<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-02T00:00:00Z">2024-12-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">42</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compute-Constrained Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16208v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16208v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Oscar Yin, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data selection can reduce the amount of training data needed to finetune
LLMs; however, the efficacy of data selection scales directly with its compute.
Motivated by the practical challenge of compute-constrained finetuning, we
consider the setting in which both the cost of selecting data and training are
budgeted for. We first formalize the problem of data selection with a
cost-aware utility function, and model the data selection problem as trading
off initial-selection cost for training gain. We run a comprehensive sweep of
experiments across multiple tasks, varying compute budget by scaling finetuning
tokens, model sizes, and data selection compute. Interestingly we find that
many powerful data selection methods are almost never compute-optimal, and that
cheaper data selection alternatives dominate both from a theoretical and
empirical perspective. For compute-optimal training, we find that perplexity
and gradient data selection require training-to-selection model size ratios of
5x and 10x, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RIRAG: Regulatory Information Retrieval and Answer Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuba Gokhan, Kexin Wang, Iryna Gurevych, Ted Briscoe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regulatory documents, issued by governmental regulatory bodies, establish
rules, guidelines, and standards that organizations must adhere to for legal
compliance. These documents, characterized by their length, complexity and
frequent updates, are challenging to interpret, requiring significant
allocation of time and expertise on the part of organizations to ensure ongoing
compliance. Regulatory Natural Language Processing (RegNLP) is a
multidisciplinary field aimed at simplifying access to and interpretation of
regulatory rules and obligations. We introduce a task of generating
question-passages pairs, where questions are automatically created and paired
with relevant regulatory passages, facilitating the development of regulatory
question-answering systems. We create the ObliQA dataset, containing 27,869
questions derived from the collection of Abu Dhabi Global Markets (ADGM)
financial regulation documents, design a baseline Regulatory Information
Retrieval and Answer Generation (RIRAG) system and evaluate it with RePASs, a
novel evaluation metric that tests whether generated answers accurately capture
all relevant obligations while avoiding contradictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Differentiates Educational Literature? A Multimodal Fusion Approach
  of <span class="highlight-title">Transformer</span>s and Computational Linguistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17593v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17593v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jordan J. Bird
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of new literature into the English curriculum remains a
challenge since educators often lack scalable tools to rapidly evaluate
readability and adapt texts for diverse classroom needs. This study proposes to
address this gap through a multimodal approach that combines transformer-based
text classification with linguistic feature analysis to align texts with UK Key
Stages. Eight state-of-the-art Transformers were fine-tuned on segmented text
data, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,
500 deep neural network topologies were searched for the classification of
linguistic characteristics, achieving an F1 score of 0.392. The fusion of these
modalities shows a significant improvement, with every multimodal approach
outperforming all unimodal models. In particular, the ELECTRA Transformer fused
with the neural network achieved an F1 score of 0.996. Unimodal and multimodal
approaches are shown to have statistically significant differences in all
validation metrics (accuracy, precision, recall, F1 score) except for inference
time. The proposed approach is finally encapsulated in a stakeholder-facing web
application, providing non-technical stakeholder access to real-time insights
on text complexity, reading difficulty, curriculum alignment, and
recommendations for learning age range. The application empowers data-driven
decision making and reduces manual workload by integrating AI-based
recommendations into lesson planning for English literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artificial intelligence contribution to translation industry: looking
  back and forward 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Q. Shormani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study provides a comprehensive analysis of artificial intelligence (AI)
contribution to translation industry (ACTI) research, synthesizing it over
forty-one years from 1980-2024. 13220 articles were retrieved from three
sources, namely WoS, Scopus, and Lens. We provided two types of analysis, viz.,
scientometric and thematic, focusing on cluster, subject categories, keywords,
burstness, centrality and research centers as for the former. For the latter,
we thematically review 18 articles, selected purposefully from the articles
involved, centering on purpose, approach, findings, and contribution to ACTI
future directions. The findings reveal that in the past AI contribution to
translation industry was not rigorous, resulting in rule-based machine
translation and statistical machine translation whose output was not
satisfactory. However, the more AI develops, the more machine translation
develops, incorporating Neural Networking Algorithms and (Deep) Language
Learning Models like ChatGPT whose translation output has developed
considerably. However, much rigorous research is still needed to overcome
several problems encountering translation industry, specifically concerning
low-source languages, multi-dialectical and free word order languages, and
cultural and religious registers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezra Karger, Houtan Bastani, Chen Yueh-Han, Zachary Jacobs, Danny Halawi, Fred Zhang, Philip E. Tetlock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasts of future events are essential inputs into informed
decision-making. Machine learning (ML) systems have the potential to deliver
forecasts at scale, but there is no framework for evaluating the accuracy of ML
systems on a standardized set of forecasting questions. To address this gap, we
introduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML
systems on an automatically generated and regularly updated set of 1,000
forecasting questions. To avoid any possibility of data leakage, ForecastBench
is comprised solely of questions about future events that have no known answer
at the time of submission. We quantify the capabilities of current ML systems
by collecting forecasts from expert (human) forecasters, the general public,
and LLMs on a random subset of questions from the benchmark ($N=200$). While
LLMs have achieved super-human performance on many benchmarks, they perform
less well here: expert forecasters outperform the top-performing LLM (p-value
$<0.01$). We display system and human scores in a public leaderboard at
www.forecastbench.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Speech-Text <span class="highlight-title">Pre-train</span>ing with Synthetic Interleaved Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech language models (SpeechLMs) accept speech input and produce speech
output, allowing for more natural human-computer interaction compared to
text-based large language models (LLMs). Traditional approaches for developing
SpeechLMs are constrained by the limited availability of unsupervised speech
data and parallel speech-text data, which are significantly less abundant than
text pre-training data, thereby limiting their scalability as LLMs. We propose
a novel approach to scaling speech-text pre-training by leveraging large-scale
synthetic interleaved data derived from text corpora, eliminating the need for
parallel speech-text datasets. Our method efficiently constructs speech-text
interleaved data by sampling text spans from existing text corpora and
synthesizing corresponding speech spans using a text-to-token model, bypassing
the need to generate actual speech. We also employ a supervised speech
tokenizer derived from an automatic speech recognition (ASR) model by
incorporating a vector-quantized bottleneck into the encoder. This supervised
training approach results in discrete speech tokens with strong semantic
preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining
speech reconstruction quality. Starting from a pre-trained language model and
scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved
speech-text data), we achieve state-of-the-art performance in speech language
modeling and spoken question answering, improving performance on spoken
questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further
demonstrate that by fine-tuning the pre-trained model with speech dialogue
data, we can develop an end-to-end spoken chatbot that achieves competitive
performance comparable to existing baselines in both conversational abilities
and speech quality, even operating exclusively in the speech domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Limits to Predicting Online Speech Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mina Remeli, Moritz Hardt, Robert C. Williamson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the predictability of online speech on social media, and whether
predictability improves with information outside a user's own posts. Recent
theoretical results suggest that posts from a user's social circle are as
predictive of the user's future posts as that of the user's past posts.
Motivated by the success of large language models, we empirically test this
hypothesis. We define predictability as a measure of the model's uncertainty,
i.e., its negative log-likelihood on future tokens given context. As the basis
of our study, we collect 10M tweets for ``tweet-tuning'' base models and a
further 6.25M posts from more than five thousand X (previously Twitter) users
and their peers. Across four large language models ranging in size from 1.5
billion to 70 billion parameters, we find that predicting a user's posts from
their peers' posts performs poorly. Moreover, the value of the user's own posts
for prediction is consistently higher than that of their peers'. We extend our
investigation with a detailed analysis on what's learned in-context and the
robustness of our findings. From context, base models learn to correctly
predict @-mentions and hashtags. Moreover, our results replicate if instead of
prompting the model with additional context, we finetune on it. Across the
board, we find that predicting the posts of individual users remains hard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Review</span>er2: Optimizing <span class="highlight-title">Review</span> Generation Through <span class="highlight-title">Prompt</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaolin Gao, Kianté Brantley, Thorsten Joachims
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in LLMs offer new opportunities for assisting authors in
improving their work. In this paper, we envision a use case where authors can
receive LLM-generated reviews that uncover weak points in the current draft.
While initial methods for automated review generation already exist, these
methods tend to produce reviews that lack detail, and they do not cover the
range of opinions that human reviewers produce. To address this shortcoming, we
propose an efficient two-stage review generation framework called Reviewer2.
Unlike prior work, this approach explicitly models the distribution of possible
aspects that the review may address. We show that this leads to more detailed
reviews that better cover the range of aspects that human reviewers identify in
the draft. As part of the research, we generate a large-scale review dataset of
27k papers and 99k reviews that we annotate with aspect prompts, which we make
available as a resource for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Meta-<span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian de Wynter, Xun Wang, Qilong Gu, Si-Qing Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern generative language models are capable of interpreting input strings
as instructions, or prompts, and carry out tasks based on them. Many approaches
to prompting and pre-training these models involve the automated generation of
these prompts: meta-prompting, or prompting to obtain prompts. We propose a
theoretical framework based on category theory to generalize and describe them.
This framework is flexible enough to account for stochasticity, and allows us
to obtain formal results around task agnosticity and equivalence of various
meta-prompting approaches. Experimentally, we test our framework in two active
areas of model research: creativity and ideation. We find that user preference
strongly favors (p < 0.01) the prompts generated under meta-prompting, as well
as their corresponding outputs, over a series of hardcoded baseline prompts
that include the original task definition. Using our framework, we argue that
meta-prompting is more effective than basic prompting at generating desirable
outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisScience: An Extensive Benchmark for Evaluating K12 Educational
  Multi-modal Scientific Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihuan Jiang, Zhen Yang, Jinhao Chen, Zhengxiao Du, Weihan Wang, Bin Xu, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal large language models (MLLMs) have demonstrated promising
capabilities across various tasks by integrating textual and visual information
to achieve visual understanding in complex scenarios. Despite the availability
of several benchmarks aims to evaluating MLLMs in tasks from visual question
answering to complex problem-solving, most focus predominantly on mathematics
or general visual understanding tasks. This reveals a critical gap in current
benchmarks, which often overlook the inclusion of other key scientific
disciplines such as physics and chemistry. To address this gap, we meticulously
construct a comprehensive benchmark, named VisScience, which is utilized to
assess the multi-modal scientific reasoning across the three disciplines of
mathematics, physics, and chemistry. This benchmark comprises 3,000 questions
drawn from K12 education - spanning elementary school through high school -
equally distributed across three disciplines, with 1,000 questions per
discipline. The questions within VisScience span 21 distinct subjects and are
categorized into five difficulty levels, offering a broad spectrum of topics
within each discipline. With VisScience, we present a detailed evaluation of
the performance of 25 representative MLLMs in scientific reasoning.
Experimental results demonstrate that closed-source MLLMs generally outperform
open-source models. The best performance observed include a 53.4\% accuracy in
mathematics by Claude3.5-Sonnet, 38.2\% in physics by GPT-4o, and 47.0\% in
chemistry by Gemini-1.5-Pro. These results underscore the strengths and
limitations of MLLMs, suggesting areas for future improvement and highlighting
the importance of developing models that can effectively handle the diverse
demands of multi-modal scientific reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>89 pages, 70 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Yang, Jinhao Chen, Zhengxiao Du, Wenmeng Yu, Weihan Wang, Wenyi Hong, Zhihuan Jiang, Bin Xu, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant capabilities in
mathematical reasoning, particularly with text-based mathematical problems.
However, current multi-modal large language models (MLLMs), especially those
specialized in mathematics, tend to focus predominantly on solving geometric
problems but ignore the diversity of visual information available in other
areas of mathematics. Moreover, the geometric information for these specialized
mathematical MLLMs is derived from several public datasets, which are typically
limited in diversity and complexity. To address these limitations, we aim to
construct a fine-tuning dataset named MathVL, and develop a series of
specialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised
Fine-Tuning (SFT) on MathVL with various parameter-scale backbones. To
extensively evaluate the effectiveness of MathGLM-Vision, we conduct
experiments on several public benchmarks and our curated MathVL-test consisting
of 2,000 problems. Experimental results demonstrate that MathGLM-Vision
achieves significant improvements compared with some existing models, including
backbone models and open-source mathematical MLLMs. These findings indicate the
importance of diversity dataset in enhancing the mathematical reasoning
abilities of MLLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages,19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Scirè, Andrei Stefan Bejgu, Simone Tedeschi, Karim Ghonim, Federico Martelli, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  After the introduction of Large Language Models (LLMs), there have been
substantial improvements in the performance of Natural Language Generation
(NLG) tasks, including Text Summarization and Machine Translation. However,
LLMs still produce outputs containing hallucinations, that is, content not
grounded in factual information. Therefore, developing methods to assess the
factuality of LLMs has become urgent.
  Indeed, resources for factuality evaluation have recently emerged. Although
challenging, these resources face one or more of the following limitations: (i)
they are tailored to a specific task or domain; (ii) they are limited in size,
thereby preventing the training of new factuality evaluators; (iii) they are
designed for simpler verification tasks, such as claim verification.
  To address these issues, we introduce LLM-Oasis, to the best of our knowledge
the largest resource for training end-to-end factuality evaluators. LLM-Oasis
is constructed by extracting claims from Wikipedia, falsifying a subset of
these claims, and generating pairs of factual and unfactual texts. We then rely
on human annotators to both validate the quality of our dataset and to create a
gold standard test set for benchmarking factuality evaluation systems.
  Our experiments demonstrate that LLM-Oasis presents a significant challenge
for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our
proposed end-to-end factuality evaluation task, highlighting its potential to
drive future research in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages. To be submitted to CL journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRDD: A <span class="highlight-title">Dataset</span> for Greek Dialectal NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00802v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00802v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stergios Chatzikyriakidis, Chatrine Qwaider, Ilias Kolokousis, Christina Koula, Dimitris Papadakis, Efthymia Sakellariou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a dataset for the computational study of a number
of Modern Greek dialects. It consists of raw text data from four dialects of
Modern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset is
of considerable size, albeit imbalanced, and presents the first attempt to
create large scale dialectal resources of this type for Modern Greek dialects.
We then use the dataset to perform dialect idefntification. We experiment with
traditional ML algorithms, as well as simple DL architectures. The results show
very good performance on the task, potentially revealing that the dialects in
question have distinct enough characteristics allowing even simple ML models to
perform well on the task. Error analysis is performed for the top performing
algorithms showing that in a number of cases the errors are due to insufficient
dataset cleaning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Refine: Improving Natural Language Explanation Generation by
  Learning in Tandem <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian Möller, Vera Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language explanations (NLEs) are vital for elucidating the reasoning
behind large language model (LLM) decisions. Many techniques have been
developed to generate NLEs using LLMs. However, like humans, LLMs might not
always produce optimal NLEs on first attempt. Inspired by human learning
processes, we introduce Cross-Refine, which employs role modeling by deploying
two LLMs as generator and critic, respectively. The generator outputs a first
NLE and then refines this initial explanation using feedback and suggestions
provided by the critic. Cross-Refine does not require any supervised training
data or additional training. We validate Cross-Refine across three NLP tasks
using three state-of-the-art open-source LLMs through automatic and human
evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which
only utilizes self-feedback to refine the explanations. Our findings from
automatic evaluation and a user study indicate that Cross-Refine outperforms
Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful
LLMs, whereas Self-Refine only yields strong results with ChatGPT.
Additionally, we conduct an ablation study to assess the importance of feedback
and suggestions. Both of them play an important role in refining explanations.
We further evaluate Cross-Refine on a bilingual dataset in English and German.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025; long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Induction and Transduction for Abstract Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02272v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02272v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, Kevin Ellis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When learning an input-output mapping from very few examples, is it better to
first infer a latent function that explains the examples, or is it better to
directly predict new test outputs, e.g. using a neural network? We study this
question on ARC by training neural models for induction (inferring latent
functions) and transduction (directly predicting the test output for a given
test input). We train on synthetically generated variations of Python programs
that solve ARC training tasks. We find inductive and transductive models solve
different kinds of test problems, despite having the same training problems and
sharing the same neural architecture: Inductive program synthesis excels at
precise computations, and at composing multiple concepts, while transduction
succeeds on fuzzier perceptual concepts. Ensembling them approaches human-level
performance on ARC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Zeroth-Order Methods for Scalable Large Language
  Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07818v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07818v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Z Liu, J Lou, W Bao, Y Hu, B Li, Z Qin, K Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning on task-specific datasets is a widely-embraced paradigm of
harnessing the powerful capability of pretrained LLMs for various downstream
tasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy
concerns, differentially private (DP) fine-tuning of pretrained LLMs has been
widely used to safeguarding the privacy of task-specific datasets. Lying at the
design core of DP LLM fine-tuning methods is the satisfactory tradeoff among
privacy, utility, and scalability. Most existing methods build upon the seminal
work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,
DP-SGD-based fine-tuning methods are unfortunately limited by the inherent
inefficiency of SGD.
  In this paper, we investigate the potential of DP zeroth-order methods for
LLM pretraining, which avoids the scalability bottleneck of SGD by
approximating the gradient with the more efficient zeroth-order gradient.
Rather than treating the zeroth-order method as a drop-in replacement for SGD,
this paper presents a comprehensive study both theoretically and empirically.
First, we propose the stagewise DP zeroth-order method (DP-ZOSO) that
dynamically schedules key hyperparameters. This design is grounded on the
synergy between DP random perturbation and the gradient approximation error of
the zeroth-order method, and its effect on fine-tuning trajectory.
  We provide theoretical analysis for both proposed methods. We conduct
extensive empirical analysis on both encoder-only masked language model and
decoder-only autoregressive language model, achieving impressive results in
terms of scalability and utility regardless of the class of tasks (compared
with DPZero, DP-ZOPO improves $4.5\%$ on SST-5, $5.5\%$ on MNLI with
RoBERTa-Large and 9.2\% on CB, 3.9\% on BoolQ with OPT-2.7b when $\epsilon=4$,
demonstrates more significant enhancement in performance on more complicated
tasks).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time <span class="highlight-title">Transformer</span>-based Open-Vocabulary Detection with Efficient
  Fusion Head 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, Kyusong Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end transformer-based detectors (DETRs) have shown exceptional
performance in both closed-set and open-vocabulary object detection (OVD) tasks
through the integration of language modalities. However, their demanding
computational requirements have hindered their practical application in
real-time object detection (OD) scenarios. In this paper, we scrutinize the
limitations of two leading models in the OVDEval benchmark, OmDet and
Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based
real-time OVD model features an innovative Efficient Fusion Head (EFH) module
designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO.
Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with
TensorRT and language cache techniques applied. Notably, in zero-shot scenarios
on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on
par with current state-of-the-art supervised models. Furthermore, it
establishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an
AP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of
OmDet-Turbo in industrial applications is underscored by its exceptional
performance on benchmark datasets and superior inference speed, positioning it
as a compelling choice for real-time object detection tasks. Code:
\url{https://github.com/om-ai-lab/OmDet}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding LLM Embeddings for Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Tang, Bangding Yang, Xingyou Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of large language models (LLMs) for flexibly processing
information as strings, a natural application is regression, specifically by
preprocessing string representations into LLM embeddings as downstream features
for metric prediction. In this paper, we provide one of the first comprehensive
investigations into embedding-based regression and demonstrate that LLM
embeddings as features can be better for high-dimensional regression tasks than
using traditional feature engineering. This regression performance can be
explained in part due to LLM embeddings over numeric data inherently preserving
Lipschitz continuity over the feature space. Furthermore, we quantify the
contribution of different model effects, most notably model size and language
understanding, which we find surprisingly do not always improve regression
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine Tuning vs. Retrieval Augmented Generation for Less Popular
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01432v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01432v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting
strong performance across diverse tasks and domains. However, it has been
observed that the performance diminishes when dealing with less-popular or
low-frequency concepts and entities, for example in domain specific
applications. The two prominent approaches to enhance the performance of LMs on
low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning
(FT) over synthetic data. This paper explores and evaluates the impact of RAG
and FT on customizing LMs in handling low-frequency entities on question
answering tasks. We conduct extensive experiments on twelve LMs of varying size
and type and different fine tuning, data augmentation, and retrieval models.
Our findings indicate that while FT boosts the performance across entities of
varying popularity, RAG surpasses FT by a large margin particularly for least
popular factual knowledge. Additionally, the success of both RAG and FT
approaches is amplified by improving retrieval and data augmentation
techniques. Fine tuning, while beneficial for small LMs, requires extensive
resources. To address this issue, we propose the new Stimulus RAG approach that
surpasses the effectiveness of fine tuning based approaches, thereby
eliminating the need for the costly data augmentation and fine tuning step for
enriching LMs with less popular factual knowledge. The code is available at
\url{https://github.com/informagi/RAGvsFT}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual-Personalizing Adapter for Federated Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19211v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19211v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyuan Yang, Guodong Long, Tao Shen, Jing Jiang, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, foundation models, particularly large language models (LLMs), have
demonstrated an impressive ability to adapt to various tasks by fine-tuning
diverse instruction data. Notably, federated foundation models (FedFM) emerge
as a privacy preservation method to fine-tune models collaboratively under
federated learning (FL) settings by leveraging many distributed datasets with
non-IID data. To alleviate communication and computation overhead,
parameter-efficient methods are introduced for efficiency, and some research
adapted personalization methods to FedFM for better user preferences alignment.
However, a critical gap in existing research is the neglect of test-time
distribution shifts in real-world applications, and conventional methods for
test-time distribution shifts in personalized FL are less effective for FedFM
due to their failure to adapt to complex distribution shift scenarios and the
requirement to train all parameters. To bridge this gap, we refine the setting
in FedFM, termed test-time personalization, which aims to learn personalized
federated foundation models on clients while effectively handling test-time
distribution shifts simultaneously. To address challenges in this setting, we
explore a simple yet effective solution, a Federated Dual-Personalizing Adapter
(FedDPA) architecture. By co-working with a foundation model, a global adapter
and a local adapter jointly tackle the test-time distribution shifts and
client-specific personalization. Additionally, we introduce an instance-wise
dynamic weighting mechanism that dynamically integrates the global and local
adapters for each test instance during inference, facilitating effective
test-time personalization. The effectiveness of the proposed method has been
evaluated on benchmark datasets across different NLP tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explicitly Representing Syntax Improves Sentence-to-layout Prediction of
  Unexpected Situations <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14212v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14212v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wolf Nuyts, Ruben Cartuyvels, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing visual entities in a natural language sentence and arranging them
in a 2D spatial layout require a compositional understanding of language and
space. This task of layout prediction is valuable in text-to-image synthesis as
it allows localized and controlled in-painting of the image. In this
comparative study it is shown that we can predict layouts from language
representations that implicitly or explicitly encode sentence syntax, if the
sentences mention similar entity-relationships to the ones seen during
training. To test compositional understanding, we collect a test set of
grammatically correct sentences and layouts describing compositions of entities
and relations that unlikely have been seen during training. Performance on this
test set substantially drops, showing that current models rely on correlations
in the training data and have difficulties in understanding the structure of
the input sentences. We propose a novel structural loss function that better
enforces the syntactic structure of the input sentence and show large
performance gains in the task of 2D spatial layout prediction conditioned on
text. The loss has the potential to be used in other generation tasks where a
tree-like structure underlies the conditioning modality. Code, trained models
and the USCOCO evaluation set are available via github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TACL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLLM-LLaVA-FL: Multimodal Large Language Model Assisted Federated
  Learning <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyi Zhang, Hao Frank Yang, Ang Li, Xin Guo, Pu Wang, Haiming Wang, Yiran Chen, Hai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous studies on federated learning (FL) often encounter performance
degradation due to data heterogeneity among different clients. In light of the
recent advances in multimodal large language models (MLLMs), such as GPT-4v and
LLaVA, which demonstrate their exceptional proficiency in multimodal tasks,
such as image captioning and multimodal question answering. We introduce a
novel federated learning framework, named Multimodal Large Language Model
Assisted Federated Learning (MLLM-LLaVA-FL), which employs powerful MLLMs at
the server end to address the heterogeneous and long-tailed challenges. Owing
to the advanced cross-modality representation capabilities and the extensive
open-vocabulary prior knowledge of MLLMs, our framework is adept at harnessing
the extensive, yet previously underexploited, open-source data accessible from
websites and powerful server-side computational resources. Hence, the
MLLM-LLaVA-FL not only enhances the performance but also avoids increasing the
risk of privacy leakage and the computational burden on local devices,
distinguishing it from prior methodologies. Our framework has three key stages.
Initially, we conduct global visual-text pretraining of the model. This
pretraining is facilitated by utilizing the extensive open-source data
available online, with the assistance of MLLMs. Subsequently, the pretrained
model is distributed among various clients for local training. Finally, once
the locally trained models are transmitted back to the server, a global
alignment is carried out under the supervision of MLLMs to further enhance the
performance. Experimental evaluations on established benchmarks, show that our
framework delivers promising performance in the typical scenarios with data
heterogeneity and long-tail distribution across different clients in FL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Query-Guided <span class="highlight-title">Self-Supervised</span> Summarization of Nursing Notes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya Gao, Hans Moen, Saila Koivusalo, Miika Koskinen, Pekka Marttinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nursing notes, an important part of Electronic Health Records (EHRs), track a
patient's health during a care episode. Summarizing key information in nursing
notes can help clinicians quickly understand patients' conditions. However,
existing summarization methods in the clinical setting, especially abstractive
methods, have overlooked nursing notes and require reference summaries for
training. We introduce QGSumm, a novel query-guided self-supervised domain
adaptation approach for abstractive nursing note summarization. The method uses
patient-related clinical queries for guidance, and hence does not need
reference summaries for training. Through automatic experiments and manual
evaluation by an expert clinician, we study our approach and other
state-of-the-art Large Language Models (LLMs) for nursing note summarization.
Our experiments show: 1) GPT-4 is competitive in maintaining information in the
original nursing notes, 2) QGSumm can generate high-quality summaries with a
good balance between recall of the original content and hallucination rate
lower than other top methods. Ultimately, our work offers a new perspective on
conditional text summarization, tailored to clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning and Machine Learning, Advancing Big Data Analytics and
  Management: Object-Oriented Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19916v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19916v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyang Wang, Ziqian Bi, Keyu Chen, Jiawei Xu, Qian Niu, Junyu Liu, Benji Peng, Ming Li, Sen Zhang, Xuanhe Pan, Jinlang Wang, Pohsun Feng, Caitlyn Heqi Yin, Yizhu Wen, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-Oriented Programming (OOP) has become a crucial paradigm for managing
the growing complexity of modern software systems, particularly in fields like
machine learning, deep learning, large language models (LLM), and data
analytics. This work provides a comprehensive introduction to the integration
of OOP techniques within these domains, with a focus on improving code
modularity, maintainability, and scalability. We begin by outlining the
evolution of computing and the rise of OOP, followed by an in-depth discussion
of key OOP principles such as encapsulation, inheritance, polymorphism, and
abstraction. The practical application of these principles is demonstrated
using Python, a widely adopted language in AI and data science. Furthermore, we
examine how design patterns and modular programming can be employed to enhance
the structure and efficiency of machine learning systems. In subsequent
sections, we apply these OOP concepts to real-world AI tasks, including the
encapsulation of preprocessing workflows, machine learning model training, and
evaluation. Detailed examples illustrate how OOP can be used to build reusable,
scalable machine learning systems while maintaining code clarity and reducing
redundancy.This work is intended to serve as a bridge for both beginners and
experienced developers, equipping them with the necessary knowledge to apply
OOP methodologies in AI-driven projects, ultimately fostering the development
of more robust and maintainable systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahan Tu, Rui Hu, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination poses a persistent challenge for multimodal large language
models (MLLMs). However, existing benchmarks for evaluating hallucinations are
generally static, which may overlook the potential risk of data contamination.
To address this issue, we propose ODE, an open-set, dynamic protocol designed
to evaluate object hallucinations in MLLMs at both the existence and attribute
levels. ODE employs a graph-based structure to represent real-world object
concepts, their attributes, and the distributional associations between them.
This structure facilitates the extraction of concept combinations based on
diverse distributional criteria, generating varied samples for structured
queries that evaluate hallucinations in both generative and discriminative
tasks. Through the generation of new samples, dynamic concept combinations, and
varied distribution frequencies, ODE mitigates the risk of data contamination
and broadens the scope of evaluation. This protocol is applicable to both
general and specialized scenarios, including those with limited data.
Experimental results demonstrate the effectiveness of our protocol, revealing
that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated
samples, which indicates potential data contamination. Furthermore, these
generated samples aid in analyzing hallucination patterns and fine-tuning
models, offering an effective approach to mitigating hallucinations in MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Prompt</span>ing Methods for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Xiaoqian Liu, Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting is a mainstream paradigm for adapting large language models to
specific natural language processing tasks without modifying internal
parameters. Therefore, detailed supplementary knowledge needs to be integrated
into external prompts, which inevitably brings extra human efforts and
computational burdens for practical applications. As an effective solution to
mitigate resource consumption, Efficient Prompting Methods have attracted a
wide range of attention. We provide mathematical expressions at a high level to
deeply discuss Automatic Prompt Engineering for different prompt components and
Prompt Compression in continuous and discrete spaces. Finally, we highlight
promising future directions to inspire researchers interested in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Entropy Decay during Language Model <span class="highlight-title">Pretrain</span>ing Hinders New
  Knowledge Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyeon Kim, Hyunji Lee, Hyowon Cho, Joel Jang, Hyeonbin Hwang, Seungpil Won, Youbin Ahn, Dohaeng Lee, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate how a model's tendency to broadly integrate its
parametric knowledge evolves throughout pretraining, and how this behavior
affects overall performance, particularly in terms of knowledge acquisition and
forgetting. We introduce the concept of knowledge entropy, which quantifies the
range of memory sources the model engages with; high knowledge entropy
indicates that the model utilizes a wide range of memory sources, while low
knowledge entropy suggests reliance on specific sources with greater certainty.
Our analysis reveals a consistent decline in knowledge entropy as pretraining
advances. We also find that the decline is closely associated with a reduction
in the model's ability to acquire and retain knowledge, leading us to conclude
that diminishing knowledge entropy (smaller number of active memory sources)
impairs the model's knowledge acquisition and retention capabilities. We find
further support for this by demonstrating that increasing the activity of
inactive memory sources enhances the model's capacity for knowledge acquisition
and retention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLaPE: Gold Label-agnostic <span class="highlight-title">Prompt</span> Evaluation and Optimization for Large
  Language Model <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanchang Zhang, Zhuosheng Zhang, Hai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid progress of large language models (LLMs), their task
performance remains sensitive to prompt design. Recent studies have explored
leveraging the LLM itself as an optimizer to identify optimal prompts that
maximize task accuracy. However, when evaluating prompts, such approaches
heavily rely on elusive manually annotated gold labels to calculate task
accuracy for each candidate prompt, which hinders the widespread implementation
and generality. To overcome the limitation, this work proposes a gold
label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold
labels. Motivated by the observed correlation between self-consistency and the
accuracy of the answer, we adopt self-consistency as the initial evaluation
score. Subsequently, we refine the scores of prompts producing identical
answers to be mutually consistent. Experimental results show that GLaPE
provides reliable evaluations uniform with accuracy, even in the absence of
gold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt
optimization yields effective prompts comparable to accuracy-based ones. The
code is publicly available at https://github.com/thunderous77/GLaPE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Pixels to Insights: A <span class="highlight-title">Survey</span> on Automatic Chart Understanding in
  the Era of Large Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12027v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12027v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kung-Hsiang Huang, Hou Pong Chan, Yi R. Fung, Haoyi Qiu, Mingyang Zhou, Shafiq Joty, Shih-Fu Chang, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data visualization in the form of charts plays a pivotal role in data
analysis, offering critical insights and aiding in informed decision-making.
Automatic chart understanding has witnessed significant advancements with the
rise of large foundation models in recent years. Foundation models, such as
large language models, have revolutionized various natural language processing
tasks and are increasingly being applied to chart understanding tasks. This
survey paper provides a comprehensive overview of the recent developments,
challenges, and future directions in chart understanding within the context of
these foundation models. We review fundamental building blocks crucial for
studying chart understanding tasks. Additionally, we explore various tasks and
their evaluation metrics and sources of both charts and textual inputs. Various
modeling strategies are then examined, encompassing both classification-based
and generation-based approaches, along with tool augmentation techniques that
enhance chart understanding performance. Furthermore, we discuss the
state-of-the-art performance of each task and discuss how we can improve the
performance. Challenges and future directions are addressed, highlighting the
importance of several topics, such as domain-specific charts, lack of efforts
in developing evaluation metrics, and agent-oriented settings. This survey
paper serves as a comprehensive resource for researchers and practitioners in
the fields of natural language processing, computer vision, and data analysis,
providing valuable insights and directions for future research in chart
understanding leveraging large foundation models. The studies mentioned in this
paper, along with emerging new research, will be continually updated at:
https://github.com/khuangaf/Awesome-Chart-Understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Knowledge and Data Engineering (TKDE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T2Vid: Translating Long Text into Multi-Image is the Catalyst for
  Video-LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan Yang, Zuwei Long, Yuhan Dai, Tong Xu, Xing Sun, Ran He, Caifeng Shan, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Multimodal Large Language Models (MLLMs) in the image domain
has garnered wide attention from the research community. Drawing on previous
successful experiences, researchers have recently explored extending the
success to the video understanding realms. Apart from training from scratch, an
efficient way is to utilize the pre-trained image-LLMs, leading to two
mainstream approaches, i.e. zero-shot inference and further fine-tuning with
video data. In this work, our study of these approaches harvests an effective
data augmentation method. We first make a deeper inspection of the zero-shot
inference way and identify two limitations, i.e. limited generalization and
lack of temporal understanding capabilities. Thus, we further investigate the
fine-tuning approach and find a low learning efficiency when simply using all
the video data samples, which can be attributed to a lack of instruction
diversity. Aiming at this issue, we develop a method called T2Vid to synthesize
video-like samples to enrich the instruction diversity in the training corpus.
Integrating these data enables a simple and efficient training scheme, which
achieves performance comparable to or even superior to using full video
datasets by training with just 15% the sample size. Meanwhile, we find that the
proposed scheme can boost the performance of long video understanding without
training with long video samples. We hope our study will spark more thinking
about using MLLMs for video understanding and curation of high-quality data.
The code is released at https://github.com/xjtupanda/T2Vid.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/xjtupanda/T2Vid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshara Prabhakar, Yuanzhi Li, Karthik Narasimhan, Sham Kakade, Eran Malach, Samy Jelassi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient
fine-tuning of Large Language Models (LLMs). We study how different LoRA
modules can be merged to achieve skill composition -- testing the performance
of the merged model on a target task that involves combining multiple skills,
each skill coming from a single LoRA. This setup is favorable when it is
difficult to obtain training data for the target task and when it can be
decomposed into multiple skills. First, we identify practically occurring
use-cases that can be studied under the realm of skill composition, e.g.
solving hard math-word problems with code, creating a bot to answer questions
on proprietary manuals or about domain-specialized corpora. Our main
contribution is to show that concatenation of LoRAs (CAT), which optimally
weights LoRAs that were individually trained on different skills, outperforms
existing model- and data- merging techniques; for instance on math-word
problems, CAT beats these methods by an average of 43% and 12% respectively.
Thus, this paper advocates model merging as an efficient way to solve
compositional tasks and underscores CAT as a simple, compute-friendly and
effective procedure. To our knowledge, this is the first work demonstrating the
superiority of model merging over data mixing for binary skill composition
tasks. Code and data are available at https://github.com/aksh555/LoRA-Soups
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Industry track; 9 pages plus references and appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Critical Tokens Matter: Token-Level Contrastive Estimation Enhances
  LLM's Reasoning Capability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited remarkable performance on
reasoning tasks. They utilize autoregressive token generation to construct
reasoning trajectories, enabling the development of a coherent chain of
thought. In this work, we explore the impact of individual tokens on the final
outcomes of reasoning tasks. We identify the existence of ``critical tokens''
that lead to incorrect reasoning trajectories in LLMs. Specifically, we find
that LLMs tend to produce positive outcomes when forced to decode other tokens
instead of critical tokens. Motivated by this observation, we propose a novel
approach - cDPO - designed to automatically recognize and conduct token-level
rewards for the critical tokens during the alignment process. Specifically, we
develop a contrastive estimation approach to automatically identify critical
tokens. It is achieved by comparing the generation likelihood of positive and
negative models. To achieve this, we separately fine-tune the positive and
negative models on various reasoning trajectories, consequently, they are
capable of identifying identify critical tokens within incorrect trajectories
that contribute to erroneous outcomes. Moreover, to further align the model
with the critical token information during the alignment process, we extend the
conventional DPO algorithms to token-level DPO and utilize the differential
likelihood from the aforementioned positive and negative model as important
weight for token-level DPO learning.Experimental results on GSM8K and MATH500
benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math
(7B) demonstrate the effectiveness of the propsoed approach cDPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self and Cross-Model Distillation for LLMs: Effective Methods for
  Refusal Pattern Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11285v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11285v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Li, Yi Liu, Chongyang Liu, Xiaoning Ren, Ling Shi, Weisong Sun, Yinxing Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) like OpenAI's GPT series, Anthropic's Claude,
and Meta's LLaMa have shown remarkable capabilities in text generation.
However, their susceptibility to toxic prompts presents significant security
challenges. This paper investigates alignment techniques, including Supervised
Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), to
mitigate these risks. We conduct an empirical study on refusal patterns across
nine LLMs, revealing that models with uniform refusal patterns, such as
Claude3, exhibit higher security. Based on these findings, we propose
self-distilling and cross-model distilling methods to enhance LLM security. Our
results show that these methods significantly improve refusal rates and reduce
unsafe content, with cross-model distilling achieving refusal rates close to
Claude3's 94.51%. These findings underscore the potential of distillation-based
alignment in securing LLMs against toxic prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The method used in the paper has obvious problems and ambiguities.
  The security enhancement method we used cannot be considered distillation,
  but it is described as distillation in the paper, and the experiment lacks
  comparison and baseline, which has been criticized by many peers. In order to
  avoid further dissemination, we have decided to withdraw the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have shown remarkable advancements in
multimodal reasoning tasks. However, they still often generate inaccurate or
irrelevant responses due to issues like hallucinated image understandings or
unrefined reasoning paths. To address these challenges, we introduce Critic-V,
a novel framework inspired by the Actor-Critic paradigm to boost the reasoning
capability of VLMs. This framework decouples the reasoning process and critic
process by integrating two independent components: the Reasoner, which
generates reasoning paths based on visual and textual inputs, and the Critic,
which provides constructive critique to refine these paths. In this approach,
the Reasoner generates reasoning responses according to text prompts, which can
evolve iteratively as a policy based on feedback from the Critic. This
interaction process was theoretically driven by a reinforcement learning
framework where the Critic offers natural language critiques instead of scalar
rewards, enabling more nuanced feedback to boost the Reasoner's capability on
complex reasoning tasks. The Critic model is trained using Direct Preference
Optimization (DPO), leveraging a preference dataset of critiques ranked by
Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results
show that the Critic-V framework significantly outperforms existing methods,
including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning
accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner
and constructive feedback from the preference-optimized Critic enables a more
reliable and context-sensitive multimodal reasoning process. Our approach
provides a promising solution to enhance the reliability of VLMs, improving
their performance in real-world reasoning-heavy multimodal applications such as
autonomous driving and embodied intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Bias in Queer Representation within Large Language Models: A
  Collaborative Agent Approach <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Huang, Arya Somasundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often perpetuate biases in pronoun usage,
leading to misrepresentation or exclusion of queer individuals. This paper
addresses the specific problem of biased pronoun usage in LLM outputs,
particularly the inappropriate use of traditionally gendered pronouns ("he,"
"she") when inclusive language is needed to accurately represent all
identities. We introduce a collaborative agent pipeline designed to mitigate
these biases by analyzing and optimizing pronoun usage for inclusivity. Our
multi-agent framework includes specialized agents for both bias detection and
correction. Experimental evaluations using the Tango dataset-a benchmark
focused on gender pronoun usage-demonstrate that our approach significantly
improves inclusive pronoun classification, achieving a 32.6 percentage point
increase over GPT-4o in correctly disagreeing with inappropriate traditionally
gendered pronouns $(\chi^2 = 38.57, p < 0.0001)$. These results accentuate the
potential of agent-driven frameworks in enhancing fairness and inclusivity in
AI-generated content, demonstrating their efficacy in reducing biases and
promoting socially responsible AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Queer in AI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Understanding Domain Adapted Sentence Embeddings for Document
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujoy Roychowdhury, Sumit Soman, H. G. Ranjani, Vansh Chhabra, Neeraj Gunda, Shashank Gautam, Subhadip Bandyopadhyay, Sai Krishna Bala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A plethora of sentence embedding models makes it challenging to choose one,
especially for technical domains rich with specialized vocabulary. In this
work, we domain adapt embeddings using telecom, health and science datasets for
question answering. We evaluate embeddings obtained from publicly available
models and their domain-adapted variants, on both point retrieval accuracies,
as well as their (95\%) confidence intervals. We establish a systematic method
to obtain thresholds for similarity scores for different embeddings. As
expected, we observe that fine-tuning improves mean bootstrapped accuracies. We
also observe that it results in tighter confidence intervals, which further
improve when pre-training is preceded by fine-tuning. We introduce metrics
which measure the distributional overlaps of top-$K$, correct and random
document similarities with the question. Further, we show that these metrics
are correlated with retrieval accuracy and similarity thresholds. Recent
literature shows conflicting effects of isotropy on retrieval accuracies. Our
experiments establish that the isotropy of embeddings (as measured by two
independent state-of-the-art isotropy metric definitions) is poorly correlated
with retrieval performance. We show that embeddings for domain-specific
sentences have little overlap with those for domain-agnostic ones, and
fine-tuning moves them further apart. Based on our results, we provide
recommendations for use of our methodology and metrics by researchers and
practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TTSDS -- Text-to-Speech Distribution Score 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12707v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12707v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Minixhofer, Ondřej Klejch, Peter Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recently published Text-to-Speech (TTS) systems produce audio close to
real speech. However, TTS evaluation needs to be revisited to make sense of the
results obtained with the new architectures, approaches and datasets. We
propose evaluating the quality of synthetic speech as a combination of multiple
factors such as prosody, speaker identity, and intelligibility. Our approach
assesses how well synthetic speech mirrors real speech by obtaining correlates
of each factor and measuring their distance from both real speech datasets and
noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and
show that our score computed as an unweighted average of factors strongly
correlates with the human evaluations from each time period.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Statistical Framework of Watermarks for Large Language Models: Pivot,
  Detection Efficiency and Optimal Rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01245v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01245v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since ChatGPT was introduced in November 2022, embedding (nearly)
unnoticeable statistical signals into text generated by large language models
(LLMs), also known as watermarking, has been used as a principled approach to
provable detection of LLM-generated text from its human-written counterpart. In
this paper, we introduce a general and flexible framework for reasoning about
the statistical efficiency of watermarks and designing powerful detection
rules. Inspired by the hypothesis testing formulation of watermark detection,
our framework starts by selecting a pivotal statistic of the text and a secret
key -- provided by the LLM to the verifier -- to enable controlling the false
positive rate (the error of mistakenly detecting human-written text as
LLM-generated). Next, this framework allows one to evaluate the power of
watermark detection rules by obtaining a closed-form expression of the
asymptotic false negative rate (the error of incorrectly classifying
LLM-generated text as human-written). Our framework further reduces the problem
of determining the optimal detection rule to solving a minimax optimization
program. We apply this framework to two representative watermarks -- one of
which has been internally implemented at OpenAI -- and obtain several findings
that can be instrumental in guiding the practice of implementing watermarks. In
particular, we derive optimal detection rules for these watermarks under our
framework. These theoretically derived detection rules are demonstrated to be
competitive and sometimes enjoy a higher power than existing detection
approaches through numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Annals of Statistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models Benefit from Preparation with Elicited Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01345v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01345v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacan Yu, Hannah An, Lenhart K. Schubert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The zero-shot chain of thought (CoT) approach is often used in question
answering (QA) by language models (LMs) for tasks that require multiple
reasoning steps. However, some QA tasks hinge more on accessing relevant
knowledge than on chaining reasoning steps. We introduce a simple prompting
technique, called PREP, that involves using two instances of LMs: the first
(LM1) generates relevant information, and the second (LM2) receives the
information from the user and answers the question. This design is intended to
make better use of the LM's instruction-following capability. PREP is
applicable across various QA tasks without domain-specific prompt engineering.
PREP is developed on a dataset of 100 QA questions, derived from an extensive
schematic dataset specifying artifact parts and material composition. These
questions ask which of two artifacts is less likely to share materials with
another artifact. Such questions probe the LM's knowledge of shared materials
in the part structure of different artifacts. We test our method on our
parts-and-materials dataset and three published commonsense reasoning datasets.
The average accuracy of our method is consistently higher than that of all the
other tested methods across all the tested datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VersaTune: An Efficient Data Composition Framework for Training
  Multi-Capability LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11266v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11266v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keer Lu, Keshi Zhao, Zheng Liang, Da Pan, Shusen Zhang, Xin Wu, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pretrained models, particularly Large Language Models (LLMs),
have exhibited remarkable capabilities in handling multiple tasks across
domains due to their emergent properties. These capabilities are further
augmented during the Supervised Fine-Tuning (SFT) phase. Despite their
potential, existing work mainly focuses on domain-specific enhancements during
fine-tuning, the challenge of which lies in catastrophic forgetting of
knowledge across other domains. In this study, we introduce VersaTune, a novel
data composition framework designed for enhancing LLMs' overall multi-ability
performances during training. We categorize knowledge into distinct domains
including law, medicine, finance, science, code, etc. We begin with detecting
the distribution of domain-specific knowledge within the base model, followed
by the training data composition that aligns with the model's existing
knowledge distribution. During the training process, domain weights are
dynamically adjusted based on their learnable potential and forgetting degree.
Experimental results demonstrate that VersaTune achieves significant
improvements in multi-domain performance, with an 35.21% enhancement in
comprehensive multi-domain tasks. Additionally, in scenarios where specific
domain optimization is required, VersaTune reduces the degradation of
performance in other domains by 38.77%, without compromising the target
domain's training efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating LLMs for Hardware Design and Test 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Blocklove, Siddharth Garg, Ramesh Karri, Hammond Pearce
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated capabilities for producing
code in Hardware Description Languages (HDLs). However, most of the focus
remains on their abilities to write functional code, not test code. The
hardware design process consists of both design and test, and so eschewing
validation and verification leaves considerable potential benefit unexplored,
given that a design and test framework may allow for progress towards full
automation of the digital design pipeline. In this work, we perform one of the
first studies exploring how a LLM can both design and test hardware modules
from provided specifications. Using a suite of 8 representative benchmarks, we
examined the capabilities and limitations of the state-of-the-art
conversational LLMs when producing Verilog for functional and verification
purposes. We taped out the benchmarks on a Skywater 130nm shuttle and received
the functional chip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Punctuation Restoration for Singaporean Spoken Languages: English,
  Malay, and Mandarin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Rao, Ho Thi-Nga, Chng Eng-Siong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the work of restoring punctuation for ASR transcripts
generated by multilingual ASR systems. The focus languages are English,
Mandarin, and Malay which are three of the most popular languages in Singapore.
To the best of our knowledge, this is the first system that can tackle
punctuation restoration for these three languages simultaneously. Traditional
approaches usually treat the task as a sequential labeling task, however, this
work adopts a slot-filling approach that predicts the presence and type of
punctuation marks at each word boundary. The approach is similar to the
Masked-Language Model approach employed during the pre-training stages of BERT,
but instead of predicting the masked word, our model predicts masked
punctuation. Additionally, we find that using Jieba1 instead of only using the
built-in SentencePiece tokenizer of XLM-R can significantly improve the
performance of punctuating Mandarin transcripts. Experimental results on
English and Mandarin IWSLT2022 datasets and Malay News show that the proposed
approach achieved state-of-the-art results for Mandarin with 73.8% F1-score
while maintaining a reasonable F1-score for English and Malay, i.e. 74.7% and
78% respectively. Our source code that allows reproducing the results and
building a simple web-based application for demonstration purposes is available
on Github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at APSIPA 2022, Chiang-Mai, Thailand</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">83</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moner: Motion Correction in Undersampled Radial MRI with Unsupervised
  Neural Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Wu, Chenhe Du, XuanYu Tian, Jingyi Yu, Yuyao Zhang, Hongjiang Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion correction (MoCo) in radial MRI is a challenging problem due to the
unpredictability of subject's motion. Current state-of-the-art (SOTA) MoCo
algorithms often use extensive high-quality MR images to pre-train neural
networks, obtaining excellent reconstructions. However, the need for
large-scale datasets significantly increases costs and limits model
generalization. In this work, we propose Moner, an unsupervised MoCo method
that jointly solves artifact-free MR images and accurate motion from
undersampled, rigid motion-corrupted k-space data, without requiring training
data. Our core idea is to leverage the continuous prior of implicit neural
representation (INR) to constrain this ill-posed inverse problem, enabling
ideal solutions. Specifically, we incorporate a quasi-static motion model into
the INR, granting its ability to correct subject's motion. To stabilize model
optimization, we reformulate radial MRI as a back-projection problem using the
Fourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash
encoding strategy, significantly enhancing MoCo accuracy. Experiments on
multiple MRI datasets show our Moner achieves performance comparable to SOTA
MoCo techniques on in-domain data, while demonstrating significant improvements
on out-of-domain data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Generalizability of Diffusion Models Requires Rethinking
  the Hidden Gaussian Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24060v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24060v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Yixiang Dai, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the generalizability of diffusion models by looking
into the hidden properties of the learned score functions, which are
essentially a series of deep denoisers trained on various noise levels. We
observe that as diffusion models transition from memorization to
generalization, their corresponding nonlinear diffusion denoisers exhibit
increasing linearity. This discovery leads us to investigate the linear
counterparts of the nonlinear diffusion models, which are a series of linear
models trained to match the function mappings of the nonlinear diffusion
denoisers. Surprisingly, these linear denoisers are approximately the optimal
denoisers for a multivariate Gaussian distribution characterized by the
empirical mean and covariance of the training dataset. This finding implies
that diffusion models have the inductive bias towards capturing and utilizing
the Gaussian structure (covariance information) of the training dataset for
data generation. We empirically demonstrate that this inductive bias is a
unique property of diffusion models in the generalization regime, which becomes
increasingly evident when the model's capacity is relatively small compared to
the training dataset size. In the case that the model is highly
overparameterized, this inductive bias emerges during the initial training
phases before the model fully memorizes its training data. Our study provides
crucial insights into understanding the notable strong generalization
phenomenon recently observed in real-world diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OminiControl: Minimal and Universal Control for Diffusion <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce OminiControl, a highly versatile and
parameter-efficient framework that integrates image conditions into pre-trained
Diffusion Transformer (DiT) models. At its core, OminiControl leverages a
parameter reuse mechanism, enabling the DiT to encode image conditions using
itself as a powerful backbone and process them with its flexible multi-modal
attention processors. Unlike existing methods, which rely heavily on additional
encoder modules with complex architectures, OminiControl (1) effectively and
efficiently incorporates injected image conditions with only ~0.1% additional
parameters, and (2) addresses a wide range of image conditioning tasks in a
unified manner, including subject-driven generation and spatially-aligned
conditions such as edges, depth, and more. Remarkably, these capabilities are
achieved by training on images generated by the DiT itself, which is
particularly beneficial for subject-driven generation. Extensive evaluations
demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted
models in both subject-driven and spatially-aligned conditional generation.
Additionally, we release our training dataset, Subjects200K, a diverse
collection of over 200,000 identity-consistent images, along with an efficient
data synthesis pipeline to advance research in subject-consistent generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Chen, Guangcong Wang, Jiahao Zhu, Jianhuang Lai, Xiaohua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has recently created impressive assets for
various applications. However, the copyright of these assets is not well
protected as existing watermarking methods are not suited for 3DGS considering
security, capacity, and invisibility. Besides, these methods often require
hours or even days for optimization, limiting the application scenarios. In
this paper, we propose GuardSplat, an innovative and efficient framework that
effectively protects the copyright of 3DGS assets. Specifically, 1) We first
propose a CLIP-guided Message Decoupling Optimization module for training the
message decoder, leveraging CLIP's aligning capability and rich representations
to achieve a high extraction accuracy with minimal optimization costs,
presenting exceptional capability and efficiency. 2) Then, we propose a
Spherical-harmonic-aware (SH-aware) Message Embedding module tailored for 3DGS,
which employs a set of SH offsets to seamlessly embed the message into the SH
features of each 3D Gaussian while maintaining the original 3D structure. It
enables the 3DGS assets to be watermarked with minimal fidelity trade-offs and
prevents malicious users from removing the messages from the model files,
meeting the demands for invisibility and security. 3) We further propose an
Anti-distortion Message Extraction module to improve robustness against various
visual distortions. Extensive experiments demonstrate that GuardSplat
outperforms the state-of-the-art methods and achieves fast optimization speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://narcissusex.github.io/GuardSplat and Code:
  https://github.com/NarcissusEx/GuardSplat</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConvMixFormer- A Resource-efficient Convolution Mixer for
  <span class="highlight-title">Transformer</span>-based Dynamic Hand Gesture Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07118v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07118v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have demonstrated remarkable success in many domains such
as natural language processing (NLP) and computer vision. With the growing
interest in transformer-based architectures, they are now utilized for gesture
recognition. So, we also explore and devise a novel ConvMixFormer architecture
for dynamic hand gestures. The transformers use quadratic scaling of the
attention features with the sequential data, due to which these models are
computationally complex and heavy. We have considered this drawback of the
transformer and designed a resource-efficient model that replaces the
self-attention in the transformer with the simple convolutional layer-based
token mixer. The computational cost and the parameters used for the
convolution-based mixer are comparatively less than the quadratic
self-attention. Convolution-mixer helps the model capture the local spatial
features that self-attention struggles to capture due to their sequential
processing nature. Further, an efficient gate mechanism is employed instead of
a conventional feed-forward network in the transformer to help the model
control the flow of features within different stages of the proposed model.
This design uses fewer learnable parameters which is nearly half the vanilla
transformer that helps in fast and efficient training. The proposed method is
evaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has
achieved state-of-the-art results on single and multimodal inputs. We have also
shown the parameter efficiency of the proposed ConvMixFormer model compared to
other methods. The source code is available at
https://github.com/mallikagarg/ConvMixFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Temporally Consistent Video Depth from Video Diffusion Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01493v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01493v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Vitor Guizilini, Yue Wang, Matteo Poggi, Yiyi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the challenge of streamed video depth estimation, which
expects not only per-frame accuracy but, more importantly, cross-frame
consistency. We argue that sharing contextual information between frames or
clips is pivotal in fostering temporal consistency. Thus, instead of directly
developing a depth estimator from scratch, we reformulate this predictive task
into a conditional generation problem to provide contextual information within
a clip and across clips. Specifically, we propose a consistent context-aware
training and inference strategy for arbitrarily long videos to provide
cross-clip context. We sample independent noise levels for each frame within a
clip during training while using a sliding window strategy and initializing
overlapping frames with previously predicted frames without adding noise.
Moreover, we design an effective training strategy to provide context within a
clip. Extensive experimental results validate our design choices and
demonstrate the superiority of our approach, dubbed ChronoDepth. Project page:
https://xdimlab.github.io/ChronoDepth/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PartGS:Learning Part-aware 3D Representations by Fusing 2D Gaussians and
  Superquadrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3D
Gaussians, are commonly used to represent 3D objects or scenes. However, human
perception typically understands 3D objects at a higher level as a composition
of parts or structures rather than points or voxels. Representing 3D objects or
scenes as semantic parts can benefit further understanding and applications. In
this paper, we introduce $\textbf{PartGS}$, $\textbf{part}$-aware 3D
reconstruction by a hybrid representation of 2D $\textbf{G}$aussians and
$\textbf{S}$uperquadrics, which parses objects or scenes into semantic parts,
digging 3D structural clues from multi-view image inputs. Accurate structured
geometry reconstruction and high-quality rendering are achieved at the same
time. Our method simultaneously optimizes superquadric meshes and Gaussians by
coupling their parameters within our hybrid representation. On one hand, this
hybrid representation inherits the advantage of superquadrics to represent
different shape primitives, supporting flexible part decomposition of scenes.
On the other hand, 2D Gaussians capture complex texture and geometry details,
ensuring high-quality appearance and geometry reconstruction. Our method is
fully unsupervised and outperforms existing state-of-the-art approaches in
extensive experiments on DTU, ShapeNet, and real-life datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DGNN-YOLO: Dynamic Graph Neural Networks with YOLO11 for Small Object
  Detection and Tracking in Traffic Surveillance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahriar Soudeep, M. F. Mridha, Md Abrar Jahin, Nilanjan Dey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate detection and tracking of small objects such as pedestrians,
cyclists, and motorbikes are critical for traffic surveillance systems, which
are crucial in improving road safety and decision-making in intelligent
transportation systems. However, traditional methods struggle with challenges
such as occlusion, low resolution, and dynamic traffic conditions,
necessitating innovative approaches to address these limitations. This paper
introduces DGNN-YOLO, a novel framework integrating dynamic graph neural
networks (DGNN) with YOLO11 to enhance small object detection and tracking in
traffic surveillance systems. The framework leverages YOLO11's advanced spatial
feature extraction capabilities for precise object detection and incorporates
DGNN to model spatial-temporal relationships for robust real-time tracking
dynamically. By constructing and updating graph structures, DGNN-YOLO
effectively represents objects as nodes and their interactions as edges,
ensuring adaptive and accurate tracking in complex and dynamic environments.
Extensive experiments demonstrate that DGNN-YOLO consistently outperforms
state-of-the-art methods in detecting and tracking small objects under diverse
traffic conditions, achieving the highest precision (0.8382), recall (0.6875),
and mAP@0.5:0.95 (0.6476), showcasing its robustness and scalability,
particularly in challenging scenarios involving small and occluded objects.
This work provides a scalable, real-time traffic surveillance and analysis
solution, significantly contributing to intelligent transportation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure-Aware Human Body Reshaping with Adaptive Affinity-Graph
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiwen Deng, Yangcen Liu, Wen Li, Guoqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a source portrait, the automatic human body reshaping task aims at
editing it to an aesthetic body shape. As the technology has been widely used
in media, several methods have been proposed mainly focusing on generating
optical flow to warp the body shape. However, those previous works only
consider the local transformation of different body parts (arms, torso, and
legs), ignoring the global affinity, and limiting the capacity to ensure
consistency and quality across the entire body. In this paper, we propose a
novel Adaptive Affinity-Graph Network (AAGN), which extracts the global
affinity between different body parts to enhance the quality of the generated
optical flow. Specifically, our AAGN primarily introduces the following
designs: (1) we propose an Adaptive Affinity-Graph (AAG) Block that leverages
the characteristic of a fully connected graph. AAG represents different body
parts as nodes in an adaptive fully connected graph and captures all the
affinities between nodes to obtain a global affinity map. The design could
better improve the consistency between body parts. (2) Besides, for
high-frequency details are crucial for photo aesthetics, a Body Shape
Discriminator (BSD) is designed to extract information from both high-frequency
and spatial domain. Particularly, an SRM filter is utilized to extract
high-frequency details, which are combined with spatial features as input to
the BSD. With this design, BSD guides the Flow Generator (FG) to pay attention
to various fine details rather than rigid pixel-level fitting. Extensive
experiments conducted on the BR-5K dataset demonstrate that our framework
significantly enhances the aesthetic appeal of reshaped photos, surpassing all
previous work to achieve state-of-the-art in all evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Invertible Consistency Distillation for Text-Guided Image Editing in
  Around 7 Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14539v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14539v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Starodubcev, Mikhail Khoroshikh, Artem Babenko, Dmitry Baranchuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion distillation represents a highly promising direction for achieving
faithful text-to-image generation in a few sampling steps. However, despite
recent successes, existing distilled models still do not provide the full
spectrum of diffusion abilities, such as real image inversion, which enables
many precise image manipulation methods. This work aims to enrich distilled
text-to-image diffusion models with the ability to effectively encode real
images into their latent space. To this end, we introduce invertible
Consistency Distillation (iCD), a generalized consistency distillation
framework that facilitates both high-quality image synthesis and accurate image
encoding in only 3-4 inference steps. Though the inversion problem for
text-to-image diffusion models gets exacerbated by high classifier-free
guidance scales, we notice that dynamic guidance significantly reduces
reconstruction errors without noticeable degradation in generation performance.
As a result, we demonstrate that iCD equipped with dynamic guidance may serve
as a highly effective tool for zero-shot text-guided image editing, competing
with more expensive state-of-the-art alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yandex-research.github.io/invertible-cd/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Brain Tumour Removing and Missing Modality Generation using 3D WDM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Gijs Luijten, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the second-placed solution for task 8 and the
participation solution for task 7 of BraTS 2024. The adoption of automated
brain analysis algorithms to support clinical practice is increasing. However,
many of these algorithms struggle with the presence of brain lesions or the
absence of certain MRI modalities. The alterations in the brain's morphology
leads to high variability and thus poor performance of predictive models that
were trained only on healthy brains. The lack of information that is usually
provided by some of the missing MRI modalities also reduces the reliability of
the prediction models trained with all modalities. In order to improve the
performance of these models, we propose the use of conditional 3D wavelet
diffusion models. The wavelet transform enabled full-resolution image training
and prediction on a GPU with 48 GB VRAM, without patching or downsampling,
preserving all information for prediction. The code for these tasks is
available at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning 2.0: Artificial Neurons That Matter -- Reject Correlation,
  Embrace Orthogonality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Bouhsine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a yat-product-powered neural network, the Neural Matter Network
(NMN), a breakthrough in deep learning that achieves non-linear pattern
recognition without activation functions. Our key innovation relies on the
yat-product and yat-product, which naturally induces non-linearity by
projecting inputs into a pseudo-metric space, eliminating the need for
traditional activation functions while maintaining only a softmax layer for
final class probability distribution. This approach simplifies network
architecture and provides unprecedented transparency into the network's
decision-making process. Our comprehensive empirical evaluation across
different datasets demonstrates that NMN consistently outperforms traditional
MLPs. The results challenge the assumption that separate activation functions
are necessary for effective deep-learning models. The implications of this work
extend beyond immediate architectural benefits, by eliminating intermediate
activation functions while preserving non-linear capabilities, yat-MLP
establishes a new paradigm for neural network design that combines simplicity
with effectiveness. Most importantly, our approach provides unprecedented
insights into the traditionally opaque "black-box" nature of neural networks,
offering a clearer understanding of how these models process and classify
information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>fixed proof, added softermax</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deepfake for the Good: Generating Avatars through Face-Swapping with
  Implicit Deepfake Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous emerging deep-learning techniques have had a substantial impact on
computer graphics. Among the most promising breakthroughs are the rise of
Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the
object's shape and color in neural network weights using a handful of images
with known camera positions to generate novel views. In contrast, GS provides
accelerated training and inference without a decrease in rendering quality by
encoding the object's characteristics in a collection of Gaussian
distributions. These two techniques have found many use cases in spatial
computing and other domains. On the other hand, the emergence of deepfake
methods has sparked considerable controversy. Deepfakes refers to artificial
intelligence-generated videos that closely mimic authentic footage. Using
generative models, they can modify facial features, enabling the creation of
altered identities or expressions that exhibit a remarkably realistic
appearance to a real person. Despite these controversies, deepfake can offer a
next-generation solution for avatar creation and gaming when of desirable
quality. To that end, we show how to combine all these emerging technologies to
obtain a more plausible outcome. Our ImplicitDeepfake uses the classical
deepfake algorithm to modify all training images separately and then train NeRF
and GS on modified faces. Such simple strategies can produce plausible 3D
deepfake-based avatars.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Multi-modal Large Language Models via Visual Token Grouping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minbin Huang, Runhui Huang, Han Shi, Yimeng Chen, Chuanyang Zheng, Xiangguo Sun, Xin Jiang, Zhenguo Li, Hong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Multi-modal Large Language Models (MLLMs) enhances Large
Language Models (LLMs) with the ability to perceive data formats beyond text,
significantly advancing a range of downstream applications, such as visual
question answering and image captioning. However, the substantial computational
costs associated with processing high-resolution images and videos pose a
barrier to their broader adoption. To address this challenge, compressing
vision tokens in MLLMs has emerged as a promising approach to reduce inference
costs. While existing methods conduct token reduction in the feature alignment
phase. In this paper, we introduce VisToG, a novel grouping mechanism that
leverages the capabilities of pre-trained vision encoders to group similar
image segments without the need for segmentation masks. Specifically, we
concatenate semantic tokens to represent image semantic segments after the
linear projection layer before feeding into the vision encoder. Besides, with
the isolated attention we adopt, VisToG can identify and eliminate redundant
visual tokens utilizing the prior knowledge in the pre-trained vision encoder,
which effectively reduces computational demands. Extensive experiments
demonstrate the effectiveness of VisToG, maintaining 98.1% of the original
performance while achieving a reduction of over 27\% inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLOV: Guided Large Language Models as Implicit Optimizers for Vision
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Jehanzeb Mirza, Mengjie Zhao, Zhuoyuan Mao, Sivan Doveh, Wei Lin, Paul Gavrikov, Michael Dorkenwald, Shiqi Yang, Saurav Jha, Hiromi Wakaki, Yuki Mitsufuji, Horst Possegger, Rogerio Feris, Leonid Karlinsky, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel method (GLOV) enabling Large Language Models
(LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to
enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the
downstream task description, querying it for suitable VLM prompts (e.g., for
zero-shot classification with CLIP). These prompts are ranked according to a
purity measure obtained through a fitness function. In each respective
optimization step, the ranked prompts are fed as in-context examples (with
their accuracies) to equip the LLM with the knowledge of the type of text
prompts preferred by the downstream VLM. Furthermore, we also explicitly steer
the LLM generation process in each optimization step by specifically adding an
offset difference vector of the embeddings from the positive and negative
solutions found by the LLM, in previous optimization steps, to the intermediate
layer of the network for the next generation step. This offset vector steers
the LLM generation toward the type of language preferred by the downstream VLM,
resulting in enhanced performance on the downstream vision tasks. We
comprehensively evaluate our GLOV on 16 diverse datasets using two families of
VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models
-- showing that the discovered solutions can enhance the recognition
performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/jmiemirza/GLOV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Free-Mask: A Novel Paradigm of Integration Between the Segmentation
  Diffusion Model and Image Editing to Improve Segmentation Ability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Gao, Fangxu Xing, Daniel Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current semantic segmentation models typically require a substantial amount
of manually annotated data, a process that is both time-consuming and
resource-intensive. Alternatively, leveraging advanced text-to-image models
such as Midjourney and Stable Diffusion has emerged as an efficient strategy,
enabling the automatic generation of synthetic data in place of manual
annotations. However, previous methods have been limited to generating
single-instance images, as the generation of multiple instances with Stable
Diffusion has proven unstable. To address this limitation and expand the scope
and diversity of synthetic datasets, we propose a framework \textbf{Free-Mask}
that combines a Diffusion Model for segmentation with advanced image editing
capabilities, allowing for the integration of multiple objects into images via
text-to-image models. Our method facilitates the creation of highly realistic
datasets that closely emulate open-world environments while generating accurate
segmentation masks. It reduces the labor associated with manual annotation and
also ensures precise mask generation. Experimental results demonstrate that
synthetic data generated by \textbf{Free-Mask} enables segmentation models to
outperform those trained on real data, especially in zero-shot settings.
Notably, \textbf{Free-Mask} achieves new state-of-the-art results on previously
unseen classes in the VOC 2012 benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages,5 figures,5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anomaly Detection in Medical Imaging -- A Mini <span class="highlight-title">Review</span> <span class="chip">SC2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.11986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.11986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian E. Tschuchnig, Michael Gadermayr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing digitization of medical imaging enables machine learning based
improvements in detecting, visualizing and segmenting lesions, easing the
workload for medical experts. However, supervised machine learning requires
reliable labelled data, which is is often difficult or impossible to collect or
at least time consuming and thereby costly. Therefore methods requiring only
partly labeled data (semi-supervised) or no labeling at all (unsupervised
methods) have been applied more regularly. Anomaly detection is one possible
methodology that is able to leverage semi-supervised and unsupervised methods
to handle medical imaging tasks like classification and segmentation. This
paper uses a semi-exhaustive literature review of relevant anomaly detection
papers in medical imaging to cluster into applications, highlight important
results, establish lessons learned and give further advice on how to approach
anomaly detection in medical imaging. The qualitative analysis is based on
google scholar and 4 different search terms, resulting in 120 different
analysed papers. The main results showed that the current research is mostly
motivated by reducing the need for labelled data. Also, the successful and
substantial amount of research in the brain MRI domain shows the potential for
applications in further domains like OCT and chest X-ray.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented at iDSC2021 edit: During work on this
  publication Maximilian Ernst Tschuchnig was affiliated with Salzburg
  University of Applied Sciences and University of Salzburg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Multi-Scale Multiple Instance Learning to Improve Thyroid
  Cancer Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian E. Tschuchnig, Philipp Grubmüller, Lea M. Stangassinger, Christina Kreutzer, Sébastien Couillard-Després, Gertie J. Oostingh, Anton Hittmair, Michael Gadermayr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thyroid cancer is currently the fifth most common malignancy diagnosed in
women. Since differentiation of cancer sub-types is important for treatment and
current, manual methods are time consuming and subjective, automatic
computer-aided differentiation of cancer types is crucial. Manual
differentiation of thyroid cancer is based on tissue sections, analysed by
pathologists using histological features. Due to the enormous size of gigapixel
whole slide images, holistic classification using deep learning methods is not
feasible. Patch based multiple instance learning approaches, combined with
aggregations such as bag-of-words, is a common approach. This work's
contribution is to extend a patch based state-of-the-art method by generating
and combining feature vectors of three different patch resolutions and
analysing three distinct ways of combining them. The results showed
improvements in one of the three multi-scale approaches, while the others led
to decreased scores. This provides motivation for analysis and discussion of
the individual approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented at IPTA 2022 (Best Paper) edit: During work on
  this publication Maximilian Ernst Tschuchnig was affiliated with Salzburg
  University of Applied Sciences and University of Salzburg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear
  Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Chen, Runnan Chen, Qiang Qu, Zhaoqing Wang, Tongliang Liu, Xiaoming Chen, Yuk Ying Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D Gaussian Splatting (3DGS) have substantially
improved novel view synthesis, enabling high-quality reconstruction and
real-time rendering. However, blurring artifacts, such as floating primitives
and over-reconstruction, remain challenging. Current methods address these
issues by refining scene structure, enhancing geometric representations,
addressing blur in training images, improving rendering consistency, and
optimizing density control, yet the role of kernel design remains
underexplored. We identify the soft boundaries of Gaussian ellipsoids as one of
the causes of these artifacts, limiting detail capture in high-frequency
regions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), which
replaces Gaussian kernels with linear kernels to achieve sharper and more
precise results, particularly in high-frequency regions. Through evaluations on
three datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, along
with a 30% FPS improvement over baseline 3DGS. The implementation will be made
publicly available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-task Learning To Improve Semantic Segmentation Of CBCT Scans Using
  Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Ernst Tschuchnig, Julia Coste-Marin, Philipp Steininger, Michael Gadermayr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation is a crucial task in medical image processing,
essential for segmenting organs or lesions such as tumors. In this study we aim
to improve automated segmentation in CBCTs through multi-task learning. To
evaluate effects on different volume qualities, a CBCT dataset is synthesised
from the CT Liver Tumor Segmentation Benchmark (LiTS) dataset. To improve
segmentation, two approaches are investigated. First, we perform multi-task
learning to add morphology based regularization through a volume reconstruction
task. Second, we use this reconstruction task to reconstruct the best quality
CBCT (most similar to the original CT), facilitating denoising effects. We
explore both holistic and patch-based approaches. Our findings reveal that,
especially using a patch-based approach, multi-task learning improves
segmentation in most cases and that these results can further be improved by
our denoising approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented at German Conference on Medical Image
  Computing (BVM) 2024 edit: During work on this publication Maximilian Ernst
  Tschuchnig was affiliated with Salzburg University of Applied Sciences and
  University of Salzburg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RO-SVD: A Reconfigurable Hardware Copyright Protection Framework for
  AIGC Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoheng Ran, Muhammad A. A. Abdelgawad, Zekai Zhang, Ray C. C. Cheung, Hong Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dramatic surge in the utilisation of generative artificial intelligence
(GenAI) underscores the need for a secure and efficient mechanism to
responsibly manage, use and disseminate multi-dimensional data generated by
artificial intelligence (AI). In this paper, we propose a blockchain-based
copyright traceability framework called ring oscillator-singular value
decomposition (RO-SVD), which introduces decomposition computing to approximate
low-rank matrices generated from hardware entropy sources and establishes an
AI-generated content (AIGC) copyright traceability mechanism at the device
level. By leveraging the parallelism and reconfigurability of
field-programmable gate arrays (FPGAs), our framework can be easily constructed
on existing AI-accelerated devices and provide a low-cost solution to emerging
copyright issues of AIGC. We developed a hardware-software (HW/SW) co-design
prototype based on comprehensive analysis and on-board experiments with
multiple AI-applicable FPGAs. Using AI-generated images as a case study, our
framework demonstrated effectiveness and emphasised customisation,
unpredictability, efficiency, management and reconfigurability. To the best of
our knowledge, this is the first practical hardware study discussing and
implementing copyright traceability specifically for AI-generated content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted on 20 May 2024 as a full paper at ASAP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Perception System for Real Open Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Sha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel multimodal perception system for a real open
environment. The proposed system includes an embedded computation platform,
cameras, ultrasonic sensors, GPS, and IMU devices. Unlike the traditional
frameworks, our system integrates multiple sensors with advanced computer
vision algorithms to help users walk outside reliably. The system can
efficiently complete various tasks, including navigating to specific locations,
passing through obstacle regions, and crossing intersections. Specifically, we
also use ultrasonic sensors and depth cameras to enhance obstacle avoidance
performance. The path planning module is designed to find the locally optimal
route based on various feedback and the user's current state. To evaluate the
performance of the proposed system, we design several experiments under
different scenarios. The results show that the system can help users walk
efficiently and independently in complex situations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advances in 3D Neural Stylization: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18328v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18328v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingshu Chen, Guocheng Shao, Ka Chun Shum, Binh-Son Hua, Sai-Kit Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern artificial intelligence offers a novel and transformative approach to
creating digital art across diverse styles and modalities like images, videos
and 3D data, unleashing the power of creativity and revolutionizing the way
that we perceive and interact with visual content. This paper reports on recent
advances in stylized 3D asset creation and manipulation with the expressive
power of neural networks. We establish a taxonomy for neural stylization,
considering crucial design choices such as scene representation, guidance data,
optimization strategies, and output styles. Building on such taxonomy, our
survey first revisits the background of neural stylization on 2D images, and
then presents in-depth discussions on recent neural stylization methods for 3D
data, accompanied by a benchmark evaluating selected mesh and neural field
stylization methods. Based on the insights gained from the survey, we highlight
the practical significance, open challenges, future research, and potential
impacts of neural stylization, which facilitates researchers and practitioners
to navigate the rapidly evolving landscape of 3D content creation using modern
artificial intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>curated list of papers:
  https://github.com/chenyingshu/advances_3d_neural_stylization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DUSt3R: Geometric 3D Vision Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14132v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14132v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view stereo reconstruction (MVS) in the wild requires to first estimate
the camera parameters e.g. intrinsic and extrinsic parameters. These are
usually tedious and cumbersome to obtain, yet they are mandatory to triangulate
corresponding pixels in 3D space, which is the core of all best performing MVS
algorithms. In this work, we take an opposite stance and introduce DUSt3R, a
radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction
of arbitrary image collections, i.e. operating without prior information about
camera calibration nor viewpoint poses. We cast the pairwise reconstruction
problem as a regression of pointmaps, relaxing the hard constraints of usual
projective camera models. We show that this formulation smoothly unifies the
monocular and binocular reconstruction cases. In the case where more than two
images are provided, we further propose a simple yet effective global alignment
strategy that expresses all pairwise pointmaps in a common reference frame. We
base our network architecture on standard Transformer encoders and decoders,
allowing us to leverage powerful pretrained models. Our formulation directly
provides a 3D model of the scene as well as depth information, but
interestingly, we can seamlessly recover from it, pixel matches, relative and
absolute camera. Exhaustive experiments on all these tasks showcase that the
proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on
monocular/multi-view depth estimation as well as relative pose estimation. In
summary, DUSt3R makes many geometric 3D vision tasks easy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>fixing the ref for StaticThings3D dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pixel-aligned RGB-NIR Stereo Imaging and <span class="highlight-title">Dataset</span> for Robot Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinnyeong Kim, Seung-Hwan Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating RGB and NIR stereo imaging provides complementary spectral
information, potentially enhancing robotic 3D vision in challenging lighting
conditions. However, existing datasets and imaging systems lack pixel-level
alignment between RGB and NIR images, posing challenges for downstream vision
tasks. In this paper, we introduce a robotic vision system equipped with
pixel-aligned RGB-NIR stereo cameras and a LiDAR sensor mounted on a mobile
robot. The system simultaneously captures pixel-aligned pairs of RGB stereo
images, NIR stereo images, and temporally synchronized LiDAR points. Utilizing
the mobility of the robot, we present a dataset containing continuous video
frames under diverse lighting conditions. We then introduce two methods that
utilize the pixel-aligned RGB-NIR images: an RGB-NIR image fusion method and a
feature fusion method. The first approach enables existing RGB-pretrained
vision models to directly utilize RGB-NIR information without fine-tuning. The
second approach fine-tunes existing vision models to more effectively utilize
RGB-NIR information. Experimental results demonstrate the effectiveness of
using pixel-aligned RGB-NIR images across diverse lighting conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages for main article, 32 pages for supplemental document. Fix
  typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16886v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16886v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntao Jiang, Mengmeng Wang, Huizhong Tian, Lingbo Cheng, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large models have achieved significant progress in computer vision,
challenges such as optimization complexity, the intricacy of transformer
architectures, computational constraints, and practical application demands
highlight the importance of simpler model designs in medical image
segmentation. This need is particularly pronounced in mobile medical devices,
which require lightweight, deployable models with real-time performance.
However, existing lightweight models often suffer from poor robustness across
datasets, limiting their widespread adoption. To address these challenges, this
paper introduces LV-UNet, a lightweight and vanilla model that leverages
pre-trained MobileNetv3-Large backbones and incorporates fusible modules.
LV-UNet employs an enhanced deep training strategy and switches to a deployment
mode during inference by re-parametrization, significantly reducing parameter
count and computational overhead. Experimental results on ISIC 2016, BUSI,
CVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better
trade-off between performance and the computational load. The code will be
released at \url{https://github.com/juntaoJianggavin/LV-UNet}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE BIBM2024 ML4BMI workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morph: A Motion-free Physics Optimization Framework for Human Motion
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Li, Mingshuang Luo, Ruibing Hou, Xin Zhao, Hao Liu, Hong Chang, Zimo Liu, Chen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion generation plays a vital role in applications such as digital
humans and humanoid robot control. However, most existing approaches disregard
physics constraints, leading to the frequent production of physically
implausible motions with pronounced artifacts such as floating and foot
sliding. In this paper, we propose \textbf{Morph}, a
\textbf{Mo}tion-f\textbf{r}ee \textbf{ph}ysics optimization framework,
comprising a Motion Generator and a Motion Physics Refinement module, for
enhancing physical plausibility without relying on costly real-world motion
data. Specifically, the Motion Generator is responsible for providing
large-scale synthetic motion data, while the Motion Physics Refinement Module
utilizes these synthetic data to train a motion imitator within a physics
simulator, enforcing physical constraints to project the noisy motions into a
physically-plausible space. These physically refined motions, in turn, are used
to fine-tune the Motion Generator, further enhancing its capability.
Experiments on both text-to-motion and music-to-dance generation tasks
demonstrate that our framework achieves state-of-the-art motion generation
quality while improving physical plausibility drastically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-View Large Reconstruction Model via Geometry-Aware Positional
  Encoding and Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengfei Li, Xiaoxiao Long, Yixun Liang, Weiyu Li, Yuan Liu, Peng Li, Wenhan Luo, Wenping Wang, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in the Large Reconstruction Model (LRM)
demonstrating impressive results, when extending its input from single image to
multiple images, it exhibits inefficiencies, subpar geometric and texture
quality, as well as slower convergence speed than expected. It is attributed to
that, LRM formulates 3D reconstruction as a naive images-to-3D translation
problem, ignoring the strong 3D coherence among the input images. In this
paper, we propose a Multi-view Large Reconstruction Model (M-LRM) designed to
reconstruct high-quality 3D shapes from multi-views in a 3D-aware manner.
Specifically, we introduce a multi-view consistent cross-attention scheme to
enable M-LRM to accurately query information from the input images. Moreover,
we employ the 3D priors of the input multi-view images to initialize the
triplane tokens. Compared to previous methods, the proposed M-LRM can generate
3D shapes of high fidelity. Experimental studies demonstrate that our model
achieves a significant performance gain and faster training convergence.
Project page: \url{https://murphylmf.github.io/M-LRM/}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AniFaceDiff: Animating Stylized Avatars via Parametric Conditioned
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ken Chen, Sachith Seneviratne, Wei Wang, Dongting Hu, Sanjay Saha, Md. Tarek Hasan, Sanka Rasnayaka, Tamasha Malepathirana, Mingming Gong, Saman Halgamuge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animating stylized avatars with dynamic poses and expressions has attracted
increasing attention for its broad range of applications. Previous research has
made significant progress by training controllable generative models to
synthesize animations based on reference characteristics, pose, and expression
conditions. However, the mechanisms used in these methods to control pose and
expression often inadvertently introduce unintended features from the target
motion, while also causing a loss of expression-related details, particularly
when applied to stylized animation. This paper proposes a new method based on
Stable Diffusion, called AniFaceDiff, incorporating a new conditioning module
for animating stylized avatars. First, we propose a refined spatial
conditioning approach by Facial Alignment to prevent the inclusion of identity
characteristics from the target motion. Then, we introduce an Expression
Adapter that incorporates additional cross-attention layers to address the
potential loss of expression-related information. Our approach effectively
preserves pose and expression from the target video while maintaining input
image consistency. Extensive experiments demonstrate that our method achieves
state-of-the-art results, showcasing superior image quality, preservation of
reference features, and expression accuracy, particularly for out-of-domain
animation across diverse styles, highlighting its versatility and strong
generalization capabilities. This work aims to enhance the quality of virtual
stylized animation for positive applications. To promote responsible use in
virtual environments, we contribute to the advancement of detection for
generative content by evaluating state-of-the-art detectors, highlighting
potential areas for improvement, and suggesting solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> and Benchmark of Automatic Surface Reconstruction from Point
  Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13656v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13656v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Sulzer, Renaud Marlet, Bruno Vallet, Loic Landrieu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive survey and benchmark of both traditional and
learning-based methods for surface reconstruction from point clouds. This task
is particularly challenging for real-world acquisitions due to factors such as
noise, outliers, non-uniform sampling, and missing data. Traditional approaches
often simplify the problem by imposing handcrafted priors on either the input
point clouds or the resulting surface, a process that can require tedious
hyperparameter tuning. In contrast, deep learning models have the capability to
directly learn the properties of input point clouds and desired surfaces from
data. We study the influence of handcrafted and learned priors on the precision
and robustness of surface reconstruction techniques. We evaluate various
time-tested and contemporary methods in a standardized manner. When both
trained and evaluated on point clouds with identical characteristics, the
learning-based models consistently produce higher-quality surfaces compared to
their traditional counterparts -- even in scenarios involving novel shape
categories. However, traditional methods demonstrate greater resilience to the
diverse anomalies commonly found in real-world 3D acquisitions. For the benefit
of the research community, we make our code and datasets available, inviting
further enhancements to learning-based surface reconstruction. This can be
accessed at https://github.com/raphaelsulzer/dsr-benchmark .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VQA$^2$: Visual Question Answering for Video Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03795v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03795v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Jia, Zicheng Zhang, Jiaying Qian, Haoning Wu, Wei Sun, Chunyi Li, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Xiongkuo Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent and proliferation of large multi-modal models (LMMs) have
introduced new paradigms to computer vision, transforming various tasks into a
unified visual question answering framework. Video Quality Assessment (VQA), a
classic field in low-level visual perception, focused initially on quantitative
video quality scoring. However, driven by advances in LMMs, it is now
progressing toward more holistic visual quality understanding tasks. Recent
studies in the image domain have demonstrated that Visual Question Answering
(VQA) can markedly enhance low-level visual quality evaluation. Nevertheless,
related work has not been explored in the video domain, leaving substantial
room for improvement. To address this gap, we introduce the VQA2 Instruction
Dataset - the first visual question answering instruction dataset that focuses
on video quality assessment. This dataset consists of 3 subsets and covers
various video types, containing 157,755 instruction question-answer pairs.
Then, leveraging this foundation, we present the VQA2 series models. The VQA2
series models interleave visual and motion tokens to enhance the perception of
spatial-temporal quality details in videos. We conduct extensive experiments on
video quality scoring and understanding tasks, and results demonstrate that the
VQA2series models achieve excellent performance in both tasks. Notably, our
final model, the VQA2-Assistant, exceeds the renowned GPT-4o in visual quality
understanding tasks while maintaining strong competitiveness in quality scoring
tasks. Our work provides a foundation and feasible approach for integrating
low-level video quality assessment and understanding with LMMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting MAE <span class="highlight-title">pre-train</span>ing for 3D medical image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tassilo Wald, Constantin Ulrich, Stanislav Lukyanenko, Andrei Goncharov, Alberto Paderno, Leander Maerkisch, Paul F. Jäger, Klaus Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the
potential of vast, untapped clinical datasets, for various downstream
applications that suffer from the scarcity of labeled data. While SSL has
revolutionized fields like natural language processing and computer vision, its
adoption in 3D medical image computing has been limited by three key pitfalls:
Small pre-training dataset sizes, architectures inadequate for 3D medical image
analysis, and insufficient evaluation practices. In this paper, we address
these issues by i) leveraging a large-scale dataset of 39k 3D brain MRI volumes
and ii) using a Residual Encoder U-Net architecture within the state-of-the-art
nnU-Net framework. iii) A robust development framework, incorporating 5
development and 8 testing brain MRI segmentation datasets, allowed
performance-driven design decisions to optimize the simple concept of Masked
Auto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses
previous SSL methods but also outperforms the strong nnU-Net baseline by an
average of approximately 3 Dice points setting a new state-of-the-art. Our code
and models are made available here.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Arxiv Preprint. Revised and under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Architectural Approach to Enhance Deep Long-Tailed Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Pan, Yanan Sun, Wei Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep long-tailed recognition has been widely studied to address the issue of
imbalanced data distributions in real-world scenarios. However, there has been
insufficient focus on the design of neural architectures, despite empirical
evidence suggesting that architecture can significantly impact performance. In
this paper, we attempt to mitigate long-tailed issues through architectural
improvements. To simplify the design process, we utilize Differential
Architecture Search (DARTS) to achieve this goal. Unfortunately, existing DARTS
methods struggle to perform well in long-tailed scenarios. To tackle this
challenge, we introduce Long-Tailed Differential Architecture Search (LTDAS).
Specifically, we conduct extensive experiments to explore architectural
components that demonstrate better performance on long-tailed data and propose
a new search space based on our observations. This ensures that the
architecture obtained through our search process incorporates superior
components. Additionally, we propose replacing the learnable linear classifier
with an Equiangular Tight Frame (ETF) classifier to further enhance our method.
This classifier effectively alleviates the biased search process and prevents
performance collapse. Extensive experimental evaluations demonstrate that our
approach consistently improves upon existing methods from an orthogonal
perspective and achieves state-of-the-art results with simple enhancements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time <span class="highlight-title">Transformer</span>-based Open-Vocabulary Detection with Efficient
  Fusion Head 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, Kyusong Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end transformer-based detectors (DETRs) have shown exceptional
performance in both closed-set and open-vocabulary object detection (OVD) tasks
through the integration of language modalities. However, their demanding
computational requirements have hindered their practical application in
real-time object detection (OD) scenarios. In this paper, we scrutinize the
limitations of two leading models in the OVDEval benchmark, OmDet and
Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based
real-time OVD model features an innovative Efficient Fusion Head (EFH) module
designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO.
Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with
TensorRT and language cache techniques applied. Notably, in zero-shot scenarios
on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on
par with current state-of-the-art supervised models. Furthermore, it
establishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an
AP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of
OmDet-Turbo in industrial applications is underscored by its exceptional
performance on benchmark datasets and superior inference speed, positioning it
as a compelling choice for real-time object detection tasks. Code:
\url{https://github.com/om-ai-lab/OmDet}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anticipating Object State Changes in Long Procedural Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12789v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12789v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victoria Manousaki, Konstantinos Bacharidis, Filippos Gouidis, Konstantinos Papoutsakis, Dimitris Plexousakis, Antonis Argyros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce (a) the new problem of anticipating object state
changes in images and videos during procedural activities, (b) new curated
annotation data for object state change classification based on the Ego4D
dataset, and (c) the first method for addressing this challenging problem.
Solutions to this new task have important implications in vision-based scene
understanding, automated monitoring systems, and action planning. The proposed
novel framework predicts object state changes that will occur in the near
future due to yet unseen human actions by integrating learned visual features
that represent recent visual information with natural language (NLP) features
that represent past object state changes and actions. Leveraging the extensive
and challenging Ego4D dataset which provides a large-scale collection of
first-person perspective videos across numerous interaction scenarios, we
introduce an extension noted Ego4D-OSCA that provides new curated annotation
data for the object state change anticipation task (OSCA). An extensive
experimental evaluation is presented demonstrating the proposed method's
efficacy in predicting object state changes in dynamic scenarios. The
performance of the proposed approach also underscores the potential of
integrating video and linguistic cues to enhance the predictive performance of
video understanding systems and lays the groundwork for future research on the
new task of object state change anticipation. The source code and the new
annotation data (Ego4D-OSCA) will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Multi-Task Brain Tumour Segmentation with Synthetic Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Tiago Jesus, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the winning solution of task 1 and the third-placed
solution of task 3 of the BraTS challenge. The use of automated tools in
clinical practice has increased due to the development of more and more
sophisticated and reliable algorithms. However, achieving clinical standards
and developing tools for real-life scenarios is a major challenge. To this end,
BraTS has organised tasks to find the most advanced solutions for specific
purposes. In this paper, we propose the use of synthetic data to train
state-of-the-art frameworks in order to improve the segmentation of adult
gliomas in a post-treatment scenario, and the segmentation of meningioma for
radiotherapy planning. Our results suggest that the use of synthetic data leads
to more robust algorithms, although the synthetic data generation pipeline is
not directly suited to the meningioma task. In task 1, we achieved a DSC of
0.7900, 0.8076, 0.7760, 0.8926, 0.7874, 0.8938 and a HD95 of 35.63, 30.35,
44.58, 16.87, 38.19, 17.95 for ET, NETC, RC, SNFH, TC and WT, respectively and,
in task 3, we achieved a DSC of 0.801 and HD95 of 38.26, in the testing phase.
The code for these tasks is available at
https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Statistics Predict the Sensitivity of Perceptual Quality Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09874v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09874v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Hepburn, Valero Laparra, Raúl Santos-Rodriguez, Jesús Malo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previously, Barlow and Attneave hypothesised a link between biological vision
and information maximisation. Following Shannon, information was defined using
the probability of natural images. Several physiological and psychophysical
phenomena have been derived from principles like info-max, efficient coding, or
optimal denoising. However, it remains unclear how this link is expressed in
mathematical terms from image probability. Classical derivations were subjected
to strong assumptions on the probability models and on the behaviour of the
sensors. Moreover, the direct evaluation of the hypothesis was limited by the
inability of classical image models to deliver accurate estimates of the
probability. Here, we directly evaluate image probabilities using a generative
model for natural images, and analyse how probability-related factors can be
combined to predict the sensitivity of state-of-the-art subjective image
quality metrics, a proxy for human perception. We use information theory and
regression analysis to find a simple model that when combining just two
probability-related factors achieves 0.77 correlation with subjective metrics.
This probability-based model is validated in two ways: through direct
comparison with the opinion of real observers in a subjective quality
experiment, and by reproducing basic trends of classical psychophysical facts
such as the Contrast Sensitivity Function, the Weber-law, and contrast masking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-Size-Driven Design of Convolutional Neural Networks: Virtual Axle
  Detection based on Raw Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01574v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01574v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henik Riedel, Robert Steven Lorenzen, Clemens Hübler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As infrastructure ages, the need for efficient monitoring methods becomes
increasingly critical. Bridge Weigh-In-Motion (BWIM) systems are crucial for
cost-effective determination of loads and, consequently, the residual service
life of road and railway infrastructure. However, conventional BWIM systems
require additional sensors for axle detection, which must be installed in
potentially inaccessible locations or places that interfere with bridge
operation.
  This study presents a novel approach for real-time detection of train axles
using sensors arbitrarily placed on bridges, providing an alternative to
dedicated axle detectors. The developed Virtual Axle Detector with Enhanced
Receptive Field (VADER) has been validated on a single-track railway bridge
using only acceleration measurements, detecting 99.9% of axles with a spatial
error of 3.69cm. Using raw data as input outperformed the state-of-the-art
spectrogram-based method in both speed and memory usage by 99%, thereby making
real-time application feasible for the first time.
  Additionally, we introduce the Maximum Receptive Field (MRF) rule, a novel
approach to optimise hyperparameters of Convolutional Neural Networks (CNNs)
based on the size of objects. In this context, the object size relates to the
fundamental frequency of a bridge. The MRF rule effectively narrows the
hyperparameter search space, overcoming the need for extensive hyperparameter
tuning. Since the MRF rule can theoretically be applied to all unstructured
data, it could have implications for a wide range of deep learning problems,
from earthquake prediction to object recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARTIST: Improving the Generation of Text-rich Images with Disentangled
  Diffusion Models and Large Language Models <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12044v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12044v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyi Zhang, Yufan Zhou, Jiuxiang Gu, Curtis Wigington, Tong Yu, Yiran Chen, Tong Sun, Ruiyi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated exceptional capabilities in generating a
broad spectrum of visual content, yet their proficiency in rendering text is
still limited: they often generate inaccurate characters or words that fail to
blend well with the underlying image. To address these shortcomings, we
introduce a novel framework named, ARTIST, which incorporates a dedicated
textual diffusion model to focus on the learning of text structures
specifically. Initially, we pretrain this textual model to capture the
intricacies of text representation. Subsequently, we finetune a visual
diffusion model, enabling it to assimilate textual structure information from
the pretrained textual model. This disentangled architecture design and
training strategy significantly enhance the text rendering ability of the
diffusion models for text-rich image generation. Additionally, we leverage the
capabilities of pretrained large language models to interpret user intentions
better, contributing to improved generation quality. Empirical results on the
MARIO-Eval benchmark underscore the effectiveness of the proposed method,
showing an improvement of up to 15% in various metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with
  Diffusion Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17550v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17550v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenpeng Du, Qi Chen, Tianyu He, Xu Tan, Xie Chen, Kai Yu, Sheng Zhao, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent research has made significant progress in speech-driven talking
face generation, the quality of the generated video still lags behind that of
real recordings. One reason for this is the use of handcrafted intermediate
representations like facial landmarks and 3DMM coefficients, which are designed
based on human knowledge and are insufficient to precisely describe facial
movements. Additionally, these methods require an external pretrained model for
extracting these representations, whose performance sets an upper bound on
talking face generation. To address these limitations, we propose a novel
method called DAE-Talker that leverages data-driven latent representations
obtained from a diffusion autoencoder (DAE). DAE contains an image encoder that
encodes an image into a latent vector and a DDIM image decoder that
reconstructs the image from it. We train our DAE on talking face video frames
and then extract their latent representations as the training target for a
Conformer-based speech2latent model. This allows DAE-Talker to synthesize full
video frames and produce natural head movements that align with the content of
speech, rather than relying on a predetermined head pose from a template video.
We also introduce pose modelling in speech2latent for pose controllability.
Additionally, we propose a novel method for generating continuous video frames
with the DDIM image decoder trained on individual frames, eliminating the need
for modelling the joint distribution of consecutive frames directly. Our
experiments show that DAE-Talker outperforms existing popular methods in
lip-sync, video fidelity, and pose naturalness. We also conduct ablation
studies to analyze the effectiveness of the proposed techniques and demonstrate
the pose controllability of DAE-Talker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-Driven Graph Network-Based Simulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franciszek Szewczyk, Gilles Louppe, Matthia Sabatelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifelike visualizations in design, cinematography, and gaming rely on precise
physics simulations, typically requiring extensive computational resources and
detailed physical input. This paper presents a method that can infer a system's
physical properties from a short video, eliminating the need for explicit
parameter input, provided it is close to the training condition. The learned
representation is then used within a Graph Network-based Simulator to emulate
the trajectories of physical systems. We demonstrate that the video-derived
encodings effectively capture the physical properties of the system and
showcase a linear dependence between some of the encodings and the system's
motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Compositional Text-to-Image Generation with Reliable Random
  Seeds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have demonstrated remarkable capability in
generating realistic images from arbitrary text prompts. However, they often
produce inconsistent results for compositional prompts such as "two dogs" or "a
penguin on the right of a bowl". Understanding these inconsistencies is crucial
for reliable image generation. In this paper, we highlight the significant role
of initial noise in these inconsistencies, where certain noise patterns are
more reliable for compositional prompts than others. Our analyses reveal that
different initial random seeds tend to guide the model to place objects in
distinct image areas, potentially adhering to specific patterns of camera
angles and image composition associated with the seed. To improve the model's
compositional ability, we propose a method for mining these reliable cases,
resulting in a curated training set of generated images without requiring any
manual annotation. By fine-tuning text-to-image models on these generated
images, we significantly enhance their compositional capabilities. For
numerical composition, we observe relative increases of 29.3% and 19.5% for
Stable Diffusion and PixArt-{\alpha}, respectively. Spatial composition sees
even larger gains, with 60.7% for Stable Diffusion and 21.1% for
PixArt-{\alpha}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing the automatic segmentation and analysis of 3D liver
  vasculature models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassine Machta, Omar Ali, Kevin Hakkakian, Ana Vlasceanu, Amaury Facque, Nicolas Golse, Irene Vignon-Clementel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical assessment of liver cancer patients requires identification of the
vessel trees from medical images. Specifically, the venous trees - the portal
(perfusing) and the hepatic (draining) trees are important for understanding
the liver anatomy and disease state, and perform surgery planning. This
research aims to improve the 3D segmentation, skeletonization, and subsequent
analysis of vessel trees, by creating an automatic pipeline based on deep
learning and image processing techniques.
  The first part of this work explores the impact of differentiable
skeletonization methods such as ClDice and morphological skeletonization loss,
on the overall liver vessel segmentation performance. To this aim, it studies
how to improve vessel tree connectivity.
  The second part of this study converts a single class vessel segmentation
into multi-class ones, separating the two venous trees. It builds on the
previous two-class vessel segmentation model, which vessel tree outputs might
be entangled, and on connected components and skeleton analyses of the trees.
  After providing sub-labeling of the specific anatomical branches of each
venous tree, these algorithms also enable a morphometric analysis of the vessel
trees by extracting various geometrical markers.
  In conclusion, we propose a method that successfully improves current
skeletonization methods, for extensive vascular trees that contain vessels of
different calibers. The separation algorithm creates a clean multi-class
segmentation of the vessels, validated by surgeons to provide low error. A new,
publicly shared high-quality liver vessel dataset of 77 cases is thus created.
Finally a method to annotate vessel trees according to anatomy is provided,
enabling a unique liver vessel morphometry analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Internship at Simbiotx</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MVBoost: Boost 3D Reconstruction with Multi-View Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Liu, Xiaomei Zhang, Zhiyuan Ma, Xiangyu Zhu, Zhen Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D object reconstruction have been remarkable, yet
most current 3D models rely heavily on existing 3D datasets. The scarcity of
diverse 3D datasets results in limited generalization capabilities of 3D
reconstruction models. In this paper, we propose a novel framework for boosting
3D reconstruction with multi-view refinement (MVBoost) by generating pseudo-GT
data. The key of MVBoost is combining the advantages of the high accuracy of
the multi-view generation model and the consistency of the 3D reconstruction
model to create a reliable data source. Specifically, given a single-view input
image, we employ a multi-view diffusion model to generate multiple views,
followed by a large 3D reconstruction model to produce consistent 3D data.
MVBoost then adaptively refines these multi-view images, rendered from the
consistent 3D data, to build a large-scale multi-view dataset for training a
feed-forward 3D reconstruction model. Additionally, the input view optimization
is designed to optimize the corresponding viewpoints based on the user's input
image, ensuring that the most important viewpoint is accurately tailored to the
user's needs. Extensive evaluations demonstrate that our method achieves
superior reconstruction results and robust generalization compared to prior
works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling nnU-Net for CBCT Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Isensee, Yannick Kirchhoff, Lars Kraemer, Maximilian Rokuss, Constantin Ulrich, Klaus H. Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents our approach to scaling the nnU-Net framework for
multi-structure segmentation on Cone Beam Computed Tomography (CBCT) images,
specifically in the scope of the ToothFairy2 Challenge. We leveraged the
nnU-Net ResEnc L model, introducing key modifications to patch size, network
topology, and data augmentation strategies to address the unique challenges of
dental CBCT imaging. Our method achieved a mean Dice coefficient of 0.9253 and
HD95 of 18.472 on the test set, securing a mean rank of 4.6 and with it the
first place in the ToothFairy2 challenge. The source code is publicly
available, encouraging further research and development in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fabian Isensee and Yannick Kirchhoff contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahan Tu, Rui Hu, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination poses a persistent challenge for multimodal large language
models (MLLMs). However, existing benchmarks for evaluating hallucinations are
generally static, which may overlook the potential risk of data contamination.
To address this issue, we propose ODE, an open-set, dynamic protocol designed
to evaluate object hallucinations in MLLMs at both the existence and attribute
levels. ODE employs a graph-based structure to represent real-world object
concepts, their attributes, and the distributional associations between them.
This structure facilitates the extraction of concept combinations based on
diverse distributional criteria, generating varied samples for structured
queries that evaluate hallucinations in both generative and discriminative
tasks. Through the generation of new samples, dynamic concept combinations, and
varied distribution frequencies, ODE mitigates the risk of data contamination
and broadens the scope of evaluation. This protocol is applicable to both
general and specialized scenarios, including those with limited data.
Experimental results demonstrate the effectiveness of our protocol, revealing
that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated
samples, which indicates potential data contamination. Furthermore, these
generated samples aid in analyzing hallucination patterns and fine-tuning
models, offering an effective approach to mitigating hallucinations in MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FIRE: A <span class="highlight-title">Dataset</span> for Feedback Integration and Refinement Evaluation of
  Multimodal Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengxiang Li, Zhi Gao, Bofei Zhang, Tao Yuan, Yuwei Wu, Mehrtash Harandi, Yunde Jia, Song-Chun Zhu, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision language models (VLMs) have achieved impressive progress in diverse
applications, becoming a prevalent research direction. In this paper, we build
FIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn
conversations that are derived from 27 source datasets, empowering VLMs to
spontaneously refine their responses based on user feedback across diverse
tasks. To scale up the data collection, FIRE is collected in two components:
FIRE-100K and FIRE-1M, where FIRE-100K is generated by GPT-4V, and FIRE-1M is
freely generated via models trained on FIRE-100K. Then, we build FIRE-Bench, a
benchmark to comprehensively evaluate the feedback-refining capability of VLMs,
which contains 11K feedback-refinement conversations as the test data, two
evaluation settings, and a model to provide feedback for VLMs. We develop the
FIRE-LLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, which shows
remarkable feedback-refining capability on FIRE-Bench and outperforms untrained
VLMs by 50%, making more efficient user-agent interactions and underscoring the
significance of the FIRE dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Dataset & Benchmark Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAR: <span class="highlight-title">Prompt</span>-Aware Token Reduction Method for Efficient Large Multimodal
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingen Liu, Fan Wu, Ruihui Li, Zhuo Tang, Kenli Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) demonstrate strong performance
across visual tasks, but their efficiency is hindered by significant
computational and memory demands from processing long contexts in multimodal
inputs. To address this, we introduce PAR (Prompt-Aware Token Reduction), a
novel and plug-and-play approach that reduces visual tokens efficiently without
compromising model performance. Unlike previous methods that rely heavily on
attention mechanisms and overlooking cross-modal interactions , we uses a
prompt-aware strategy to adpative identify and cluster essential visual tokens.
PAR categorizes visual context redundancy into two types: external and
internal. External redundancy is minimized through semantic retrieval, while
internal redundancy is addressed using a token routing mechanism. This method
substantially reduces computational load without requiring additional training
or complex architectural modifications. \textbf{Experimental results
demonstrate that across various visual question answering tasks, PAR reduces
FLOPs by 83\% with a compression ratio of 89\%, while retaining 97\% of
baseline accuracy.} The adaptive design of PAR achieves a 2x token reduction
ratio compared to prior approaches, enabling a better balance between
performance and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures,3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystify Mamba in Vision: A Linear Attention Perspective <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16605v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16605v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongchen Han, Ziyi Wang, Zhuofan Xia, Yizeng Han, Yifan Pu, Chunjiang Ge, Jun Song, Shiji Song, Bo Zheng, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mamba is an effective state space model with linear computation complexity.
It has recently shown impressive efficiency in dealing with high-resolution
inputs across various vision tasks. In this paper, we reveal that the powerful
Mamba model shares surprising similarities with linear attention Transformer,
which typically underperform conventional Transformer in practice. By exploring
the similarities and disparities between the effective Mamba and subpar linear
attention Transformer, we provide comprehensive analyses to demystify the key
factors behind Mamba's success. Specifically, we reformulate the selective
state space model and linear attention within a unified formulation, rephrasing
Mamba as a variant of linear attention Transformer with six major distinctions:
input gate, forget gate, shortcut, no attention normalization, single-head, and
modified block design. For each design, we meticulously analyze its pros and
cons, and empirically evaluate its impact on model performance in vision tasks.
Interestingly, the results highlight the forget gate and block design as the
core contributors to Mamba's success, while the other four designs are less
crucial. Based on these findings, we propose a Mamba-Inspired Linear Attention
(MILA) model by incorporating the merits of these two key designs into linear
attention. The resulting model outperforms various vision Mamba models in both
image classification and high-resolution dense prediction tasks, while enjoying
parallelizable computation and fast inference speed. Code is available at
https://github.com/LeapLabTHU/MLLA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SITReg: Multi-resolution architecture for symmetric, inverse consistent,
  and topology preserving image registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10211v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10211v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Honkamaa, Pekka Marttinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has emerged as a strong alternative for classical iterative
methods for deformable medical image registration, where the goal is to find a
mapping between the coordinate systems of two images. Popular classical image
registration methods enforce the useful inductive biases of symmetricity,
inverse consistency, and topology preservation by construction. However, while
many deep learning registration methods encourage these properties via loss
functions, no earlier methods enforce all of them by construction. Here, we
propose a novel registration architecture based on extracting multi-resolution
feature representations which is by construction symmetric, inverse consistent,
and topology preserving. We also develop an implicit layer for memory efficient
inversion of the deformation fields. Our method achieves state-of-the-art
registration accuracy on three datasets. The code is available at
https://github.com/honkamj/SITReg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoVLA: Comprehensive Vision-Language-Action <span class="highlight-title">Dataset</span> for Autonomous
  Driving <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hidehisa Arai, Keita Miwa, Kento Sasaki, Yu Yamaguchi, Kohei Watanabe, Shunsuke Aoki, Issei Yamamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving, particularly navigating complex and unanticipated
scenarios, demands sophisticated reasoning and planning capabilities. While
Multi-modal Large Language Models (MLLMs) offer a promising avenue for this,
their use has been largely confined to understanding complex environmental
contexts or generating high-level driving commands, with few studies extending
their application to end-to-end path planning. A major research bottleneck is
the lack of large-scale annotated datasets encompassing vision, language, and
action. To address this issue, we propose CoVLA (Comprehensive
Vision-Language-Action) Dataset, an extensive dataset comprising real-world
driving videos spanning more than 80 hours. This dataset leverages a novel,
scalable approach based on automated data processing and a caption generation
pipeline to generate accurate driving trajectories paired with detailed natural
language descriptions of driving environments and maneuvers. This approach
utilizes raw in-vehicle sensor data, allowing it to surpass existing datasets
in scale and annotation richness. Using CoVLA, we investigate the driving
capabilities of MLLMs that can handle vision, language, and action in a variety
of driving scenarios. Our results illustrate the strong proficiency of our
model in generating coherent language and action outputs, emphasizing the
potential of Vision-Language-Action (VLA) models in the field of autonomous
driving. This dataset establishes a framework for robust, interpretable, and
data-driven autonomous driving systems by providing a comprehensive platform
for training and evaluating VLA models, contributing to safer and more reliable
self-driving vehicles. The dataset is released for academic purpose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025, Project Page: https://turingmotors.github.io/covla-ad/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Cue Enhancement and Dual Low-Rank Adaptation for Efficient Visual
  Instruction Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengkun Jiao, Bin Zhu, Jingjing Chen, Chong-Wah Ngo, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning multimodal large language models (MLLMs)
presents significant challenges, including reliance on high-level visual
features that limit fine-grained detail comprehension, and data conflicts that
arise from task complexity. To address these issues, we propose an efficient
fine-tuning framework with two novel approaches: Vision Cue Enhancement (VCE)
and Dual Low-Rank Adaptation (Dual-LoRA). VCE enhances the vision projector by
integrating multi-level visual cues, improving the model's ability to capture
fine-grained visual features. Dual-LoRA introduces a dual low-rank structure
for instruction tuning, decoupling learning into skill and task spaces to
enable precise control and efficient adaptation across diverse tasks. Our
method simplifies implementation, enhances visual comprehension, and improves
adaptability. Experiments on both downstream tasks and general benchmarks
demonstrate the effectiveness of our proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Pixels to Insights: A <span class="highlight-title">Survey</span> on Automatic Chart Understanding in
  the Era of Large Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12027v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12027v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kung-Hsiang Huang, Hou Pong Chan, Yi R. Fung, Haoyi Qiu, Mingyang Zhou, Shafiq Joty, Shih-Fu Chang, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data visualization in the form of charts plays a pivotal role in data
analysis, offering critical insights and aiding in informed decision-making.
Automatic chart understanding has witnessed significant advancements with the
rise of large foundation models in recent years. Foundation models, such as
large language models, have revolutionized various natural language processing
tasks and are increasingly being applied to chart understanding tasks. This
survey paper provides a comprehensive overview of the recent developments,
challenges, and future directions in chart understanding within the context of
these foundation models. We review fundamental building blocks crucial for
studying chart understanding tasks. Additionally, we explore various tasks and
their evaluation metrics and sources of both charts and textual inputs. Various
modeling strategies are then examined, encompassing both classification-based
and generation-based approaches, along with tool augmentation techniques that
enhance chart understanding performance. Furthermore, we discuss the
state-of-the-art performance of each task and discuss how we can improve the
performance. Challenges and future directions are addressed, highlighting the
importance of several topics, such as domain-specific charts, lack of efforts
in developing evaluation metrics, and agent-oriented settings. This survey
paper serves as a comprehensive resource for researchers and practitioners in
the fields of natural language processing, computer vision, and data analysis,
providing valuable insights and directions for future research in chart
understanding leveraging large foundation models. The studies mentioned in this
paper, along with emerging new research, will be continually updated at:
https://github.com/khuangaf/Awesome-Chart-Understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Knowledge and Data Engineering (TKDE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monocular Lane Detection Based on Deep Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16316v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16316v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin He, Haiyun Guo, Kuan Zhu, Bingke Zhu, Xu Zhao, Jianwu Fang, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane detection plays an important role in autonomous driving perception
systems. As deep learning algorithms gain popularity, monocular lane detection
methods based on them have demonstrated superior performance and emerged as a
key research direction in autonomous driving perception. The core designs of
these algorithmic frameworks can be summarized as follows: (1) Task paradigm,
focusing on lane instance-level discrimination; (2) Lane modeling, representing
lanes as a set of learnable parameters in the neural network; (3) Global
context supplementation, enhancing inference on the obscure lanes; (4)
Perspective effect elimination, providing accurate 3D lanes for downstream
applications. From these perspectives, this paper presents a comprehensive
overview of existing methods, encompassing both the increasingly mature 2D lane
detection approaches and the developing 3D lane detection works. Besides, this
paper compares the performance of mainstream methods on different benchmarks
and investigates their inference speed under a unified setting for fair
comparison. Moreover, we present some extended works on lane detection,
including multi-task perception, video lane detection, online high-definition
map construction, and lane topology reasoning, to offer readers a comprehensive
roadmap for the evolution of lane detection. Finally, we point out some
potential future research directions in this field. We exhaustively collect the
papers and codes of existing works at
https://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatRex: Taming Multimodal LLM for Joint Perception and Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18363v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18363v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Jiang, Gen Luo, Yuqin Yang, Yuda Xiong, Yihao Chen, Zhaoyang Zeng, Tianhe Ren, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perception and understanding are two pillars of computer vision. While
multimodal large language models (MLLM) have demonstrated remarkable visual
understanding capabilities, they arguably lack accurate perception abilities,
e.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on
the COCO dataset, limiting many tasks requiring the combination of perception
and understanding. In this work, we aim to bridge this perception gap from both
model designing and data development perspectives. We first introduce ChatRex,
an MLLM with a decoupled perception design. Instead of having the LLM directly
predict box coordinates, we feed the output boxes from a universal proposal
network into the LLM, allowing it to output the corresponding box indices to
represent its detection results, turning the regression task into a
retrieval-based task that LLM handles more proficiently. From the data
perspective, we build a fully automated data engine and construct the
Rexverse-2M dataset which possesses multiple granularities to support the joint
training of perception and understanding. After standard two-stage training,
ChatRex demonstrates strong perception capabilities while preserving multimodal
understanding performance. The combination of these two capabilities
simultaneously unlocks many attractive applications, demonstrating the
complementary roles of both perception and understanding in MLLM. Code is
available at \url{https://github.com/IDEA-Research/ChatRex}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T2Vid: Translating Long Text into Multi-Image is the Catalyst for
  Video-LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan Yang, Zuwei Long, Yuhan Dai, Tong Xu, Xing Sun, Ran He, Caifeng Shan, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Multimodal Large Language Models (MLLMs) in the image domain
has garnered wide attention from the research community. Drawing on previous
successful experiences, researchers have recently explored extending the
success to the video understanding realms. Apart from training from scratch, an
efficient way is to utilize the pre-trained image-LLMs, leading to two
mainstream approaches, i.e. zero-shot inference and further fine-tuning with
video data. In this work, our study of these approaches harvests an effective
data augmentation method. We first make a deeper inspection of the zero-shot
inference way and identify two limitations, i.e. limited generalization and
lack of temporal understanding capabilities. Thus, we further investigate the
fine-tuning approach and find a low learning efficiency when simply using all
the video data samples, which can be attributed to a lack of instruction
diversity. Aiming at this issue, we develop a method called T2Vid to synthesize
video-like samples to enrich the instruction diversity in the training corpus.
Integrating these data enables a simple and efficient training scheme, which
achieves performance comparable to or even superior to using full video
datasets by training with just 15% the sample size. Meanwhile, we find that the
proposed scheme can boost the performance of long video understanding without
training with long video samples. We hope our study will spark more thinking
about using MLLMs for video understanding and curation of high-quality data.
The code is released at https://github.com/xjtupanda/T2Vid.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/xjtupanda/T2Vid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A design of Convolutional Neural Network model for the Diagnosis of the
  COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06394v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06394v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the spread of COVID-19 around the globe over the past year, the usage of
artificial intelligence (AI) algorithms and image processing methods to analyze
the X-ray images of patients' chest with COVID-19 has become essential. The
COVID-19 virus recognition in the lung area of a patient is one of the basic
and essential needs of clicical centers and hospitals. Most research in this
field has been devoted to papers on the basis of deep learning methods
utilizing CNNs (Convolutional Neural Network), which mainly deal with the
screening of sick and healthy people.In this study, a new structure of a
19-layer CNN has been recommended for accurately recognition of the COVID-19
from the X-ray pictures of chest. The offered CNN is developed to serve as a
precise diagnosis system for a three class (viral pneumonia, Normal, COVID) and
a four classclassification (Lung opacity, Normal, COVID-19, and pneumonia). A
comparison is conducted among the outcomes of the offered procedure and some
popular pretrained networks, including Inception, Alexnet, ResNet50,
Squeezenet, and VGG19 and based on Specificity, Accuracy, Precision,
Sensitivity, Confusion Matrix, and F1-score. The experimental results of the
offered CNN method specify its dominance over the existing published
procedures. This method can be a useful tool for clinicians in deciding
properly about COVID-19.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Important mistakes. Also, another author has contributed some to the
  revised version. So it is not appropriate for it to be with only my name</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SL-YOLO: A Stronger and Lighter Drone Target Detection Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Defan Chen, Luchan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting small objects in complex scenes, such as those captured by drones,
is a daunting challenge due to the difficulty in capturing the complex features
of small targets. While the YOLO family has achieved great success in large
target detection, its performance is less than satisfactory when faced with
small targets. Because of this, this paper proposes a revolutionary model
SL-YOLO (Stronger and Lighter YOLO) that aims to break the bottleneck of small
target detection. We propose the Hierarchical Extended Path Aggregation Network
(HEPAN), a pioneering cross-scale feature fusion method that can ensure
unparalleled detection accuracy even in the most challenging environments. At
the same time, without sacrificing detection capabilities, we design the C2fDCB
lightweight module and add the SCDown downsampling module to greatly reduce the
model's parameters and computational complexity. Our experimental results on
the VisDrone2019 dataset reveal a significant improvement in performance, with
mAP@0.5 jumping from 43.0% to 46.9% and mAP@0.5:0.95 increasing from 26.0% to
28.9%. At the same time, the model parameters are reduced from 11.1M to 9.6M,
and the FPS can reach 132, making it an ideal solution for real-time small
object detection in resource-constrained environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We are withdrawing this submission to incorporate substantial updates
  and improvements to the manuscript, including additional data and analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GausSurf: Geometry-Guided 3D Gaussian Splatting for Surface
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19454v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19454v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiepeng Wang, Yuan Liu, Peng Wang, Cheng Lin, Junhui Hou, Xin Li, Taku Komura, Wenping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting has achieved impressive performance in novel view
synthesis with real-time rendering capabilities. However, reconstructing
high-quality surfaces with fine details using 3D Gaussians remains a
challenging task. In this work, we introduce GausSurf, a novel approach to
high-quality surface reconstruction by employing geometry guidance from
multi-view consistency in texture-rich areas and normal priors in texture-less
areas of a scene. We observe that a scene can be mainly divided into two
primary regions: 1) texture-rich and 2) texture-less areas. To enforce
multi-view consistency at texture-rich areas, we enhance the reconstruction
quality by incorporating a traditional patch-match based Multi-View Stereo
(MVS) approach to guide the geometry optimization in an iterative scheme. This
scheme allows for mutual reinforcement between the optimization of Gaussians
and patch-match refinement, which significantly improves the reconstruction
results and accelerates the training process. Meanwhile, for the texture-less
areas, we leverage normal priors from a pre-trained normal estimation model to
guide optimization. Extensive experiments on the DTU and Tanks and Temples
datasets demonstrate that our method surpasses state-of-the-art methods in
terms of reconstruction quality and computation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jiepengwang.github.io/GausSurf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neighboring Slice Noise2Noise: <span class="highlight-title">Self-Supervised</span> Medical Image Denoising
  from Single Noisy Image Volume 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Langrui Zhou, Ziteng Zhou, Xinyu Huang, Xiangyu Zhang, Huiru Wang, Guang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last few years, with the rapid development of deep learning
technologies, supervised methods based on convolutional neural networks have
greatly enhanced the performance of medical image denoising. However, these
methods require large quantities of noisy-clean image pairs for training, which
greatly limits their practicality. Although some researchers have attempted to
train denoising networks using only single noisy images, existing
self-supervised methods, including blind-spot-based and data-splitting-based
methods, heavily rely on the assumption that noise is pixel-wise independent.
However, this assumption often does not hold in real-world medical images.
Therefore, in the field of medical imaging, there remains a lack of simple and
practical denoising methods that can achieve high-quality denoising performance
using only single noisy images. In this paper, we propose a novel
self-supervised medical image denoising method, Neighboring Slice Noise2Noise
(NS-N2N). The proposed method utilizes neighboring slices within a single noisy
image volume to construct weighted training data, and then trains the denoising
network using a self-supervised scheme with regional consistency loss and
inter-slice continuity loss. NS-N2N only requires a single noisy image volume
obtained from one medical imaging procedure to achieve high-quality denoising
of the image volume itself. Extensive experiments demonstrate that the proposed
method outperforms state-of-the-art self-supervised denoising methods in both
denoising performance and processing efficiency. Furthermore, since NS-N2N
operates solely in the image domain, it is free from device-specific issues
such as reconstruction geometry, making it easier to apply in various clinical
practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Feedback Network for Perceptual Point Cloud Quality
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Zhang, Qi Yang, Ziyu Shan, Yiling Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the success of the deep learning-based technique
in research of no-reference point cloud quality assessment (NR-PCQA). For a
more accurate quality prediction, many previous studies have attempted to
capture global and local features in a bottom-up manner, but ignored the
interaction and promotion between them. To solve this problem, we propose a
novel asynchronous feedback quality prediction network (AFQ-Net). Motivated by
human visual perception mechanisms, AFQ-Net employs a dual-branch structure to
deal with global and local features, simulating the left and right hemispheres
of the human brain, and constructs a feedback module between them.
Specifically, the input point clouds are first fed into a transformer-based
global encoder to generate the attention maps that highlight these semantically
rich regions, followed by being merged into the global feature. Then, we
utilize the generated attention maps to perform dynamic convolution for
different semantic regions and obtain the local feature. Finally, a
coarse-to-fine strategy is adopted to merge the two features into the final
quality score. We conduct comprehensive experiments on three datasets and
achieve superior performance over the state-of-the-art approaches on all of
these datasets. The code will be available at The code will be available at
https://github.com/zhangyujie-1998/AFQ-Net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-aware Distributed Source Coding under Dynamic Bandwidth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15523v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15523v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-han Li, Sravan Kumar Ankireddy, Ruihan Zhao, Hossein Nourkhiz Mahjoub, Ehsan Moradi-Pari, Ufuk Topcu, Sandeep Chinchali, Hyeji Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient compression of correlated data is essential to minimize
communication overload in multi-sensor networks. In such networks, each sensor
independently compresses the data and transmits them to a central node due to
limited communication bandwidth. A decoder at the central node decompresses and
passes the data to a pre-trained machine learning-based task to generate the
final output. Thus, it is important to compress the features that are relevant
to the task. Additionally, the final performance depends heavily on the total
available bandwidth. In practice, it is common to encounter varying
availability in bandwidth, and higher bandwidth results in better performance
of the task. We design a novel distributed compression framework composed of
independent encoders and a joint decoder, which we call neural distributed
principal component analysis (NDPCA). NDPCA flexibly compresses data from
multiple sources to any available bandwidth with a single model, reducing
computing and storage overhead. NDPCA achieves this by learning low-rank task
representations and efficiently distributing bandwidth among sensors, thus
providing a graceful trade-off between performance and bandwidth. Experiments
show that NDPCA improves the success rate of multi-view robotic arm
manipulation by 9% and the accuracy of object detection tasks on satellite
imagery by 14% compared to an autoencoder with uniform bandwidth allocation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have shown remarkable advancements in
multimodal reasoning tasks. However, they still often generate inaccurate or
irrelevant responses due to issues like hallucinated image understandings or
unrefined reasoning paths. To address these challenges, we introduce Critic-V,
a novel framework inspired by the Actor-Critic paradigm to boost the reasoning
capability of VLMs. This framework decouples the reasoning process and critic
process by integrating two independent components: the Reasoner, which
generates reasoning paths based on visual and textual inputs, and the Critic,
which provides constructive critique to refine these paths. In this approach,
the Reasoner generates reasoning responses according to text prompts, which can
evolve iteratively as a policy based on feedback from the Critic. This
interaction process was theoretically driven by a reinforcement learning
framework where the Critic offers natural language critiques instead of scalar
rewards, enabling more nuanced feedback to boost the Reasoner's capability on
complex reasoning tasks. The Critic model is trained using Direct Preference
Optimization (DPO), leveraging a preference dataset of critiques ranked by
Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results
show that the Critic-V framework significantly outperforms existing methods,
including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning
accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner
and constructive feedback from the preference-optimized Critic enables a more
reliable and context-sensitive multimodal reasoning process. Our approach
provides a promising solution to enhance the reliability of VLMs, improving
their performance in real-world reasoning-heavy multimodal applications such as
autonomous driving and embodied intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David B. Lindell, Sergey Tulyakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous works have recently integrated 3D camera control into foundational
text-to-video models, but the resulting camera control is often imprecise, and
video generation quality suffers. In this work, we analyze camera motion from a
first principles perspective, uncovering insights that enable precise 3D camera
manipulation without compromising synthesis quality. First, we determine that
motion induced by camera movements in videos is low-frequency in nature. This
motivates us to adjust train and test pose conditioning schedules, accelerating
training convergence while improving visual and motion quality. Then, by
probing the representations of an unconditional video diffusion transformer, we
observe that they implicitly perform camera pose estimation under the hood, and
only a sub-portion of their layers contain the camera information. This
suggested us to limit the injection of camera conditioning to a subset of the
architecture to prevent interference with other video features, leading to 4x
reduction of training parameters, improved training speed and 10% higher visual
quality. Finally, we complement the typical dataset for camera control learning
with a curated dataset of 20K diverse dynamic videos with stationary cameras.
This helps the model disambiguate the difference between camera and scene
motion, and improves the dynamics of generated pose-conditioned videos. We
compound these findings to design the Advanced 3D Camera Control (AC3D)
architecture, the new state-of-the-art model for generative video modeling with
camera control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://snap-research.github.io/ac3d/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two-Stage Approach for Brain MR Image Synthesis: 2D Image Synthesis and
  3D Refinement <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoon Cho, Seunghyuck Park, Jinah Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in automatic brain tumor segmentation
methods, their performance is not guaranteed when certain MR sequences are
missing. Addressing this issue, it is crucial to synthesize the missing MR
images that reflect the unique characteristics of the absent modality with
precise tumor representation. Typically, MRI synthesis methods generate partial
images rather than full-sized volumes due to computational constraints. This
limitation can lead to a lack of comprehensive 3D volumetric information and
result in image artifacts during the merging process. In this paper, we propose
a two-stage approach that first synthesizes MR images from 2D slices using a
novel intensity encoding method and then refines the synthesized MRI. The
proposed intensity encoding reduces artifacts when synthesizing MRI on a 2D
slice basis. Then, the \textit{Refiner}, which leverages complete 3D volume
information, further improves the quality of the synthesized images and
enhances their applicability to segmentation methods. Experimental results
demonstrate that the intensity encoding effectively minimizes artifacts in the
synthesized MRI and improves perceptual quality. Furthermore, using the
\textit{Refiner} on synthesized MRI significantly improves brain tumor
segmentation results, highlighting the potential of our approach in practical
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024 BraSyn Challenge 1st place</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imagine and Seek: Improving Composed Image Retrieval with an Imagined
  Proxy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You Li, Fan Ma, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Zero-shot Composed Image Retrieval (ZSCIR) requires retrieving images
that match the query image and the relative captions. Current methods focus on
projecting the query image into the text feature space, subsequently combining
them with features of query texts for retrieval. However, retrieving images
only with the text features cannot guarantee detailed alignment due to the
natural gap between images and text. In this paper, we introduce Imagined Proxy
for CIR (IP-CIR), a training-free method that creates a proxy image aligned
with the query image and text description, enhancing query representation in
the retrieval process. We first leverage the large language model's
generalization capability to generate an image layout, and then apply both the
query text and image for conditional generation. The robust query features are
enhanced by merging the proxy image, query image, and text semantic
perturbation. Our newly proposed balancing metric integrates text-based and
proxy retrieval similarities, allowing for more accurate retrieval of the
target image while incorporating image-side information into the process.
Experiments on three public datasets demonstrate that our method significantly
improves retrieval performances. We achieve state-of-the-art (SOTA) results on
the CIRR dataset with a Recall@K of 70.07 at K=10. Additionally, we achieved an
improvement in Recall@10 on the FashionIQ dataset, rising from 45.11 to 45.74,
and improved the baseline performance in CIRCO with a mAPK@10 score, increasing
from 32.24 to 34.26.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoisyNN: Exploring the Impact of Information Entropy Change in Learning
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10625v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10625v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowei Yu, Zhe Huang, Minheng Chen, Yao Xue, Tianming Liu, Dajiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the impact of entropy change in deep learning systems by noise
injection at different levels, including the embedding space and the image. The
series of models that employ our methodology are collectively known as Noisy
Neural Networks (NoisyNN), with examples such as NoisyViT and NoisyCNN. Noise
is conventionally viewed as a harmful perturbation in various deep learning
architectures, such as convolutional neural networks (CNNs) and vision
transformers (ViTs), as well as different learning tasks like image
classification and transfer learning. However, this work shows noise can be an
effective way to change the entropy of the learning system. We demonstrate that
specific noise can boost the performance of various deep models under certain
conditions. We theoretically prove the enhancement gained from positive noise
by reducing the task complexity defined by information entropy and
experimentally show the significant performance gain in large image datasets,
such as the ImageNet. Herein, we use the information entropy to define the
complexity of the task. We categorize the noise into two types, positive noise
(PN) and harmful noise (HN), based on whether the noise can help reduce the
task complexity. Extensive experiments of CNNs and ViTs have shown performance
improvements by proactively injecting positive noise, where we achieved an
unprecedented top 1 accuracy of 95$\%$ on ImageNet. Both theoretical analysis
and empirical evidence have confirmed that the presence of positive noise, can
benefit the learning process, while the traditionally perceived harmful noise
indeed impairs deep learning models. The different roles of noise offer new
explanations for deep models on specific tasks and provide a new paradigm for
improving model performance. Moreover, it reminds us that we can influence the
performance of learning systems via information entropy change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Task Entropy, NoisyViT, NoisyCNN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Texture, Shape and Order Matter: A New <span class="highlight-title">Transformer</span> Design for Sequential
  DeepFake Detection <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13873v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13873v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfei Li, Yuezun Li, Xin Wang, Baoyuan Wu, Jiaran Zhou, Junyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential DeepFake detection is an emerging task that predicts the
manipulation sequence in order. Existing methods typically formulate it as an
image-to-sequence problem, employing conventional Transformer architectures.
However, these methods lack dedicated design and consequently result in limited
performance. As such, this paper describes a new Transformer design, called
TSOM, by exploring three perspectives: Texture, Shape, and Order of
Manipulations. Our method features four major improvements: \ding{182} we
describe a new texture-aware branch that effectively captures subtle
manipulation traces with a Diversiform Pixel Difference Attention module.
\ding{183} Then we introduce a Multi-source Cross-attention module to seek deep
correlations among spatial and sequential features, enabling effective modeling
of complex manipulation traces. \ding{184} To further enhance the
cross-attention, we describe a Shape-guided Gaussian mapping strategy,
providing initial priors of the manipulation shape. \ding{185} Finally,
observing that the subsequent manipulation in a sequence may influence traces
left in the preceding one, we intriguingly invert the prediction order from
forward to backward, leading to notable gains as expected. Extensive
experimental results demonstrate that our method outperforms others by a large
margin, highlighting the superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnySynth: Harnessing the Power of Image Synthetic Data Generation for
  Generalized Vision-Language Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You Li, Fan Ma, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently been employed to generate high-quality images,
reducing the need for manual data collection and improving model generalization
in tasks such as object detection, instance segmentation, and image perception.
However, the synthetic framework is usually designed with meticulous human
effort for each task due to various requirements on image layout, content, and
annotation formats, restricting the application of synthetic data on more
general scenarios. In this paper, we propose AnySynth, a unified framework
integrating adaptable, comprehensive, and highly controllable components
capable of generating an arbitrary type of synthetic data given diverse
requirements. Specifically, the Task-Specific Layout Generation Module is first
introduced to produce reasonable layouts for different tasks by leveraging the
generation ability of large language models and layout priors of real-world
images. A Uni-Controlled Image Generation Module is then developed to create
high-quality synthetic images that are controllable and based on the generated
layouts. In addition, user specific reference images, and style images can be
incorporated into the generation to task requirements. Finally, the
Task-Oriented Annotation Module offers precise and detailed annotations for the
generated images across different tasks. We have validated our framework's
performance across various tasks, including Few-shot Object Detection,
Cross-domain Object Detection, Zero-shot Composed Image Retrieval, and
Multi-modal Image Perception and Grounding. The specific data synthesized by
our framework significantly improves model performance in these tasks,
demonstrating the generality and effectiveness of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Step-by-Step Instructional Diagrams to Video Demonstrations <span class="chip">CVPR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13800v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13800v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Zhang, Anoop Cherian, Yanbin Liu, Yizhak Ben-Shabat, Cristian Rodriguez, Stephen Gould
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal alignment facilitates the retrieval of instances from one modality
when queried using another. In this paper, we consider a novel setting where
such an alignment is between (i) instruction steps that are depicted as
assembly diagrams (commonly seen in Ikea assembly manuals) and (ii) video
segments from in-the-wild videos; these videos comprising an enactment of the
assembly actions in the real world. To learn this alignment, we introduce a
novel supervised contrastive learning method that learns to align videos with
the subtle details in the assembly diagrams, guided by a set of novel losses.
To study this problem and demonstrate the effectiveness of our method, we
introduce a novel dataset: IAW for Ikea assembly in the wild consisting of 183
hours of videos from diverse furniture assembly collections and nearly 8,300
illustrations from their associated instruction manuals and annotated for their
ground truth alignments. We define two tasks on this dataset: First, nearest
neighbor retrieval between video segments and illustrations, and, second,
alignment of instruction steps and the segments for each video. Extensive
experiments on IAW demonstrate superior performances of our approach against
alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GameGen-X: Interactive Open-world Game Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GameGen-X, the first diffusion transformer model specifically
designed for both generating and interactively controlling open-world game
videos. This model facilitates high-quality, open-domain generation by
simulating an extensive array of game engine features, such as innovative
characters, dynamic environments, complex actions, and diverse events.
Additionally, it provides interactive controllability, predicting and altering
future content based on the current clip, thus allowing for gameplay
simulation. To realize this vision, we first collected and built an Open-World
Video Game Dataset from scratch. It is the first and largest dataset for
open-world game video generation and control, which comprises over a million
diverse gameplay video clips sampling from over 150 games with informative
captions from GPT-4o. GameGen-X undergoes a two-stage training process,
consisting of foundation model pre-training and instruction tuning. Firstly,
the model was pre-trained via text-to-video generation and video continuation,
endowing it with the capability for long-sequence, high-quality open-domain
game video generation. Further, to achieve interactive controllability, we
designed InstructNet to incorporate game-related multi-modal control signal
experts. This allows the model to adjust latent representations based on user
inputs, unifying character interaction and scene content control for the first
time in video generation. During instruction tuning, only the InstructNet is
updated while the pre-trained foundation model is frozen, enabling the
integration of interactive controllability without loss of diversity and
quality of generated video content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://gamegen-x.github.io/ Github:
  https://github.com/GameGen-X/GameGen-X</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporally Grounding Instructional Diagrams in Unconstrained Videos <span class="chip">WACV'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12066v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12066v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Zhang, Frederic Z. Zhang, Cristian Rodriguez, Yizhak Ben-Shabat, Anoop Cherian, Stephen Gould
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the challenging problem of simultaneously localizing a sequence of
queries in the form of instructional diagrams in a video. This requires
understanding not only the individual queries but also their
interrelationships. However, most existing methods focus on grounding one query
at a time, ignoring the inherent structures among queries such as the general
mutual exclusiveness and the temporal order. Consequently, the predicted
timespans of different step diagrams may overlap considerably or violate the
temporal order, thus harming the accuracy. In this paper, we tackle this issue
by simultaneously grounding a sequence of step diagrams. Specifically, we
propose composite queries, constructed by exhaustively pairing up the visual
content features of the step diagrams and a fixed number of learnable
positional embeddings. Our insight is that self-attention among composite
queries carrying different content features suppress each other to reduce
timespan overlaps in predictions, while the cross-attention corrects the
temporal misalignment via content and position joint guidance. We demonstrate
the effectiveness of our approach on the IAW dataset for grounding step
diagrams and the YouCook2 benchmark for grounding natural language queries,
significantly outperforming existing methods while simultaneously grounding
multiple queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow
  Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungbin Cho, Junwan Kim, Jisoo Kim, Minseo Kim, Mingu Kang, Sungeun Hong, Tae-Hyun Oh, Youngjae Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion, inherently continuous and dynamic, presents significant
challenges for generative models. Despite their dominance, discrete
quantization methods, such as VQ-VAEs, suffer from inherent limitations,
including restricted expressiveness and frame-wise noise artifacts. Continuous
approaches, while producing smoother and more natural motions, often falter due
to high-dimensional complexity and limited training data. To resolve this
"discord" between discrete and continuous representations, we introduce
DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a
novel method that decodes discrete motion tokens into continuous motion through
rectified flow. By employing an iterative refinement process in the continuous
space, DisCoRD captures fine-grained dynamics and ensures smoother and more
natural motions. Compatible with any discrete-based framework, our method
enhances naturalness without compromising faithfulness to the conditioning
signals. Extensive evaluations demonstrate that DisCoRD achieves
state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on
KIT-ML. These results solidify DisCoRD as a robust solution for bridging the
divide between discrete efficiency and continuous realism. Our project page is
available at: https://whwjdqls.github.io/discord.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyperSeg: Towards Universal Visual Segmentation with Large Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Wei, Yujie Zhong, Haoxian Tan, Yong Liu, Zheng Zhao, Jie Hu, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to address universal segmentation for image and video
perception with the strong reasoning ability empowered by Visual Large Language
Models (VLLMs). Despite significant progress in current unified segmentation
methods, limitations in adaptation to both image and video scenarios, as well
as the complex reasoning segmentation, make it difficult for them to handle
various challenging instructions and achieve an accurate understanding of
fine-grained vision-language correlations. We propose HyperSeg, the first
VLLM-based universal segmentation model for pixel-level image and video
perception, encompassing generic segmentation tasks and more complex reasoning
perception tasks requiring powerful reasoning abilities and world knowledge.
Besides, to fully leverage the recognition capabilities of VLLMs and the
fine-grained visual information, HyperSeg incorporates hybrid entity
recognition and fine-grained visual perceiver modules for various segmentation
tasks. Combined with the temporal adapter, HyperSeg achieves a comprehensive
understanding of temporal information. Experimental results validate the
effectiveness of our insights in resolving universal image and video
segmentation tasks, including the more complex reasoning perception tasks. Our
code is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting and Enhancing the Fairness of DNNs with the Curvature of
  Perceptual Manifolds <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12307v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12307v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbiao Ma, Licheng Jiao, Fang Liu, Maoji Wen, Lingling Li, Wenping Ma, Shuyuan Yang, Xu Liu, Puhua Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the challenges of long-tailed classification, researchers have
proposed several approaches to reduce model bias, most of which assume that
classes with few samples are weak classes. However, recent studies have shown
that tail classes are not always hard to learn, and model bias has been
observed on sample-balanced datasets, suggesting the existence of other factors
that affect model bias. In this work, we first establish a geometric
perspective for analyzing model fairness and then systematically propose a
series of geometric measurements for perceptual manifolds in deep neural
networks. Subsequently, we comprehensively explore the effect of the geometric
characteristics of perceptual manifolds on classification difficulty and how
learning shapes the geometric characteristics of perceptual manifolds. An
unanticipated finding is that the correlation between the class accuracy and
the separation degree of perceptual manifolds gradually decreases during
training, while the negative correlation with the curvature gradually
increases, implying that curvature imbalance leads to model bias.Building upon
these observations, we propose curvature regularization to facilitate the model
to learn curvature-balanced and flatter perceptual manifolds. Evaluations on
multiple long-tailed and non-long-tailed datasets show the excellent
performance and exciting generality of our approach, especially in achieving
significant performance improvements based on current state-of-the-art
techniques. Our work opens up a geometric analysis perspective on model bias
and reminds researchers to pay attention to model bias on non-long-tailed and
even sample-balanced datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17pages, Accepted by CVPR 2023, Submitted to TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeanGaussian: Breaking Pixel or Point Cloud Correspondence in Modeling
  3D Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiamin Wu, Kenkun Liu, Han Gao, Xiaoke Jiang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rencently, Gaussian splatting has demonstrated significant success in novel
view synthesis. Current methods often regress Gaussians with pixel or point
cloud correspondence, linking each Gaussian with a pixel or a 3D point. This
leads to the redundancy of Gaussians being used to overfit the correspondence
rather than the objects represented by the 3D Gaussians themselves,
consequently wasting resources and lacking accurate geometries or textures. In
this paper, we introduce LeanGaussian, a novel approach that treats each query
in deformable Transformer as one 3D Gaussian ellipsoid, breaking the pixel or
point cloud correspondence constraints. We leverage deformable decoder to
iteratively refine the Gaussians layer-by-layer with the image features as keys
and values. Notably, the center of each 3D Gaussian is defined as 3D reference
points, which are then projected onto the image for deformable attention in 2D
space. On both the ShapeNet SRN dataset (category level) and the Google Scanned
Objects dataset (open-category level, trained with the Objaverse dataset), our
approach, outperforms prior methods by approximately 6.1\%, achieving a PSNR of
25.44 and 22.36, respectively. Additionally, our method achieves a 3D
reconstruction speed of 7.2 FPS and rendering speed 500 FPS. The code will be
released at https://github.com/jwubz123/DIG3D.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Complementary Knowledge Distillation for Efficient Dense Image
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Zhang, Pingcheng Dong, Xinting Hu, Long Chen, Kwang-Ting Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been revealed that small efficient dense image prediction (EDIP)
models, trained using the knowledge distillation (KD) framework, encounter two
key challenges, including maintaining boundary region completeness and
preserving target region connectivity, despite their favorable capacity to
recognize main object regions. In this work, we propose a complementary
boundary and context distillation (BCD) method within the KD framework for
EDIPs, which facilitates the targeted knowledge transfer from large accurate
teacher models to compact efficient student models. Specifically, the boundary
distillation component focuses on extracting explicit object-level semantic
boundaries from the hierarchical feature maps of the backbone network to
enhance the student model's mask quality in boundary regions. Concurrently, the
context distillation component leverages self-relations as a bridge to transfer
implicit pixel-level contexts from the teacher model to the student model,
ensuring strong connectivity in target regions. Our proposed BCD method is
specifically designed for EDIP tasks and is characterized by its simplicity and
efficiency. Extensive experimental results across semantic segmentation, object
detection, and instance segmentation on various representative datasets
demonstrate that our method can outperform existing methods without requiring
extra supervisions or incurring increased inference costs, resulting in
well-defined object boundaries and smooth connecting regions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EchoSight: Advancing Visual-Language Models with Wiki Knowledge <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12735v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12735v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Yan, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-based Visual Question Answering (KVQA) tasks require answering
questions about images using extensive background knowledge. Despite
significant advancements, generative models often struggle with these tasks due
to the limited integration of external knowledge. In this paper, we introduce
EchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework
that enables large language models (LLMs) to answer visual questions requiring
fine-grained encyclopedic knowledge. To strive for high-performing retrieval,
EchoSight first searches wiki articles by using visual-only information,
subsequently, these candidate articles are further reranked according to their
relevance to the combined text-image query. This approach significantly
improves the integration of multimodal knowledge, leading to enhanced retrieval
outcomes and more accurate VQA responses. Our experimental results on the
Encyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes
new state-of-the-art results in knowledge-based VQA, achieving an accuracy of
41.8% on Encyclopedic VQA and 31.3% on InfoSeek.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 findings; Project Page:
  https://go2heart.github.io/echosight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GlocalCLIP: Object-agnostic Global-Local <span class="highlight-title">Prompt</span> Learning for Zero-shot
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyul Ham, Yonggon Jung, Jun-Geol Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot anomaly detection (ZSAD) is crucial for detecting anomalous
patterns in target datasets without using training samples, specifically in
scenarios where there are distributional differences between the target domain
and training data or where data scarcity arises because of restricted access.
Although recently pretrained vision-language models demonstrate strong
zero-shot performance across various visual tasks, they focus on learning class
semantics, which makes their direct application to ZSAD challenging. To address
this scenario, we propose GlocalCLIP, which uniquely separates global and local
prompts and jointly optimizes them. This approach enables the object-agnostic
glocal semantic prompt to effectively capture general normal and anomalous
patterns without dependency on specific objects in the image. We refine the
text prompts for more precise adjustments by utilizing deep-text prompt tuning
in the text encoder. In the vision encoder, we apply V-V attention layers to
capture detailed local image features. Finally, we introduce glocal contrastive
learning to improve the complementary learning of global and local prompts,
effectively detecting anomalous patterns across various domains. The
generalization performance of GlocalCLIP in ZSAD was demonstrated on 15
real-world datasets from both the industrial and medical domains, achieving
superior performance compared to existing methods. Code will be made available
at https://github.com/YUL-git/GlocalCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 36 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Voxel-based X-ray Rendering Improves Sparse-View 3D CBCT
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadhossein Momeni, Vivek Gopalakrishnan, Neel Dey, Polina Golland, Sarah Frisken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DiffVox, a self-supervised framework for Cone-Beam Computed
Tomography (CBCT) reconstruction by directly optimizing a voxelgrid
representation using physics-based differentiable X-ray rendering. Further, we
investigate how the different implementations of the X-ray image formation
model in the renderer affect the quality of 3D reconstruction and novel view
synthesis. When combined with our regularized voxel-based learning framework,
we find that using an exact implementation of the discrete Beer-Lambert law for
X-ray attenuation in the renderer outperforms both widely used iterative CBCT
reconstruction algorithms and modern neural field approaches, particularly when
given only a few input views. As a result, we reconstruct high-fidelity 3D CBCT
volumes from fewer X-rays, potentially reducing ionizing radiation exposure and
improving diagnostic utility. Our implementation is available at
https://github.com/hossein-momeni/DiffVox.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Det-SAM2:Technical Report on the Self-<span class="highlight-title">Prompt</span>ing Segmentation Framework
  Based on Segment Anything Model 2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiting Wang, Qiangong Zhou, Zongyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment Anything Model 2 (SAM2) demonstrates exceptional performance in video
segmentation and refinement of segmentation results. We anticipate that it can
further evolve to achieve higher levels of automation for practical
applications. Building upon SAM2, we conducted a series of practices that
ultimately led to the development of a fully automated pipeline, termed
Det-SAM2, in which object prompts are automatically generated by a detection
model to facilitate inference and refinement by SAM2. This pipeline enables
inference on infinitely long video streams with constant VRAM and RAM usage,
all while preserving the same efficiency and accuracy as the original SAM2.
  This technical report focuses on the construction of the overall Det-SAM2
framework and the subsequent engineering optimization applied to SAM2. We
present a case demonstrating an application built on the Det-SAM2 framework: AI
refereeing in a billiards scenario, derived from our business context. The
project at \url{https://github.com/motern88/Det-SAM2}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HGCLIP: Exploring Vision-Language Models with Graph Representations for
  Hierarchical Understanding <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14064v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14064v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xia, Xingtong Yu, Ming Hu, Lie Ju, Zhiyong Wang, Peibo Duan, Zongyuan Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object categories are typically organized into a multi-granularity taxonomic
hierarchy. When classifying categories at different hierarchy levels,
traditional uni-modal approaches focus primarily on image features, revealing
limitations in complex scenarios. Recent studies integrating Vision-Language
Models (VLMs) with class hierarchies have shown promise, yet they fall short of
fully exploiting the hierarchical relationships. These efforts are constrained
by their inability to perform effectively across varied granularity of
categories. To tackle this issue, we propose a novel framework (HGCLIP) that
effectively combines CLIP with a deeper exploitation of the Hierarchical class
structure via Graph representation learning. We explore constructing the class
hierarchy into a graph, with its nodes representing the textual or image
features of each category. After passing through a graph encoder, the textual
features incorporate hierarchical structure information, while the image
features emphasize class-aware features derived from prototypes through the
attention mechanism. Our approach demonstrates significant improvements on 11
diverse visual recognition benchmarks. Our codes are fully available at
https://github.com/richard-peng-xia/HGCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting and Improving Attention From the Perspective of Large
  Kernel Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05738v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05738v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghao Li, Chaoning Zhang, Boheng Zeng, Yi Lu, Pengbo Shi, Qingzi Chen, Jirui Liu, Lingyun Zhu, Yang Yang, Heng Tao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention mechanisms have significantly advanced visual models by capturing
global context effectively. However, their reliance on large-scale datasets and
substantial computational resources poses challenges in data-scarce and
resource-constrained scenarios. Moreover, traditional self-attention mechanisms
lack inherent spatial inductive biases, making them suboptimal for modeling
local features critical to tasks involving smaller datasets. In this work, we
introduce Large Kernel Convolutional Attention (LKCA), a novel formulation that
reinterprets attention operations as a single large-kernel convolution. This
design unifies the strengths of convolutional architectures locality and
translation invariance with the global context modeling capabilities of
self-attention. By embedding these properties into a computationally efficient
framework, LKCA addresses key limitations of traditional attention mechanisms.
The proposed LKCA achieves competitive performance across various visual tasks,
particularly in data-constrained settings. Experimental results on CIFAR-10,
CIFAR-100, SVHN, and Tiny-ImageNet demonstrate its ability to excel in image
classification, outperforming conventional attention mechanisms and vision
transformers in compact model settings. These findings highlight the
effectiveness of LKCA in bridging local and global feature modeling, offering a
practical and robust solution for real-world applications with limited data and
resources.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RIRAG: Regulatory Information Retrieval and Answer Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuba Gokhan, Kexin Wang, Iryna Gurevych, Ted Briscoe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regulatory documents, issued by governmental regulatory bodies, establish
rules, guidelines, and standards that organizations must adhere to for legal
compliance. These documents, characterized by their length, complexity and
frequent updates, are challenging to interpret, requiring significant
allocation of time and expertise on the part of organizations to ensure ongoing
compliance. Regulatory Natural Language Processing (RegNLP) is a
multidisciplinary field aimed at simplifying access to and interpretation of
regulatory rules and obligations. We introduce a task of generating
question-passages pairs, where questions are automatically created and paired
with relevant regulatory passages, facilitating the development of regulatory
question-answering systems. We create the ObliQA dataset, containing 27,869
questions derived from the collection of Abu Dhabi Global Markets (ADGM)
financial regulation documents, design a baseline Regulatory Information
Retrieval and Answer Generation (RIRAG) system and evaluate it with RePASs, a
novel evaluation metric that tests whether generated answers accurately capture
all relevant obligations while avoiding contradictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Multimodal Retrieval via Document Screenshot Embedding <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the real world, documents are organized in different formats and varied
modalities. Traditional retrieval pipelines require tailored document parsing
techniques and content extraction modules to prepare input for indexing. This
process is tedious, prone to errors, and has information loss. To this end, we
propose Document Screenshot Embedding (DSE), a novel retrieval paradigm that
regards document screenshots as a unified input format, which does not require
any content extraction preprocess and preserves all the information in a
document (e.g., text, image and layout). DSE leverages a large vision-language
model to directly encode document screenshots into dense representations for
retrieval. To evaluate our method, we first craft the dataset of Wiki-SS, a
1.3M Wikipedia web page screenshots as the corpus to answer the questions from
the Natural Questions dataset. In such a text-intensive document retrieval
setting, DSE shows competitive effectiveness compared to other text retrieval
methods relying on parsing. For example, DSE outperforms BM25 by 17 points in
top-1 retrieval accuracy. Additionally, in a mixed-modality task of slide
retrieval, DSE significantly outperforms OCR text retrieval methods by over 15
points in nDCG@10. These experiments show that DSE is an effective document
retrieval paradigm for diverse types of documents. Model checkpoints, code, and
Wiki-SS collection will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2024 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling and Mitigating Bias in Large Language Model Recommendations: A
  Path to Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10825v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10825v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindya Bijoy Das, Shahnewaz Karim Sakib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  excel in delivering comprehensive suggestions by deeply analyzing content and
user behavior. However, they often inherit biases from skewed training data,
favoring mainstream content while underrepresenting diverse or non-traditional
options. This study explores the interplay between bias and LLM-based
recommendation systems, focusing on music, song, and book recommendations
across diverse demographic and cultural groups. This paper analyzes bias in
LLM-based recommendation systems across multiple models (GPT, LLaMA, and
Gemini), revealing its deep and pervasive impact on outcomes. Intersecting
identities and contextual factors, like socioeconomic status, further amplify
biases, complicating fair recommendations across diverse groups. Our findings
reveal that bias in these systems is deeply ingrained, yet even simple
interventions like prompt engineering can significantly reduce it. We further
propose a retrieval-augmented generation strategy to mitigate bias more
effectively. Numerical experiments validate these strategies, demonstrating
both the pervasive nature of bias and the impact of the proposed solutions.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">97</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compute-Constrained Data Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16208v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16208v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Oscar Yin, Alexander M. Rush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data selection can reduce the amount of training data needed to finetune
LLMs; however, the efficacy of data selection scales directly with its compute.
Motivated by the practical challenge of compute-constrained finetuning, we
consider the setting in which both the cost of selecting data and training are
budgeted for. We first formalize the problem of data selection with a
cost-aware utility function, and model the data selection problem as trading
off initial-selection cost for training gain. We run a comprehensive sweep of
experiments across multiple tasks, varying compute budget by scaling finetuning
tokens, model sizes, and data selection compute. Interestingly we find that
many powerful data selection methods are almost never compute-optimal, and that
cheaper data selection alternatives dominate both from a theoretical and
empirical perspective. For compute-optimal training, we find that perplexity
and gradient data selection require training-to-selection model size ratios of
5x and 10x, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Note on Doubly Robust Estimator in Regression Continuity Designs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07978v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07978v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This note introduces a doubly robust (DR) estimator for regression
discontinuity (RD) designs. RD designs provide a quasi-experimental framework
for estimating treatment effects, where treatment assignment depends on whether
a running variable surpasses a predefined cutoff. A common approach in RD
estimation is the use of nonparametric regression methods, such as local linear
regression. However, the validity of these methods still relies on the
consistency of the nonparametric estimators. In this study, we propose the
DR-RD estimator, which combines two distinct estimators for the conditional
expected outcomes. The primary advantage of the DR-RD estimator lies in its
ability to ensure the consistency of the treatment effect estimation as long as
at least one of the two estimators is consistent. Consequently, our DR-RD
estimator enhances robustness of treatment effect estimators in RD designs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There is a critical error in the previous submission. We have revised
  the original claim and present a weakened result</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect
  Verifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Stroebl, Sayash Kapoor, Arvind Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has generated hope that inference scaling could allow weaker
language models to match or exceed the accuracy of stronger models, such as by
repeatedly sampling solutions to a coding problem until it passes unit tests.
The central thesis of this paper is that there is no free lunch for inference
scaling: indefinite accuracy improvement through resampling can only be
realized if the "verifier" (in this case, a set of unit tests) is perfect. When
the verifier is imperfect, as it almost always is in domains such as reasoning
or coding (for example, unit tests have imperfect coverage), there is a nonzero
probability of false positives: incorrect solutions that pass the verifier.
Resampling cannot decrease this probability, so it imposes an upper bound to
the accuracy of resampling-based inference scaling even with an infinite
compute budget. We find that there is a very strong correlation between the
model's single-sample accuracy (i.e. accuracy without unit tests) and its false
positive rate on coding benchmarks HumanEval and MBPP, whose unit tests have
limited coverage. Therefore, no amount of inference scaling of weaker models
can enable them to match the single-sample accuracy of a sufficiently strong
model (Fig. 1a). When we consider that false positives have a negative utility
compared to abstaining from producing a solution, it bends the inference
scaling curve further downward. Empirically, we find that the optimal number of
samples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we
show that beyond accuracy, false positives may have other undesirable
qualities, such as poor adherence to coding style conventions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topology-Based Reconstruction Prevention for Decentralised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05248v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05248v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florine W. Dekker, Zekeriya Erkin, Mauro Conti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralised learning has recently gained traction as an alternative to
federated learning in which both data and coordination are distributed. To
preserve the confidentiality of users' data, decentralised learning relies on
differential privacy, multi-party computation, or both. However, running
multiple privacy-preserving summations in sequence may allow adversaries to
perform reconstruction attacks. Current reconstruction countermeasures either
cannot trivially be adapted to the distributed setting, or add excessive
amounts of noise.
  In this work, we first show that passive honest-but-curious adversaries can
infer other users' private data after several privacy-preserving summations.
For example, in subgraphs with 18 users, we show that only three passive
honest-but-curious adversaries succeed at reconstructing private data 11.0% of
the time, requiring an average of 8.8 summations per adversary. The success
rate depends only on the adversaries' direct neighbourhood, and is independent
of the size of the full network. We consider weak adversaries that do not
control the graph topology, cannot exploit the summation's inner workings, and
do not have auxiliary knowledge; and show that these adversaries can still
infer private data.
  We analyse how reconstruction relates to topology and propose the first
topology-based decentralised defence against reconstruction attacks. We show
that reconstruction requires a number of adversaries linear in the length of
the network's shortest cycle. Consequently, exact attacks over
privacy-preserving summations are impossible in acyclic networks.
  Our work is a stepping stone for a formal theory of topology-based
decentralised reconstruction defences. Such a theory would generalise our
countermeasure beyond summation, define confidentiality in terms of entropy,
and describe the interactions with (topology-aware) differential privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 19 figures, for associated experiment source code see
  doi:10.4121/21572601.v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Estimation of Learning Rates Using a Non-Linear Autoregressive
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramin Okhrati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new class of adaptive non-linear autoregressive (Nlar) models
incorporating the concept of momentum, which dynamically estimate both the
learning rates and momentum as the number of iterations increases. In our
method, the growth of the gradients is controlled using a scaling (clipping)
function, leading to stable convergence. Within this framework, we propose
three distinct estimators for learning rates and provide theoretical proof of
their convergence. We further demonstrate how these estimators underpin the
development of effective Nlar optimizers. The performance of the proposed
estimators and optimizers is rigorously evaluated through extensive experiments
across several datasets and a reinforcement learning environment. The results
highlight two key features of the Nlar optimizers: robust convergence despite
variations in underlying parameters, including large initial learning rates,
and strong adaptability with rapid convergence during the initial epochs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Typos corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CREW: Facilitating Human-AI Teaming Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyu Zhang, Zhengran Ji, Boyuan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing deployment of artificial intelligence (AI) technologies,
the potential of humans working with AI agents has been growing at a great
speed. Human-AI teaming is an important paradigm for studying various aspects
when humans and AI agents work together. The unique aspect of Human-AI teaming
research is the need to jointly study humans and AI agents, demanding
multidisciplinary research efforts from machine learning to human-computer
interaction, robotics, cognitive science, neuroscience, psychology, social
science, and complex systems. However, existing platforms for Human-AI teaming
research are limited, often supporting oversimplified scenarios and a single
task, or specifically focusing on either human-teaming research or multi-agent
AI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming
research in real-time decision-making scenarios and engage collaborations from
multiple scientific disciplines, with a strong emphasis on human involvement.
It includes pre-built tasks for cognitive studies and Human-AI teaming with
expandable potentials from our modular design. Following conventional cognitive
neuroscience research, CREW also supports multimodal human physiological signal
recording for behavior analysis. Moreover, CREW benchmarks real-time
human-guided reinforcement learning agents using state-of-the-art algorithms
and well-tuned baselines. With CREW, we were able to conduct 50 human subject
studies within a week to verify the effectiveness of our benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project website is at: http://generalroboticslab.com/CREW</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Tales of Single-Phase Contrastive Hebbian Learning <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08573v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08573v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasmus Kjær Høier, Christopher Zach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The search for ``biologically plausible'' learning algorithms has converged
on the idea of representing gradients as activity differences. However, most
approaches require a high degree of synchronization (distinct phases during
learning) and introduce substantial computational overhead, which raises doubts
regarding their biological plausibility as well as their potential utility for
neuromorphic computing. Furthermore, they commonly rely on applying
infinitesimal perturbations (nudges) to output units, which is impractical in
noisy environments. Recently it has been shown that by modelling artificial
neurons as dyads with two oppositely nudged compartments, it is possible for a
fully local learning algorithm named ``dual propagation'' to bridge the
performance gap to backpropagation, without requiring separate learning phases
or infinitesimal nudging. However, the algorithm has the drawback that its
numerical stability relies on symmetric nudging, which may be restrictive in
biological and analog implementations. In this work we first provide a solid
foundation for the objective underlying the dual propagation method, which also
reveals a surprising connection with adversarial robustness. Second, we
demonstrate how dual propagation is related to a particular adjoint state
method, which is stable regardless of asymmetric nudging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024; 21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inducing Group Fairness in <span class="highlight-title">Prompt</span>-Based Language Model Decisions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Atwood, Nino Scherrer, Preethi Lahoti, Ananth Balashankar, Flavien Prost, Ahmad Beirami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifiers are used throughout industry to enforce policies, ranging from
the detection of toxic content to age-appropriate content filtering. While
these classifiers serve important functions, it is also essential that they are
built in ways that minimize unfair biases for users.
  One such fairness consideration is called group fairness, which desires that
different sub-population of users receive equal treatment. This is a
well-studied problem in the context of 'classical' classifiers. However, the
emergence of prompt-based language model (LM) decision making has created new
opportunities to solve text-based classification tasks, and the fairness
properties of these new classifiers are not yet well understood. Further, the
`remediation toolkit' is incomplete for LM-based decision makers and little is
understood about how to improve decision maker group fairness while maintaining
classifier performance.
  This work sets out to add more tools to that toolbox. We introduce
adaptations of existing effective approaches from the classical classifier
fairness to the prompt-based classifier space. We also devise simple methods
that take advantage of the new structure of prompt-based decision makers and
operate at the prompt level. We compare these approaches empirically on real
data. Our results suggest that adaptations of approaches that are effective for
classical classifiers remain effective in the LM-based classifier environment.
However, there is room for further exploration of prompt-based remediation
methods (and other remediation methods that take advantage of LM structure).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regression Trees Know Calculus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Wycoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regression trees have emerged as a preeminent tool for solving real-world
regression problems due to their ability to deal with nonlinearities,
interaction effects and sharp discontinuities. In this article, we rather study
regression trees applied to well-behaved, differentiable functions, and
determine the relationship between node parameters and the local gradient of
the function being approximated. We find a simple estimate of the gradient
which can be efficiently computed using quantities exposed by popular tree
learning libraries. This allows the tools developed in the context of
differentiable algorithms, like neural nets and Gaussian processes, to be
deployed to tree-based models. To demonstrate this, we study measures of model
sensitivity defined in terms of integrals of gradients and demonstrate how to
compute them for regression trees using the proposed gradient estimates.
Quantitative and qualitative numerical experiments reveal the capability of
gradients estimated by regression trees to improve predictive analysis, solve
tasks in uncertainty quantification, and provide interpretation of model
behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments very welcome!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Message-Passing and Zeroth-Order Optimization Based
  Distributed Learning with a Use-Case in Resource Allocation in Communication
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04604v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04604v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pourya Behmandpoor, Marc Moonen, Panagiotis Patrinos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed learning and adaptation have received significant interest and
found wide-ranging applications in machine learning and signal processing.
While various approaches, such as shared-memory optimization, multi-task
learning, and consensus-based learning (e.g., federated learning and learning
over graphs), focus on optimizing either local costs or a global cost, there
remains a need for further exploration of their interconnections. This paper
specifically focuses on a scenario where agents collaborate towards a common
task (i.e., optimizing a global cost equal to aggregated local costs) while
effectively having distinct individual tasks (i.e., optimizing individual local
parameters in a local cost). Each agent's actions can potentially impact other
agents' performance through interactions. Notably, each agent has access to
only its local zeroth-order oracle (i.e., cost function value) and shares
scalar values, rather than gradient vectors, with other agents, leading to
communication bandwidth efficiency and agent privacy. Agents employ
zeroth-order optimization to update their parameters, and the asynchronous
message-passing between them is subject to bounded but possibly random
communication delays. This paper presents theoretical convergence analyses and
establishes a convergence rate for nonconvex problems. Furthermore, it
addresses the relevant use-case of deep learning-based resource allocation in
communication networks and conducts numerical experiments in which agents,
acting as transmitters, collaboratively train their individual policies to
maximize a global reward, e.g., a sum of data rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Generalizability of Diffusion Models Requires Rethinking
  the Hidden Gaussian Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24060v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24060v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Yixiang Dai, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the generalizability of diffusion models by looking
into the hidden properties of the learned score functions, which are
essentially a series of deep denoisers trained on various noise levels. We
observe that as diffusion models transition from memorization to
generalization, their corresponding nonlinear diffusion denoisers exhibit
increasing linearity. This discovery leads us to investigate the linear
counterparts of the nonlinear diffusion models, which are a series of linear
models trained to match the function mappings of the nonlinear diffusion
denoisers. Surprisingly, these linear denoisers are approximately the optimal
denoisers for a multivariate Gaussian distribution characterized by the
empirical mean and covariance of the training dataset. This finding implies
that diffusion models have the inductive bias towards capturing and utilizing
the Gaussian structure (covariance information) of the training dataset for
data generation. We empirically demonstrate that this inductive bias is a
unique property of diffusion models in the generalization regime, which becomes
increasingly evident when the model's capacity is relatively small compared to
the training dataset size. In the case that the model is highly
overparameterized, this inductive bias emerges during the initial training
phases before the model fully memorizes its training data. Our study provides
crucial insights into understanding the notable strong generalization
phenomenon recently observed in real-world diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OminiControl: Minimal and Universal Control for Diffusion <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce OminiControl, a highly versatile and
parameter-efficient framework that integrates image conditions into pre-trained
Diffusion Transformer (DiT) models. At its core, OminiControl leverages a
parameter reuse mechanism, enabling the DiT to encode image conditions using
itself as a powerful backbone and process them with its flexible multi-modal
attention processors. Unlike existing methods, which rely heavily on additional
encoder modules with complex architectures, OminiControl (1) effectively and
efficiently incorporates injected image conditions with only ~0.1% additional
parameters, and (2) addresses a wide range of image conditioning tasks in a
unified manner, including subject-driven generation and spatially-aligned
conditions such as edges, depth, and more. Remarkably, these capabilities are
achieved by training on images generated by the DiT itself, which is
particularly beneficial for subject-driven generation. Extensive evaluations
demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted
models in both subject-driven and spatially-aligned conditional generation.
Additionally, we release our training dataset, Subjects200K, a diverse
collection of over 200,000 identity-consistent images, along with an efficient
data synthesis pipeline to advance research in subject-consistent generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Differentiates Educational Literature? A Multimodal Fusion Approach
  of <span class="highlight-title">Transformer</span>s and Computational Linguistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17593v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17593v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jordan J. Bird
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of new literature into the English curriculum remains a
challenge since educators often lack scalable tools to rapidly evaluate
readability and adapt texts for diverse classroom needs. This study proposes to
address this gap through a multimodal approach that combines transformer-based
text classification with linguistic feature analysis to align texts with UK Key
Stages. Eight state-of-the-art Transformers were fine-tuned on segmented text
data, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,
500 deep neural network topologies were searched for the classification of
linguistic characteristics, achieving an F1 score of 0.392. The fusion of these
modalities shows a significant improvement, with every multimodal approach
outperforming all unimodal models. In particular, the ELECTRA Transformer fused
with the neural network achieved an F1 score of 0.996. Unimodal and multimodal
approaches are shown to have statistically significant differences in all
validation metrics (accuracy, precision, recall, F1 score) except for inference
time. The proposed approach is finally encapsulated in a stakeholder-facing web
application, providing non-technical stakeholder access to real-time insights
on text complexity, reading difficulty, curriculum alignment, and
recommendations for learning age range. The application empowers data-driven
decision making and reduces manual workload by integrating AI-based
recommendations into lesson planning for English literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering group dynamics in coordinated time series via hierarchical
  recurrent switching-state models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.14973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.14973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael T. Wojnowicz, Kaitlin Gili, Preetish Rath, Eric Miller, Jeffrey Miller, Clifford Hancock, Meghan O'Donovan, Seth Elkin-Frankston, Tad T. Brunyé, Michael C. Hughes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We seek a computationally efficient model for a collection of time series
arising from multiple interacting entities (a.k.a. "agents"). Recent models of
spatiotemporal patterns across individuals fail to incorporate explicit
system-level collective behavior that can influence the trajectories of
individual entities. To address this gap in the literature, we present a new
hierarchical switching-state model that can be trained in an unsupervised
fashion to simultaneously learn both system-level and individual-level
dynamics. We employ a latent system-level discrete state Markov chain that
provides top-down influence on latent entity-level chains which in turn govern
the emission of each observed time series. Recurrent feedback from the
observations to the latent chains at both entity and system levels allows
recent situational context to inform how dynamics unfold at all levels in
bottom-up fashion. We hypothesize that including both top-down and bottom-up
influences on group dynamics will improve interpretability of the learned
dynamics and reduce error when forecasting. Our hierarchical switching
recurrent dynamical model can be learned via closed-form variational coordinate
ascent updates to all latent chains that scale linearly in the number of
entities. This is asymptotically no more costly than fitting a separate model
for each entity. Analysis of both synthetic data and real basketball team
movements suggests our lean parametric model can achieve competitive forecasts
compared to larger neural network models that require far more computational
resources. Further experiments on soldier data as well as a synthetic task with
64 cooperating entities show how our approach can yield interpretable insights
about team dynamics over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Conditional Independence Test in the Presence of Discretization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17644v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17644v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyang Sun, Yu Yao, Huangyuan Hao, Yumou Qiu, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Testing conditional independence has many applications, such as in Bayesian
network learning and causal discovery. Different test methods have been
proposed. However, existing methods generally can not work when only
discretized observations are available. Specifically, consider $X_1$,
$\tilde{X}_2$ and $X_3$ are observed variables, where $\tilde{X}_2$ is a
discretization of latent variables $X_2$. Applying existing test methods to the
observations of $X_1$, $\tilde{X}_2$ and $X_3$ can lead to a false conclusion
about the underlying conditional independence of variables $X_1$, $X_2$ and
$X_3$. Motivated by this, we propose a conditional independence test
specifically designed to accommodate the presence of such discretization. To
achieve this, we design the bridge equations to recover the parameter
reflecting the statistical information of the underlying latent continuous
variables. An appropriate test statistic and its asymptotic distribution under
the null hypothesis of conditional independence have also been derived. Both
theoretical results and empirical validation have been provided, demonstrating
the effectiveness of our test methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConvMixFormer- A Resource-efficient Convolution Mixer for
  <span class="highlight-title">Transformer</span>-based Dynamic Hand Gesture Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07118v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07118v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have demonstrated remarkable success in many domains such
as natural language processing (NLP) and computer vision. With the growing
interest in transformer-based architectures, they are now utilized for gesture
recognition. So, we also explore and devise a novel ConvMixFormer architecture
for dynamic hand gestures. The transformers use quadratic scaling of the
attention features with the sequential data, due to which these models are
computationally complex and heavy. We have considered this drawback of the
transformer and designed a resource-efficient model that replaces the
self-attention in the transformer with the simple convolutional layer-based
token mixer. The computational cost and the parameters used for the
convolution-based mixer are comparatively less than the quadratic
self-attention. Convolution-mixer helps the model capture the local spatial
features that self-attention struggles to capture due to their sequential
processing nature. Further, an efficient gate mechanism is employed instead of
a conventional feed-forward network in the transformer to help the model
control the flow of features within different stages of the proposed model.
This design uses fewer learnable parameters which is nearly half the vanilla
transformer that helps in fast and efficient training. The proposed method is
evaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has
achieved state-of-the-art results on single and multimodal inputs. We have also
shown the parameter efficiency of the proposed ConvMixFormer model compared to
other methods. The source code is available at
https://github.com/mallikagarg/ConvMixFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DGNN-YOLO: Dynamic Graph Neural Networks with YOLO11 for Small Object
  Detection and Tracking in Traffic Surveillance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahriar Soudeep, M. F. Mridha, Md Abrar Jahin, Nilanjan Dey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate detection and tracking of small objects such as pedestrians,
cyclists, and motorbikes are critical for traffic surveillance systems, which
are crucial in improving road safety and decision-making in intelligent
transportation systems. However, traditional methods struggle with challenges
such as occlusion, low resolution, and dynamic traffic conditions,
necessitating innovative approaches to address these limitations. This paper
introduces DGNN-YOLO, a novel framework integrating dynamic graph neural
networks (DGNN) with YOLO11 to enhance small object detection and tracking in
traffic surveillance systems. The framework leverages YOLO11's advanced spatial
feature extraction capabilities for precise object detection and incorporates
DGNN to model spatial-temporal relationships for robust real-time tracking
dynamically. By constructing and updating graph structures, DGNN-YOLO
effectively represents objects as nodes and their interactions as edges,
ensuring adaptive and accurate tracking in complex and dynamic environments.
Extensive experiments demonstrate that DGNN-YOLO consistently outperforms
state-of-the-art methods in detecting and tracking small objects under diverse
traffic conditions, achieving the highest precision (0.8382), recall (0.6875),
and mAP@0.5:0.95 (0.6476), showcasing its robustness and scalability,
particularly in challenging scenarios involving small and occluded objects.
This work provides a scalable, real-time traffic surveillance and analysis
solution, significantly contributing to intelligent transportation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Graph Rewiring via Virtual Nodes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chendi Qian, Andrei Manolache, Christopher Morris, Mathias Niepert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message-passing graph neural networks (MPNNs) have emerged as a powerful
paradigm for graph-based machine learning. Despite their effectiveness, MPNNs
face challenges such as under-reaching and over-squashing, where limited
receptive fields and structural bottlenecks hinder information flow in the
graph. While graph transformers hold promise in addressing these issues, their
scalability is limited due to quadratic complexity regarding the number of
nodes, rendering them impractical for larger graphs. Here, we propose
implicitly rewired message-passing neural networks (IPR-MPNNs), a novel
approach that integrates implicit probabilistic graph rewiring into MPNNs. By
introducing a small number of virtual nodes, i.e., adding additional nodes to a
given graph and connecting them to existing nodes, in a differentiable,
end-to-end manner, IPR-MPNNs enable long-distance message propagation,
circumventing quadratic complexity. Theoretically, we demonstrate that
IPR-MPNNs surpass the expressiveness of traditional MPNNs. Empirically, we
validate our approach by showcasing its ability to mitigate under-reaching and
over-squashing effects, achieving state-of-the-art performance across multiple
graph datasets. Notably, IPR-MPNNs outperform graph transformers while
maintaining significantly faster computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 38th Conference on Neural Information Processing Systems
  (NeurIPS 2024), Vancouver, Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezra Karger, Houtan Bastani, Chen Yueh-Han, Zachary Jacobs, Danny Halawi, Fred Zhang, Philip E. Tetlock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasts of future events are essential inputs into informed
decision-making. Machine learning (ML) systems have the potential to deliver
forecasts at scale, but there is no framework for evaluating the accuracy of ML
systems on a standardized set of forecasting questions. To address this gap, we
introduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML
systems on an automatically generated and regularly updated set of 1,000
forecasting questions. To avoid any possibility of data leakage, ForecastBench
is comprised solely of questions about future events that have no known answer
at the time of submission. We quantify the capabilities of current ML systems
by collecting forecasts from expert (human) forecasters, the general public,
and LLMs on a random subset of questions from the benchmark ($N=200$). While
LLMs have achieved super-human performance on many benchmarks, they perform
less well here: expert forecasters outperform the top-performing LLM (p-value
$<0.01$). We display system and human scores in a public leaderboard at
www.forecastbench.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Informed Real NVP for Satellite Power System Fault Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Cena, Umberto Albertin, Mauro Martini, Silvia Bucci, Marcello Chiaberge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unique challenges posed by the space environment, characterized by
extreme conditions and limited accessibility, raise the need for robust and
reliable techniques to identify and prevent satellite faults. Fault detection
methods in the space sector are required to ensure mission success and to
protect valuable assets. In this context, this paper proposes an Artificial
Intelligence (AI) based fault detection methodology and evaluates its
performance on ADAPT (Advanced Diagnostics and Prognostics Testbed), an
Electrical Power System (EPS) dataset, crafted in laboratory by NASA. Our study
focuses on the application of a physics-informed (PI) real-valued non-volume
preserving (Real NVP) model for fault detection in space systems. The efficacy
of this method is systematically compared against other AI approaches such as
Gated Recurrent Unit (GRU) and Autoencoder-based techniques. Results show that
our physics-informed approach outperforms existing methods of fault detection,
demonstrating its suitability for addressing the unique challenges of satellite
EPS sub-system faults. Furthermore, we unveil the competitive advantage of
physics-informed loss in AI models to address specific space needs, namely
robustness, reliability, and power constraints, crucial for space exploration
and satellite missions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>C. Cena, U. Albertin, M. Martini, S. Bucci and M. Chiaberge,
  "Physics-Informed Real NVP for Satellite Power System Fault Detection," 2024
  IEEE International Conference on Advanced Intelligent Mechatronics (AIM),
  Boston, MA, USA, 2024, pp. 679-684, doi: 10.1109/AIM55361.2024.10636990</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Self-Supervised</span> Task for Fault Detection in Satellite Multivariate
  Time Series <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Cena, Silvia Bucci, Alessandro Balossino, Marcello Chiaberge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the space sector, due to environmental conditions and restricted
accessibility, robust fault detection methods are imperative for ensuring
mission success and safeguarding valuable assets. This work proposes a novel
approach leveraging Physics-Informed Real NVP neural networks, renowned for
their ability to model complex and high-dimensional distributions, augmented
with a self-supervised task based on sensors' data permutation. It focuses on
enhancing fault detection within the satellite multivariate time series. The
experiments involve various configurations, including pre-training with
self-supervision, multi-task learning, and standalone self-supervised training.
Results indicate significant performance improvements across all settings. In
particular, employing only the self-supervised loss yields the best overall
results, suggesting its efficacy in guiding the network to extract relevant
features for fault detection. This study presents a promising direction for
improving fault detection in space systems and warrants further exploration in
other datasets and applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SPAICE: AI in and for Space, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Brain Tumour Removing and Missing Modality Generation using 3D WDM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Gijs Luijten, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the second-placed solution for task 8 and the
participation solution for task 7 of BraTS 2024. The adoption of automated
brain analysis algorithms to support clinical practice is increasing. However,
many of these algorithms struggle with the presence of brain lesions or the
absence of certain MRI modalities. The alterations in the brain's morphology
leads to high variability and thus poor performance of predictive models that
were trained only on healthy brains. The lack of information that is usually
provided by some of the missing MRI modalities also reduces the reliability of
the prediction models trained with all modalities. In order to improve the
performance of these models, we propose the use of conditional 3D wavelet
diffusion models. The wavelet transform enabled full-resolution image training
and prediction on a GPU with 48 GB VRAM, without patching or downsampling,
preserving all information for prediction. The code for these tasks is
available at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Limits to Predicting Online Speech Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mina Remeli, Moritz Hardt, Robert C. Williamson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the predictability of online speech on social media, and whether
predictability improves with information outside a user's own posts. Recent
theoretical results suggest that posts from a user's social circle are as
predictive of the user's future posts as that of the user's past posts.
Motivated by the success of large language models, we empirically test this
hypothesis. We define predictability as a measure of the model's uncertainty,
i.e., its negative log-likelihood on future tokens given context. As the basis
of our study, we collect 10M tweets for ``tweet-tuning'' base models and a
further 6.25M posts from more than five thousand X (previously Twitter) users
and their peers. Across four large language models ranging in size from 1.5
billion to 70 billion parameters, we find that predicting a user's posts from
their peers' posts performs poorly. Moreover, the value of the user's own posts
for prediction is consistently higher than that of their peers'. We extend our
investigation with a detailed analysis on what's learned in-context and the
robustness of our findings. From context, base models learn to correctly
predict @-mentions and hashtags. Moreover, our results replicate if instead of
prompting the model with additional context, we finetune on it. Across the
board, we find that predicting the posts of individual users remains hard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Text to Insight: Large Language Models for Materials Science Data
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mara Schilling-Wilhelmi, Martiño Ríos-García, Sherjeel Shabih, María Victoria Gil, Santiago Miret, Christoph T. Koch, José A. Márquez, Kevin Maik Jablonka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vast majority of materials science knowledge exists in unstructured
natural language, yet structured data is crucial for innovative and systematic
materials design. Traditionally, the field has relied on manual curation and
partial automation for data extraction for specific use cases. The advent of
large language models (LLMs) represents a significant shift, potentially
enabling efficient extraction of structured, actionable data from unstructured
text by non-experts. While applying LLMs to materials science data extraction
presents unique challenges, domain knowledge offers opportunities to guide and
validate LLM outputs. This review provides a comprehensive overview of
LLM-based structured data extraction in materials science, synthesizing current
knowledge and outlining future directions. We address the lack of standardized
guidelines and present frameworks for leveraging the synergy between LLMs and
materials science expertise. This work serves as a foundational resource for
researchers aiming to harness LLMs for data-driven materials research. The
insights presented here could significantly enhance how researchers across
disciplines access and utilize scientific information, potentially accelerating
the development of novel materials for critical societal needs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Meta-<span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian de Wynter, Xun Wang, Qilong Gu, Si-Qing Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern generative language models are capable of interpreting input strings
as instructions, or prompts, and carry out tasks based on them. Many approaches
to prompting and pre-training these models involve the automated generation of
these prompts: meta-prompting, or prompting to obtain prompts. We propose a
theoretical framework based on category theory to generalize and describe them.
This framework is flexible enough to account for stochasticity, and allows us
to obtain formal results around task agnosticity and equivalence of various
meta-prompting approaches. Experimentally, we test our framework in two active
areas of model research: creativity and ideation. We find that user preference
strongly favors (p < 0.01) the prompts generated under meta-prompting, as well
as their corresponding outputs, over a series of hardcoded baseline prompts
that include the original task definition. Using our framework, we argue that
meta-prompting is more effective than basic prompting at generating desirable
outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ranking by Lifts: A Cost-Benefit Approach to Large-Scale A/B Tests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pallavi Basu, Ron Berman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A/B testers that conduct large-scale tests often prioritize lifts as the main
outcome metric and want to be able to control costs resulting from false
rejections of the null. This work develops a decision-theoretic framework for
maximizing profits subject to false discovery rate (FDR) control. We build an
empirical Bayes solution for the problem via a greedy knapsack approach. We
derive an oracle rule based on ranking the ratio of expected lifts and the cost
of wrong rejections using the local false discovery rate (lfdr) statistic. Our
oracle decision rule is valid and optimal for large-scale tests. Further, we
establish asymptotic validity for the data-driven procedure and demonstrate
finite-sample validity in experimental studies. We also demonstrate the merit
of the proposed method over other FDR control methods. Finally, we discuss an
application to data collected by experiments on the Optimizely platform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning 2.0: Artificial Neurons That Matter -- Reject Correlation,
  Embrace Orthogonality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Bouhsine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a yat-product-powered neural network, the Neural Matter Network
(NMN), a breakthrough in deep learning that achieves non-linear pattern
recognition without activation functions. Our key innovation relies on the
yat-product and yat-product, which naturally induces non-linearity by
projecting inputs into a pseudo-metric space, eliminating the need for
traditional activation functions while maintaining only a softmax layer for
final class probability distribution. This approach simplifies network
architecture and provides unprecedented transparency into the network's
decision-making process. Our comprehensive empirical evaluation across
different datasets demonstrates that NMN consistently outperforms traditional
MLPs. The results challenge the assumption that separate activation functions
are necessary for effective deep-learning models. The implications of this work
extend beyond immediate architectural benefits, by eliminating intermediate
activation functions while preserving non-linear capabilities, yat-MLP
establishes a new paradigm for neural network design that combines simplicity
with effectiveness. Most importantly, our approach provides unprecedented
insights into the traditionally opaque "black-box" nature of neural networks,
offering a clearer understanding of how these models process and classify
information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>fixed proof, added softermax</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Diffusion for Neural Spiking Data <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaivardhan Kapoor, Auguste Schulz, Julius Vetter, Felix Pei, Richard Gao, Jakob H. Macke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern datasets in neuroscience enable unprecedented inquiries into the
relationship between complex behaviors and the activity of many simultaneously
recorded neurons. While latent variable models can successfully extract
low-dimensional embeddings from such recordings, using them to generate
realistic spiking data, especially in a behavior-dependent manner, still poses
a challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS),
a diffusion-based generative model with a low-dimensional latent space: LDNS
employs an autoencoder with structured state-space (S4) layers to project
discrete high-dimensional spiking data into continuous time-aligned latents. On
these inferred latents, we train expressive (conditional) diffusion models,
enabling us to sample neural activity with realistic single-neuron and
population spiking statistics. We validate LDNS on synthetic data, accurately
recovering latent structure, firing rates, and spiking statistics. Next, we
demonstrate its flexibility by generating variable-length data that mimics
human cortical activity during attempted speech. We show how to equip LDNS with
an expressive observation model that accounts for single-neuron dynamics not
mediated by the latent state, further increasing the realism of generated
samples. Finally, conditional LDNS trained on motor cortical activity during
diverse reaching behaviors can generate realistic spiking data given reach
direction or unseen reach trajectories. In summary, LDNS simultaneously enables
inference of low-dimensional latents and realistic conditional generation of
neural spiking datasets, opening up further possibilities for simulating
experimentally testable hypotheses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning in the Presence of Repetition <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Hemati, Lorenzo Pellegrini, Xiaotian Duan, Zixuan Zhao, Fangfang Xia, Marc Masana, Benedikt Tscheschner, Eduardo Veas, Yuxiang Zheng, Shiji Zhao, Shao-Yuan Li, Sheng-Jun Huang, Vincenzo Lomonaco, Gido M. van de Ven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) provides a framework for training models in
ever-evolving environments. Although re-occurrence of previously seen objects
or tasks is common in real-world problems, the concept of repetition in the
data stream is not often considered in standard benchmarks for CL. Unlike with
the rehearsal mechanism in buffer-based strategies, where sample repetition is
controlled by the strategy, repetition in the data stream naturally stems from
the environment. This report provides a summary of the CLVision challenge at
CVPR 2023, which focused on the topic of repetition in class-incremental
learning. The report initially outlines the challenge objective and then
describes three solutions proposed by finalist teams that aim to effectively
exploit the repetition in the stream to learn continually. The experimental
results from the challenge highlight the effectiveness of ensemble-based
solutions that employ multiple versions of similar modules, each trained on
different but overlapping subsets of classes. This report underscores the
transformative potential of taking a different perspective in CL by employing
repetition in the data stream to foster innovative strategy design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted version, to appear in Neural Networks; Challenge Report of
  the 4th Workshop on Continual Learning in Computer Vision at CVPR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair Generalized Linear Mixed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09273v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09273v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Pablo Burgard, João Vitor Pamplona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using machine learning for automated prediction, it is important to
account for fairness in the prediction. Fairness in machine learning aims to
ensure that biases in the data and model inaccuracies do not lead to
discriminatory decisions. E.g., predictions from fair machine learning models
should not discriminate against sensitive variables such as sexual orientation
and ethnicity. The training data often in obtained from social surveys. In
social surveys, oftentimes the data collection process is a strata sampling,
e.g. due to cost restrictions. In strata samples, the assumption of
independence between the observation is not fulfilled. Hence, if the machine
learning models do not account for the strata correlations, the results may be
biased. Especially high is the bias in cases where the strata assignment is
correlated to the variable of interest. We present in this paper an algorithm
that can handle both problems simultaneously, and we demonstrate the impact of
stratified sampling on the quality of fair machine learning predictions in a
reproducible simulation study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:2405.06433</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fair Mixed Effects Support Vector Machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06433v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06433v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Pablo Burgard, João Vitor Pamplona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ensure unbiased and ethical automated predictions, fairness must be a core
principle in machine learning applications. Fairness in machine learning aims
to mitigate biases present in the training data and model imperfections that
could lead to discriminatory outcomes. This is achieved by preventing the model
from making decisions based on sensitive characteristics like ethnicity or
sexual orientation. A fundamental assumption in machine learning is the
independence of observations. However, this assumption often does not hold true
for data describing social phenomena, where data points are often clustered
based. Hence, if the machine learning models do not account for the cluster
correlations, the results may be biased. Especially high is the bias in cases
where the cluster assignment is correlated to the variable of interest. We
present a fair mixed effects support vector machine algorithm that can handle
both problems simultaneously. With a reproducible simulation study we
demonstrate the impact of clustered data on the quality of fair machine
learning predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProtFAD: Introducing function-aware domains as implicit modality towards
  protein function prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqing Wang, Zhiwei Nie, Yonghong He, Athanasios V. Vasilakos, Zhixiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein function prediction is currently achieved by encoding its sequence or
structure, where the sequence-to-function transcendence and high-quality
structural data scarcity lead to obvious performance bottlenecks. Protein
domains are "building blocks" of proteins that are functionally independent,
and their combinations determine the diverse biological functions. However,
most existing studies have yet to thoroughly explore the intricate functional
information contained in the protein domains. To fill this gap, we propose a
synergistic integration approach for a function-aware domain representation,
and a domain-joint contrastive learning strategy to distinguish different
protein functions while aligning the modalities. Specifically, we align the
domain semantics with GO terms and text description to pre-train domain
embeddings. Furthermore, we partition proteins into multiple sub-views based on
continuous joint domains for contrastive training under the supervision of a
novel triplet InfoNCE loss. Our approach significantly and comprehensively
outperforms the state-of-the-art methods on various benchmarks, and clearly
differentiates proteins carrying distinct functions compared to the competitor.
Our implementation is available at
https://github.com/AI-HPC-Research-Team/ProtFAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anomaly Detection in Medical Imaging -- A Mini <span class="highlight-title">Review</span> <span class="chip">SC2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.11986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.11986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian E. Tschuchnig, Michael Gadermayr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing digitization of medical imaging enables machine learning based
improvements in detecting, visualizing and segmenting lesions, easing the
workload for medical experts. However, supervised machine learning requires
reliable labelled data, which is is often difficult or impossible to collect or
at least time consuming and thereby costly. Therefore methods requiring only
partly labeled data (semi-supervised) or no labeling at all (unsupervised
methods) have been applied more regularly. Anomaly detection is one possible
methodology that is able to leverage semi-supervised and unsupervised methods
to handle medical imaging tasks like classification and segmentation. This
paper uses a semi-exhaustive literature review of relevant anomaly detection
papers in medical imaging to cluster into applications, highlight important
results, establish lessons learned and give further advice on how to approach
anomaly detection in medical imaging. The qualitative analysis is based on
google scholar and 4 different search terms, resulting in 120 different
analysed papers. The main results showed that the current research is mostly
motivated by reducing the need for labelled data. Also, the successful and
substantial amount of research in the brain MRI domain shows the potential for
applications in further domains like OCT and chest X-ray.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented at iDSC2021 edit: During work on this
  publication Maximilian Ernst Tschuchnig was affiliated with Salzburg
  University of Applied Sciences and University of Salzburg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moral Alignment for LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making agents based on pre-trained Large Language Models (LLMs) are
increasingly being deployed across various domains of human activity. While
their applications are currently rather specialized, several research efforts
are under way to develop more generalist agents. As LLM-based systems become
more agentic, their influence on human activity will grow and the transparency
of this will decrease. Consequently, developing effective methods for aligning
them to human values is vital.
  The prevailing practice in alignment often relies on human preference data
(e.g., in RLHF or DPO), in which values are implicit and are essentially
deduced from relative preferences over different model outputs. In this work,
instead of relying on human feedback, we introduce the design of reward
functions that explicitly encode core human values for Reinforcement
Learning-based fine-tuning of foundation agent models. Specifically, we use
intrinsic rewards for the moral alignment of LLM agents.
  We evaluate our approach using the traditional philosophical frameworks of
Deontological Ethics and Utilitarianism, quantifying moral rewards for agents
in terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)
environment. We also show how moral fine-tuning can be deployed to enable an
agent to unlearn a previously developed selfish strategy. Finally, we find that
certain moral strategies learned on the IPD game generalize to several other
matrix game environments. In summary, we demonstrate that fine-tuning with
intrinsic rewards is a promising general solution for aligning LLM agents to
human values, and it might represent a more transparent and cost-effective
alternative to currently predominant alignment techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constraining Generative Models for Engineering Design with Negative Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lyle Regenwetter, Giorgio Giannone, Akash Srivastava, Dan Gutfreund, Faez Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have recently achieved remarkable success and widespread
adoption in society, yet they often struggle to generate realistic and accurate
outputs. This challenge extends beyond language and vision into fields like
engineering design, where safety-critical engineering standards and
non-negotiable physical laws tightly constrain what outputs are considered
acceptable. In this work, we introduce a novel training method to guide a
generative model toward constraint-satisfying outputs using `negative data' --
examples of what to avoid. Our negative-data generative model (NDGM)
formulation easily outperforms classic models, generating 1/6 as many
constraint-violating samples using 1/8 as much data in certain problems. It
also consistently outperforms other baselines, achieving a balance between
constraint satisfaction and distributional similarity that is unsurpassed by
any other model in 12 of the 14 problems tested. This widespread superiority is
rigorously demonstrated across numerous synthetic tests and real engineering
problems, such as ship hull synthesis with hydrodynamic constraints and vehicle
design with impact safety constraints. Our benchmarks showcase both the
best-in-class performance of our new NDGM formulation and the overall dominance
of NDGMs versus classic generative models. We publicly release the code and
benchmarks at https://github.com/Lyleregenwetter/NDGMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autobidders with Budget and ROI Constraints: Efficiency, Regret, and
  Pacing Dynamics <span class="chip">COLT 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13306v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13306v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brendan Lucier, Sarath Pattathil, Aleksandrs Slivkins, Mengxiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a game between autobidding algorithms that compete in an online
advertising platform. Each autobidder is tasked with maximizing its
advertiser's total value over multiple rounds of a repeated auction, subject to
budget and return-on-investment constraints. We propose a gradient-based
learning algorithm that is guaranteed to satisfy all constraints and achieves
vanishing individual regret. Our algorithm uses only bandit feedback and can be
used with the first- or second-price auction, as well as with any
"intermediate" auction format. Our main result is that when these autobidders
play against each other, the resulting expected liquid welfare over all rounds
is at least half of the expected optimal liquid welfare achieved by any
allocation. This holds whether or not the bidding dynamics converges to an
equilibrium.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared at COLT 2024. Numerical experiments added since Jun'24
  version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Multi-Scale Multiple Instance Learning to Improve Thyroid
  Cancer Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian E. Tschuchnig, Philipp Grubmüller, Lea M. Stangassinger, Christina Kreutzer, Sébastien Couillard-Després, Gertie J. Oostingh, Anton Hittmair, Michael Gadermayr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thyroid cancer is currently the fifth most common malignancy diagnosed in
women. Since differentiation of cancer sub-types is important for treatment and
current, manual methods are time consuming and subjective, automatic
computer-aided differentiation of cancer types is crucial. Manual
differentiation of thyroid cancer is based on tissue sections, analysed by
pathologists using histological features. Due to the enormous size of gigapixel
whole slide images, holistic classification using deep learning methods is not
feasible. Patch based multiple instance learning approaches, combined with
aggregations such as bag-of-words, is a common approach. This work's
contribution is to extend a patch based state-of-the-art method by generating
and combining feature vectors of three different patch resolutions and
analysing three distinct ways of combining them. The results showed
improvements in one of the three multi-scale approaches, while the others led
to decreased scores. This provides motivation for analysis and discussion of
the individual approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented at IPTA 2022 (Best Paper) edit: During work on
  this publication Maximilian Ernst Tschuchnig was affiliated with Salzburg
  University of Applied Sciences and University of Salzburg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample Complexity Bounds for Linear System Identification from a Finite
  Set 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Chatzikiriakos, Andrea Iannelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers a finite sample perspective on the problem of
identifying an LTI system from a finite set of possible systems using
trajectory data. To this end, we use the maximum likelihood estimator to
identify the true system and provide an upper bound for its sample complexity.
Crucially, the derived bound does not rely on a potentially restrictive
stability assumption. Additionally, we leverage tools from information theory
to provide a lower bound to the sample complexity that holds independently of
the used estimator. The derived sample complexity bounds are analyzed
analytically and numerically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Refine: Improving Natural Language Explanation Generation by
  Learning in Tandem <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian Möller, Vera Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language explanations (NLEs) are vital for elucidating the reasoning
behind large language model (LLM) decisions. Many techniques have been
developed to generate NLEs using LLMs. However, like humans, LLMs might not
always produce optimal NLEs on first attempt. Inspired by human learning
processes, we introduce Cross-Refine, which employs role modeling by deploying
two LLMs as generator and critic, respectively. The generator outputs a first
NLE and then refines this initial explanation using feedback and suggestions
provided by the critic. Cross-Refine does not require any supervised training
data or additional training. We validate Cross-Refine across three NLP tasks
using three state-of-the-art open-source LLMs through automatic and human
evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which
only utilizes self-feedback to refine the explanations. Our findings from
automatic evaluation and a user study indicate that Cross-Refine outperforms
Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful
LLMs, whereas Self-Refine only yields strong results with ChatGPT.
Additionally, we conduct an ablation study to assess the importance of feedback
and suggestions. Both of them play an important role in refining explanations.
We further evaluate Cross-Refine on a bilingual dataset in English and German.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025; long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15798v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15798v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, Shinkook Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) generation with Stable Diffusion models (SDMs) involves
high computing demands due to billion-scale parameters. To enhance efficiency,
recent studies have reduced sampling steps and applied network quantization
while retaining the original architectures. The lack of architectural reduction
attempts may stem from worries over expensive retraining for such massive
models. In this work, we uncover the surprising potential of block pruning and
feature distillation for low-cost general-purpose T2I. By removing several
residual and attention blocks from the U-Net of SDMs, we achieve 30%~50%
reduction in model size, MACs, and latency. We show that distillation
retraining is effective even under limited resources: using only 13 A100 days
and a tiny dataset, our compact models can imitate the original SDMs (v1.4 and
v2.1-base with over 6,000 A100 days). Benefiting from the transferred
knowledge, our BK-SDMs deliver competitive results on zero-shot MS-COCO against
larger multi-billion parameter models. We further demonstrate the applicability
of our lightweight backbones in personalized generation and image-to-image
translation. Deployment of our models on edge devices attains 4-second
inference. Code and models can be found at:
https://github.com/Nota-NetsPresso/BK-SDM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024 Camera-Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MASP: Scalable GNN-based Planning for Multi-Agent Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Yang, Xinting Yang, Chao Yu, Jiayu Chen, Wenbo Ding, Huazhong Yang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate multi-agent navigation tasks, where multiple agents need to
reach initially unassigned goals in a limited time. Classical planning-based
methods suffer from expensive computation overhead at each step and offer
limited expressiveness for complex cooperation strategies. In contrast,
reinforcement learning (RL) has recently become a popular approach for
addressing this issue. However, RL struggles with low data efficiency and
cooperation when directly exploring (nearly) optimal policies in a large
exploration space, especially with an increased number of agents(e.g., 10+
agents) or in complex environments (e.g., 3-D simulators). In this paper, we
propose the Multi-Agent Scalable Graph-based Planner (MASP), a goal-conditioned
hierarchical planner for navigation tasks with a substantial number of agents
in the decentralized setting. MASP employs a hierarchical framework to reduce
space complexity by decomposing a large exploration space into multiple
goal-conditioned subspaces, where a high-level policy assigns agents goals, and
a low-level policy navigates agents toward designated goals. For agent
cooperation and the adaptation to varying team sizes, we model agents and goals
as graphs to better capture their relationship. The high-level policy, the Goal
Matcher, leverages a graph-based Self-Encoder and Cross-Encoder to optimize
goal assignment by updating the agent and the goal graphs. The low-level
policy, the Coordinated Action Executor, introduces the Group Information
Fusion to facilitate group division and extract agent relationships across
groups, enhancing training efficiency for agent cooperation. The results
demonstrate that MASP outperforms RL and planning-based baselines in task
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE RA-L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Generative Priors Improve World Models Sequence Modelling
  Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07836v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07836v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Meo, Mircea Lica, Zarif Ikram, Akihiro Nakano, Vedant Shah, Aniket Rajiv Didolkar, Dianbo Liu, Anirudh Goyal, Justin Dauwels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (RL) has become the leading approach for creating
artificial agents in complex environments. Model-based approaches, which are RL
methods with world models that predict environment dynamics, are among the most
promising directions for improving data efficiency, forming a critical step
toward bridging the gap between research and real-world deployment. In
particular, world models enhance sample efficiency by learning in imagination,
which involves training a generative sequence model of the environment in a
self-supervised manner. Recently, Masked Generative Modelling has emerged as a
more efficient and superior inductive bias for modelling and generating token
sequences. Building on the Efficient Stochastic Transformer-based World Models
(STORM) architecture, we replace the traditional MLP prior with a Masked
Generative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our
model on two downstream tasks: reinforcement learning and video prediction.
GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari
100k benchmark. Moreover, we apply Transformer-based World Models to continuous
action environments for the first time, addressing a significant gap in prior
research. To achieve this, we employ a state mixer function that integrates
latent state representations with actions, enabling our model to handle
continuous control tasks. We validate this approach through qualitative and
quantitative analyses on the DeepMind Control Suite, showcasing the
effectiveness of Transformer-based World Models in this new domain. Our results
highlight the versatility and efficacy of the MaskGIT dynamics prior, paving
the way for more accurate world models and effective RL policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topology Only <span class="highlight-title">Pre-Train</span>ing: Towards Generalised Multi-Domain Graph
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03976v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03976v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex O. Davies, Riku W. Green, Nirav S. Ajmeri, Telmo M. Silva Filho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The principal benefit of unsupervised representation learning is that a
pre-trained model can be fine-tuned where data or labels are scarce. Existing
approaches for graph representation learning are domain specific, maintaining
consistent node and edge features across the pre-training and target datasets.
This has precluded transfer to multiple domains. We present Topology Only
Pre-Training (ToP), a graph pre-training method based on node and edge feature
exclusion. We show positive transfer on evaluation datasets from multiple
domains, including domains not present in pre-training data, running directly
contrary to assumptions made in contemporary works. On 75% of experiments, ToP
models perform significantly $p \leq 0.01$ better than a supervised baseline.
Performance is significantly positive on 85.7% of tasks when node and edge
features are used in fine-tuning. We further show that out-of-domain topologies
can produce more useful pre-training than in-domain. Under ToP we show better
transfer from non-molecule pre-training, compared to molecule pre-training, on
79% of molecular benchmarks. Against the limited set of other generalist graph
models ToP performs strongly, including against models with many orders of
magnitude larger. These findings show that ToP opens broad areas of research in
both transfer learning on scarcely populated graph domains and in graph
foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures, 5 tables. For in-development code see
  https://github.com/neutralpronoun/general-gcl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-turn Reinforcement Learning from Preference Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lior Shani, Aviv Rosenberg, Asaf Cassel, Oran Lang, Daniele Calandriello, Avital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, Avinatan Hassidim, Yossi Matias, Rémi Munos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) has become the standard
approach for aligning Large Language Models (LLMs) with human preferences,
allowing LLMs to demonstrate remarkable abilities in various tasks. Existing
methods work by emulating the preferences at the single decision (turn) level,
limiting their capabilities in settings that require planning or multi-turn
interactions to achieve a long-term goal. In this paper, we address this issue
by developing novel methods for Reinforcement Learning (RL) from preference
feedback between two full multi-turn conversations. In the tabular setting, we
present a novel mirror-descent-based policy optimization algorithm for the
general multi-turn preference-based RL problem, and prove its convergence to
Nash equilibrium. To evaluate performance, we create a new environment,
Education Dialogue, where a teacher agent guides a student in learning a random
topic, and show that a deep RL variant of our algorithm outperforms RLHF
baselines. Finally, we show that in an environment with explicit rewards, our
algorithm recovers the same performance as a reward-based RL baseline, despite
relying solely on a weaker preference signal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Methods for generating and evaluating synthetic longitudinal patient
  data: a systematic <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12380v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12380v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katariina Perkonoja, Kari Auranen, Joni Virta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth in data availability has facilitated research and
development, yet not all industries have benefited equally due to legal and
privacy constraints. The healthcare sector faces significant challenges in
utilizing patient data because of concerns about data security and
confidentiality. To address this, various privacy-preserving methods, including
synthetic data generation, have been proposed. Synthetic data replicate
existing data as closely as possible, acting as a proxy for sensitive
information. While patient data are often longitudinal, this aspect remains
underrepresented in existing reviews of synthetic data generation in
healthcare. This paper maps and describes methods for generating and evaluating
synthetic longitudinal patient data in real-life settings through a systematic
literature review, conducted following the PRISMA guidelines and incorporating
data from five databases up to May 2024. Thirty-nine methods were identified,
with four addressing all challenges of longitudinal data generation, though
none included privacy-preserving mechanisms. Resemblance was evaluated in most
studies, utility in the majority, and privacy in just over half. Only a small
fraction of studies assessed all three aspects. Our findings highlight the need
for further research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Induction and Transduction for Abstract Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02272v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02272v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, Kevin Ellis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When learning an input-output mapping from very few examples, is it better to
first infer a latent function that explains the examples, or is it better to
directly predict new test outputs, e.g. using a neural network? We study this
question on ARC by training neural models for induction (inferring latent
functions) and transduction (directly predicting the test output for a given
test input). We train on synthetically generated variations of Python programs
that solve ARC training tasks. We find inductive and transductive models solve
different kinds of test problems, despite having the same training problems and
sharing the same neural architecture: Inductive program synthesis excels at
precise computations, and at composing multiple concepts, while transduction
succeeds on fuzzier perceptual concepts. Ensembling them approaches human-level
performance on ARC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Zeroth-Order Methods for Scalable Large Language
  Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07818v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07818v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Z Liu, J Lou, W Bao, Y Hu, B Li, Z Qin, K Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning on task-specific datasets is a widely-embraced paradigm of
harnessing the powerful capability of pretrained LLMs for various downstream
tasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy
concerns, differentially private (DP) fine-tuning of pretrained LLMs has been
widely used to safeguarding the privacy of task-specific datasets. Lying at the
design core of DP LLM fine-tuning methods is the satisfactory tradeoff among
privacy, utility, and scalability. Most existing methods build upon the seminal
work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,
DP-SGD-based fine-tuning methods are unfortunately limited by the inherent
inefficiency of SGD.
  In this paper, we investigate the potential of DP zeroth-order methods for
LLM pretraining, which avoids the scalability bottleneck of SGD by
approximating the gradient with the more efficient zeroth-order gradient.
Rather than treating the zeroth-order method as a drop-in replacement for SGD,
this paper presents a comprehensive study both theoretically and empirically.
First, we propose the stagewise DP zeroth-order method (DP-ZOSO) that
dynamically schedules key hyperparameters. This design is grounded on the
synergy between DP random perturbation and the gradient approximation error of
the zeroth-order method, and its effect on fine-tuning trajectory.
  We provide theoretical analysis for both proposed methods. We conduct
extensive empirical analysis on both encoder-only masked language model and
decoder-only autoregressive language model, achieving impressive results in
terms of scalability and utility regardless of the class of tasks (compared
with DPZero, DP-ZOPO improves $4.5\%$ on SST-5, $5.5\%$ on MNLI with
RoBERTa-Large and 9.2\% on CB, 3.9\% on BoolQ with OPT-2.7b when $\epsilon=4$,
demonstrates more significant enhancement in performance on more complicated
tasks).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning General Representation of 12-Lead Electrocardiogram with a
  Joint-Embedding Predictive Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08559v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08559v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sehun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrocardiogram (ECG) captures the heart's electrical signals, offering
valuable information for diagnosing cardiac conditions. However, the scarcity
of labeled data makes it challenging to fully leverage supervised learning in
medical domain. Self-supervised learning (SSL) offers a promising solution,
enabling models to learn from unlabeled data and uncover meaningful patterns.
In this paper, we show that masked modeling in the latent space can be a
powerful alternative to existing self-supervised methods in the ECG domain. We
introduce ECG-JEPA, a SSL model for 12-lead ECG analysis that learns semantic
representations of ECG data by predicting in the hidden latent space, bypassing
the need to reconstruct raw signals. This approach offers several advantages in
the ECG domain: (1) it avoids producing unnecessary details, such as noise,
which is common in ECG; and (2) it addresses the limitations of na\"ive L2 loss
between raw signals. Another key contribution is the introduction of
Cross-Pattern Attention (CroPA), a specialized masked attention mechanism
tailored for 12-lead ECG data. ECG-JEPA is trained on the union of several open
ECG datasets, totaling approximately 180,000 samples, and achieves
state-of-the-art performance in various downstream tasks including ECG
classification and feature prediction. Our code is openly available at
https://github.com/sehunfromdaegu/ECG_JEPA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simulation-based inference with scattering representations: scattering
  is all you need <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11883v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11883v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiyam Lin, Benjamin Joachimi, Jason D. McEwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate the successful use of scattering representations without
further compression for simulation-based inference (SBI) with images (i.e.
field-level), illustrated with a cosmological case study. Scattering
representations provide a highly effective representational space for
subsequent learning tasks, although the higher dimensional compressed space
introduces challenges. We overcome these through spatial averaging, coupled
with more expressive density estimators. Compared to alternative methods, such
an approach does not require additional simulations for either training or
computing derivatives, is interpretable, and resilient to covariate shift. As
expected, we show that a scattering only approach extracts more information
than traditional second order summary statistics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, accepted by NeurIPS workshop on Machine Learning
  and the Physical Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting MAE <span class="highlight-title">pre-train</span>ing for 3D medical image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tassilo Wald, Constantin Ulrich, Stanislav Lukyanenko, Andrei Goncharov, Alberto Paderno, Leander Maerkisch, Paul F. Jäger, Klaus Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the
potential of vast, untapped clinical datasets, for various downstream
applications that suffer from the scarcity of labeled data. While SSL has
revolutionized fields like natural language processing and computer vision, its
adoption in 3D medical image computing has been limited by three key pitfalls:
Small pre-training dataset sizes, architectures inadequate for 3D medical image
analysis, and insufficient evaluation practices. In this paper, we address
these issues by i) leveraging a large-scale dataset of 39k 3D brain MRI volumes
and ii) using a Residual Encoder U-Net architecture within the state-of-the-art
nnU-Net framework. iii) A robust development framework, incorporating 5
development and 8 testing brain MRI segmentation datasets, allowed
performance-driven design decisions to optimize the simple concept of Masked
Auto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses
previous SSL methods but also outperforms the strong nnU-Net baseline by an
average of approximately 3 Dice points setting a new state-of-the-art. Our code
and models are made available here.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Arxiv Preprint. Revised and under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Protecting Federated Learning from Extreme Model Poisoning Attacks via
  Multidimensional Time Series Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16668v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16668v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Gabrielli, Dimitri Belli, Zoe Matrullo, Vittorio Miori, Gabriele Tolomei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current defense mechanisms against model poisoning attacks in federated
learning (FL) systems have proven effective up to a certain threshold of
malicious clients. In this work, we introduce FLANDERS, a novel pre-aggregation
filter for FL resilient to large-scale model poisoning attacks, i.e., when
malicious clients far exceed legitimate participants. FLANDERS treats the
sequence of local models sent by clients in each FL round as a matrix-valued
time series. Then, it identifies malicious client updates as outliers in this
time series by comparing actual observations with estimates generated by a
matrix autoregressive forecasting model maintained by the server. Experiments
conducted in several non-iid FL setups show that FLANDERS significantly
improves robustness across a wide spectrum of attacks when paired with standard
and robust existing aggregation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable Generation of Privacy-preserving Synthetic Electronic Health
  Record Time Series via Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15290v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15290v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhang Tian, Bernie Chen, Allan Guo, Shiyi Jiang, Anru R. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) are rich sources of patient-level data,
offering valuable resources for medical data analysis. However, privacy
concerns often restrict access to EHRs, hindering downstream analysis. Current
EHR de-identification methods are flawed and can lead to potential privacy
leakage. Additionally, existing publicly available EHR databases are limited,
preventing the advancement of medical research using EHR. This study aims to
overcome these challenges by generating realistic and privacy-preserving
synthetic electronic health records (EHRs) time series efficiently. We
introduce a new method for generating diverse and realistic synthetic EHR time
series data using Denoising Diffusion Probabilistic Models (DDPM). We conducted
experiments on six databases: Medical Information Mart for Intensive Care III
and IV (MIMIC-III/IV), the eICU Collaborative Research Database (eICU), and
non-EHR datasets on Stocks and Energy. We compared our proposed method with
eight existing methods. Our results demonstrate that our approach significantly
outperforms all existing methods in terms of data fidelity while requiring less
training effort. Additionally, data generated by our method yields a lower
discriminative accuracy compared to other baseline methods, indicating the
proposed method can generate data with less privacy risk. The proposed
diffusion-model-based method can reliably and efficiently generate synthetic
EHR time series, which facilitates the downstream medical data analysis. Our
numerical results show the superiority of the proposed method over all other
existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonequilbrium physics of generative diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11932v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11932v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhendong Yu, Haiping Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative diffusion models apply the concept of Langevin dynamics in physics
to machine leaning, attracting a lot of interests from engineering, statistics
and physics, but a complete picture about inherent mechanisms is still lacking.
In this paper, we provide a transparent physics analysis of diffusion models,
formulating the fluctuation theorem, entropy production, equilibrium measure,
and Franz-Parisi potential to understand the dynamic process and intrinsic
phase transitions. Our analysis is rooted in a path integral representation of
both forward and backward dynamics, and in treating the reverse diffusion
generative process as a statistical inference, where the time-dependent state
variables serve as quenched disorder akin to that in spin glass theory. Our
study thus links stochastic thermodynamics, statistical inference and geometry
based analysis together to yield a coherent picture about how the generative
diffusion models work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures, 31 refs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding LLM Embeddings for Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Tang, Bangding Yang, Xingyou Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of large language models (LLMs) for flexibly processing
information as strings, a natural application is regression, specifically by
preprocessing string representations into LLM embeddings as downstream features
for metric prediction. In this paper, we provide one of the first comprehensive
investigations into embedding-based regression and demonstrate that LLM
embeddings as features can be better for high-dimensional regression tasks than
using traditional feature engineering. This regression performance can be
explained in part due to LLM embeddings over numeric data inherently preserving
Lipschitz continuity over the feature space. Furthermore, we quantify the
contribution of different model effects, most notably model size and language
understanding, which we find surprisingly do not always improve regression
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty quantification for fast reconstruction methods using
  augmented equivariant bootstrap: Application to radio interferometry <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23178v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23178v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Cherif, Tobías I. Liaudat, Jonathan Kern, Christophe Kervazo, Jérôme Bobin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of next-generation radio interferometers like the Square Kilometer
Array promises to revolutionise our radio astronomy observational capabilities.
The unprecedented volume of data these devices generate requires fast and
accurate image reconstruction algorithms to solve the ill-posed radio
interferometric imaging problem. Most state-of-the-art reconstruction methods
lack trustworthy and scalable uncertainty quantification, which is critical for
the rigorous scientific interpretation of radio observations. We propose an
unsupervised technique based on a conformalized version of a radio-augmented
equivariant bootstrapping method, which allows us to quantify uncertainties for
fast reconstruction methods. Noticeably, we rely on reconstructions from
ultra-fast unrolled algorithms. The proposed method brings more reliable
uncertainty estimations to our problem than existing alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures. Accepted at the Machine Learning and the
  Physical Sciences Workshop, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Multi-Task Brain Tumour Segmentation with Synthetic Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Tiago Jesus, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the winning solution of task 1 and the third-placed
solution of task 3 of the BraTS challenge. The use of automated tools in
clinical practice has increased due to the development of more and more
sophisticated and reliable algorithms. However, achieving clinical standards
and developing tools for real-life scenarios is a major challenge. To this end,
BraTS has organised tasks to find the most advanced solutions for specific
purposes. In this paper, we propose the use of synthetic data to train
state-of-the-art frameworks in order to improve the segmentation of adult
gliomas in a post-treatment scenario, and the segmentation of meningioma for
radiotherapy planning. Our results suggest that the use of synthetic data leads
to more robust algorithms, although the synthetic data generation pipeline is
not directly suited to the meningioma task. In task 1, we achieved a DSC of
0.7900, 0.8076, 0.7760, 0.8926, 0.7874, 0.8938 and a HD95 of 35.63, 30.35,
44.58, 16.87, 38.19, 17.95 for ET, NETC, RC, SNFH, TC and WT, respectively and,
in task 3, we achieved a DSC of 0.801 and HD95 of 38.26, in the testing phase.
The code for these tasks is available at
https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual-Personalizing Adapter for Federated Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19211v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19211v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyuan Yang, Guodong Long, Tao Shen, Jing Jiang, Michael Blumenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, foundation models, particularly large language models (LLMs), have
demonstrated an impressive ability to adapt to various tasks by fine-tuning
diverse instruction data. Notably, federated foundation models (FedFM) emerge
as a privacy preservation method to fine-tune models collaboratively under
federated learning (FL) settings by leveraging many distributed datasets with
non-IID data. To alleviate communication and computation overhead,
parameter-efficient methods are introduced for efficiency, and some research
adapted personalization methods to FedFM for better user preferences alignment.
However, a critical gap in existing research is the neglect of test-time
distribution shifts in real-world applications, and conventional methods for
test-time distribution shifts in personalized FL are less effective for FedFM
due to their failure to adapt to complex distribution shift scenarios and the
requirement to train all parameters. To bridge this gap, we refine the setting
in FedFM, termed test-time personalization, which aims to learn personalized
federated foundation models on clients while effectively handling test-time
distribution shifts simultaneously. To address challenges in this setting, we
explore a simple yet effective solution, a Federated Dual-Personalizing Adapter
(FedDPA) architecture. By co-working with a foundation model, a global adapter
and a local adapter jointly tackle the test-time distribution shifts and
client-specific personalization. Additionally, we introduce an instance-wise
dynamic weighting mechanism that dynamically integrates the global and local
adapters for each test instance during inference, facilitating effective
test-time personalization. The effectiveness of the proposed method has been
evaluated on benchmark datasets across different NLP tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02595v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02595v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nouhaila Innan, Alberto Marchisio, Mohamed Bennai, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces the Quantum Federated Neural Network for Financial
Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine
Learning (QML) and quantum computing with Federated Learning (FL) for financial
fraud detection. Using quantum technologies' computational power and the robust
data privacy protections offered by FL, QFNN-FFD emerges as a secure and
efficient method for identifying fraudulent transactions within the financial
sector. Implementing a dual-phase training model across distributed clients
enhances data integrity and enables superior performance metrics, achieving
precision rates consistently above 95%. Additionally, QFNN-FFD demonstrates
exceptional resilience by maintaining an impressive 80% accuracy, highlighting
its robustness and readiness for real-world applications. This combination of
high performance, security, and robustness against noise positions QFNN-FFD as
a transformative advancement in financial technology solutions and establishes
it as a new benchmark for privacy-focused fraud detection systems. This
framework facilitates the broader adoption of secure, quantum-enhanced
financial services and inspires future innovations that could use QML to tackle
complex challenges in other areas requiring high confidentiality and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Statistics Predict the Sensitivity of Perceptual Quality Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09874v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09874v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Hepburn, Valero Laparra, Raúl Santos-Rodriguez, Jesús Malo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previously, Barlow and Attneave hypothesised a link between biological vision
and information maximisation. Following Shannon, information was defined using
the probability of natural images. Several physiological and psychophysical
phenomena have been derived from principles like info-max, efficient coding, or
optimal denoising. However, it remains unclear how this link is expressed in
mathematical terms from image probability. Classical derivations were subjected
to strong assumptions on the probability models and on the behaviour of the
sensors. Moreover, the direct evaluation of the hypothesis was limited by the
inability of classical image models to deliver accurate estimates of the
probability. Here, we directly evaluate image probabilities using a generative
model for natural images, and analyse how probability-related factors can be
combined to predict the sensitivity of state-of-the-art subjective image
quality metrics, a proxy for human perception. We use information theory and
regression analysis to find a simple model that when combining just two
probability-related factors achieves 0.77 correlation with subjective metrics.
This probability-based model is validated in two ways: through direct
comparison with the opinion of real observers in a subjective quality
experiment, and by reproducing basic trends of classical psychophysical facts
such as the Contrast Sensitivity Function, the Weber-law, and contrast masking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Adaptive Quantum Kernel Principal Components Analysis for Compact
  Readout of Chemiresistive Sensor Arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeheng Wang, Timothy van der Laan, Muhammad Usman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of Internet of Things (IoT) devices necessitates efficient
data compression techniques to handle the vast amounts of data generated by
these devices. Chemiresistive sensor arrays (CSAs), a simple-to-fabricate but
crucial component in IoT systems, generate large volumes of data due to their
simultaneous multi-sensor operations. Classical principal component analysis
(cPCA) methods, a common solution to the data compression challenge, face
limitations in preserving critical information during dimensionality reduction.
In this study, we present self-adaptive quantum kernel (SAQK) PCA as a superior
alternative to enhance information retention. Our findings demonstrate that
SAQK PCA outperforms cPCA in various back-end machine-learning tasks,
especially in low-dimensional scenarios where access to quantum bits is
limited. These results highlight the potential of noisy intermediate-scale
quantum (NISQ) computers to revolutionize data processing in real-world IoT
applications by improving the efficiency and reliability of CSA data
compression and readout, despite the current constraints on qubit availability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Correction to "Wasserstein distance estimates for the distributions of
  numerical approximations to ergodic stochastic differential equations" 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08711v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08711v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Paulin, Peter A. Whalley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A method for analyzing non-asymptotic guarantees of numerical discretizations
of ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and
Zygalakis in ``Wasserstein distance estimates for the distributions of
numerical approximations to ergodic stochastic differential equations". They
analyze the UBU integrator which is strong order two and only requires one
gradient evaluation per step, resulting in desirable non-asymptotic guarantees,
in particular $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ steps to reach a distance
of $\epsilon > 0$ in Wasserstein-2 distance away from the target distribution.
However, there is a mistake in the local error estimates in Sanz-Serna and
Zygalakis (2021), in particular, a stronger assumption is needed to achieve
these complexity estimates. This note reconciles the theory with the dimension
dependence observed in practice in many applications of interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLLM-LLaVA-FL: Multimodal Large Language Model Assisted Federated
  Learning <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyi Zhang, Hao Frank Yang, Ang Li, Xin Guo, Pu Wang, Haiming Wang, Yiran Chen, Hai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous studies on federated learning (FL) often encounter performance
degradation due to data heterogeneity among different clients. In light of the
recent advances in multimodal large language models (MLLMs), such as GPT-4v and
LLaVA, which demonstrate their exceptional proficiency in multimodal tasks,
such as image captioning and multimodal question answering. We introduce a
novel federated learning framework, named Multimodal Large Language Model
Assisted Federated Learning (MLLM-LLaVA-FL), which employs powerful MLLMs at
the server end to address the heterogeneous and long-tailed challenges. Owing
to the advanced cross-modality representation capabilities and the extensive
open-vocabulary prior knowledge of MLLMs, our framework is adept at harnessing
the extensive, yet previously underexploited, open-source data accessible from
websites and powerful server-side computational resources. Hence, the
MLLM-LLaVA-FL not only enhances the performance but also avoids increasing the
risk of privacy leakage and the computational burden on local devices,
distinguishing it from prior methodologies. Our framework has three key stages.
Initially, we conduct global visual-text pretraining of the model. This
pretraining is facilitated by utilizing the extensive open-source data
available online, with the assistance of MLLMs. Subsequently, the pretrained
model is distributed among various clients for local training. Finally, once
the locally trained models are transmitted back to the server, a global
alignment is carried out under the supervision of MLLMs to further enhance the
performance. Experimental evaluations on established benchmarks, show that our
framework delivers promising performance in the typical scenarios with data
heterogeneity and long-tail distribution across different clients in FL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Strongly-polynomial time and validation analysis of policy gradient
  methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19437v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19437v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caleb Ju, Guanghui Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel termination criterion, termed the advantage gap
function, for finite state and action Markov decision processes (MDP) and
reinforcement learning (RL). By incorporating this advantage gap function into
the design of step size rules and deriving a new linear rate of convergence
that is independent of the stationary state distribution of the optimal policy,
we demonstrate that policy gradient methods can solve MDPs in
strongly-polynomial time. To the best of our knowledge, this is the first time
that such strong convergence properties have been established for policy
gradient methods. Moreover, in the stochastic setting, where only stochastic
estimates of policy gradients are available, we show that the advantage gap
function provides close approximations of the optimality gap for each
individual state and exhibits a sublinear rate of convergence at every state.
The advantage gap function can be easily estimated in the stochastic case, and
when coupled with easily computable upper bounds on policy values, they provide
a convenient way to validate the solutions generated by policy gradient
methods. Therefore, our developments offer a principled and computable measure
of optimality for RL, whereas current practice tends to rely on
algorithm-to-algorithm or baselines comparisons with no certificate of
optimality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add numerical experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BricksRL: A Platform for Democratizing Robotics and Reinforcement
  Learning Research and Education with LEGO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Dittert, Vincent Moens, Gianni De Fabritiis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present BricksRL, a platform designed to democratize access to robotics
for reinforcement learning research and education. BricksRL facilitates the
creation, design, and training of custom LEGO robots in the real world by
interfacing them with the TorchRL library for reinforcement learning agents.
The integration of TorchRL with the LEGO hubs, via Bluetooth bidirectional
communication, enables state-of-the-art reinforcement learning training on GPUs
for a wide variety of LEGO builds. This offers a flexible and cost-efficient
approach for scaling and also provides a robust infrastructure for
robot-environment-algorithm communication. We present various experiments
across tasks and robot configurations, providing built plans and training
results. Furthermore, we demonstrate that inexpensive LEGO robots can be
trained end-to-end in the real world to achieve simple tasks, with training
times typically under 120 minutes on a normal laptop. Moreover, we show how
users can extend the capabilities, exemplified by the successful integration of
non-LEGO sensors. By enhancing accessibility to both robotics and reinforcement
learning, BricksRL establishes a strong foundation for democratized robotic
learning in research and educational settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive
  Tensor Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Yang, Ziyue Liu, Samridhi Choudhary, Xinfeng Xie, Cao Gao, Siegfried Kunzmann, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large AI models such as LLMs and DLRMs costs massive GPUs and
computing time. The high training cost has become only affordable to big tech
companies, meanwhile also causing increasing concerns about the environmental
impact. This paper presents CoMERA, a Computing- and Memory-Efficient training
method via Rank-Adaptive tensor optimization. CoMERA achieves rank-adaptive
tensor-compressed (pre)-training via a multi-objective optimization formulation
and improves the training to provide both a high compression ratio and
excellent accuracy in the training process. Our optimized numerical computation
(e.g., optimized tensorized embedding and tensor-network contractions) and GPU
implementation eliminate part of the run-time overhead in the tensorized
training on GPU. This leads to, for the first time, $2-3\times$ speedup per
training epoch compared with standard training. CoMERA also outperforms the
recent GaLore in terms of both memory and computing efficiency. Specifically,
CoMERA is $2\times$ faster per training epoch and $9\times$ more
memory-efficient than GaLore on a tested six-encoder transformer with
single-batch training. Our method also shows $\sim 2\times$ speedup than
standard pre-training on a BERT-like code-generation LLM while achieving
$4.23\times$ compression ratio in pre-training. With further HPC optimization,
CoMERA may reduce the pre-training cost of many other LLMs. An implementation
of CoMERA is available at https://github.com/ziyangjoy/CoMERA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-Driven Graph Network-Based Simulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franciszek Szewczyk, Gilles Louppe, Matthia Sabatelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifelike visualizations in design, cinematography, and gaming rely on precise
physics simulations, typically requiring extensive computational resources and
detailed physical input. This paper presents a method that can infer a system's
physical properties from a short video, eliminating the need for explicit
parameter input, provided it is close to the training condition. The learned
representation is then used within a Graph Network-based Simulator to emulate
the trajectories of physical systems. We demonstrate that the video-derived
encodings effectively capture the physical properties of the system and
showcase a linear dependence between some of the encodings and the system's
motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Query-Guided <span class="highlight-title">Self-Supervised</span> Summarization of Nursing Notes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya Gao, Hans Moen, Saila Koivusalo, Miika Koskinen, Pekka Marttinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nursing notes, an important part of Electronic Health Records (EHRs), track a
patient's health during a care episode. Summarizing key information in nursing
notes can help clinicians quickly understand patients' conditions. However,
existing summarization methods in the clinical setting, especially abstractive
methods, have overlooked nursing notes and require reference summaries for
training. We introduce QGSumm, a novel query-guided self-supervised domain
adaptation approach for abstractive nursing note summarization. The method uses
patient-related clinical queries for guidance, and hence does not need
reference summaries for training. Through automatic experiments and manual
evaluation by an expert clinician, we study our approach and other
state-of-the-art Large Language Models (LLMs) for nursing note summarization.
Our experiments show: 1) GPT-4 is competitive in maintaining information in the
original nursing notes, 2) QGSumm can generate high-quality summaries with a
good balance between recall of the original content and hallucination rate
lower than other top methods. Ultimately, our work offers a new perspective on
conditional text summarization, tailored to clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artificial Intelligence Mangrove Monitoring System Based on Deep
  Learning and Sentinel-2 Satellite Data in the UAE (2017-2024) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linlin Tan, Haishan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mangroves play a crucial role in maintaining coastal ecosystem health and
protecting biodiversity. Therefore, continuous mapping of mangroves is
essential for understanding their dynamics. Earth observation imagery typically
provides a cost-effective way to monitor mangrove dynamics. However, there is a
lack of regional studies on mangrove areas in the UAE. This study utilizes the
UNet++ deep learning model combined with Sentinel-2 multispectral data and
manually annotated labels to monitor the spatiotemporal dynamics of densely
distributed mangroves (coverage greater than 70%) in the UAE from 2017 to 2024,
achieving an mIoU of 87.8% on the validation set. Results show that the total
mangrove area in the UAE in 2024 was approximately 9,142.21 hectares, an
increase of 2,061.33 hectares compared to 2017, with carbon sequestration
increasing by approximately 194,383.42 tons, equivalent to fixing about
713,367.36 tons of carbon dioxide. Abu Dhabi has the largest mangrove area and
plays a dominant role in the UAE's mangrove growth, increasing by 1,855.6
hectares between 2017-2024, while other emirates have also contributed to
mangrove expansion through stable and sustainable growth in mangrove areas.
This comprehensive growth pattern reflects the collective efforts of all
emirates in mangrove restoration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Deep Learning Based Super-Resolution For The Shallow Water
  Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Witte, Fabricio Rodrigues Lapolli, Philip Freese, Sebastian Götschel, Daniel Ruprecht, Peter Korn, Christopher Kadow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using the nonlinear shallow water equations as benchmark, we demonstrate that
a simulation with the ICON-O ocean model with a 20km resolution that is
frequently corrected by a U-net-type neural network can achieve discretization
errors of a simulation with 10km resolution. The network, originally developed
for image-based super-resolution in post-processing, is trained to compute the
difference between solutions on both meshes and is used to correct the coarse
mesh every 12h. Our setup is the Galewsky test case, modeling transition of a
barotropic instability into turbulent flow. We show that the ML-corrected
coarse resolution run correctly maintains a balance flow and captures the
transition to turbulence in line with the higher resolution simulation. After 8
day of simulation, the $L_2$-error of the corrected run is similar to a
simulation run on the finer mesh. While mass is conserved in the corrected
runs, we observe some spurious generation of kinetic energy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Compositional Text-to-Image Generation with Reliable Random
  Seeds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have demonstrated remarkable capability in
generating realistic images from arbitrary text prompts. However, they often
produce inconsistent results for compositional prompts such as "two dogs" or "a
penguin on the right of a bowl". Understanding these inconsistencies is crucial
for reliable image generation. In this paper, we highlight the significant role
of initial noise in these inconsistencies, where certain noise patterns are
more reliable for compositional prompts than others. Our analyses reveal that
different initial random seeds tend to guide the model to place objects in
distinct image areas, potentially adhering to specific patterns of camera
angles and image composition associated with the seed. To improve the model's
compositional ability, we propose a method for mining these reliable cases,
resulting in a curated training set of generated images without requiring any
manual annotation. By fine-tuning text-to-image models on these generated
images, we significantly enhance their compositional capabilities. For
numerical composition, we observe relative increases of 29.3% and 19.5% for
Stable Diffusion and PixArt-{\alpha}, respectively. Spatial composition sees
even larger gains, with 60.7% for Stable Diffusion and 21.1% for
PixArt-{\alpha}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble data assimilation to diagnose AI-based weather prediction
  model: A case with ClimaX version 0.3.1 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17781v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17781v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunji Kotsuki, Kenta Shiraishi, Atsushi Okazaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI)-based weather prediction research is growing
rapidly and has shown to be competitive with the advanced dynamic numerical
weather prediction models. However, research combining AI-based weather
prediction models with data assimilation remains limited partially because
long-term sequential data assimilation cycles are required to evaluate data
assimilation systems. This study proposes using ensemble data assimilation for
diagnosing AI-based weather prediction models, and marked the first successful
implementation of ensemble Kalman filter with AI-based weather prediction
models. Our experiments with an AI-based model ClimaX demonstrated that the
ensemble data assimilation cycled stably for the AI-based weather prediction
model using covariance inflation and localization techniques within the
ensemble Kalman filter. While ClimaX showed some limitations in capturing
flow-dependent error covariance compared to dynamical models, the AI-based
ensemble forecasts provided reasonable and beneficial error covariance in
sparsely observed regions. In addition, ensemble data assimilation revealed
that error growth based on ensemble ClimaX predictions was weaker than that of
dynamical NWP models, leading to higher inflation factors. A series of
experiments demonstrated that ensemble data assimilation can be used to
diagnose properties of AI weather prediction models such as physical
consistency and accurate error growth representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preserving Data Privacy for ML-driven Applications in Open Radio Access
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranshav Gajjar, Azuka Chiejina, Vijay K. Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning offers a promising solution to improve spectrum access
techniques by utilizing data-driven approaches to manage and share limited
spectrum resources for emerging applications. For several of these
applications, the sensitive wireless data (such as spectrograms) are stored in
a shared database or multistakeholder cloud environment and are therefore prone
to privacy leaks. This paper aims to address such privacy concerns by examining
the representative case study of shared database scenarios in 5G Open Radio
Access Network (O-RAN) networks where we have a shared database within the
near-real-time (near-RT) RAN intelligent controller. We focus on securing the
data that can be used by machine learning (ML) models for spectrum sharing and
interference mitigation applications without compromising the model and network
performances. The underlying idea is to leverage a (i) Shuffling-based
learnable encryption technique to encrypt the data, following which, (ii)
employ a custom Vision transformer (ViT) as the trained ML model that is
capable of performing accurate inferences on such encrypted data. The paper
offers a thorough analysis and comparisons with analogous convolutional neural
networks (CNN) as well as deeper architectures (such as ResNet-50) as
baselines. Our experiments showcase that the proposed approach significantly
outperforms the baseline CNN with an improvement of 24.5% and 23.9% for the
percent accuracy and F1-Score respectively when operated on encrypted data.
Though deeper ResNet-50 architecture is obtained as a slightly more accurate
model, with an increase of 4.4%, the proposed approach boasts a reduction of
parameters by 99.32%, and thus, offers a much-improved prediction time by
nearly 60%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLaPE: Gold Label-agnostic <span class="highlight-title">Prompt</span> Evaluation and Optimization for Large
  Language Model <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanchang Zhang, Zhuosheng Zhang, Hai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid progress of large language models (LLMs), their task
performance remains sensitive to prompt design. Recent studies have explored
leveraging the LLM itself as an optimizer to identify optimal prompts that
maximize task accuracy. However, when evaluating prompts, such approaches
heavily rely on elusive manually annotated gold labels to calculate task
accuracy for each candidate prompt, which hinders the widespread implementation
and generality. To overcome the limitation, this work proposes a gold
label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold
labels. Motivated by the observed correlation between self-consistency and the
accuracy of the answer, we adopt self-consistency as the initial evaluation
score. Subsequently, we refine the scores of prompts producing identical
answers to be mutually consistent. Experimental results show that GLaPE
provides reliable evaluations uniform with accuracy, even in the absence of
gold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt
optimization yields effective prompts comparable to accuracy-based ones. The
code is publicly available at https://github.com/thunderous77/GLaPE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Counterfactual Distributions via Kernel Nearest Neighbors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyuseong Choi, Jacob Feitelberg, Caleb Chin, Anish Agarwal, Raaz Dwivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider a setting with multiple units (e.g., individuals, cohorts,
geographic locations) and outcomes (e.g., treatments, times, items), where the
goal is to learn a multivariate distribution for each unit-outcome entry, such
as the distribution of a user's weekly spend and engagement under a specific
mobile app version. A common challenge is the prevalence of missing not at
random data, where observations are available only for certain unit-outcome
combinations and the observation availability can be correlated with the
properties of distributions themselves, i.e., there is unobserved confounding.
An additional challenge is that for any observed unit-outcome entry, we only
have a finite number of samples from the underlying distribution. We tackle
these two challenges by casting the problem into a novel distributional matrix
completion framework and introduce a kernel based distributional generalization
of nearest neighbors to estimate the underlying distributions. By leveraging
maximum mean discrepancies and a suitable factor model on the kernel mean
embeddings of the underlying distributions, we establish consistent recovery of
the underlying distributions even when data is missing not at random and
positivity constraints are violated. Furthermore, we demonstrate that our
nearest neighbors approach is robust to heteroscedastic noise, provided we have
access to two or more measurements for the observed unit-outcome entries, a
robustness not present in prior works on nearest neighbors with single
measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrences reveal shared causal drivers of complex time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Gilpin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmeasured causal forces influence diverse experimental time series, such as
the transcription factors that regulate genes, or the descending neurons that
steer motor circuits. Combining the theory of skew-product dynamical systems
with topological data analysis, we show that simultaneous recurrence events
across multiple time series reveal the structure of their shared unobserved
driving signal. We introduce a physics-based unsupervised learning algorithm
that reconstructs causal drivers by iteratively building a recurrence graph
with glass-like structure. As the amount of data increases, a percolation
transition on this graph leads to weak ergodicity breaking for random walks --
revealing the shared driver's dynamics, even from strongly-corrupted
measurements. We relate reconstruction accuracy to the rate of information
transfer from a chaotic driver to the response systems, and we find that
effective reconstruction proceeds through gradual approximation of the driver's
dynamical attractor. Through extensive benchmarks against classical signal
processing and machine learning techniques, we demonstrate our method's ability
to extract causal drivers from diverse experimental datasets spanning ecology,
genomics, fluid dynamics, and physiology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Physical Review X (to appear). Code available online at
  https://github.com/williamgilpin/shrec</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stock Movement Prediction with Multimodal Stable Fusion via Gated
  Cross-Attention Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Hang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate prediction of stock movements is crucial for investment
strategies. Stock prices are subject to the influence of various forms of
information, including financial indicators, sentiment analysis, news
documents, and relational structures. Predominant analytical approaches,
however, tend to address only unimodal or bimodal sources, neglecting the
complexity of multimodal data. Further complicating the landscape are the
issues of data sparsity and semantic conflicts between these modalities, which
are frequently overlooked by current models, leading to unstable performance
and limiting practical applicability. To address these shortcomings, this study
introduces a novel architecture, named Multimodal Stable Fusion with Gated
Cross-Attention (MSGCA), designed to robustly integrate multimodal input for
stock movement prediction. The MSGCA framework consists of three integral
components: (1) a trimodal encoding module, responsible for processing
indicator sequences, dynamic documents, and a relational graph, and
standardizing their feature representations; (2) a cross-feature fusion module,
where primary and consistent features guide the multimodal fusion of the three
modalities via a pair of gated cross-attention networks; and (3) a prediction
module, which refines the fused features through temporal and dimensional
reduction to execute precise movement forecasting. Empirical evaluations
demonstrate that the MSGCA framework exceeds current leading methods, achieving
performance gains of 8.1%, 6.1%, 21.7% and 31.6% on four multimodal datasets,
respectively, attributed to its enhanced multimodal fusion stability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling and Mitigating Bias in Large Language Model Recommendations: A
  Path to Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10825v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10825v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindya Bijoy Das, Shahnewaz Karim Sakib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  excel in delivering comprehensive suggestions by deeply analyzing content and
user behavior. However, they often inherit biases from skewed training data,
favoring mainstream content while underrepresenting diverse or non-traditional
options. This study explores the interplay between bias and LLM-based
recommendation systems, focusing on music, song, and book recommendations
across diverse demographic and cultural groups. This paper analyzes bias in
LLM-based recommendation systems across multiple models (GPT, LLaMA, and
Gemini), revealing its deep and pervasive impact on outcomes. Intersecting
identities and contextual factors, like socioeconomic status, further amplify
biases, complicating fair recommendations across diverse groups. Our findings
reveal that bias in these systems is deeply ingrained, yet even simple
interventions like prompt engineering can significantly reduce it. We further
propose a retrieval-augmented generation strategy to mitigate bias more
effectively. Numerical experiments validate these strategies, demonstrating
both the pervasive nature of bias and the impact of the proposed solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational autoencoders with latent high-dimensional steady geometric
  flows for dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10137v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10137v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Gracyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop Riemannian approaches to variational autoencoders (VAEs) for
PDE-type ambient data with regularizing geometric latent dynamics, which we
refer to as VAE-DLM, or VAEs with dynamical latent manifolds. We redevelop the
VAE framework such that manifold geometries, subject to our geometric flow,
embedded in Euclidean space are learned in the intermediary latent space
developed by encoders and decoders. By tailoring the geometric flow in which
the latent space evolves, we induce latent geometric properties of our
choosing, which are reflected in empirical performance. We reformulate the
traditional evidence lower bound (ELBO) loss with a considerate choice of
prior. We develop a linear geometric flow with a steady-state regularizing
term. This flow requires only automatic differentiation of one time derivative,
and can be solved in moderately high dimensions in a physics-informed approach,
allowing more expressive latent representations. We discuss how this flow can
be formulated as a gradient flow, and maintains entropy away from metric
singularity. This, along with an eigenvalue penalization condition, helps
ensure the manifold is sufficiently large in measure, nondegenerate, and a
canonical geometry, which contribute to a robust representation. Our methods
focus on the modified multi-layer perceptron architecture with tanh activations
for the manifold encoder-decoder. We demonstrate, on our datasets of interest,
our methods perform at least as well as the traditional VAE, and oftentimes
better. Our methods can outperform this and a VAE endowed with our proposed
architecture by up to 25% reduction in out-of-distribution (OOD) error and
potentially greater. We highlight our method on ambient PDEs whose solutions
maintain minimal variation in late times. We provide empirical justification
towards how we can improve robust learning for external dynamics with VAEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor fixes; added details to proofs in the appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T2Vid: Translating Long Text into Multi-Image is the Catalyst for
  Video-LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan Yang, Zuwei Long, Yuhan Dai, Tong Xu, Xing Sun, Ran He, Caifeng Shan, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Multimodal Large Language Models (MLLMs) in the image domain
has garnered wide attention from the research community. Drawing on previous
successful experiences, researchers have recently explored extending the
success to the video understanding realms. Apart from training from scratch, an
efficient way is to utilize the pre-trained image-LLMs, leading to two
mainstream approaches, i.e. zero-shot inference and further fine-tuning with
video data. In this work, our study of these approaches harvests an effective
data augmentation method. We first make a deeper inspection of the zero-shot
inference way and identify two limitations, i.e. limited generalization and
lack of temporal understanding capabilities. Thus, we further investigate the
fine-tuning approach and find a low learning efficiency when simply using all
the video data samples, which can be attributed to a lack of instruction
diversity. Aiming at this issue, we develop a method called T2Vid to synthesize
video-like samples to enrich the instruction diversity in the training corpus.
Integrating these data enables a simple and efficient training scheme, which
achieves performance comparable to or even superior to using full video
datasets by training with just 15% the sample size. Meanwhile, we find that the
proposed scheme can boost the performance of long video understanding without
training with long video samples. We hope our study will spark more thinking
about using MLLMs for video understanding and curation of high-quality data.
The code is released at https://github.com/xjtupanda/T2Vid.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/xjtupanda/T2Vid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshara Prabhakar, Yuanzhi Li, Karthik Narasimhan, Sham Kakade, Eran Malach, Samy Jelassi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient
fine-tuning of Large Language Models (LLMs). We study how different LoRA
modules can be merged to achieve skill composition -- testing the performance
of the merged model on a target task that involves combining multiple skills,
each skill coming from a single LoRA. This setup is favorable when it is
difficult to obtain training data for the target task and when it can be
decomposed into multiple skills. First, we identify practically occurring
use-cases that can be studied under the realm of skill composition, e.g.
solving hard math-word problems with code, creating a bot to answer questions
on proprietary manuals or about domain-specialized corpora. Our main
contribution is to show that concatenation of LoRAs (CAT), which optimally
weights LoRAs that were individually trained on different skills, outperforms
existing model- and data- merging techniques; for instance on math-word
problems, CAT beats these methods by an average of 43% and 12% respectively.
Thus, this paper advocates model merging as an efficient way to solve
compositional tasks and underscores CAT as a simple, compute-friendly and
effective procedure. To our knowledge, this is the first work demonstrating the
superiority of model merging over data mixing for binary skill composition
tasks. Code and data are available at https://github.com/aksh555/LoRA-Soups
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Industry track; 9 pages plus references and appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal on-chip polarization handling with deep photonic networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aycan Deniz Vit, Ujal Rzayev, Bahrem Serhat Danis, Ali Najjar Amiri, Kazim Gorgulu, Emir Salih Magden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel design paradigm for arbitrarily capable deep photonic
networks of cascaded Mach-Zehnder Interferometers (MZIs) for on-chip universal
polarization handling. Using a device architecture made of cascaded
Mach-Zehnder interferometers, we modify and train the phase difference between
interferometer arms for both polarizations through wide operation bandwidths.
Three proof-of-concept polarization handling devices are illustrated using a
software-defined, physics-informed neural framework, to achieve user-specified
target device responses as functions of polarization and wavelength. These
devices include a polarization splitter, a polarization-independent power
splitter, and an arbitrary polarization-dependent splitter to illustrate the
capabilities of the design framework. The performance for all three devices is
optimized using transfer matrix calculations; and their final responses are
verified through 3D-FDTD simulations. All devices demonstrate state-of-the-art
performance metrics with over 20 dB extinction, and flat-top transmission bands
through bandwidths of 120 nm. In addition to the functional diversity enabled,
the optimization for each device is completed in under a minute, highlighting
the computational efficiency of the design paradigm presented. These results
demonstrate the versatility of the deep photonic network design ecosystem in
polarization management, unveiling promising prospects for advanced on-chip
applications in optical communications, sensing, and computing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Critical Tokens Matter: Token-Level Contrastive Estimation Enhances
  LLM's Reasoning Capability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited remarkable performance on
reasoning tasks. They utilize autoregressive token generation to construct
reasoning trajectories, enabling the development of a coherent chain of
thought. In this work, we explore the impact of individual tokens on the final
outcomes of reasoning tasks. We identify the existence of ``critical tokens''
that lead to incorrect reasoning trajectories in LLMs. Specifically, we find
that LLMs tend to produce positive outcomes when forced to decode other tokens
instead of critical tokens. Motivated by this observation, we propose a novel
approach - cDPO - designed to automatically recognize and conduct token-level
rewards for the critical tokens during the alignment process. Specifically, we
develop a contrastive estimation approach to automatically identify critical
tokens. It is achieved by comparing the generation likelihood of positive and
negative models. To achieve this, we separately fine-tune the positive and
negative models on various reasoning trajectories, consequently, they are
capable of identifying identify critical tokens within incorrect trajectories
that contribute to erroneous outcomes. Moreover, to further align the model
with the critical token information during the alignment process, we extend the
conventional DPO algorithms to token-level DPO and utilize the differential
likelihood from the aforementioned positive and negative model as important
weight for token-level DPO learning.Experimental results on GSM8K and MATH500
benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math
(7B) demonstrate the effectiveness of the propsoed approach cDPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fighting Bias with Bias: A Machine Learning Approach to Assess Human
  Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18122v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18122v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanxue Dong, Maria De-arteaga, Maytal Saar-Tsechansky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biased human decisions have consequential impacts across various domains,
yielding unfair treatment of individuals and resulting in suboptimal outcomes
for organizations and society. In recognition of this fact, organizations
regularly design and deploy interventions aimed at mitigating these biases.
However, measuring human decision biases remains an important but elusive task.
Organizations are frequently concerned with mistaken decisions
disproportionately affecting one group. In practice, however, this is typically
not possible to assess due to the scarcity of a gold standard: a label that
indicates what the correct decision would have been. In this work, we propose a
machine learning-based framework to assess bias in human-generated decisions
when gold standard labels are scarce. We provide theoretical guarantees and
empirical evidence demonstrating the superiority of our method over existing
alternatives. This proposed methodology establishes a foundation for
transparency in human decision-making, carrying substantial implications for
managerial duties, and offering potential for alleviating algorithmic biases
when human decisions are used as labels to train algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GPU-Accelerated Counterfactual Regret Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14778v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14778v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juho Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual regret minimization is a family of algorithms of no-regret
learning dynamics capable of solving large-scale imperfect information games.
We propose implementing this algorithm as a series of dense and sparse matrix
and vector operations, thereby making it highly parallelizable for a graphical
processing unit, at a cost of higher memory usage. Our experiments show that
our implementation performs up to about 401.2 times faster than OpenSpiel's
Python implementation and, on an expanded set of games, up to about 203.6 times
faster than OpenSpiel's C++ implementation and the speedup becomes more
pronounced as the size of the game being solved grows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Realizable Continuous-Space Shields for Safe Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungmin Kim, Davide Corsi, Andoni Rodriguez, JB Lanier, Benjami Parellada, Pierre Baldi, Cesar Sanchez, Roy Fox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Deep Reinforcement Learning (DRL) has achieved remarkable success
across various domains, it remains vulnerable to occasional catastrophic
failures without additional safeguards. An effective solution to prevent these
failures is to use a shield that validates and adjusts the agent's actions to
ensure compliance with a provided set of safety specifications. For real-world
robotic domains, it is essential to define safety specifications over
continuous state and action spaces to accurately account for system dynamics
and compute new actions that minimally deviate from the agent's original
decision. In this paper, we present the first shielding approach specifically
designed to ensure the satisfaction of safety requirements in continuous state
and action spaces, making it suitable for practical robotic applications. Our
method builds upon realizability, an essential property that confirms the
shield will always be able to generate a safe action for any state in the
environment. We formally prove that realizability can be verified for stateful
shields, enabling the incorporation of non-Markovian safety requirements, such
as loop avoidance. Finally, we demonstrate the effectiveness of our approach in
ensuring safety without compromising the policy's success rate by applying it
to a navigation problem and a multi-agent particle environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Kim, Corsi, and Rodriguez contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Understanding Domain Adapted Sentence Embeddings for Document
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujoy Roychowdhury, Sumit Soman, H. G. Ranjani, Vansh Chhabra, Neeraj Gunda, Shashank Gautam, Subhadip Bandyopadhyay, Sai Krishna Bala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A plethora of sentence embedding models makes it challenging to choose one,
especially for technical domains rich with specialized vocabulary. In this
work, we domain adapt embeddings using telecom, health and science datasets for
question answering. We evaluate embeddings obtained from publicly available
models and their domain-adapted variants, on both point retrieval accuracies,
as well as their (95\%) confidence intervals. We establish a systematic method
to obtain thresholds for similarity scores for different embeddings. As
expected, we observe that fine-tuning improves mean bootstrapped accuracies. We
also observe that it results in tighter confidence intervals, which further
improve when pre-training is preceded by fine-tuning. We introduce metrics
which measure the distributional overlaps of top-$K$, correct and random
document similarities with the question. Further, we show that these metrics
are correlated with retrieval accuracy and similarity thresholds. Recent
literature shows conflicting effects of isotropy on retrieval accuracies. Our
experiments establish that the isotropy of embeddings (as measured by two
independent state-of-the-art isotropy metric definitions) is poorly correlated
with retrieval performance. We show that embeddings for domain-specific
sentences have little overlap with those for domain-agnostic ones, and
fine-tuning moves them further apart. Based on our results, we provide
recommendations for use of our methodology and metrics by researchers and
practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TTSDS -- Text-to-Speech Distribution Score 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12707v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12707v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Minixhofer, Ondřej Klejch, Peter Bell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recently published Text-to-Speech (TTS) systems produce audio close to
real speech. However, TTS evaluation needs to be revisited to make sense of the
results obtained with the new architectures, approaches and datasets. We
propose evaluating the quality of synthetic speech as a combination of multiple
factors such as prosody, speaker identity, and intelligibility. Our approach
assesses how well synthetic speech mirrors real speech by obtaining correlates
of each factor and measuring their distance from both real speech datasets and
noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and
show that our score computed as an unweighted average of factors strongly
correlates with the human evaluations from each time period.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Acceleration of Nesterov's Accelerated Gradient for Rectangular
  Matrix Factorization and Linear Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenghao Xu, Yuqing Wang, Tuo Zhao, Rachel Ward, Molei Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the convergence rate of first-order methods for rectangular matrix
factorization, which is a canonical nonconvex optimization problem.
Specifically, given a rank-$r$ matrix $\mathbf{A}\in\mathbb{R}^{m\times n}$, we
prove that gradient descent (GD) can find a pair of $\epsilon$-optimal
solutions $\mathbf{X}_T\in\mathbb{R}^{m\times d}$ and
$\mathbf{Y}_T\in\mathbb{R}^{n\times d}$, where $d\geq r$, satisfying
$\lVert\mathbf{X}_T\mathbf{Y}_T^\top-\mathbf{A}\rVert_\mathrm{F}\leq\epsilon\lVert\mathbf{A}\rVert_\mathrm{F}$
in $T=O(\kappa^2\log\frac{1}{\epsilon})$ iterations with high probability,
where $\kappa$ denotes the condition number of $\mathbf{A}$. Furthermore, we
prove that Nesterov's accelerated gradient (NAG) attains an iteration
complexity of $O(\kappa\log\frac{1}{\epsilon})$, which is the best-known bound
of first-order methods for rectangular matrix factorization. Different from
small balanced random initialization in the existing literature, we adopt an
unbalanced initialization, where $\mathbf{X}_0$ is large and $\mathbf{Y}_0$ is
$0$. Moreover, our initialization and analysis can be further extended to
linear neural networks, where we prove that NAG can also attain an accelerated
linear convergence rate. In particular, we only require the width of the
network to be greater than or equal to the rank of the output label matrix. In
contrast, previous results achieving the same rate require excessive widths
that additionally depend on the condition number and the rank of the input data
matrix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages (checklist included)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow
  Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungbin Cho, Junwan Kim, Jisoo Kim, Minseo Kim, Mingu Kang, Sungeun Hong, Tae-Hyun Oh, Youngjae Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion, inherently continuous and dynamic, presents significant
challenges for generative models. Despite their dominance, discrete
quantization methods, such as VQ-VAEs, suffer from inherent limitations,
including restricted expressiveness and frame-wise noise artifacts. Continuous
approaches, while producing smoother and more natural motions, often falter due
to high-dimensional complexity and limited training data. To resolve this
"discord" between discrete and continuous representations, we introduce
DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a
novel method that decodes discrete motion tokens into continuous motion through
rectified flow. By employing an iterative refinement process in the continuous
space, DisCoRD captures fine-grained dynamics and ensures smoother and more
natural motions. Compatible with any discrete-based framework, our method
enhances naturalness without compromising faithfulness to the conditioning
signals. Extensive evaluations demonstrate that DisCoRD achieves
state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on
KIT-ML. These results solidify DisCoRD as a robust solution for bridging the
divide between discrete efficiency and continuous realism. Our project page is
available at: https://whwjdqls.github.io/discord.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Statistical Framework of Watermarks for Large Language Models: Pivot,
  Detection Efficiency and Optimal Rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01245v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01245v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since ChatGPT was introduced in November 2022, embedding (nearly)
unnoticeable statistical signals into text generated by large language models
(LLMs), also known as watermarking, has been used as a principled approach to
provable detection of LLM-generated text from its human-written counterpart. In
this paper, we introduce a general and flexible framework for reasoning about
the statistical efficiency of watermarks and designing powerful detection
rules. Inspired by the hypothesis testing formulation of watermark detection,
our framework starts by selecting a pivotal statistic of the text and a secret
key -- provided by the LLM to the verifier -- to enable controlling the false
positive rate (the error of mistakenly detecting human-written text as
LLM-generated). Next, this framework allows one to evaluate the power of
watermark detection rules by obtaining a closed-form expression of the
asymptotic false negative rate (the error of incorrectly classifying
LLM-generated text as human-written). Our framework further reduces the problem
of determining the optimal detection rule to solving a minimax optimization
program. We apply this framework to two representative watermarks -- one of
which has been internally implemented at OpenAI -- and obtain several findings
that can be instrumental in guiding the practice of implementing watermarks. In
particular, we derive optimal detection rules for these watermarks under our
framework. These theoretically derived detection rules are demonstrated to be
competitive and sometimes enjoy a higher power than existing detection
approaches through numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Annals of Statistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved AdaBoost for Virtual Reality Experience Prediction Based on
  Long Short-Term Memory Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Fan, Zhicheng Ding, Ruixin Huang, Chang Zhou, Xuyang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A classification prediction algorithm based on Long Short-Term Memory Network
(LSTM) improved AdaBoost is used to predict virtual reality (VR) user
experience. The dataset is randomly divided into training and test sets in the
ratio of 7:3.During the training process, the model's loss value decreases from
0.65 to 0.31, which shows that the model gradually reduces the discrepancy
between the prediction results and the actual labels, and improves the accuracy
and generalisation ability.The final loss value of 0.31 indicates that the
model fits the training data well, and is able to make predictions and
classifications more accurately. The confusion matrix for the training set
shows a total of 177 correct predictions and 52 incorrect predictions, with an
accuracy of 77%, precision of 88%, recall of 77% and f1 score of 82%. The
confusion matrix for the test set shows a total of 167 correct and 53 incorrect
predictions with 75% accuracy, 87% precision, 57% recall and 69% f1 score. In
summary, the classification prediction algorithm based on LSTM with improved
AdaBoost shows good prediction ability for virtual reality user experience.
This study is of great significance to enhance the application of virtual
reality technology in user experience. By combining LSTM and AdaBoost
algorithms, significant progress has been made in user experience prediction,
which not only improves the accuracy and generalisation ability of the model,
but also provides useful insights for related research in the field of virtual
reality. This approach can help developers better understand user requirements,
optimise virtual reality product design, and enhance user satisfaction,
promoting the wide application of virtual reality technology in various fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been peer-reviewed in The 2nd International Conference
  on Software Engineering and Machine Learning and published in Applied and
  Computational Engineering, DOI:
  https://doi.org/10.54254/2755-2721/77/20240678</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating LLMs for Hardware Design and Test 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Blocklove, Siddharth Garg, Ramesh Karri, Hammond Pearce
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated capabilities for producing
code in Hardware Description Languages (HDLs). However, most of the focus
remains on their abilities to write functional code, not test code. The
hardware design process consists of both design and test, and so eschewing
validation and verification leaves considerable potential benefit unexplored,
given that a design and test framework may allow for progress towards full
automation of the digital design pipeline. In this work, we perform one of the
first studies exploring how a LLM can both design and test hardware modules
from provided specifications. Using a suite of 8 representative benchmarks, we
examined the capabilities and limitations of the state-of-the-art
conversational LLMs when producing Verilog for functional and verification
purposes. We taped out the benchmarks on a Skywater 130nm shuttle and received
the functional chip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Dynamics: Vehicle Dynamics Modeling with a Physics-Constrained
  Neural Network for Autonomous Racing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Chrosniak, Jingyun Ning, Madhur Behl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous racing is a critical research area for autonomous driving,
presenting significant challenges in vehicle dynamics modeling, such as
balancing model precision and computational efficiency at high speeds
(>280km/h), where minor errors in modeling have severe consequences. Existing
physics-based models for vehicle dynamics require elaborate testing setups and
tuning, which are hard to implement, time-intensive, and cost-prohibitive.
Conversely, purely data-driven approaches do not generalize well and cannot
adequately ensure physical constraints on predictions. This paper introduces
Deep Dynamics, a physics-constrained neural network (PCNN) for vehicle dynamics
modeling of an autonomous racecar. It combines physics coefficient estimation
and dynamical equations to accurately predict vehicle states at high speeds and
includes a unique Physics Guard layer to ensure internal coefficient estimates
remain within their nominal physical ranges. Open-loop and closed-loop
performance assessments, using a physics-based simulator and full-scale
autonomous Indy racecar data, highlight Deep Dynamics as a promising approach
for modeling racecar vehicle dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the IEEE Robotics and Automation Letters and presented
  at the IEEE International Conference on Intelligent Robots and Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noisy Nonnegative Tucker Decomposition with Sparse Factors and Missing
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiongjun Zhang, Michael K. Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensor decomposition is a powerful tool for extracting physically meaningful
latent factors from multi-dimensional nonnegative data, and has been an
increasing interest in a variety of fields such as image processing, machine
learning, and computer vision. In this paper, we propose a sparse nonnegative
Tucker decomposition and completion method for the recovery of underlying
nonnegative data under noisy observations. Here the underlying nonnegative data
tensor is decomposed into a core tensor and several factor matrices with all
entries being nonnegative and the factor matrices being sparse. The loss
function is derived by the maximum likelihood estimation of the noisy
observations, and the $\ell_0$ norm is employed to enhance the sparsity of the
factor matrices. We establish the error bound of the estimator of the proposed
model under generic noise scenarios, which is then specified to the
observations with additive Gaussian noise, additive Laplace noise, and Poisson
observations, respectively. Our theoretical results are better than those by
existing tensor-based or matrix-based methods. Moreover, the minimax lower
bounds are shown to be matched with the derived upper bounds up to logarithmic
factors. Numerical examples on both synthetic and real-world data sets
demonstrate the superiority of the proposed method for nonnegative tensor data
completion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Block Coordinate Descent Method for Nonsmooth Composite Optimization
  under Orthogonality Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03641v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03641v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganzhao Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonsmooth composite optimization with orthogonality constraints has a wide
range of applications in statistical learning and data science. However, this
problem is challenging due to its nonsmooth objective and computationally
expensive, non-convex constraints. In this paper, we propose a new approach
called \textbf{OBCD}, which leverages Block Coordinate Descent to address these
challenges. \textbf{OBCD} is a feasible method with a small computational
footprint. In each iteration, it updates $k$ rows of the solution matrix, where
$k \geq 2$, by globally solving a small nonsmooth optimization problem under
orthogonality constraints. We prove that the limiting points of \textbf{OBCD},
referred to as (global) block-$k$ stationary points, offer stronger optimality
than standard critical points. Furthermore, we show that \textbf{OBCD}
converges to $\epsilon$-block-$k$ stationary points with an ergodic convergence
rate of $\mathcal{O}(1/\epsilon)$. Additionally, under the Kurdyka-Lojasiewicz
(KL) inequality, we establish the non-ergodic convergence rate of
\textbf{OBCD}. We also extend \textbf{OBCD} by incorporating breakpoint
searching methods for subproblem solving and greedy strategies for working set
selection. Comprehensive experiments demonstrate the superior performance of
our approach across various tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling more efficient and cost-effective AI/ML systems with Collective
  Mind, virtualized MLOps, MLPerf, Collective Knowledge Playground and
  reproducible optimization tournaments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grigori Fursin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This white paper introduces my educational community initiative to learn how
to run AI, ML and other emerging workloads in the most efficient and
cost-effective way across diverse models, data sets, software and hardware.
This project leverages Collective Mind (CM), virtualized MLOps and DevOps
(CM4MLOps), MLPerf benchmarks, and the Collective Knowledge playground (CK),
which I have developed in collaboration with the community and MLCommons.
  I created Collective Mind as a small and portable Python package with minimal
dependencies, a unified CLI and Python API to help researchers and engineers
automate repetitive, tedious, and time-consuming tasks. I also designed CM as a
distributed framework, continuously enhanced by the community through the CM4*
repositories, which function as the unified interface for organizing and
managing various collections of automations and artifacts. For example,
CM4MLOps repository includes many automations, also known as CM scripts, to
streamline the process of building, running, benchmarking, and optimizing AI,
ML, and other workflows across ever-evolving models, data, and systems.
  I donated CK, CM and CM4MLOps to MLCommons to foster collaboration between
academia and industry to learn how to co-design more efficient and
cost-effective AI systems while capturing and encoding knowledge within
Collective Mind, protecting intellectual property, enabling portable skills,
and accelerating the transition of the state-of-the-art research into
production. My ultimate goal is to collaborate with the community to complete
my two-decade journey toward creating self-optimizing software and hardware
that can automatically learn how to run any workload in the most efficient and
cost-effective manner based on user requirements and constraints such as cost,
latency, throughput, accuracy, power consumption, size, and other critical
factors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Information Theoretic Approach to Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01401v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01401v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Foster, Kyle Fogarty, Stefan Schoepf, Zack Dugue, Cengiz Öztireli, Alexandra Brintrup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To comply with AI and data regulations, the need to forget private or
copyrighted information from trained machine learning models is increasingly
important. The key challenge in unlearning is forgetting the necessary data in
a timely manner, while preserving model performance. In this work, we address
the zero-shot unlearning scenario, whereby an unlearning algorithm must be able
to remove data given only a trained model and the data to be forgotten. We
explore unlearning from an information theoretic perspective, connecting the
influence of a sample to the information gain a model receives by observing it.
From this, we derive a simple but principled zero-shot unlearning method based
on the geometry of the model. Our approach takes the form of minimising the
gradient of a learned function with respect to a small neighbourhood around a
target forget point. This induces a smoothing effect, causing forgetting by
moving the boundary of the classifier. We explore the intuition behind why this
approach can jointly unlearn forget samples while preserving general model
performance through a series of low-dimensional experiments. We perform
extensive empirical evaluation of our method over a range of contemporary
benchmarks, verifying that our method is competitive with state-of-the-art
performance under the strict constraints of zero-shot unlearning. Code for the
project can be found at
https://github.com/jwf40/Information-Theoretic-Unlearning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated, new low-dimensional experiments and updated perspective on
  unlearning from an information theoretic view</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with
  Diffusion Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17550v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17550v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenpeng Du, Qi Chen, Tianyu He, Xu Tan, Xie Chen, Kai Yu, Sheng Zhao, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent research has made significant progress in speech-driven talking
face generation, the quality of the generated video still lags behind that of
real recordings. One reason for this is the use of handcrafted intermediate
representations like facial landmarks and 3DMM coefficients, which are designed
based on human knowledge and are insufficient to precisely describe facial
movements. Additionally, these methods require an external pretrained model for
extracting these representations, whose performance sets an upper bound on
talking face generation. To address these limitations, we propose a novel
method called DAE-Talker that leverages data-driven latent representations
obtained from a diffusion autoencoder (DAE). DAE contains an image encoder that
encodes an image into a latent vector and a DDIM image decoder that
reconstructs the image from it. We train our DAE on talking face video frames
and then extract their latent representations as the training target for a
Conformer-based speech2latent model. This allows DAE-Talker to synthesize full
video frames and produce natural head movements that align with the content of
speech, rather than relying on a predetermined head pose from a template video.
We also introduce pose modelling in speech2latent for pose controllability.
Additionally, we propose a novel method for generating continuous video frames
with the DDIM image decoder trained on individual frames, eliminating the need
for modelling the joint distribution of consecutive frames directly. Our
experiments show that DAE-Talker outperforms existing popular methods in
lip-sync, video fidelity, and pose naturalness. We also conduct ablation
studies to analyze the effectiveness of the proposed techniques and demonstrate
the pose controllability of DAE-Talker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-01T00:00:00Z">2024-12-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruction Tuning for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10792v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10792v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper surveys research works in the quickly advancing field of
instruction tuning (IT), which can also be referred to as supervised
fine-tuning (SFT)\footnote{In this paper, unless specified otherwise,
supervised fine-tuning (SFT) and instruction tuning (IT) are used
interchangeably.}, a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users' objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of SFT, the
construction of SFT datasets, the training of SFT models, and applications to
different modalities, domains and application, along with analysis on aspects
that influence the outcome of SFT (e.g., generation of instruction outputs,
size of the instruction dataset, etc). We also review the potential pitfalls of
SFT along with criticism against it, along with efforts pointing out current
deficiencies of existing strategies and suggest some avenues for fruitful
research. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V5; Last update: Dec. 1, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CineXDrama: Relevance Detection and Sentiment Analysis of Bangla YouTube
  Comments on Movie-Drama using <span class="highlight-title">Transformer</span>s: Insights from Interpretability
  Tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usafa Akther Rifa, Pronay Debnath, Busra Kamal Rafa, Shamaun Safa Hridi, Md. Aminur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, YouTube has become the leading platform for Bangla movies
and dramas, where viewers express their opinions in comments that convey their
sentiments about the content. However, not all comments are relevant for
sentiment analysis, necessitating a filtering mechanism. We propose a system
that first assesses the relevance of comments and then analyzes the sentiment
of those deemed relevant. We introduce a dataset of 14,000 manually collected
and preprocessed comments, annotated for relevance (relevant or irrelevant) and
sentiment (positive or negative). Eight transformer models, including
BanglaBERT, were used for classification tasks, with BanglaBERT achieving the
highest accuracy (83.99% for relevance detection and 93.3% for sentiment
analysis). The study also integrates LIME to interpret model decisions,
enhancing transparency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Fifth International Conference on
  Advances in Electrical, Computing, Communications and Sustainable
  Technologies (ICAECT 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counting Like <span class="highlight-title">Transformer</span>s: Compiling Temporal Counting Logic Into
  Softmax <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Yang, David Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deriving formal bounds on the expressivity of transformers, as well as
studying transformers that are constructed to implement known algorithms, are
both effective methods for better understanding the computational power of
transformers. Towards both ends, we introduce the temporal counting logic
$\textsf{K}_\text{t}$[#] alongside the RASP variant $\textsf{C-RASP}$. We show
they are equivalent to each other, and that together they are the best-known
lower bound on the formal expressivity of future-masked soft attention
transformers with unbounded input size. We prove this by showing all
$\textsf{K}_\text{t}$[#] formulas can be compiled into these transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPCSpell: A <span class="highlight-title">Transformer</span>-based Detector-Purificator-Corrector Framework
  for Spelling Error Correction of Bangla and Resource Scarce Indic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.03730v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.03730v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehedi Hasan Bijoy, Nahid Hossain, Salekul Islam, Swakkhar Shatabda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spelling error correction is the task of identifying and rectifying
misspelled words in texts. It is a potential and active research topic in
Natural Language Processing because of numerous applications in human language
understanding. The phonetically or visually similar yet semantically distinct
characters make it an arduous task in any language. Earlier efforts on spelling
error correction in Bangla and resource-scarce Indic languages focused on
rule-based, statistical, and machine learning-based methods which we found
rather inefficient. In particular, machine learning-based approaches, which
exhibit superior performance to rule-based and statistical methods, are
ineffective as they correct each character regardless of its appropriateness.
In this paper, we propose a novel detector-purificator-corrector framework,
DPCSpell based on denoising transformers by addressing previous issues. In
addition to that, we present a method for large-scale corpus creation from
scratch which in turn resolves the resource limitation problem of any
left-to-right scripted language. The empirical outcomes demonstrate the
effectiveness of our approach, which outperforms previous state-of-the-art
methods by attaining an exact match (EM) score of 94.78%, a precision score of
0.9487, a recall score of 0.9478, an f1 score of 0.948, an f0.5 score of
0.9483, and a modified accuracy (MA) score of 95.16% for Bangla spelling error
correction. The models and corpus are publicly available at
https://tinyurl.com/DPCSpell.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 4 figures, and 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical <span class="highlight-title">Prompt</span>ing Taxonomy: A Universal Evaluation Framework for
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12644v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12644v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devichand Budagam, Ashutosh Kumar, Mahsa Khoshnoodi, Sankalp KJ, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the effectiveness of large language models (LLMs) in performing
different tasks is crucial for understanding their strengths and weaknesses.
This paper presents the Hierarchical Prompting Taxonomy (HPT), grounded on
human cognitive principles and designed to assess LLMs by examining the
cognitive demands of various tasks. The HPT uses the Hierarchical Prompting
Framework (HPF), a prompt selection framework that organizes five distinct
prompting strategies by their cognitive load on LLMs. This study introduces the
Hierarchical Prompting Index (HPI) to measure task complexity, which
demonstrates LLMs' abilities across different datasets and serves as a
universal metric for task complexity. The HPT offers a reliable method for
evaluating LLMs' problem-solving skills in diverse scenarios, leading to
clearer conclusions. Extensive experiments with multiple datasets and LLMs show
that the HPF enhances LLM performance by 2\% to 63\% compared to standard
benchmark datasets, confirming the effectiveness of the HPT. To support future
research in this domain, the implementations of HPT and HPF are publicly
available
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">BERT</span> or FastText? A Comparative Analysis of Contextual as well as
  Non-Contextual Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhay Shanbhag, Suramya Jadhav, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) for low-resource languages presents
significant challenges, particularly due to the scarcity of high-quality
annotated data and linguistic resources. The choice of embeddings plays a
critical role in enhancing the performance of NLP tasks, such as news
classification, sentiment analysis, and hate speech detection, especially for
low-resource languages like Marathi. In this study, we investigate the impact
of various embedding techniques- Contextual BERT-based, Non-Contextual
BERT-based, and FastText-based on NLP classification tasks specific to the
Marathi language. Our research includes a thorough evaluation of both
compressed and uncompressed embeddings, providing a comprehensive overview of
how these embeddings perform across different scenarios. Specifically, we
compare two BERT model embeddings, Muril and MahaBERT, as well as two FastText
model embeddings, IndicFT and MahaFT. Our evaluation includes applying
embeddings to a Multiple Logistic Regression (MLR) classifier for task
performance assessment, as well as TSNE visualizations to observe the spatial
distribution of these embeddings. The results demonstrate that contextual
embeddings outperform non-contextual embeddings. Furthermore, BERT-based
non-contextual embeddings extracted from the first BERT embedding layer yield
better results than FastText-based embeddings, suggesting a potential
alternative to FastText embeddings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieving Implicit and Explicit Emotional Events Using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19128v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19128v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guimin Hu, Hasti Seifi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have garnered significant attention in recent
years due to their impressive performance. While considerable research has
evaluated these models from various perspectives, the extent to which LLMs can
perform implicit and explicit emotion retrieval remains largely unexplored. To
address this gap, this study investigates LLMs' emotion retrieval capabilities
in commonsense. Through extensive experiments involving multiple models, we
systematically evaluate the ability of LLMs on emotion retrieval. Specifically,
we propose a supervised contrastive probing method to verify LLMs' performance
for implicit and explicit emotion retrieval, as well as the diversity of the
emotional events they retrieve. The results offer valuable insights into the
strengths and limitations of LLMs in handling emotion retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking Korean Verbs: A User-Friendly Exploration into the Verb
  Lexicon <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seohyun Song, Eunkyul Leah Jo, Yige Chen, Jeen-Pyo Hong, Kyuwon Kim, Jin Wee, Miyoung Kang, KyungTae Lim, Jungyeul Park, Chulwoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Sejong dictionary dataset offers a valuable resource, providing extensive
coverage of morphology, syntax, and semantic representation. This dataset can
be utilized to explore linguistic information in greater depth. The labeled
linguistic structures within this dataset form the basis for uncovering
relationships between words and phrases and their associations with target
verbs. This paper introduces a user-friendly web interface designed for the
collection and consolidation of verb-related information, with a particular
focus on subcategorization frames. Additionally, it outlines our efforts in
mapping this information by aligning subcategorization frames with
corresponding illustrative sentence examples. Furthermore, we provide a Python
library that would simplify syntactic parsing and semantic role labeling. These
tools are intended to assist individuals interested in harnessing the Sejong
dictionary dataset to develop applications for Korean language processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 System Demonstrations (Submitted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ManiTweet: A New Benchmark for Identifying Manipulation of News on
  Social Media <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14225v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14225v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kung-Hsiang Huang, Hou Pong Chan, Kathleen McKeown, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considerable advancements have been made to tackle the misrepresentation of
information derived from reference articles in the domains of fact-checking and
faithful summarization. However, an unaddressed aspect remains - the
identification of social media posts that manipulate information within
associated news articles. This task presents a significant challenge, primarily
due to the prevalence of personal opinions in such posts. We present a novel
task, identifying manipulation of news on social media, which aims to detect
manipulation in social media posts and identify manipulated or inserted
information. To study this task, we have proposed a data collection schema and
curated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and
corresponding articles. Our analysis demonstrates that this task is highly
challenging, with large language models (LLMs) yielding unsatisfactory
performance. Additionally, we have developed a simple yet effective basic model
that outperforms LLMs significantly on the ManiTweet dataset. Finally, we have
conducted an exploratory analysis of human-written tweets, unveiling intriguing
connections between manipulation and the domain and factuality of news
articles, as well as revealing that manipulated sentences are more likely to
encapsulate the main story or consequences of a news outlet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval Augmented Instruction Tuning for Open NER with Large Language
  Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyu Xie, Jian Zhang, Yan Zhang, Yuanyuan Liang, Qi Li, Hongwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The strong capability of large language models (LLMs) has been applied to
information extraction (IE) through either retrieval augmented prompting or
instruction tuning (IT). However, the best way to incorporate information with
LLMs for IE remains an open question. In this paper, we explore Retrieval
Augmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named
entity recognition (NER). Specifically, for each training sample, we retrieve
semantically similar examples from the training dataset as the context and
prepend them to the input of the original instruction. To evaluate our RA-IT
approach more thoroughly, we construct a Chinese IT dataset for open NER and
evaluate RA-IT in both English and Chinese scenarios. Experimental results
verify the effectiveness of RA-IT across various data sizes and in both English
and Chinese scenarios. We also conduct thorough studies to explore the impacts
of various retrieval strategies in the proposed RA-IT framework. Code and data
are available at: https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be appeared at COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Human-Centric LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14491v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14491v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yi Wang, Nicholas Sukiennik, Tong Li, Weikang Su, Qianyue Hao, Jingbo Xu, Zihan Huang, Fengli Xu, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of large language models (LLMs) and their capacity to
simulate human cognition and behavior has given rise to LLM-based frameworks
and tools that are evaluated and applied based on their ability to perform
tasks traditionally performed by humans, namely those involving cognition,
decision-making, and social interaction. This survey provides a comprehensive
examination of such human-centric LLM capabilities, focusing on their
performance in both individual tasks (where an LLM acts as a stand-in for a
single human) and collective tasks (where multiple LLMs coordinate to mimic
group dynamics). We first evaluate LLM competencies across key areas including
reasoning, perception, and social cognition, comparing their abilities to
human-like skills. Then, we explore real-world applications of LLMs in
human-centric domains such as behavioral science, political science, and
sociology, assessing their effectiveness in replicating human behaviors and
interactions. Finally, we identify challenges and future research directions,
such as improving LLM adaptability, emotional intelligence, and cultural
sensitivity, while addressing inherent biases and enhancing frameworks for
human-AI collaboration. This survey aims to provide a foundational
understanding of LLMs from a human-centric perspective, offering insights into
their current capabilities and potential for future development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Circuit Complexity Bounds for RoPE-based <span class="highlight-title">Transformer</span> Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Chen, Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterizing the express power of the Transformer architecture is critical
to understanding its capacity limits and scaling law. Recent works provide the
circuit complexity bounds to Transformer-like architecture. On the other hand,
Rotary Position Embedding ($\mathsf{RoPE}$) has emerged as a crucial technique
in modern large language models, offering superior performance in capturing
positional information compared to traditional position embeddings, which shows
great potential in application prospects, particularly for the long context
scenario. Empirical evidence also suggests that $\mathsf{RoPE}$-based
Transformer architectures demonstrate greater generalization capabilities
compared to conventional Transformer models. In this work, we establish a
circuit complexity bound for Transformers with $\mathsf{RoPE}$ attention. Our
key contribution is that we show that unless $\mathsf{TC}^0 = \mathsf{NC}^1$, a
$\mathsf{RoPE}$-based Transformer with $\mathrm{poly}(n)$-precision, $O(1)$
layers, hidden dimension $d \leq O(n)$ cannot solve the Arithmetic formula
evaluation problem or the Boolean formula value problem. This result
significantly demonstrates the fundamental limitation of the expressivity of
the $\mathsf{RoPE}$-based Transformer architecture, although it achieves giant
empirical success. Our theoretical result not only establishes the complexity
bound but also may instruct further work on the $\mathsf{RoPE}$-based
Transformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models
  for Integrated Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Yu, Zhengyuan Yang, Lingfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MM-Vet, with open-ended vision-language questions targeting at evaluating
integrated capabilities, has become one of the most popular benchmarks for
large multimodal model evaluation. MM-Vet assesses six core vision-language
(VL) capabilities: recognition, knowledge, spatial awareness, language
generation, OCR, and math. However, its question format is restricted to single
image-text pairs, lacking the interleaved image and text sequences prevalent in
real-world scenarios. To address this limitation, we introduce MM-Vet v2, which
includes a new VL capability called "image-text sequence understanding",
evaluating models' ability to process VL sequences. Furthermore, we maintain
the high quality of evaluation samples while further expanding the evaluation
set size. Using MM-Vet v2 to benchmark large multimodal models, we found that
Claude 3.5 Sonnet is the best model with a score of 71.8, slightly
outperforming GPT-4o which scored 71.0. Among open-weight models,
InternVL2-Llama3-76B leads with a score of 68.4. The code, data, and
leaderboard are accessible at https://github.com/yuweihao/MM-Vet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, data and leaderboard: https://github.com/yuweihao/MM-Vet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02490v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02490v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MM-Vet, an evaluation benchmark that examines large multimodal
models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various
intriguing abilities, such as solving math problems written on the blackboard,
reasoning about events and celebrities in news images, and explaining visual
jokes. Rapid model advancements pose challenges to evaluation benchmark
development. Problems include: (1) How to systematically structure and evaluate
the complicated multimodal tasks; (2) How to design evaluation metrics that
work well across question and answer types; and (3) How to give model insights
beyond a simple performance ranking. To this end, we present MM-Vet, designed
based on the insight that the intriguing ability to solve complicated tasks is
often achieved by a generalist model being able to integrate different core
vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and
examines the 16 integrations of interest derived from the capability
combination. For evaluation metrics, we propose an LLM-based evaluator for
open-ended outputs. The evaluator enables the evaluation across different
question types and answer styles, resulting in a unified scoring metric. We
evaluate representative LMMs on MM-Vet, providing insights into the
capabilities of different LMM system paradigms and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024. Code, data and leaderboard:
  https://github.com/yuweihao/MM-Vet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Batch Calibration: Rethinking Calibration for In-Context Learning and
  <span class="highlight-title">Prompt</span> Engineering <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, Subhrajit Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting and in-context learning (ICL) have become efficient learning
paradigms for large language models (LLMs). However, LLMs suffer from prompt
brittleness and various bias factors in the prompt, including but not limited
to the formatting, the choice verbalizers, and the ICL examples. To address
this problem that results in unexpected performance degradation, calibration
methods have been developed to mitigate the effects of these biases while
recovering LLM performance. In this work, we first conduct a systematic
analysis of the existing calibration methods, where we both provide a unified
view and reveal the failure cases. Inspired by these analyses, we propose Batch
Calibration (BC), a simple yet intuitive method that controls the contextual
bias from the batched input, unifies various prior approaches, and effectively
addresses the aforementioned issues. BC is zero-shot, inference-only, and
incurs negligible additional costs. In the few-shot setup, we further extend BC
to allow it to learn the contextual bias from labeled data. We validate the
effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate
state-of-the-art performance over previous calibration baselines across more
than 10 natural language understanding and image classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">38</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Inverse Rendering with Interpretable Basis BRDFs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17994v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17994v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoon-Gyu Chung, Seokjun Choi, Seung-Hwan Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse rendering seeks to reconstruct both geometry and spatially varying
BRDFs (SVBRDFs) from captured images. To address the inherent ill-posedness of
inverse rendering, basis BRDF representations are commonly used, modeling
SVBRDFs as spatially varying blends of a set of basis BRDFs. However, existing
methods often yield basis BRDFs that lack intuitive separation and have limited
scalability to scenes of varying complexity. In this paper, we introduce a
differentiable inverse rendering method that produces interpretable basis
BRDFs. Our approach models a scene using 2D Gaussians, where the reflectance of
each Gaussian is defined by a weighted blend of basis BRDFs. We efficiently
render an image from the 2D Gaussians and basis BRDFs using differentiable
rasterization and impose a rendering loss with the input images. During this
analysis-by-synthesis optimization process of differentiable inverse rendering,
we dynamically adjust the number of basis BRDFs to fit the target scene while
encouraging sparsity in the basis weights. This ensures that the reflectance of
each Gaussian is represented by only a few basis BRDFs. This approach enables
the reconstruction of accurate geometry and interpretable basis BRDFs that are
spatially separated. Consequently, the resulting scene representation,
comprising basis BRDFs and 2D Gaussians, supports physically-based novel-view
relighting and intuitive scene editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a different paper from my previous paper "Differentiable
  Point-based Inverse Rendering". It must not be removed automatically</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive framework for evaluation of deep neural networks in
  detection and quantification of lymphoma from PET/CT images: clinical
  insights, pitfalls, and observer agreement analyses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09614v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09614v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadab Ahamed, Yixi Xu, Sara Kurkowska, Claire Gowdy, Joo H. O, Ingrid Bloise, Don Wilson, Patrick Martineau, François Bénard, Fereshteh Yousefirizi, Rahul Dodhia, Juan M. Lavista, William B. Weeks, Carlos F. Uribe, Arman Rahmim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses critical gaps in automated lymphoma segmentation from
PET/CT images, focusing on issues often overlooked in existing literature.
While deep learning has been applied for lymphoma lesion segmentation, few
studies incorporate out-of-distribution testing, raising concerns about model
generalizability across diverse imaging conditions and patient populations. We
highlight the need to compare model performance with expert human annotators,
including intra- and inter-observer variability, to understand task difficulty
better. Most approaches focus on overall segmentation accuracy but overlook
lesion-specific metrics important for precise lesion detection and disease
quantification.To address these gaps, we propose a clinically-relevant
framework for evaluating deep neural networks. Using this lesion-specific
evaluation, we assess the performance of four deep segmentation networks
(ResUNet, SegResNet, DynUNet, and SwinUNETR) across 611 cases from
multi-institutional datasets, covering various lymphoma subtypes and lesion
characteristics. Beyond standard metrics like the Dice similarity coefficient
(DSC), we evaluate clinical lesion measures and their prediction errors. We
also introduce detection criteria for lesion localization and propose a new
detection Criterion 3 based on metabolic characteristics. We show that networks
perform better on large, intense lesions with higher metabolic
activity.Finally, we compare network performance to expert human observers via
intra- and inter-observer variability analyses, demonstrating that network
errors closely resemble those made by experts. Some small, faint lesions remain
challenging for both humans and networks. This study aims to improve automated
lesion segmentation's clinical relevance, supporting better treatment decisions
for lymphoma patients. The code is available at:
https://github.com/microsoft/lymphoma-segmentation-dnn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 15 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Right Place, Right Time! Generalizing ObjectNav to Dynamic Environments
  with Portable Targets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Sashank Dorbala, Bhrij Patel, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ObjectNav is a popular task in Embodied AI, where an agent navigates to a
target object in an unseen environment. Prior literature makes the assumption
of a static environment with stationary objects, which lacks realism. To
address this, we present a novel formulation to generalize ObjectNav to dynamic
environments with non-stationary objects, and refer to it as Portable ObjectNav
or P-ObjectNav. In our formulation, we first address several challenging issues
with dynamizing existing topological scene graphs by developing a novel method
that introduces multiple transition behaviors to portable objects in the scene.
We use this technique to dynamize Matterport3D, a popular simulator for
evaluating embodied tasks. We then present a benchmark for P-ObjectNav using a
combination of heuristic, reinforcement learning, and Large Language Model
(LLM)-based navigation approaches on the dynamized environment, while
introducing novel evaluation metrics tailored for our task. Our work
fundamentally challenges the "static-environment" notion of prior ObjectNav
work; the code and dataset for P-ObjectNav will be made publicly available to
foster research on embodied navigation in dynamic scenes. We provide an
anonymized repository for our code and dataset:
https://anonymous.4open.science/r/PObjectNav-1C6D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Blockchain and Biometrics: A <span class="highlight-title">Survey</span> on Technical Aspects and a
  First Legal Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Ghafourian, Bilgesu Sumer, Ruben Vera-Rodriguez, Julian Fierrez, Ruben Tolosana, Aythami Moralez, Els Kindt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biometric recognition as a unique, hard-to-forge, and efficient way of
identification and verification has become an indispensable part of the current
digital world. The fast evolution of this technology has been a strong
incentive for integrating it into many applications. Meanwhile, blockchain, the
very attractive decentralized ledger technology, has been widely received both
by the research and industry in the past years and it is being increasingly
deployed nowadays in many different applications, such as money transfer, IoT,
healthcare, or logistics. Recently, researchers have started to speculate what
would be the pros and cons and what would be the best applications when these
two technologies cross paths. This paper provides a survey of technical
literature research on the combination of blockchain and biometrics and
includes a first legal analysis of this integration to shed light on challenges
and potentials. While this combination is still in its infancy and a growing
body of literature discusses specific blockchain applications and solutions in
an advanced technological set-up, this paper presents a holistic understanding
of blockchains applicability in the biometric sector. This study demonstrates
that combining blockchain and biometrics would be beneficial for novel
applications in biometrics such as the PKI mechanism, distributed trusted
service, and identity management. However, blockchain networks at their current
stage are not efficient and economical for real-time applications. From a legal
point of view, the allocation of accountability remains a main issue, while
other difficulties remain, such as conducting a proper Data Protection Impact
Assessment. Finally, it supplies technical and legal recommendations to reap
the benefits and mitigate the risks of the combination.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SaFL: Sybil-aware Federated Learning with Application to Face
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Ghafourian, Julian Fierrez, Ruben Vera-Rodriguez, Ruben Tolosana, Aythami Morales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a machine learning paradigm to conduct
collaborative learning among clients on a joint model. The primary goal is to
share clients' local training parameters with an integrating server while
preserving their privacy. This method permits to exploit the potential of
massive mobile users' data for the benefit of machine learning models'
performance while keeping sensitive data on local devices. On the downside, FL
raises security and privacy concerns that have just started to be studied. To
address some of the key threats in FL, researchers have proposed to use secure
aggregation methods (e.g. homomorphic encryption, secure multiparty
computation, etc.). These solutions improve some security and privacy metrics,
but at the same time bring about other serious threats such as poisoning
attacks, backdoor attacks, and free running attacks. This paper proposes a new
defense method against poisoning attacks in FL called SaFL (Sybil-aware
Federated Learning) that minimizes the effect of sybils with a novel
time-variant aggregation scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Document Haystacks: Vision-Language Reasoning Over Piles of 1000+
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Chen, Dannong Xu, Junjie Fei, Chun-Mei Feng, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) have achieved impressive progress in
vision-language understanding, yet they face limitations in real-world
applications requiring complex reasoning over a large number of images.
Existing benchmarks for multi-image question-answering are limited in scope,
each question is paired with only up to 30 images, which does not fully capture
the demands of large-scale retrieval tasks encountered in the real-world
usages. To reduce these gaps, we introduce two document haystack benchmarks,
dubbed DocHaystack and InfoHaystack, designed to evaluate LMM performance on
large-scale visual document retrieval and understanding. Additionally, we
propose V-RAG, a novel, vision-centric retrieval-augmented generation (RAG)
framework that leverages a suite of multimodal vision encoders, each optimized
for specific strengths, and a dedicated question-document relevance module.
V-RAG sets a new standard, with a 9% and 11% improvement in Recall@1 on the
challenging DocHaystack-1000 and InfoHaystack-1000 benchmarks, respectively,
compared to the previous best baseline models. Additionally, integrating V-RAG
with LMMs enables them to efficiently operate across thousands of images,
yielding significant improvements on our DocHaystack and InfoHaystack
benchmarks. Our code and datasets are available at
https://github.com/Vision-CAIR/dochaystacks
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the correct arxiv version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Shift Invariance in Convolutional Neural Networks with
  Translation Invariant Polyphase Sampling <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourajit Saha, Tejas Gokhale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Downsampling operators break the shift invariance of convolutional neural
networks (CNNs) and this affects the robustness of features learned by CNNs
when dealing with even small pixel-level shift. Through a large-scale
correlation analysis framework, we study shift invariance of CNNs by inspecting
existing downsampling operators in terms of their maximum-sampling bias (MSB),
and find that MSB is negatively correlated with shift invariance. Based on this
crucial insight, we propose a learnable pooling operator called Translation
Invariant Polyphase Sampling (TIPS) and two regularizations on the intermediate
feature maps of TIPS to reduce MSB and learn translation-invariant
representations. TIPS can be integrated into any CNN and can be trained
end-to-end with marginal computational overhead. Our experiments demonstrate
that TIPS results in consistent performance gains in terms of accuracy, shift
consistency, and shift fidelity on multiple benchmarks for image classification
and semantic segmentation compared to previous methods and also leads to
improvements in adversarial and distributional robustness. TIPS results in the
lowest MSB compared to all previous methods, thus explaining our strong
empirical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Transferable Features for Implicit Neural Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushal Vyas, Ahmed Imtiaz Humayun, Aniket Dashpute, Richard G. Baraniuk, Ashok Veeraraghavan, Guha Balakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representations (INRs) have demonstrated success in a variety
of applications, including inverse problems and neural rendering. An INR is
typically trained to capture one signal of interest, resulting in learned
neural features that are highly attuned to that signal. Assumed to be less
generalizable, we explore the aspect of transferability of such learned neural
features for fitting similar signals. We introduce a new INR training
framework, STRAINER that learns transferrable features for fitting INRs to new
signals from a given distribution, faster and with better reconstruction
quality. Owing to the sequential layer-wise affine operations in an INR, we
propose to learn transferable representations by sharing initial encoder layers
across multiple INRs with independent decoder layers. At test time, the learned
encoder representations are transferred as initialization for an otherwise
randomly initialized INR. We find STRAINER to yield extremely powerful
initialization for fitting images from the same domain and allow for $\approx
+10dB$ gain in signal quality early on compared to an untrained INR itself.
STRAINER also provides a simple way to encode data-driven priors in INRs. We
evaluate STRAINER on multiple in-domain and out-of-domain signal fitting tasks
and inverse problems and further provide detailed analysis and discussion on
the transferability of STRAINER's features. Our demo can be accessed at
https://colab.research.google.com/drive/1fBZAwqE8C_lrRPAe-hQZJTWrMJuAKtG2?usp=sharing .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://kushalvyas.github.io/strainer.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation
  Framework for Efficient Device Model Generalization <span class="chip">WWW'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05227v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05227v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei Wang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Device Model Generalization (DMG) is a practical yet under-investigated
research topic for on-device machine learning applications. It aims to improve
the generalization ability of pre-trained models when deployed on
resource-constrained devices, such as improving the performance of pre-trained
cloud models on smart mobiles. While quite a lot of works have investigated the
data distribution shift across clouds and devices, most of them focus on model
fine-tuning on personalized data for individual devices to facilitate DMG.
Despite their promising, these approaches require on-device re-training, which
is practically infeasible due to the overfitting problem and high time delay
when performing gradient calculation on real-time data. In this paper, we argue
that the computational cost brought by fine-tuning can be rather unnecessary.
We consequently present a novel perspective to improving DMG without increasing
computational cost, i.e., device-specific parameter generation which directly
maps data distribution to parameters. Specifically, we propose an efficient
Device-cloUd collaborative parametErs generaTion framework DUET. DUET is
deployed on a powerful cloud server that only requires the low cost of
forwarding propagation and low time delay of data transmission between the
device and the cloud. By doing so, DUET can rehearse the device-specific model
weight realizations conditioned on the personalized real-time data for an
individual device. Importantly, our DUET elegantly connects the cloud and
device as a 'duet' collaboration, frees the DMG from fine-tuning, and enables a
faster and more accurate DMG paradigm. We conduct an extensive experimental
study of DUET on three public datasets, and the experimental results confirm
our framework's effectiveness and generalisability for different DMG tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on WWW'23: Proceedings of the ACM on Web Conference 2023
  (pp. 3077 - 3085)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GATE OpenING: A Comprehensive Benchmark for Judging Open-ended
  Interleaved Image-Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Zhou, Xiaopeng Peng, Jiajun Song, Chuanhao Li, Zhaopan Xu, Yue Yang, Ziyao Guo, Hao Zhang, Yuqi Lin, Yefei He, Lirui Zhao, Shuo Liu, Tianhua Li, Yuxuan Xie, Xiaojun Chang, Yu Qiao, Wenqi Shao, Kaipeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have made significant strides in
visual understanding and generation tasks. However, generating interleaved
image-text content remains a challenge, which requires integrated multimodal
understanding and generation abilities. While the progress in unified models
offers new solutions, existing benchmarks are insufficient for evaluating these
methods due to data size and diversity limitations. To bridge this gap, we
introduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400
high-quality human-annotated instances across 56 real-world tasks. OpenING
covers diverse daily scenarios such as travel guide, design, and brainstorming,
offering a robust platform for challenging interleaved generation methods. In
addition, we present IntJudge, a judge model for evaluating open-ended
multimodal generation methods. Trained with a novel data pipeline, our IntJudge
achieves an agreement rate of 82. 42% with human judgments, outperforming
GPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that
current interleaved generation methods still have substantial room for
improvement. Key findings on interleaved image-text generation are further
presented to guide the development of next-generation models. The OpenING is
open-sourced at https://opening-benchmark.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Cross-View-Consistent <span class="highlight-title">Self-Supervised</span> Surround Depth Estimation <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laiyan Ding, Hualie Jiang, Jie Li, Yongquan Chen, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation is a cornerstone for autonomous driving, yet acquiring
per-pixel depth ground truth for supervised learning is challenging.
Self-Supervised Surround Depth Estimation (SSSDE) from consecutive images
offers an economical alternative. While previous SSSDE methods have proposed
different mechanisms to fuse information across images, few of them explicitly
consider the cross-view constraints, leading to inferior performance,
particularly in overlapping regions. This paper proposes an efficient and
consistent pose estimation design and two loss functions to enhance cross-view
consistency for SSSDE. For pose estimation, we propose to use only front-view
images to reduce training memory and sustain pose estimation consistency. The
first loss function is the dense depth consistency loss, which penalizes the
difference between predicted depths in overlapping regions. The second one is
the multi-view reconstruction consistency loss, which aims to maintain
consistency between reconstruction from spatial and spatial-temporal contexts.
Additionally, we introduce a novel flipping augmentation to improve the
performance further. Our techniques enable a simple neural model to achieve
state-of-the-art performance on the DDAD and nuScenes datasets. Last but not
least, our proposed techniques can be easily applied to other methods. The code
will be made public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiKASA: Multi-Key-Anchor & Scene-Aware <span class="highlight-title">Transformer</span> for 3D Visual
  Grounding <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03077v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03077v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun-Peng Chang, Shaoxiang Wang, Alain Pagani, Didier Stricker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding involves matching natural language descriptions with
their corresponding objects in 3D spaces. Existing methods often face
challenges with accuracy in object recognition and struggle in interpreting
complex linguistic queries, particularly with descriptions that involve
multiple anchors or are view-dependent. In response, we present the MiKASA
(Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model
integrates a self-attention-based scene-aware object encoder and an original
multi-key-anchor technique, enhancing object recognition accuracy and the
understanding of spatial relationships. Furthermore, MiKASA improves the
explainability of decision-making, facilitating error diagnosis. Our model
achieves the highest overall accuracy in the Referit3D challenge for both the
Sr3D and Nr3D datasets, particularly excelling by a large margin in categories
that require viewpoint-dependent descriptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPA: Dual Prototypes Alignment for Unsupervised Adaptation of
  Vision-Language Models <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eman Ali, Sathira Silva, Muhammad Haris Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs), e.g., CLIP, have shown remarkable potential in
zero-shot image classification. However, adapting these models to new domains
remains challenging, especially in unsupervised settings where labeled data is
unavailable. Recent research has proposed pseudo-labeling approaches to adapt
CLIP in an unsupervised manner using unlabeled target data. Nonetheless, these
methods struggle due to noisy pseudo-labels resulting from the misalignment
between CLIP's visual and textual representations. This study introduces DPA,
an unsupervised domain adaptation method for VLMs. DPA introduces the concept
of dual prototypes, acting as distinct classifiers, along with the convex
combination of their outputs, thereby leading to accurate pseudo-label
construction. Next, it ranks pseudo-labels to facilitate robust self-training,
particularly during early training. Finally, it addresses visual-textual
misalignment by aligning textual prototypes with image prototypes to further
improve the adaptation performance. Experiments on 13 downstream vision tasks
demonstrate that DPA significantly outperforms zero-shot CLIP and the
state-of-the-art unsupervised adaptation baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TED-VITON: <span class="highlight-title">Transformer</span>-Empowered Diffusion Models for Virtual Try-On 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenchen Wan, Yanwu Xu, Zhaoqing Wang, Feng Liu, Tongliang Liu, Mingming Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional
efficacy in generating realistic images and preserving garment details, largely
attributed to the robust generative capabilities of text-to-image (T2I)
diffusion backbones. However, the T2I models that underpin these methods have
become outdated, thereby limiting the potential for further improvement in VTO.
Additionally, current methods face notable challenges in accurately rendering
text on garments without distortion and preserving fine-grained details, such
as textures and material fidelity. The emergence of Diffusion Transformer (DiT)
based T2I models has showcased impressive performance and offers a promising
opportunity for advancing VTO. Directly applying existing VTO techniques to
transformer-based T2I models is ineffective due to substantial architectural
differences, which hinder their ability to fully leverage the models' advanced
capabilities for improved text generation. To address these challenges and
unlock the full potential of DiT-based T2I models for VTO, we propose
TED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter
for enhancing garment-specific features, a Text Preservation Loss to ensure
accurate and distortion-free text rendering, and a constraint mechanism to
generate prompts by optimizing Large Language Model (LLM). These innovations
enable state-of-the-art (SOTA) performance in visual quality and text fidelity,
establishing a new benchmark for VTO task. Project page:
\url{https://zhenchenwan.github.io/TED-VITON/}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: \href{https://github.com/ZhenchenWan/TED-VITON}{this
  URL}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh
  Tokenization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02555v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02555v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meshes are the de facto 3D representation in the industry but are
labor-intensive to produce. Recently, a line of research has focused on
autoregressively generating meshes. This approach processes meshes into a
sequence composed of vertices and then generates them vertex by vertex, similar
to how a language model generates text. These methods have achieved some
success but still struggle to generate complex meshes. One primary reason for
this limitation is their inefficient tokenization methods. To address this
issue, we introduce MeshAnything V2, an advanced mesh generation model designed
to create Artist-Created Meshes that align precisely with specified shapes. A
key innovation behind MeshAnything V2 is our novel Adjacent Mesh Tokenization
(AMT) method. Unlike traditional approaches that represent each face using
three vertices, AMT optimizes this by employing a single vertex wherever
feasible, effectively reducing the token sequence length by about half on
average. This not only streamlines the tokenization process but also results in
more compact and well-structured sequences, enhancing the efficiency of mesh
generation. With these improvements, MeshAnything V2 effectively doubles the
face limit compared to previous models, delivering superior performance without
increasing computational costs. We will make our code and models publicly
available. Project Page: https://buaacyw.github.io/meshanything-v2/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://buaacyw.github.io/meshanything-v2/ Github:
  https://github.com/buaacyw/MeshAnythingV2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly-Supervised Semantic Segmentation with Image-Level Labels: from
  Traditional Models to Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaozheng Chen, Qianru Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of deep learning has driven significant progress in
image semantic segmentation - a fundamental task in computer vision. Semantic
segmentation algorithms often depend on the availability of pixel-level labels
(i.e., masks of objects), which are expensive, time-consuming, and
labor-intensive. Weakly-supervised semantic segmentation (WSSS) is an effective
solution to avoid such labeling. It utilizes only partial or incomplete
annotations and provides a cost-effective alternative to fully-supervised
semantic segmentation. In this journal, our focus is on the WSSS with
image-level labels, which is the most challenging form of WSSS. Our work has
two parts. First, we conduct a comprehensive survey on traditional methods,
primarily focusing on those presented at premier research conferences. We
categorize them into four groups based on where their methods operate:
pixel-wise, image-wise, cross-image, and external data. Second, we investigate
the applicability of visual foundation models, such as the Segment Anything
Model (SAM), in the context of WSSS. We scrutinize SAM in two intriguing
scenarios: text prompting and zero-shot learning. We provide insights into the
potential and challenges of deploying visual foundational models for WSSS,
facilitating future developments in this exciting research area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Computing Surveys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReVersion: Diffusion-Based Relation Inversion from Images <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C. K. Chan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models gain increasing popularity for their generative
capabilities. Recently, there have been surging needs to generate customized
images by inverting diffusion models from exemplar images, and existing
inversion methods mainly focus on capturing object appearances (i.e., the
"look"). However, how to invert object relations, another important pillar in
the visual world, remains unexplored. In this work, we propose the Relation
Inversion task, which aims to learn a specific relation (represented as
"relation prompt") from exemplar images. Specifically, we learn a relation
prompt with a frozen pre-trained text-to-image diffusion model. The learned
relation prompt can then be applied to generate relation-specific images with
new objects, backgrounds, and styles.
  To tackle the Relation Inversion task, we propose the ReVersion Framework.
Specifically, we propose a novel "relation-steering contrastive learning"
scheme to steer the relation prompt towards relation-dense regions, and
disentangle it away from object appearances. We further devise "relation-focal
importance sampling" to emphasize high-level interactions over low-level
appearances (e.g., texture, color). To comprehensively evaluate this new task,
we contribute the ReVersion Benchmark, which provides various exemplar images
with diverse relations. Extensive experiments validate the superiority of our
approach over existing methods across a wide range of visual relations. Our
proposed task and method could be good inspirations for future research in
various domains like generative inversion, few-shot learning, and visual
relation detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia (Conference Track) 2024, Project page:
  https://ziqihuangg.github.io/projects/reversion.html Code:
  https://github.com/ziqihuangg/ReVersion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Matters When Repurposing Diffusion Models for General Dense
  Perception Tasks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06090v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06090v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extensive pre-training with large data is indispensable for downstream
geometry and semantic visual perception tasks. Thanks to large-scale
text-to-image (T2I) pretraining, recent works show promising results by simply
fine-tuning T2I diffusion models for dense perception tasks. However, several
crucial design decisions in this process still lack comprehensive
justification, encompassing the necessity of the multi-step stochastic
diffusion mechanism, training strategy, inference ensemble strategy, and
fine-tuning data quality. In this work, we conduct a thorough investigation
into critical factors that affect transfer efficiency and performance when
using diffusion priors. Our key findings are: 1) High-quality fine-tuning data
is paramount for both semantic and geometry perception tasks. 2) The stochastic
nature of diffusion models has a slightly negative impact on deterministic
visual perception tasks. 3) Apart from fine-tuning the diffusion model with
only latent space supervision, task-specific image-level supervision is
beneficial to enhance fine-grained details. These observations culminate in the
development of GenPercept, an effective deterministic one-step fine-tuning
paradigm tailed for dense visual perception tasks. Different from the previous
multi-step methods, our paradigm has a much faster inference speed, and can be
seamlessly integrated with customized perception decoders and loss functions
for image-level supervision, which is critical to improving the fine-grained
details of predictions. Comprehensive experiments on diverse dense visual
perceptual tasks, including monocular depth estimation, surface normal
estimation, image segmentation, and matting, are performed to demonstrate the
remarkable adaptability and effectiveness of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is at: https://github.com/aim-uofa/GenPercept</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Denoising-Contrastive Alignment for Continuous Sign Language Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03614v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03614v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leming Guo, Wanli Xue, Shengyong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous sign language recognition (CSLR) aims to recognize signs in
untrimmed sign language videos to textual glosses. A key challenge of CSLR is
achieving effective cross-modality alignment between video and gloss sequences
to enhance video representation. However, current cross-modality alignment
paradigms often neglect the role of textual grammar to guide the video
representation in learning global temporal context, which adversely affects
recognition performance. To tackle this limitation, we propose a
Denoising-Contrastive Alignment (DCA) paradigm. DCA creatively leverages
textual grammar to enhance video representations through two complementary
approaches: modeling the instance correspondence between signs and glosses from
a discrimination perspective and aligning their global context from a
generative perspective. Specifically, DCA accomplishes flexible instance-level
correspondence between signs and glosses using a contrastive loss. Building on
this, DCA models global context alignment between the video and gloss sequences
by denoising the gloss representation from noise, guided by video
representation. Additionally, DCA introduces gradient modulation to optimize
the alignment and recognition gradients, ensuring a more effective learning
process. By integrating gloss-wise and global context knowledge, DCA
significantly enhances video representations for CSLR tasks. Experimental
results across public benchmarks validate the effectiveness of DCA and confirm
its video representation enhancement feasibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Class Abnormality Classification in Video Capsule Endoscopy Using
  Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnav Samal, Ranya Batsyas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report outlines Team Seq2Cure's deep learning approach for the Capsule
Vision 2024 Challenge, leveraging an ensemble of convolutional neural networks
(CNNs) and transformer-based architectures for multi-class abnormality
classification in video capsule endoscopy frames. The dataset comprised over
50,000 frames from three public sources and one private dataset, labeled across
10 abnormality classes. To overcome the limitations of traditional CNNs in
capturing global context, we integrated CNN and transformer models within a
multi-model ensemble. Our approach achieved a balanced accuracy of 86.34
percent and a mean AUC-ROC score of 0.9908 on the validation set, earning our
submission 5th place in the challenge. Code is available at
http://github.com/arnavs04/capsule-vision-2024 .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning-driven pulmonary artery and vein segmentation reveals
  demography-associated vasculature anatomical differences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuetan Chu, Gongning Luo, Longxi Zhou, Shaodong Cao, Guolin Ma, Xianglin Meng, Juexiao Zhou, Changchun Yang, Dexuan Xie, Dan Mu, Ricardo Henao, Gianluca Setti, Xigang Xiao, Lianming Wu, Zhaowen Qiu, Xin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulmonary artery-vein segmentation is crucial for disease diagnosis and
surgical planning and is traditionally achieved by Computed Tomography
Pulmonary Angiography (CTPA). However, concerns regarding adverse health
effects from contrast agents used in CTPA have constrained its clinical
utility. In contrast, identifying arteries and veins using non-contrast CT, a
conventional and low-cost clinical examination routine, has long been
considered impossible. Here we propose a High-abundant Pulmonary Artery-vein
Segmentation (HiPaS) framework achieving accurate artery-vein segmentation on
both non-contrast CT and CTPA across various spatial resolutions. HiPaS first
performs spatial normalization on raw CT volumes via a super-resolution module,
and then iteratively achieves segmentation results at different branch levels
by utilizing the lower-level vessel segmentation as a prior for higher-level
vessel segmentation. We trained and validated HiPaS on our established
multi-centric dataset comprising 1,073 CT volumes with meticulous manual
annotations. Both quantitative experiments and clinical evaluation demonstrated
the superior performance of HiPaS, achieving an average dice score of 91.8% and
a sensitivity of 98.0%. Further experiments showed the non-inferiority of HiPaS
segmentation on non-contrast CT compared to segmentation on CTPA. Employing
HiPaS, we have conducted an anatomical study of pulmonary vasculature on 11,784
participants in China (six sites), discovering a new association of pulmonary
vessel anatomy with sex, age, and disease states: vessel abundance suggests a
significantly higher association with females than males with slightly
decreasing with age, and is also influenced by certain diseases, under the
controlling of lung volumes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoMM: A Coherent Interleaved Image-Text <span class="highlight-title">Dataset</span> for Multimodal
  Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chen, Lin Li, Yongqi Yang, Bin Wen, Fan Yang, Tingting Gao, Yu Wu, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interleaved image-text generation has emerged as a crucial multimodal task,
aiming at creating sequences of interleaved visual and textual content given a
query. Despite notable advancements in recent multimodal large language models
(MLLMs), generating integrated image-text sequences that exhibit narrative
coherence and entity and style consistency remains challenging due to poor
training data quality. To address this gap, we introduce CoMM, a high-quality
Coherent interleaved image-text MultiModal dataset designed to enhance the
coherence, consistency, and alignment of generated multimodal content.
Initially, CoMM harnesses raw data from diverse sources, focusing on
instructional content and visual storytelling, establishing a foundation for
coherent and consistent content. To further refine the data quality, we devise
a multi-perspective filter strategy that leverages advanced pre-trained models
to ensure the development of sentences, consistency of inserted images, and
semantic alignment between them. Various quality evaluation metrics are
designed to prove the high quality of the filtered dataset. Meanwhile,
extensive few-shot experiments on various downstream tasks demonstrate CoMM's
effectiveness in significantly enhancing the in-context learning capabilities
of MLLMs. Moreover, we propose four new tasks to evaluate MLLMs' interleaved
generation abilities, supported by a comprehensive evaluation framework. We
believe CoMM opens a new avenue for advanced MLLMs with superior multimodal
in-context learning and understanding ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PACA: Perspective-Aware Cross-Attention Representation for Zero-Shot
  Scene Rearrangement <span class="chip">WACV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutong Jin, Ruiyu Wang, Kuangyi Chen, Florian T. Pokorny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene rearrangement, like table tidying, is a challenging task in robotic
manipulation due to the complexity of predicting diverse object arrangements.
Web-scale trained generative models such as Stable Diffusion can aid by
generating natural scenes as goals. To facilitate robot execution, object-level
representations must be extracted to match the real scenes with the generated
goals and to calculate object pose transformations. Current methods typically
use a multi-step design that involves separate models for generation,
segmentation, and feature encoding, which can lead to a low success rate due to
error accumulation. Furthermore, they lack control over the viewing
perspectives of the generated goals, restricting the tasks to 3-DoF settings.
In this paper, we propose PACA, a zero-shot pipeline for scene rearrangement
that leverages perspective-aware cross-attention representation derived from
Stable Diffusion. Specifically, we develop a representation that integrates
generation, segmentation, and feature encoding into a single step to produce
object-level representations. Additionally, we introduce perspective control,
thus enabling the matching of 6-DoF camera views and extending past approaches
that were limited to 3-DoF top-down views. The efficacy of our method is
demonstrated through its zero-shot performance in real robot experiments across
various scenes, achieving an average matching accuracy and execution success
rate of 87% and 67%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SI<span class="highlight-title">Transformer</span>: Shared Information-Guided <span class="highlight-title">Transformer</span> for Extreme
  Multimodal Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15829v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15829v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Liu, Lintao Wang, Xiaogang Zhu, Xuequan Lu, Zhiyong Wang, Kun Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes an
attractive summarization approach by integrating various types of information
to create extremely concise yet informative summaries for individual
modalities. Existing methods overlook the issue that multimodal data often
contains more topic irrelevant information, which can mislead the model into
producing inaccurate summaries especially for extremely short ones. In this
paper, we propose SITransformer, a Shared Information-guided Transformer for
extreme multimodal summarization. It has a shared information guided pipeline
which involves a cross-modal shared information extractor and a cross-modal
interaction module. The extractor formulates semantically shared salient
information from different modalities by devising a novel filtering process
consisting of a differentiable top-k selector and a shared-information guided
gating unit. As a result, the common, salient, and relevant contents across
modalities are identified. Next, a transformer with cross-modal attentions is
developed for intra- and inter-modality learning with the shared information
guidance to produce the extreme summary. Comprehensive experiments demonstrate
that SITransformer significantly enhances the summarization quality for both
video and text summaries for XMSMO. Our code will be publicly available at
https://github.com/SichengLeoLiu/MMAsia24-XMSMO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, submitted to ACM Multimedia Asia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Open Vocabulary to Open World: Teaching Vision Language Models to
  Detect Novel Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhao Li, Zhengkang Xiang, Joseph West, Kourosh Khoshelham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional object detection methods operate under the closed-set assumption,
where models can only detect a fixed number of objects predefined in the
training set. Recent works on open vocabulary object detection (OVD) enable the
detection of objects defined by an unbounded vocabulary, which reduces the cost
of training models for specific tasks. However, OVD heavily relies on accurate
prompts provided by an ''oracle'', which limits their use in critical
applications such as driving scene perception. OVD models tend to misclassify
near-out-of-distribution (NOOD) objects that have similar semantics to known
classes, and ignore far-out-of-distribution (FOOD) objects. To address theses
limitations, we propose a framework that enables OVD models to operate in open
world settings, by identifying and incrementally learning novel objects. To
detect FOOD objects, we propose Open World Embedding Learning (OWEL) and
introduce the concept of Pseudo Unknown Embedding which infers the location of
unknown classes in a continuous semantic space based on the information of
known classes. We also propose Multi-Scale Contrastive Anchor Learning (MSCAL),
which enables the identification of misclassified unknown objects by promoting
the intra-class consistency of object embeddings at different scales. The
proposed method achieves state-of-the-art performance in common open world
object detection and autonomous driving benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-supervised
  Change Detector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04788v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04788v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyu Li, Xiangyong Cao, Yupeng Deng, Jiayi Song, Junmin Liu, Deyu Meng, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change Detection (CD) aims to identify pixels with semantic changes between
images. However, annotating massive numbers of pixel-level images is
labor-intensive and costly, especially for multi-temporal images, which require
pixel-wise comparisons by human experts. Considering the excellent performance
of visual language models (VLMs) for zero-shot, open-vocabulary, etc. with
prompt-based reasoning, it is promising to utilize VLMs to make better CD under
limited labeled data. In this paper, we propose a VLM guidance-based
semi-supervised CD method, namely SemiCD-VL. The insight of SemiCD-VL is to
synthesize free change labels using VLMs to provide additional supervision
signals for unlabeled data. However, almost all current VLMs are designed for
single-temporal images and cannot be directly applied to bi- or multi-temporal
images. Motivated by this, we first propose a VLM-based mixed change event
generation (CEG) strategy to yield pseudo labels for unlabeled CD data. Since
the additional supervised signals provided by these VLM-driven pseudo labels
may conflict with the pseudo labels from the consistency regularization
paradigm (e.g. FixMatch), we propose the dual projection head for de-entangling
different signal sources. Further, we explicitly decouple the bi-temporal
images semantic representation through two auxiliary segmentation decoders,
which are also guided by VLM. Finally, to make the model more adequately
capture change representations, we introduce metric-aware supervision by
feature-level contrastive loss in auxiliary branches. Extensive experiments
show the advantage of SemiCD-VL. For instance, SemiCD-VL improves the FixMatch
baseline by +5.3 IoU on WHU-CD and by +2.4 IoU on LEVIR-CD with 5% labels. In
addition, our CEG strategy, in an un-supervised manner, can achieve performance
far superior to state-of-the-art un-supervised CD methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Point Attention <span class="highlight-title">Transformer</span> for 3D Shape Reassembly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahan Li, Chaoran Cheng, Jianzhu Ma, Ge Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shape assembly, which aims to reassemble separate parts into a complete
object, has gained significant interest in recent years. Existing methods
primarily rely on networks to predict the poses of individual parts, but often
fail to effectively capture the geometric interactions between the parts and
their poses. In this paper, we present the Geometric Point Attention
Transformer (GPAT), a network specifically designed to address the challenges
of reasoning about geometric relationships. In the geometric point attention
module, we integrate both global shape information and local pairwise geometric
features, along with poses represented as rotation and translation vectors for
each part. To enable iterative updates and dynamic reasoning, we introduce a
geometric recycling scheme, where each prediction is fed into the next
iteration for refinement. We evaluate our model on both the semantic and
geometric assembly tasks, showing that it outperforms previous methods in
absolute pose estimation, achieving accurate pose predictions and high
alignment accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HAAT: Hybrid Attention Aggregation <span class="highlight-title">Transformer</span> for Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song-Jiang Lai, Tsun-Hin Cheung, Ka-Chun Fung, Kai-wen Xue, Kin-Man Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the research area of image super-resolution, Swin-transformer-based models
are favored for their global spatial modeling and shifting window attention
mechanism. However, existing methods often limit self-attention to non
overlapping windows to cut costs and ignore the useful information that exists
across channels. To address this issue, this paper introduces a novel model,
the Hybrid Attention Aggregation Transformer (HAAT), designed to better
leverage feature information. HAAT is constructed by integrating
Swin-Dense-Residual-Connected Blocks (SDRCB) with Hybrid Grid Attention Blocks
(HGAB). SDRCB expands the receptive field while maintaining a streamlined
architecture, resulting in enhanced performance. HGAB incorporates channel
attention, sparse attention, and window attention to improve nonlocal feature
fusion and achieve more visually compelling results. Experimental evaluations
demonstrate that HAAT surpasses state-of-the-art methods on benchmark datasets.
  Keywords: Image super-resolution, Computer vision, Attention mechanism,
Transformer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for
  Semantic- and Spatial-Aware 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Song, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detection is an important task that has been widely applied in
autonomous driving. To perform this task, a new trend is to fuse multi-modal
inputs, i.e., LiDAR and camera. Under such a trend, recent methods fuse these
two modalities by unifying them in the same 3D space. However, during direct
fusion in a unified space, the drawbacks of both modalities (LiDAR features
struggle with detailed semantic information and the camera lacks accurate 3D
spatial information) are also preserved, diluting semantic and spatial
awareness of the final unified representation. To address the issue, this
letter proposes a novel bidirectional complementary LiDAR-camera fusion
framework, called BiCo-Fusion that can achieve robust semantic- and
spatial-aware 3D object detection. The key insight is to fuse LiDAR and camera
features in a bidirectional complementary way to enhance the semantic awareness
of the LiDAR and the 3D spatial awareness of the camera. The enhanced features
from both modalities are then adaptively fused to build a semantic- and
spatial-aware unified representation. Specifically, we introduce Pre-Fusion
consisting of a Voxel Enhancement Module (VEM) to enhance the semantic
awareness of voxel features from 2D camera features and Image Enhancement
Module (IEM) to enhance the 3D spatial awareness of camera features from 3D
voxel features. We then introduce Unified Fusion (U-Fusion) to adaptively fuse
the enhanced features from the last stage to build a unified representation.
Extensive experiments demonstrate the superiority of our BiCo-Fusion against
the prior arts. Project page: https://t-ys.github.io/BiCo-Fusion/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models
  for Integrated Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Yu, Zhengyuan Yang, Lingfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MM-Vet, with open-ended vision-language questions targeting at evaluating
integrated capabilities, has become one of the most popular benchmarks for
large multimodal model evaluation. MM-Vet assesses six core vision-language
(VL) capabilities: recognition, knowledge, spatial awareness, language
generation, OCR, and math. However, its question format is restricted to single
image-text pairs, lacking the interleaved image and text sequences prevalent in
real-world scenarios. To address this limitation, we introduce MM-Vet v2, which
includes a new VL capability called "image-text sequence understanding",
evaluating models' ability to process VL sequences. Furthermore, we maintain
the high quality of evaluation samples while further expanding the evaluation
set size. Using MM-Vet v2 to benchmark large multimodal models, we found that
Claude 3.5 Sonnet is the best model with a score of 71.8, slightly
outperforming GPT-4o which scored 71.0. Among open-weight models,
InternVL2-Llama3-76B leads with a score of 68.4. The code, data, and
leaderboard are accessible at https://github.com/yuweihao/MM-Vet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, data and leaderboard: https://github.com/yuweihao/MM-Vet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02490v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02490v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MM-Vet, an evaluation benchmark that examines large multimodal
models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various
intriguing abilities, such as solving math problems written on the blackboard,
reasoning about events and celebrities in news images, and explaining visual
jokes. Rapid model advancements pose challenges to evaluation benchmark
development. Problems include: (1) How to systematically structure and evaluate
the complicated multimodal tasks; (2) How to design evaluation metrics that
work well across question and answer types; and (3) How to give model insights
beyond a simple performance ranking. To this end, we present MM-Vet, designed
based on the insight that the intriguing ability to solve complicated tasks is
often achieved by a generalist model being able to integrate different core
vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and
examines the 16 integrations of interest derived from the capability
combination. For evaluation metrics, we propose an LLM-based evaluator for
open-ended outputs. The evaluator enables the evaluation across different
question types and answer styles, resulting in a unified scoring metric. We
evaluate representative LMMs on MM-Vet, providing insights into the
capabilities of different LMM system paradigms and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024. Code, data and leaderboard:
  https://github.com/yuweihao/MM-Vet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Potential Field Based Deep Metric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhang Bhatnagar, Narendra Ahuja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep metric learning (DML) involves training a network to learn a
semantically meaningful representation space. Many current approaches mine
n-tuples of examples and model interactions within each tuplets. We present a
novel, compositional DML model, inspired by electrostatic fields in physics
that, instead of in tuples, represents the influence of each example
(embedding) by a continuous potential field, and superposes the fields to
obtain their combined global potential field. We use attractive/repulsive
potential fields to represent interactions among embeddings from images of the
same/different classes. Contrary to typical learning methods, where mutual
influence of samples is proportional to their distance, we enforce reduction in
such influence with distance, leading to a decaying field. We show that such
decay helps improve performance on real world datasets with large intra-class
variations and label noise. Like other proxy-based methods, we also use proxies
to succinctly represent sub-populations of examples. We evaluate our method on
three standard DML benchmarks- Cars-196, CUB-200-2011, and SOP datasets where
it outperforms state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Inversion-based Measure of Memorization for Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Ma, Qingming Li, Xuhong Zhang, Tianyu Du, Ruixiao Lin, Zonghui Wang, Shouling Ji, Wenzhi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The past few years have witnessed substantial advances in image generation
powered by diffusion models. However, it was shown that diffusion models are
vulnerable to training data memorization, raising concerns regarding copyright
infringement and privacy invasion. This study delves into a rigorous analysis
of memorization in diffusion models. We introduce an inversion-based measure of
memorization, InvMM, which searches for a sensitive latent noise distribution
accounting for the replication of an image. For accurate estimation of the
memorization score, we propose an adaptive algorithm that balances the
normality and sensitivity of the inverted distribution. Comprehensive
experiments, conducted on both unconditional and text-guided diffusion models,
demonstrate that InvMM is capable of detecting heavily memorized images and
elucidating the effect of various factors on memorization. Additionally, we
discuss how memorization differs from membership. In practice, InvMM serves as
a useful tool for model developers to reliably assess the risk of memorization,
thereby contributing to the enhancement of trustworthiness and
privacy-preserving capabilities of diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Variational Translator for Bridging Image Restoration and
  High-Level Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Wu, Zhi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research tries to extend image restoration capabilities from human
perception to machine perception, thereby enhancing the performance of
high-level vision tasks in degraded environments. These methods, primarily
based on supervised learning, typically involve the retraining of restoration
networks or high-level vision networks. However, collecting paired data in
real-world scenarios and retraining large-scale models are challenge. To this
end, we propose an unsupervised learning method called \textbf{Va}riational
\textbf{T}ranslator (VaT), which does not require retraining existing
restoration and high-level vision networks. Instead, it establishes a
lightweight network that serves as an intermediate bridge between them. By
variational inference, VaT approximates the joint distribution of restoration
output and high-level vision input, dividing the optimization objective into
preserving content and maximizing marginal likelihood associated with
high-level vision tasks. By cleverly leveraging self-training paradigms, VaT
achieves the above optimization objective without requiring labels. As a
result, the translated images maintain a close resemblance to their original
content while also demonstrating exceptional performance on high-level vision
tasks. Extensive experiments in dehazing and low-light enhancement for
detection and classification show the superiority of our method over other
state-of-the-art unsupervised counterparts, even significantly surpassing
supervised methods in some complex real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12279v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12279v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Zong, Zhaohuan Zhan, Guang Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a two-phase text-to-floorplan generation method, which
guides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)
and refines them into the final floorplans through conditional diffusion model.
We incorporate a Chain-of-Thought approach to prompt the LLM based on user text
specifications, enabling a more user-friendly and intuitive house layout
design. This method allows users to describe their needs in natural language,
enhancing accessibility and providing clearer geometric constraints. The final
floorplans generated by Layout-LLM through conditional diffusion refinement are
more accurate and better meet user requirements. Experimental results
demonstrate that our approach achieves state-of-the-art performance across all
metrics, validating its effectiveness in practical home design applications. We
plan to release our code for public use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EZ-HOI: VLM Adaptation via Guided <span class="highlight-title">Prompt</span> Learning for Zero-Shot HOI
  Detection <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinqian Lei, Bo Wang, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting Human-Object Interactions (HOI) in zero-shot settings, where models
must handle unseen classes, poses significant challenges. Existing methods that
rely on aligning visual encoders with large Vision-Language Models (VLMs) to
tap into the extensive knowledge of VLMs, require large, computationally
expensive models and encounter training difficulties. Adapting VLMs with prompt
learning offers an alternative to direct alignment. However, fine-tuning on
task-specific datasets often leads to overfitting to seen classes and
suboptimal performance on unseen classes, due to the absence of unseen class
labels. To address these challenges, we introduce a novel prompt learning-based
framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce
Large Language Model (LLM) and VLM guidance for learnable prompts, integrating
detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.
However, because training datasets contain seen-class labels alone, fine-tuning
VLMs on such datasets tends to optimize learnable prompts for seen classes
instead of unseen ones. Therefore, we design prompt learning for unseen classes
using information from related seen classes, with LLMs utilized to highlight
the differences between unseen and related seen classes. Quantitative
evaluations on benchmark datasets demonstrate that our EZ-HOI achieves
state-of-the-art performance across various zero-shot settings with only 10.35%
to 33.95% of the trainable parameters compared to existing methods. Code is
available at https://github.com/ChelsieLei/EZ-HOI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vastextures: Vast repository of textures and PBR materials extracted
  from real-world images using unsupervised methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagi Eppel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vastextures is a vast repository of 500,000 textures and PBR materials
extracted from real-world images using an unsupervised process. The extracted
materials and textures are extremely diverse and cover a vast range of
real-world patterns, but at the same time less refined compared to existing
repositories. The repository is composed of 2D textures cropped from natural
images and SVBRDF/PBR materials generated from these textures. Textures and PBR
materials are essential for CGI. Existing materials repositories focus on
games, animation, and arts, that demand a limited amount of high-quality
assets. However, virtual worlds and synthetic data are becoming increasingly
important for training A.I systems for computer vision. This application
demands a huge amount of diverse assets but at the same time less affected by
noisy and unrefined assets. Vastexture aims to address this need by creating a
free, huge, and diverse assets repository that covers as many real-world
materials as possible. The materials are automatically extracted from natural
images in two steps: 1) Automatically scanning a giant amount of images to
identify and crop regions with uniform textures. This is done by splitting the
image into a grid of cells and identifying regions in which all of the cells
share a similar statistical distribution. 2) Extracting the properties of the
PBR material from the cropped texture. This is done by randomly guessing every
correlation between the properties of the texture image and the properties of
the PBR material. The resulting PBR materials exhibit a vast amount of
real-world patterns as well as unexpected emergent properties. Neutral nets
trained on this repository outperformed nets trained using handcrafted assets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Vastexture was published as part of Learning Zero-Shot Material
  States Segmentation, by Implanting Natural Image Patterns in Synthetic Data,
  refer to this work in citations. This document gives a more detailed and
  technical discussion of this repository</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Support-Set Context Matters for Bongard Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Raghuraman, Adam W. Harley, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current machine learning methods struggle to solve Bongard problems, which
are a type of IQ test that requires deriving an abstract "concept" from a set
of positive and negative "support" images, and then classifying whether or not
a new query image depicts the key concept. On Bongard-HOI, a benchmark for
natural-image Bongard problems, most existing methods have reached at best 69%
accuracy (where chance is 50%). Low accuracy is often attributed to neural
nets' lack of ability to find human-like symbolic rules. In this work, we point
out that many existing methods are forfeiting accuracy due to a much simpler
problem: they do not adapt image features given information contained in the
support set as a whole, and rely instead on information extracted from
individual supports. This is a critical issue, because the "key concept" in a
typical Bongard problem can often only be distinguished using multiple
positives and multiple negatives. We explore simple methods to incorporate this
context and show substantial gains over prior works, leading to new
state-of-the-art accuracy on Bongard-LOGO (75.3%) and Bongard-HOI (76.4%)
compared to methods with equivalent vision backbone architectures and strong
performance on the original Bongard problem set (60.8%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR October 2024. Code:
  https://github.com/nraghuraman/bongard-context</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation
  Framework for Efficient Device Model Generalization <span class="chip">WWW'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05227v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05227v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei Wang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Device Model Generalization (DMG) is a practical yet under-investigated
research topic for on-device machine learning applications. It aims to improve
the generalization ability of pre-trained models when deployed on
resource-constrained devices, such as improving the performance of pre-trained
cloud models on smart mobiles. While quite a lot of works have investigated the
data distribution shift across clouds and devices, most of them focus on model
fine-tuning on personalized data for individual devices to facilitate DMG.
Despite their promising, these approaches require on-device re-training, which
is practically infeasible due to the overfitting problem and high time delay
when performing gradient calculation on real-time data. In this paper, we argue
that the computational cost brought by fine-tuning can be rather unnecessary.
We consequently present a novel perspective to improving DMG without increasing
computational cost, i.e., device-specific parameter generation which directly
maps data distribution to parameters. Specifically, we propose an efficient
Device-cloUd collaborative parametErs generaTion framework DUET. DUET is
deployed on a powerful cloud server that only requires the low cost of
forwarding propagation and low time delay of data transmission between the
device and the cloud. By doing so, DUET can rehearse the device-specific model
weight realizations conditioned on the personalized real-time data for an
individual device. Importantly, our DUET elegantly connects the cloud and
device as a 'duet' collaboration, frees the DMG from fine-tuning, and enables a
faster and more accurate DMG paradigm. We conduct an extensive experimental
study of DUET on three public datasets, and the experimental results confirm
our framework's effectiveness and generalisability for different DMG tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on WWW'23: Proceedings of the ACM on Web Conference 2023
  (pp. 3077 - 3085)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligent Model Update Strategy for Sequential Recommendation <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07335v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07335v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, Kun Kuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern online platforms are increasingly employing recommendation systems to
address information overload and improve user engagement. There is an evolving
paradigm in this research field that recommendation network learning occurs
both on the cloud and on edges with knowledge transfer in between (i.e.,
edge-cloud collaboration). Recent works push this field further by enabling
edge-specific context-aware adaptivity, where model parameters are updated in
real-time based on incoming on-edge data. However, we argue that frequent data
exchanges between the cloud and edges often lead to inefficiency and waste of
communication/computation resources, as considerable parameter updates might be
redundant. To investigate this problem, we introduce Intelligent Edge-Cloud
Parameter Request Model, abbreviated as IntellectReq.
  IntellectReq is designed to operate on edge, evaluating the cost-benefit
landscape of parameter requests with minimal computation and communication
overhead. We formulate this as a novel learning task, aimed at the detection of
out-of-distribution data, thereby fine-tuning adaptive communication
strategies. Further, we employ statistical mapping techniques to convert
real-time user behavior into a normal distribution, thereby employing
multi-sample outputs to quantify the model's uncertainty and thus its
generalization capabilities. Rigorous empirical validation on four
widely-adopted benchmarks evaluates our approach, evidencing a marked
improvement in the efficiency and generalizability of edge-cloud collaborative
and dynamic recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on WWW'24(Oral): Proceedings of the ACM on Web Conference
  2024 (pp. 3117-3128)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Retrieval-Augmented Generation for Persian University
  Knowledge Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arshia Hemmat, Kianoosh Vadaei, Mohammad Hassan Heydari, Afsaneh Fatemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative approach using Retrieval-Augmented
Generation (RAG) pipelines with Large Language Models (LLMs) to enhance
information retrieval and query response systems for university-related
question answering. By systematically extracting data from the university
official webpage and employing advanced prompt engineering techniques, we
generate accurate, contextually relevant responses to user queries.
  We developed a comprehensive university benchmark, UniversityQuestionBench
(UQB), to rigorously evaluate our system performance, based on common key
metrics in the filed of RAG pipelines, assessing accuracy and reliability
through various metrics and real-world scenarios. Our experimental results
demonstrate significant improvements in the precision and relevance of
generated responses, enhancing user experience and reducing the time required
to obtain relevant answers. In summary, this paper presents a novel application
of RAG pipelines and LLMs, supported by a meticulously prepared university
benchmark, offering valuable insights into advanced AI techniques for academic
data retrieval and setting the stage for future research in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table, Submitted to 15th IKT conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Data-aware Distance Comparison Operations for High-Dimensional
  Approximate Nearest Neighbor Search <span class="chip">VLDB 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liwei Deng, Penghao Chen, Ximu Zeng, Tianfu Wang, Yan Zhao, Kai Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-dimensional approximate $K$ nearest neighbor search (AKNN) is a
fundamental task for various applications, including information retrieval.
Most existing algorithms for AKNN can be decomposed into two main components,
i.e., candidate generation and distance comparison operations (DCOs). While
different methods have unique ways of generating candidates, they all share the
same DCO process. In this study, we focus on accelerating the process of DCOs
that dominates the time cost in most existing AKNN algorithms. To achieve this,
we propose an Data-Aware Distance Estimation approach, called DADE, which
approximates the exact distance in a lower-dimensional space. We theoretically
prove that the distance estimation in DADE is unbiased in terms of data
distribution. Furthermore, we propose an optimized estimation based on the
unbiased distance estimation formulation. In addition, we propose a hypothesis
testing approach to adaptively determine the number of dimensions needed to
estimate the exact distance with sufficient confidence. We integrate DADE into
widely-used AKNN search algorithms, e.g., IVF and HNSW, and conduct extensive
experiments to demonstrate the superiority.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by VLDB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Potential Field Based Deep Metric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhang Bhatnagar, Narendra Ahuja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep metric learning (DML) involves training a network to learn a
semantically meaningful representation space. Many current approaches mine
n-tuples of examples and model interactions within each tuplets. We present a
novel, compositional DML model, inspired by electrostatic fields in physics
that, instead of in tuples, represents the influence of each example
(embedding) by a continuous potential field, and superposes the fields to
obtain their combined global potential field. We use attractive/repulsive
potential fields to represent interactions among embeddings from images of the
same/different classes. Contrary to typical learning methods, where mutual
influence of samples is proportional to their distance, we enforce reduction in
such influence with distance, leading to a decaying field. We show that such
decay helps improve performance on real world datasets with large intra-class
variations and label noise. Like other proxy-based methods, we also use proxies
to succinctly represent sub-populations of examples. We evaluate our method on
three standard DML benchmarks- Cars-196, CUB-200-2011, and SOP datasets where
it outperforms state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G-RAG: Knowledge Expansion in Material Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radeen Mostafa, Mirza Nihal Baig, Mashaekh Tausif Ehsan, Jakir Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of Material Science, effective information retrieval systems are
essential for facilitating research. Traditional Retrieval-Augmented Generation
(RAG) approaches in Large Language Models (LLMs) often encounter challenges
such as outdated information, hallucinations, limited interpretability due to
context constraints, and inaccurate retrieval. To address these issues, Graph
RAG integrates graph databases to enhance the retrieval process. Our proposed
method processes Material Science documents by extracting key entities
(referred to as MatIDs) from sentences, which are then utilized to query
external Wikipedia knowledge bases (KBs) for additional relevant information.
We implement an agent-based parsing technique to achieve a more detailed
representation of the documents. Our improved version of Graph RAG called G-RAG
further leverages a graph database to capture relationships between these
entities, improving both retrieval accuracy and contextual understanding. This
enhanced approach demonstrates significant improvements in performance for
domains that require precise information retrieval, such as Material Science.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">50</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UTG: Towards a Unified View of Snapshot and Event Based Models for
  Temporal Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenyang Huang, Farimah Poursafaei, Reihaneh Rabbany, Guillaume Rabusseau, Emanuele Rossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real world graphs are inherently dynamic, constantly evolving with node
and edge additions. These graphs can be represented by temporal graphs, either
through a stream of edge events or a sequence of graph snapshots. Until now,
the development of machine learning methods for both types has occurred largely
in isolation, resulting in limited experimental comparison and theoretical
crosspollination between the two. In this paper, we introduce Unified Temporal
Graph (UTG), a framework that unifies snapshot-based and event-based machine
learning models under a single umbrella, enabling models developed for one
representation to be applied effectively to datasets of the other. We also
propose a novel UTG training procedure to boost the performance of
snapshot-based models in the streaming setting. We comprehensively evaluate
both snapshot and event-based models across both types of temporal graphs on
the temporal link prediction task. Our main findings are threefold: first, when
combined with UTG training, snapshot-based models can perform competitively
with event-based models such as TGN and GraphMixer even on event datasets.
Second, snapshot-based models are at least an order of magnitude faster than
most event-based models during inference. Third, while event-based methods such
as NAT and DyGFormer outperforms snapshot-based methods on both types of
temporal graphs, this is because they leverage joint neighborhood structural
features thus emphasizing the potential to incorporate these features into
snapshotbased models as well. These findings highlight the importance of
comparing model architectures independent of the data format and suggest the
potential of combining the efficiency of snapshot-based models with the
performance of event-based models in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Deployment of <span class="highlight-title">Transformer</span> Models in Analog In-Memory Computing
  Hardware 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Li, Corey Lammie, Manuel Le Gallo, Bipin Rajendran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analog in-memory computing (AIMC) has emerged as a promising solution to
overcome the von Neumann bottleneck, accelerating neural network computations
and improving computational efficiency. While AIMC has demonstrated success
with architectures such as CNNs, MLPs, and RNNs, deploying transformer-based
models using AIMC presents unique challenges. Transformers are expected to
handle diverse downstream tasks and adapt to new user data or instructions
after deployment, which requires more flexible approaches to suit AIMC
constraints.
  In this paper, we propose a novel method for deploying pre-trained
transformer models onto AIMC hardware. Unlike traditional approaches requiring
hardware-aware training, our technique allows direct deployment without the
need for retraining the original model. Instead, we utilize lightweight,
low-rank adapters -- compact modules stored in digital cores -- to adapt the
model to hardware constraints. We validate our approach on MobileBERT,
demonstrating accuracy on par with, or even exceeding, a traditional
hardware-aware training approach. Our method is particularly appealing in
multi-task scenarios, as it enables a single analog model to be reused across
multiple tasks. Moreover, it supports on-chip adaptation to new hardware
constraints and tasks without updating analog weights, providing a flexible and
versatile solution for real-world AI applications. Code is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing supply chain security with automated machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Wang, Lutfu S. Sua, Bahram Alidaee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing scale and complexity of global supply chains have led to new
challenges spanning various fields, such as supply chain disruptions due to
long waiting lines at the ports, material shortages, and inflation. Coupled
with the size of supply chains and the availability of vast amounts of data,
efforts towards tackling such challenges have led to an increasing interest in
applying machine learning methods in many aspects of supply chains. Unlike
other solutions, ML techniques, including Random Forest, XGBoost, LightGBM, and
Neural Networks, make predictions and approximate optimal solutions faster.
This paper presents an automated ML framework to enhance supply chain security
by detecting fraudulent activities, predicting maintenance needs, and
forecasting material backorders. Using datasets of varying sizes, results show
that fraud detection achieves an 88% accuracy rate using sampling methods,
machine failure prediction reaches 93.4% accuracy, and material backorder
prediction achieves 89.3% accuracy. Hyperparameter tuning significantly
improved the performance of these models, with certain supervised techniques
like XGBoost and LightGBM reaching up to 100% precision. This research
contributes to supply chain security by streamlining data preprocessing,
feature selection, model optimization, and inference deployment, addressing
critical challenges and boosting operational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive framework for evaluation of deep neural networks in
  detection and quantification of lymphoma from PET/CT images: clinical
  insights, pitfalls, and observer agreement analyses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09614v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09614v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadab Ahamed, Yixi Xu, Sara Kurkowska, Claire Gowdy, Joo H. O, Ingrid Bloise, Don Wilson, Patrick Martineau, François Bénard, Fereshteh Yousefirizi, Rahul Dodhia, Juan M. Lavista, William B. Weeks, Carlos F. Uribe, Arman Rahmim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses critical gaps in automated lymphoma segmentation from
PET/CT images, focusing on issues often overlooked in existing literature.
While deep learning has been applied for lymphoma lesion segmentation, few
studies incorporate out-of-distribution testing, raising concerns about model
generalizability across diverse imaging conditions and patient populations. We
highlight the need to compare model performance with expert human annotators,
including intra- and inter-observer variability, to understand task difficulty
better. Most approaches focus on overall segmentation accuracy but overlook
lesion-specific metrics important for precise lesion detection and disease
quantification.To address these gaps, we propose a clinically-relevant
framework for evaluating deep neural networks. Using this lesion-specific
evaluation, we assess the performance of four deep segmentation networks
(ResUNet, SegResNet, DynUNet, and SwinUNETR) across 611 cases from
multi-institutional datasets, covering various lymphoma subtypes and lesion
characteristics. Beyond standard metrics like the Dice similarity coefficient
(DSC), we evaluate clinical lesion measures and their prediction errors. We
also introduce detection criteria for lesion localization and propose a new
detection Criterion 3 based on metabolic characteristics. We show that networks
perform better on large, intense lesions with higher metabolic
activity.Finally, we compare network performance to expert human observers via
intra- and inter-observer variability analyses, demonstrating that network
errors closely resemble those made by experts. Some small, faint lesions remain
challenging for both humans and networks. This study aims to improve automated
lesion segmentation's clinical relevance, supporting better treatment decisions
for lymphoma patients. The code is available at:
https://github.com/microsoft/lymphoma-segmentation-dnn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 15 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruction Tuning for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10792v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10792v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper surveys research works in the quickly advancing field of
instruction tuning (IT), which can also be referred to as supervised
fine-tuning (SFT)\footnote{In this paper, unless specified otherwise,
supervised fine-tuning (SFT) and instruction tuning (IT) are used
interchangeably.}, a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users' objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of SFT, the
construction of SFT datasets, the training of SFT models, and applications to
different modalities, domains and application, along with analysis on aspects
that influence the outcome of SFT (e.g., generation of instruction outputs,
size of the instruction dataset, etc). We also review the potential pitfalls of
SFT along with criticism against it, along with efforts pointing out current
deficiencies of existing strategies and suggest some avenues for fruitful
research. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V5; Last update: Dec. 1, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Models That Prove Their Own Correctness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15722v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15722v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noga Amit, Shafi Goldwasser, Orr Paradise, Guy Rothblum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we trust the correctness of a learned model on a particular input of
interest? Model accuracy is typically measured *on average* over a distribution
of inputs, giving no guarantee for any fixed input. This paper proposes a
theoretically-founded solution to this problem: to train *Self-Proving models*
that prove the correctness of their output to a verification algorithm $V$ via
an Interactive Proof. Self-Proving models satisfy that, with high probability
over a random input, the model generates a correct output *and* successfully
proves its correctness to $V\!$. The *soundness* property of $V$ guarantees
that, for *every* input, no model can convince $V$ of the correctness of an
incorrect output. Thus, a Self-Proving model proves correctness of most of its
outputs, while *all* incorrect outputs (of any model) are detected by $V$. We
devise a generic method for learning Self-Proving models, and we prove
convergence bounds under certain assumptions. The theoretical framework and
results are complemented by experiments on an arithmetic capability: computing
the greatest common divisor (GCD) of two integers. Our learning method is used
to train a Self-Proving transformer that computes the GCD *and* proves the
correctness of its answer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Constraint Integration for Simultaneously Optimizing Crystal
  Structures with Multiple Targeted Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08562v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08562v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akihiro Fujii, Yoshitaka Ushiku, Koji Shimizu, Anh Khoa Augustin Lu, Satoshi Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In materials science, finding crystal structures that have targeted
properties is crucial. While recent methodologies such as Bayesian optimization
and deep generative models have made some advances on this issue, these methods
often face difficulties in adaptively incorporating various constraints, such
as electrical neutrality and targeted properties optimization, while keeping
the desired specific crystal structure. To address these challenges, we have
developed the Simultaneous Multi-property Optimization using Adaptive Crystal
Synthesizer (SMOACS), which utilizes state-of-the-art property prediction
models and their gradients to directly optimize input crystal structures for
targeted properties simultaneously. SMOACS enables the integration of adaptive
constraints into the optimization process without necessitating model
retraining. Thanks to this feature, SMOACS has succeeded in simultaneously
optimizing targeted properties while maintaining perovskite structures, even
with models trained on diverse crystal types. We have demonstrated the band gap
optimization while meeting a challenging constraint, that is, maintaining
electrical neutrality in large atomic configurations up to 135 atom sites,
where the verification of the electrical neutrality is challenging. The
properties of the most promising materials have been confirmed by density
functional theory calculations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counting Like <span class="highlight-title">Transformer</span>s: Compiling Temporal Counting Logic Into
  Softmax <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Yang, David Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deriving formal bounds on the expressivity of transformers, as well as
studying transformers that are constructed to implement known algorithms, are
both effective methods for better understanding the computational power of
transformers. Towards both ends, we introduce the temporal counting logic
$\textsf{K}_\text{t}$[#] alongside the RASP variant $\textsf{C-RASP}$. We show
they are equivalent to each other, and that together they are the best-known
lower bound on the formal expressivity of future-masked soft attention
transformers with unbounded input size. We prove this by showing all
$\textsf{K}_\text{t}$[#] formulas can be compiled into these transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Blockchain and Biometrics: A <span class="highlight-title">Survey</span> on Technical Aspects and a
  First Legal Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Ghafourian, Bilgesu Sumer, Ruben Vera-Rodriguez, Julian Fierrez, Ruben Tolosana, Aythami Moralez, Els Kindt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biometric recognition as a unique, hard-to-forge, and efficient way of
identification and verification has become an indispensable part of the current
digital world. The fast evolution of this technology has been a strong
incentive for integrating it into many applications. Meanwhile, blockchain, the
very attractive decentralized ledger technology, has been widely received both
by the research and industry in the past years and it is being increasingly
deployed nowadays in many different applications, such as money transfer, IoT,
healthcare, or logistics. Recently, researchers have started to speculate what
would be the pros and cons and what would be the best applications when these
two technologies cross paths. This paper provides a survey of technical
literature research on the combination of blockchain and biometrics and
includes a first legal analysis of this integration to shed light on challenges
and potentials. While this combination is still in its infancy and a growing
body of literature discusses specific blockchain applications and solutions in
an advanced technological set-up, this paper presents a holistic understanding
of blockchains applicability in the biometric sector. This study demonstrates
that combining blockchain and biometrics would be beneficial for novel
applications in biometrics such as the PKI mechanism, distributed trusted
service, and identity management. However, blockchain networks at their current
stage are not efficient and economical for real-time applications. From a legal
point of view, the allocation of accountability remains a main issue, while
other difficulties remain, such as conducting a proper Data Protection Impact
Assessment. Finally, it supplies technical and legal recommendations to reap
the benefits and mitigate the risks of the combination.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SaFL: Sybil-aware Federated Learning with Application to Face
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Ghafourian, Julian Fierrez, Ruben Vera-Rodriguez, Ruben Tolosana, Aythami Morales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a machine learning paradigm to conduct
collaborative learning among clients on a joint model. The primary goal is to
share clients' local training parameters with an integrating server while
preserving their privacy. This method permits to exploit the potential of
massive mobile users' data for the benefit of machine learning models'
performance while keeping sensitive data on local devices. On the downside, FL
raises security and privacy concerns that have just started to be studied. To
address some of the key threats in FL, researchers have proposed to use secure
aggregation methods (e.g. homomorphic encryption, secure multiparty
computation, etc.). These solutions improve some security and privacy metrics,
but at the same time bring about other serious threats such as poisoning
attacks, backdoor attacks, and free running attacks. This paper proposes a new
defense method against poisoning attacks in FL called SaFL (Sybil-aware
Federated Learning) that minimizes the effect of sybils with a novel
time-variant aggregation scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rotation Invariant Quantization for Model Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03106v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03106v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Kampeas, Yury Nahshan, Hanoch Kremer, Gil Lederman, Shira Zaloshinski, Zheng Li, Emir Haleva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training Neural Network (NN) model compression is an attractive approach
for deploying large, memory-consuming models on devices with limited memory
resources. In this study, we investigate the rate-distortion tradeoff for NN
model compression. First, we suggest a Rotation-Invariant Quantization (RIQ)
technique that utilizes a single parameter to quantize the entire NN model,
yielding a different rate at each layer, i.e., mixed-precision quantization.
Then, we prove that our rotation-invariant approach is optimal in terms of
compression. We rigorously evaluate RIQ and demonstrate its capabilities on
various models and tasks. For example, RIQ facilitates $\times 19.4$ and
$\times 52.9$ compression ratios on pre-trained VGG dense and pruned models,
respectively, with $<0.4\%$ accuracy degradation. Code is available in
\href{https://github.com/ehaleva/RIQ}{github.com/ehaleva/RIQ}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Shift Invariance in Convolutional Neural Networks with
  Translation Invariant Polyphase Sampling <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourajit Saha, Tejas Gokhale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Downsampling operators break the shift invariance of convolutional neural
networks (CNNs) and this affects the robustness of features learned by CNNs
when dealing with even small pixel-level shift. Through a large-scale
correlation analysis framework, we study shift invariance of CNNs by inspecting
existing downsampling operators in terms of their maximum-sampling bias (MSB),
and find that MSB is negatively correlated with shift invariance. Based on this
crucial insight, we propose a learnable pooling operator called Translation
Invariant Polyphase Sampling (TIPS) and two regularizations on the intermediate
feature maps of TIPS to reduce MSB and learn translation-invariant
representations. TIPS can be integrated into any CNN and can be trained
end-to-end with marginal computational overhead. Our experiments demonstrate
that TIPS results in consistent performance gains in terms of accuracy, shift
consistency, and shift fidelity on multiple benchmarks for image classification
and semantic segmentation compared to previous methods and also leads to
improvements in adversarial and distributional robustness. TIPS results in the
lowest MSB compared to all previous methods, thus explaining our strong
empirical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Vertical Federated Learning for Practical Training on
  Vertically Partitioned Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avi Amalanshu, Yash Sirvi, David I. Inouye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vertical Federated Learning (VFL) is an emergent distributed machine learning
paradigm for collaborative learning between clients who have disjoint features
of common entities. However, standard VFL lacks fault tolerance, with each
participant and connection being a single point of failure. Prior attempts to
induce fault tolerance in VFL focus on the scenario of "straggling clients",
usually entailing that all messages eventually arrive or that there is an upper
bound on the number of late messages. To handle the more general problem of
arbitrary crashes, we propose Decoupled VFL (DVFL). To handle training with
faults, DVFL decouples training between communication rounds using local
unsupervised objectives. By further decoupling label supervision from
aggregation, DVFL also enables redundant aggregators. As secondary benefits,
DVFL can enhance data efficiency and provides immunity against gradient-based
attacks. In this work, we implement DVFL for split neural networks with a
self-supervised autoencoder loss. When there are faults, DVFL outperforms the
best VFL-based alternative (97.58% vs 96.95% on an MNIST task). Even under
perfect conditions, performance is comparable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised manuscript. Nothing removed, additional baseline results
  added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">BERT</span> or FastText? A Comparative Analysis of Contextual as well as
  Non-Contextual Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhay Shanbhag, Suramya Jadhav, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) for low-resource languages presents
significant challenges, particularly due to the scarcity of high-quality
annotated data and linguistic resources. The choice of embeddings plays a
critical role in enhancing the performance of NLP tasks, such as news
classification, sentiment analysis, and hate speech detection, especially for
low-resource languages like Marathi. In this study, we investigate the impact
of various embedding techniques- Contextual BERT-based, Non-Contextual
BERT-based, and FastText-based on NLP classification tasks specific to the
Marathi language. Our research includes a thorough evaluation of both
compressed and uncompressed embeddings, providing a comprehensive overview of
how these embeddings perform across different scenarios. Specifically, we
compare two BERT model embeddings, Muril and MahaBERT, as well as two FastText
model embeddings, IndicFT and MahaFT. Our evaluation includes applying
embeddings to a Multiple Logistic Regression (MLR) classifier for task
performance assessment, as well as TSNE visualizations to observe the spatial
distribution of these embeddings. The results demonstrate that contextual
embeddings outperform non-contextual embeddings. Furthermore, BERT-based
non-contextual embeddings extracted from the first BERT embedding layer yield
better results than FastText-based embeddings, suggesting a potential
alternative to FastText embeddings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Universality Theorem for Deep and Shallow
  Joint-Group-Equivariant Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13682v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13682v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sho Sonoda, Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a constructive universal approximation theorem for learning
machines equipped with joint-group-equivariant feature maps, called the
joint-equivariant machines, based on the group representation theory.
"Constructive" here indicates that the distribution of parameters is given in a
closed-form expression known as the ridgelet transform.
Joint-group-equivariance encompasses a broad class of feature maps that
generalize classical group-equivariance. Particularly, fully-connected networks
are not group-equivariant but are joint-group-equivariant. Our main theorem
also unifies the universal approximation theorems for both shallow and deep
networks. Until this study, the universality of deep networks has been shown in
a different manner from the universality of shallow networks, but our results
discuss them on common ground. Now we can understand the approximation schemes
of various learning machines in a unified manner. As applications, we show the
constructive universal approximation properties of four examples: depth-$n$
joint-equivariant machine, depth-$n$ fully-connected network, depth-$n$
group-convolutional network, and a new depth-$2$ network with quadratic forms
whose universality has not been known.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy
  Principles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Mesana, Clément Bénesse, Hadrien Lautraite, Gilles Caporossi, Sébastien Gambs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce WaKA (Wasserstein K-nearest-neighbors
Attribution), a novel attribution method that leverages principles from the
LiRA (Likelihood Ratio Attack) framework and k-nearest neighbors classifiers
(k-NN). WaKA efficiently measures the contribution of individual data points to
the model's loss distribution, analyzing every possible k-NN that can be
constructed using the training set, without requiring to sample subsets of the
training set. WaKA is versatile and can be used a posteriori as a membership
inference attack (MIA) to assess privacy risks or a priori for privacy
influence measurement and data valuation. Thus, WaKA can be seen as bridging
the gap between data attribution and membership inference attack (MIA) by
providing a unified framework to distinguish between a data point's value and
its privacy risk. For instance, we have shown that self-attribution values are
more strongly correlated with the attack success rate than the contribution of
a point to the model generalization. WaKA's different usage were also evaluated
across diverse real-world datasets, demonstrating performance very close to
LiRA when used as an MIA on k-NN classifiers, but with greater computational
efficiency. Additionally, WaKA shows greater robustness than Shapley Values for
data minimization tasks (removal or addition) on imbalanced datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PoCo: Policy Composition from and for Heterogeneous Robot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02511v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02511v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lirui Wang, Jialiang Zhao, Yilun Du, Edward H. Adelson, Russ Tedrake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training general robotic policies from heterogeneous data for different tasks
is a significant challenge. Existing robotic datasets vary in different
modalities such as color, depth, tactile, and proprioceptive information, and
collected in different domains such as simulation, real robots, and human
videos. Current methods usually collect and pool all data from one domain to
train a single policy to handle such heterogeneity in tasks and domains, which
is prohibitively expensive and difficult. In this work, we present a flexible
approach, dubbed Policy Composition, to combine information across such diverse
modalities and domains for learning scene-level and task-level generalized
manipulation skills, by composing different data distributions represented with
diffusion models. Our method can use task-level composition for multi-task
manipulation and be composed with analytic cost functions to adapt policy
behaviors at inference time. We train our method on simulation, human, and real
robot data and evaluate in tool-use tasks. The composed policy achieves robust
and dexterous performance under varying scenes and tasks and outperforms
baselines from a single data source in both simulation and real-world
experiments. See https://liruiw.github.io/policycomp for more details .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>R:SS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Burning RED: Unlocking Subtask-Driven Reinforcement Learning and
  Risk-Awareness in Average-Reward Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10578v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10578v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Sebastian Rojas, Chi-Guhn Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Average-reward Markov decision processes (MDPs) provide a foundational
framework for sequential decision-making under uncertainty. However,
average-reward MDPs have remained largely unexplored in reinforcement learning
(RL) settings, with the majority of RL-based efforts having been allocated to
episodic and discounted MDPs. In this work, we study a unique structural
property of average-reward MDPs and utilize it to introduce Reward-Extended
Differential (or RED) reinforcement learning: a novel RL framework that can be
used to effectively and efficiently solve various subtasks simultaneously in
the average-reward setting. We introduce a family of RED learning algorithms
for prediction and control, including proven-convergent algorithms for the
tabular case. We then showcase the power of these algorithms by demonstrating
how they can be used to learn a policy that optimizes, for the first time, the
well-known conditional value-at-risk (CVaR) risk measure in a fully-online
manner, without the use of an explicit bi-level optimization scheme or an
augmented state-space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Aggregators in Neural Algorithmic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijia Xu, Petar Veličković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural algorithmic reasoning (NAR) is an emerging field that seeks to design
neural networks that mimic classical algorithmic computations. Today, graph
neural networks (GNNs) are widely used in neural algorithmic reasoners due to
their message passing framework and permutation equivariance. In this extended
abstract, we challenge this design choice, and replace the equivariant
aggregation function with a recurrent neural network. While seemingly
counter-intuitive, this approach has appropriate grounding when nodes have a
natural ordering -- and this is the case frequently in established reasoning
benchmarks like CLRS-30. Indeed, our recurrent NAR (RNAR) model performs very
strongly on such tasks, while handling many others gracefully. A notable
achievement of RNAR is its decisive state-of-the-art result on the Heapsort and
Quickselect tasks, both deemed as a significant challenge for contemporary
neural algorithmic reasoners -- especially the latter, where RNAR achieves a
mean micro-F1 score of 87%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Third Learning on Graphs Conference (LoG 2024). 10
  pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One to beat them all: "RYU" -- a unifying framework for the construction
  of safe balls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thu-Le Tran, Clément Elvira, Hong-Phuong Dang, Cédric Herzet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new framework, called "RYU" for constructing
"safe" regions -- specifically, bounded sets that are guaranteed to contain the
dual solution of a target optimization problem. Our framework applies to the
standard case where the objective function is composed of two components: a
closed, proper, convex function with Lipschitz-smooth gradient and another
closed, proper, convex function. We show that the RYU framework not only
encompasses but also improves upon the state-of-the-art methods proposed over
the past decade for this class of optimization problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Retrieval-Augmented Generation for Persian University
  Knowledge Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arshia Hemmat, Kianoosh Vadaei, Mohammad Hassan Heydari, Afsaneh Fatemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative approach using Retrieval-Augmented
Generation (RAG) pipelines with Large Language Models (LLMs) to enhance
information retrieval and query response systems for university-related
question answering. By systematically extracting data from the university
official webpage and employing advanced prompt engineering techniques, we
generate accurate, contextually relevant responses to user queries.
  We developed a comprehensive university benchmark, UniversityQuestionBench
(UQB), to rigorously evaluate our system performance, based on common key
metrics in the filed of RAG pipelines, assessing accuracy and reliability
through various metrics and real-world scenarios. Our experimental results
demonstrate significant improvements in the precision and relevance of
generated responses, enhancing user experience and reducing the time required
to obtain relevant answers. In summary, this paper presents a novel application
of RAG pipelines and LLMs, supported by a meticulously prepared university
benchmark, offering valuable insights into advanced AI techniques for academic
data retrieval and setting the stage for future research in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table, Submitted to 15th IKT conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Reasoning Costs -- The Path of Optimization for Chain of
  Thought via Sparse Attention Mechanism <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09111v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09111v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to address the chain of thought in the large language model
inference cost surge, this research proposes to use a sparse attention
mechanism that only focuses on a few relevant tokens. The researcher
constructed a new attention mechanism and used GiantRabbit trained with custom
GPTs as an experimental tool. The experiment tested and compared the reasoning
time, correctness score and chain of thought length of this model and o1
Preview in solving the linear algebra test questions of MIT OpenCourseWare. The
results show that GiantRabbit's reasoning time and chain of thought length are
significantly lower than o1 Preview. It verifies the feasibility of sparse
attention mechanism for optimizing chain of thought reasoning. Detailed
architectural details and experimental process have been uploaded to Github,
the link is:https://github.com/brucewang123456789/GeniusTrail.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The main text is 5 pages, totaling 9 pages; 4 figures, 1 table. It
  have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Federated Learning Over the Air: Combating Heavy-Tailed Noise
  with Median Anchored Clipping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15100v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15100v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxing Li, Zihan Chen, Kai Fong Ernest Chong, Bikramjit Das, Tony Q. S. Quek, Howard H. Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging over-the-air computations for model aggregation is an effective
approach to cope with the communication bottleneck in federated edge learning.
By exploiting the superposition properties of multi-access channels, this
approach facilitates an integrated design of communication and computation,
thereby enhancing system privacy while reducing implementation costs. However,
the inherent electromagnetic interference in radio channels often exhibits
heavy-tailed distributions, giving rise to exceptionally strong noise in
globally aggregated gradients that can significantly deteriorate the training
performance. To address this issue, we propose a novel gradient clipping
method, termed Median Anchored Clipping (MAC), to combat the detrimental
effects of heavy-tailed noise. We also derive analytical expressions for the
convergence rate of model training with analog over-the-air federated learning
under MAC, which quantitatively demonstrates the effect of MAC on training
performance. Extensive experimental results show that the proposed MAC
algorithm effectively mitigates the impact of heavy-tailed noise, hence
substantially enhancing system robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the full version of the paper, and the appendix contains a
  complete convergence analysis under non-convex conditions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skew-Probabilistic Neural Networks for Learning from Imbalanced Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shraddha M. Naik, Tanujit Chakraborty, Madhurima Panja, Abdenour Hadid, Bibhas Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world datasets often exhibit imbalanced data distribution, where certain
class levels are severely underrepresented. In such cases, traditional pattern
classifiers have shown a bias towards the majority class, impeding accurate
predictions for the minority class. This paper introduces an imbalanced
data-oriented classifier using probabilistic neural networks (PNN) with a
skew-normal kernel function to address this major challenge. PNN is known for
providing probabilistic outputs, enabling quantification of prediction
confidence, interpretability, and the ability to handle limited data. By
leveraging the skew-normal distribution, which offers increased flexibility,
particularly for imbalanced and non-symmetric data, our proposed
Skew-Probabilistic Neural Networks (SkewPNN) can better represent underlying
class densities. Hyperparameter fine-tuning is imperative to optimize the
performance of the proposed approach on imbalanced datasets. To this end, we
employ a population-based heuristic algorithm, the Bat optimization algorithm,
to explore the hyperparameter space effectively. We also prove the statistical
consistency of the density estimates, suggesting that the true distribution
will be approached smoothly as the sample size increases. Theoretical analysis
of the computational complexity of the proposed SkewPNN and BA-SkewPNN is also
provided. Numerical simulations have been conducted on different synthetic
datasets, comparing various benchmark-imbalanced learners. Real-data analysis
on several datasets shows that SkewPNN and BA-SkewPNN substantially outperform
most state-of-the-art machine-learning methods for both balanced and imbalanced
datasets (binary and multi-class categories) in most experimental settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational Bottlenecks of Training Small-scale Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19456v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19456v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saleh Ashkboos, Iman Mirzadeh, Keivan Alizadeh, Mohammad Hossein Sekhavat, Moin Nabi, Mehrdad Farajtabar, Fartash Faghri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) dominate the AI landscape, Small-scale
large Language Models (SLMs) are gaining attention due to cost and efficiency
demands from consumers. However, there is limited research on the training
behavior and computational requirements of SLMs. In this study, we explore the
computational bottlenecks of training SLMs (up to 2B parameters) by examining
the effects of various hyperparameters and configurations, including GPU type,
batch size, model size, communication protocol, attention type, and the number
of GPUs. We assess these factors on popular cloud services using metrics such
as loss per dollar and tokens per second. Our findings aim to support the
broader adoption and optimization of language model training for low-resource
AI research institutes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training a neural netwok for data reduction and better generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sylvain Sardy, Maxime van Cutsem, Xiaoyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The motivation for sparse learners is to compress the inputs (features) by
selecting only the ones needed for good generalization. Linear models with
LASSO-type regularization achieve this by setting the weights of irrelevant
features to zero, effectively identifying and ignoring them. In artificial
neural networks, this selective focus can be achieved by pruning the input
layer. Given a cost function enhanced with a sparsity-promoting penalty, our
proposal selects a regularization term $\lambda$ (without the use of
cross-validation or a validation set) that creates a local minimum in the cost
function at the origin where no features are selected. This local minimum acts
as a baseline, meaning that if there is no strong enough signal to justify a
feature inclusion, the local minimum remains at zero with a high prescribed
probability. The method is flexible, applying to complex models ranging from
shallow to deep artificial neural networks and supporting various cost
functions and sparsity-promoting penalties. We empirically show a remarkable
phase transition in the probability of retrieving the relevant features, as
well as good generalization thanks to the choice of $\lambda$, the non-convex
penalty and the optimization scheme developed. This approach can be seen as a
form of compressed sensing for complex models, allowing us to distill
high-dimensional data into a compact, interpretable subset of meaningful
features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Binary Feature Mask Optimization for Feature Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehmet E. Lorasdagi, Mehmet Y. Turali, Suleyman S. Kozat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate feature selection problem for generic machine learning models.
We introduce a novel framework that selects features considering the outcomes
of the model. Our framework introduces a novel feature masking approach to
eliminate the features during the selection process, instead of completely
removing them from the dataset. This allows us to use the same machine learning
model during feature selection, unlike other feature selection methods where we
need to train the machine learning model again as the dataset has different
dimensions on each iteration. We obtain the mask operator using the predictions
of the machine learning model, which offers a comprehensive view on the subsets
of the features essential for the predictive performance of the model. A
variety of approaches exist in the feature selection literature. However, to
our knowledge, no study has introduced a training-free framework for a generic
machine learning model to select features while considering the importance of
the feature subsets as a whole, instead of focusing on the individual features.
We demonstrate significant performance improvements on the real-life datasets
under different settings using LightGBM and Multi-Layer Perceptron as our
machine learning models. The high performance of our General Binary Mask
Optimization algorithm stems from its feature masking approach to select
features and its flexibility in the number of selected features. The algorithm
selects features based on the validation performance of the machine learning
model. Hence, the number of selected features is not predetermined and adjusts
dynamically to the dataset. Additionally, we openly share the implementation or
our code to encourage further research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Corn Yield Prediction Model with Deep Neural Networks for Smallholder
  Farmer Decision Support System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03768v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03768v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chollette C. Olisah, Lyndon Smith, Melvyn Smith, Morolake O. Lawrence, Osita Ojukwu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crop yield prediction has been modeled on the assumption that there is no
interaction between weather and soil variables. However, this paper argues that
an interaction exists, and it can be finely modelled using the Kendall
Correlation coefficient. Given the nonlinearity of the interaction between
weather and soil variables, a deep neural network regressor (DNNR) is carefully
designed with consideration to the depth, number of neurons of the hidden
layers, and the hyperparameters with their optimizations. Additionally, a new
metric, the average of absolute root squared error (ARSE) is proposed to
combine the strengths of root mean square error (RMSE) and mean absolute error
(MAE). With the ARSE metric, the proposed DNNR(s), optimised random forest
regressor (RFR) and the extreme gradient boosting regressor (XGBR) achieved
impressively small yield errors, 0.0172 t/ha, and 0.0243 t/ha, 0.0001 t/ha, and
0.001 t/ha, respectively. However, the DNNR(s), with changes to the explanatory
variables to ensure generalizability to unforeseen data, DNNR(s) performed
best. Further analysis reveals that a strong interaction does exist between
weather and soil variables. Precisely, yield is observed to increase when
precipitation is reduced and silt increased, and vice-versa. However, the
degree of decrease or increase is not quantified in this paper. Contrary to
existing yield models targeted towards agricultural policies and global food
security, the goal of the proposed corn yield model is to empower the
smallholder farmer to farm smartly and intelligently, thus the prediction model
is integrated into a mobile application that includes education, and a
farmer-to-market access module.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 Pages, 11 Figures, 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DoorINet: Door Heading Prediction through Inertial Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksei Zakharchenko, Sharon Farber, Itzik Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inertial sensors are widely used in a variety of applications. A common task
is orientation estimation. To tackle such a task, attitude and heading
reference system algorithms are applied. Relying on the gyroscope readings, the
accelerometer measurements are used to update the attitude angles, and
magnetometer measurements are utilized to update the heading angle. In indoor
environments, magnetometers suffer from interference that degrades their
performance resulting in poor heading angle estimation. Therefore, applications
that estimate the heading angle of moving objects, such as walking pedestrians,
closets, and refrigerators, are prone to error. To circumvent such situations,
we propose DoorINet, an end-to-end deep-learning framework to calculate the
heading angle from door-mounted, low-cost inertial sensors without using
magnetometers. To evaluate our approach, we record a unique dataset containing
391 minutes of accelerometer and gyroscope measurements and corresponding
ground-truth heading angle. We show that our proposed approach outperforms
commonly used, model based approaches and data-driven methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 14 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Point Attention <span class="highlight-title">Transformer</span> for 3D Shape Reassembly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahan Li, Chaoran Cheng, Jianzhu Ma, Ge Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shape assembly, which aims to reassemble separate parts into a complete
object, has gained significant interest in recent years. Existing methods
primarily rely on networks to predict the poses of individual parts, but often
fail to effectively capture the geometric interactions between the parts and
their poses. In this paper, we present the Geometric Point Attention
Transformer (GPAT), a network specifically designed to address the challenges
of reasoning about geometric relationships. In the geometric point attention
module, we integrate both global shape information and local pairwise geometric
features, along with poses represented as rotation and translation vectors for
each part. To enable iterative updates and dynamic reasoning, we introduce a
geometric recycling scheme, where each prediction is fed into the next
iteration for refinement. We evaluate our model on both the semantic and
geometric assembly tasks, showing that it outperforms previous methods in
absolute pose estimation, achieving accurate pose predictions and high
alignment accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PINNfluence: Influence Functions for Physics-Informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas R. Naujoks, Aleksander Krasowski, Moritz Weckbecker, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek, René P. Klausen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, physics-informed neural networks (PINNs) have emerged as a flexible
and promising application of deep learning to partial differential equations in
the physical sciences. While offering strong performance and competitive
inference speeds on forward and inverse problems, their black-box nature limits
interpretability, particularly regarding alignment with expected physical
behavior. In the present work, we explore the application of influence
functions (IFs) to validate and debug PINNs post-hoc. Specifically, we apply
variations of IF-based indicators to gauge the influence of different types of
collocation points on the prediction of PINNs applied to a 2D Navier-Stokes
fluid flow problem. Our results demonstrate how IFs can be adapted to PINNs to
reveal the potential for further studies. The code is publicly available at
https://github.com/aleks-krasowski/PINNfluence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Circuit Complexity Bounds for RoPE-based <span class="highlight-title">Transformer</span> Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Chen, Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterizing the express power of the Transformer architecture is critical
to understanding its capacity limits and scaling law. Recent works provide the
circuit complexity bounds to Transformer-like architecture. On the other hand,
Rotary Position Embedding ($\mathsf{RoPE}$) has emerged as a crucial technique
in modern large language models, offering superior performance in capturing
positional information compared to traditional position embeddings, which shows
great potential in application prospects, particularly for the long context
scenario. Empirical evidence also suggests that $\mathsf{RoPE}$-based
Transformer architectures demonstrate greater generalization capabilities
compared to conventional Transformer models. In this work, we establish a
circuit complexity bound for Transformers with $\mathsf{RoPE}$ attention. Our
key contribution is that we show that unless $\mathsf{TC}^0 = \mathsf{NC}^1$, a
$\mathsf{RoPE}$-based Transformer with $\mathrm{poly}(n)$-precision, $O(1)$
layers, hidden dimension $d \leq O(n)$ cannot solve the Arithmetic formula
evaluation problem or the Boolean formula value problem. This result
significantly demonstrates the fundamental limitation of the expressivity of
the $\mathsf{RoPE}$-based Transformer architecture, although it achieves giant
empirical success. Our theoretical result not only establishes the complexity
bound but also may instruct further work on the $\mathsf{RoPE}$-based
Transformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02490v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02490v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MM-Vet, an evaluation benchmark that examines large multimodal
models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various
intriguing abilities, such as solving math problems written on the blackboard,
reasoning about events and celebrities in news images, and explaining visual
jokes. Rapid model advancements pose challenges to evaluation benchmark
development. Problems include: (1) How to systematically structure and evaluate
the complicated multimodal tasks; (2) How to design evaluation metrics that
work well across question and answer types; and (3) How to give model insights
beyond a simple performance ranking. To this end, we present MM-Vet, designed
based on the insight that the intriguing ability to solve complicated tasks is
often achieved by a generalist model being able to integrate different core
vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and
examines the 16 integrations of interest derived from the capability
combination. For evaluation metrics, we propose an LLM-based evaluator for
open-ended outputs. The evaluator enables the evaluation across different
question types and answer styles, resulting in a unified scoring metric. We
evaluate representative LMMs on MM-Vet, providing insights into the
capabilities of different LMM system paradigms and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024. Code, data and leaderboard:
  https://github.com/yuweihao/MM-Vet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Potential Field Based Deep Metric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhang Bhatnagar, Narendra Ahuja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep metric learning (DML) involves training a network to learn a
semantically meaningful representation space. Many current approaches mine
n-tuples of examples and model interactions within each tuplets. We present a
novel, compositional DML model, inspired by electrostatic fields in physics
that, instead of in tuples, represents the influence of each example
(embedding) by a continuous potential field, and superposes the fields to
obtain their combined global potential field. We use attractive/repulsive
potential fields to represent interactions among embeddings from images of the
same/different classes. Contrary to typical learning methods, where mutual
influence of samples is proportional to their distance, we enforce reduction in
such influence with distance, leading to a decaying field. We show that such
decay helps improve performance on real world datasets with large intra-class
variations and label noise. Like other proxy-based methods, we also use proxies
to succinctly represent sub-populations of examples. We evaluate our method on
three standard DML benchmarks- Cars-196, CUB-200-2011, and SOP datasets where
it outperforms state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InvestESG: A multi-agent reinforcement learning benchmark for studying
  climate investment as a social dilemma 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09856v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09856v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxuan Hou, Jiayi Yuan, Joel Z. Leibo, Natasha Jaques
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark
designed to study the impact of Environmental, Social, and Governance (ESG)
disclosure mandates on corporate climate investments. Supported by both PyTorch
and JAX implementation, the benchmark models an intertemporal social dilemma
where companies balance short-term profit losses from climate mitigation
efforts and long-term benefits from reducing climate risk, while ESG-conscious
investors attempt to influence corporate behavior through their investment
decisions, in a scalable and hardware-accelerated manner. Companies allocate
capital across mitigation, greenwashing, and resilience, with varying
strategies influencing climate outcomes and investor preferences. Our
experiments show that without ESG-conscious investors with sufficient capital,
corporate mitigation efforts remain limited under the disclosure mandate.
However, when a critical mass of investors prioritizes ESG, corporate
cooperation increases, which in turn reduces climate risks and enhances
long-term financial stability. Additionally, providing more information about
global climate risks encourages companies to invest more in mitigation, even
without investor involvement. Our findings align with empirical research using
real-world data, highlighting MARL's potential to inform policy by providing
insights into large-scale socio-economic challenges through efficient testing
of alternative policy and market designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Hessian Fittings with Lie Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11858v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11858v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi-Lin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report studies the fitting of Hessian or its inverse for stochastic
optimizations using a Hessian fitting criterion from the preconditioned
stochastic gradient descent (PSGD) method, which is intimately related to many
commonly used second-order and adaptive gradient optimizers, e.g., BFGS,
Gaussian-Newton algorithm, natural gradient descent, AdaGrad, etc. Our analyses
reveal the efficiency and reliability differences among a wide range of
preconditioner fitting methods, from closed-form to iterative solutions, using
Hessian-vector products or stochastic gradients only, with Hessian fittings in
the Euclidean space, the manifold of symmetric positive definite (SPL)
matrices, to a variety of Lie groups. The most intriguing discovery is that the
Hessian fitting itself as an optimization problem is strongly convex under mild
conditions in certain general Lie groups. This discovery turns Hessian fitting
into a well-behaved Lie group optimization problem and facilitates the designs
of highly efficient and elegant Lie group sparse preconditioner fitting methods
for large-scale stochastic optimizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages; 6 figures; 3 tables; code
  https://github.com/lixilinx/psgd_torch</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unmasking Trees for Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05593v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05593v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Calvin McCarter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite much work on advanced deep learning and generative modeling
techniques for tabular data generation and imputation, traditional methods have
continued to win on imputation benchmarks. We herein present UnmaskingTrees, a
simple method for tabular imputation (and generation) employing
gradient-boosted decision trees which are used to incrementally unmask
individual features. This approach offers state-of-the-art performance on
imputation, and on generation given training data with missingness; and it has
competitive performance on vanilla generation. To solve the conditional
generation subproblem, we propose a tabular probabilistic prediction method,
BaltoBot, which fits a balanced tree of boosted tree classifiers. Unlike older
methods, it requires no parametric assumption on the conditional distribution,
accommodating features with multimodal distributions; unlike newer diffusion
methods, it offers fast sampling, closed-form density estimation, and flexible
handling of discrete variables. We finally consider our two approaches as
meta-algorithms, demonstrating in-context learning-based generative modeling
with TabPFN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v0.3.0 of UnmaskingTrees software</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Introduction to Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majid Ghasemi, Dariush Ebrahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL), a subfield of Artificial Intelligence (AI),
focuses on training agents to make decisions by interacting with their
environment to maximize cumulative rewards. This paper provides an overview of
RL, covering its core concepts, methodologies, and resources for further
learning. It offers a thorough explanation of fundamental components such as
states, actions, policies, and reward signals, ensuring readers develop a solid
foundational understanding. Additionally, the paper presents a variety of RL
algorithms, categorized based on the key factors such as model-free,
model-based, value-based, policy-based, and other key factors. Resources for
learning and implementing RL, such as books, courses, and online communities
are also provided. By offering a clear, structured introduction, this paper
aims to simplify the complexities of RL for beginners, providing a
straightforward pathway to understanding and applying real-time techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SongBsAb: A Dual Prevention Approach against Singing Voice Conversion
  based Illegal Song Covers <span class="chip">NDSS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangke Chen, Yedi Zhang, Fu Song, Ting Wang, Xiaoning Du, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singing voice conversion (SVC) automates song covers by converting a source
singing voice from a source singer into a new singing voice with the same
lyrics and melody as the source, but sounds like being covered by the target
singer of some given target singing voices. However, it raises serious concerns
about copyright and civil right infringements. We propose SongBsAb, the first
proactive approach to tackle SVC-based illegal song covers. SongBsAb adds
perturbations to singing voices before releasing them, so that when they are
used, the process of SVC will be interfered, leading to unexpected singing
voices. Perturbations are carefully crafted to (1) provide a dual prevention,
i.e., preventing the singing voice from being used as the source and target
singing voice in SVC, by proposing a gender-transformation loss and a high/low
hierarchy multi-target loss, respectively; and (2) be harmless, i.e., no
side-effect on the enjoyment of protected songs, by refining a psychoacoustic
model-based loss with the backing track as an additional masker, a unique
accompanying element for singing voices compared to ordinary speech voices. We
also adopt a frame-level interaction reduction-based loss and encoder ensemble
to enhance the transferability of SongBsAb to unknown SVC models. We
demonstrate the prevention effectiveness, harmlessness, and robustness of
SongBsAb on five diverse and promising SVC models, using both English and
Chinese datasets, and both objective and human study-based subjective metrics.
Our work fosters an emerging research direction for mitigating illegal
automated song covers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 32nd Network and Distributed System Security
  (NDSS) Symposium 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clustering with Neural Network and Index 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03853v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03853v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new model called Clustering with Neural Network and Index (CNNI) is
introduced. CNNI uses a Neural Network to cluster data points. Training of the
Neural Network mimics supervised learning, with an internal clustering
evaluation index acting as the loss function. An experiment is conducted to
test the feasibility of the new model, and compared with results of other
clustering models like K-means and Gaussian Mixture Model (GMM). The result
shows CNNI can work properly for clustering data; CNNI equipped with MMJ-SC,
achieves the first parametric (inductive) clustering model that can deal with
non-convex shaped (non-flat geometry) data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sliced-Wasserstein-based Anomaly Detection and Open <span class="highlight-title">Dataset</span> for
  Localized Critical Peak Rebates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Pallage, Bertrand Scherrer, Salma Naccache, Christophe Bélanger, Antoine Lesage-Landry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a new unsupervised anomaly (outlier) detection (AD)
method using the sliced-Wasserstein metric. This filtering technique is
conceptually interesting for MLOps pipelines deploying machine learning models
in critical sectors, e.g., energy, as it offers a conservative data selection.
Additionally, we open the first dataset showcasing localized critical peak
rebate demand response in a northern climate. We demonstrate the capabilities
of our method on synthetic datasets as well as standard AD datasets and use it
in the making of a first benchmark for our open-source localized critical peak
rebate dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Job-SDF: A Multi-Granularity <span class="highlight-title">Dataset</span> for Job Skill Demand Forecasting
  and Benchmarking <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11920v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11920v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Chuan Qin, Chuyu Fang, Chao Wang, Chen Zhu, Fuzhen Zhuang, Hengshu Zhu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a rapidly evolving job market, skill demand forecasting is crucial as it
enables policymakers and businesses to anticipate and adapt to changes,
ensuring that workforce skills align with market needs, thereby enhancing
productivity and competitiveness. Additionally, by identifying emerging skill
requirements, it directs individuals towards relevant training and education
opportunities, promoting continuous self-learning and development. However, the
absence of comprehensive datasets presents a significant challenge, impeding
research and the advancement of this field. To bridge this gap, we present
Job-SDF, a dataset designed to train and benchmark job-skill demand forecasting
models. Based on 10.35 million public job advertisements collected from major
online recruitment platforms in China between 2021 and 2023, this dataset
encompasses monthly recruitment demand for 2,324 types of skills across 521
companies. Our dataset uniquely enables evaluating skill demand forecasting
models at various granularities, including occupation, company, and regional
levels. We benchmark a range of models on this dataset, evaluating their
performance in standard scenarios, in predictions focused on lower value
ranges, and in the presence of structural breaks, providing new insights for
further research. Our code and dataset are publicly accessible via the
https://github.com/Job-SDF/benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Dynamic Message Passing on Graphs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junshu Sun, Chenxue Yang, Xiangyang Ji, Qingming Huang, Shuhui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message passing plays a vital role in graph neural networks (GNNs) for
effective feature learning. However, the over-reliance on input topology
diminishes the efficacy of message passing and restricts the ability of GNNs.
Despite efforts to mitigate the reliance, existing study encounters
message-passing bottlenecks or high computational expense problems, which
invokes the demands for flexible message passing with low complexity. In this
paper, we propose a novel dynamic message-passing mechanism for GNNs. It
projects graph nodes and learnable pseudo nodes into a common space with
measurable spatial relations between them. With nodes moving in the space,
their evolving relations facilitate flexible pathway construction for a dynamic
message-passing process. Associating pseudo nodes to input graphs with their
measured relations, graph nodes can communicate with each other intermediately
through pseudo nodes under linear complexity. We further develop a GNN model
named $\mathtt{\mathbf{N^2}}$ based on our dynamic message-passing mechanism.
$\mathtt{\mathbf{N^2}}$ employs a single recurrent layer to recursively
generate the displacements of nodes and construct optimal dynamic pathways.
Evaluation on eighteen benchmarks demonstrates the superior performance of
$\mathtt{\mathbf{N^2}}$ over popular GNNs. $\mathtt{\mathbf{N^2}}$ successfully
scales to large-scale benchmarks and requires significantly fewer parameters
for graph classification with the shared recurrent layer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL <span class="chip">NeurIPS '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15876v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15876v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Woosung Koh, Wonbeen Oh, Siyeol Kim, Suhin Shin, Hyeongjin Kim, Jaein Jang, Junghyun Lee, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent reinforcement learning has demonstrated significant potential in
addressing complex cooperative tasks across various real-world applications.
However, existing MARL approaches often rely on the restrictive assumption that
the number of entities (e.g., agents, obstacles) remains constant between
training and inference. This overlooks scenarios where entities are dynamically
removed or added during the inference trajectory -- a common occurrence in
real-world environments like search and rescue missions and dynamic combat
situations. In this paper, we tackle the challenge of intra-trajectory dynamic
entity composition under zero-shot out-of-domain (OOD) generalization, where
such dynamic changes cannot be anticipated beforehand. Our empirical studies
reveal that existing MARL methods suffer significant performance degradation
and increased uncertainty in these scenarios. In response, we propose
FlickerFusion, a novel OOD generalization method that acts as a universally
applicable augmentation technique for MARL backbone methods. FlickerFusion
stochastically drops out parts of the observation space, emulating being
in-domain when inferenced OOD. The results show that FlickerFusion not only
achieves superior inference rewards but also uniquely reduces uncertainty
vis-\`a-vis the backbone, compared to existing methods. Benchmarks,
implementations, and model weights are organized and open-sourced at
flickerfusion305.github.io, accompanied by ample demo video renderings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS '24 Open-World Agents Workshop (v2: minor revision)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree-Wasserstein Distance for High Dimensional Data with a Latent
  Feature Hierarchy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya-Wei Eileen Lin, Ronald R. Coifman, Gal Mishne, Ronen Talmon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding meaningful distances between high-dimensional data samples is an
important scientific task. To this end, we propose a new tree-Wasserstein
distance (TWD) for high-dimensional data with two key aspects. First, our TWD
is specifically designed for data with a latent feature hierarchy, i.e., the
features lie in a hierarchical space, in contrast to the usual focus on
embedding samples in hyperbolic space. Second, while the conventional use of
TWD is to speed up the computation of the Wasserstein distance, we use its
inherent tree as a means to learn the latent feature hierarchy. The key idea of
our method is to embed the features into a multi-scale hyperbolic space using
diffusion geometry and then present a new tree decoding method by establishing
analogies between the hyperbolic embedding and trees. We show that our TWD
computed based on data observations provably recovers the TWD defined with the
latent feature hierarchy and that its computation is efficient and scalable. We
showcase the usefulness of the proposed TWD in applications to word-document
and single-cell RNA-sequencing datasets, demonstrating its advantages over
existing TWDs and methods based on pre-trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DistRL: An Asynchronous Distributed Reinforcement Learning Framework for
  On-Device Control Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14803v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14803v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On-device control agents, especially on mobile devices, are responsible for
operating mobile devices to fulfill users' requests, enabling seamless and
intuitive interactions. Integrating Multimodal Large Language Models (MLLMs)
into these agents enhances their ability to understand and execute complex
commands, thereby improving user experience. However, fine-tuning MLLMs for
on-device control presents significant challenges due to limited data
availability and inefficient online training processes. This paper introduces
DistRL, a novel framework designed to enhance the efficiency of online RL
fine-tuning for mobile device control agents. DistRL employs centralized
training and decentralized data acquisition to ensure efficient fine-tuning in
the context of dynamic online interactions. Additionally, the framework is
backed by our tailor-made RL algorithm, which effectively balances exploration
with the prioritized utilization of collected data to ensure stable and robust
training. Our experiments show that, on average, DistRL delivers a 3X
improvement in training efficiency and enables training data collection 2.4X
faster than the leading synchronous multi-machine methods. Notably, after
training, DistRL achieves a 20% relative improvement in success rate compared
to state-of-the-art methods on general Android tasks from an open benchmark,
significantly outperforming existing approaches while maintaining the same
training time. These results validate DistRL as a scalable and efficient
solution, offering substantial improvements in both training efficiency and
agent performance for real-world, in-the-wild device control tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper and Appendix, 26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Batch Calibration: Rethinking Calibration for In-Context Learning and
  <span class="highlight-title">Prompt</span> Engineering <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, Subhrajit Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting and in-context learning (ICL) have become efficient learning
paradigms for large language models (LLMs). However, LLMs suffer from prompt
brittleness and various bias factors in the prompt, including but not limited
to the formatting, the choice verbalizers, and the ICL examples. To address
this problem that results in unexpected performance degradation, calibration
methods have been developed to mitigate the effects of these biases while
recovering LLM performance. In this work, we first conduct a systematic
analysis of the existing calibration methods, where we both provide a unified
view and reveal the failure cases. Inspired by these analyses, we propose Batch
Calibration (BC), a simple yet intuitive method that controls the contextual
bias from the batched input, unifies various prior approaches, and effectively
addresses the aforementioned issues. BC is zero-shot, inference-only, and
incurs negligible additional costs. In the few-shot setup, we further extend BC
to allow it to learn the contextual bias from labeled data. We validate the
effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate
state-of-the-art performance over previous calibration baselines across more
than 10 natural language understanding and image classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alternators For Sequence Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Rezaei, Adji Bousso Dieng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces alternators, a novel family of non-Markovian dynamical
models for sequences. An alternator features two neural networks: the
observation trajectory network (OTN) and the feature trajectory network (FTN).
The OTN and the FTN work in conjunction, alternating between outputting samples
in the observation space and some feature space, respectively, over a cycle.
The parameters of the OTN and the FTN are not time-dependent and are learned
via a minimum cross-entropy criterion over the trajectories. Alternators are
versatile. They can be used as dynamical latent-variable generative models or
as sequence-to-sequence predictors. Alternators can uncover the latent dynamics
underlying complex sequential data, accurately forecast and impute missing
data, and sample new trajectories. We showcase the capabilities of alternators
in three applications. We first used alternators to model the Lorenz equations,
often used to describe chaotic behavior. We then applied alternators to
Neuroscience, to map brain activity to physical activity. Finally, we applied
alternators to Climate Science, focusing on sea-surface temperature
forecasting. In all our experiments, we found alternators are stable to train,
fast to sample from, yield high-quality generated samples and latent variables,
and often outperform strong baselines such as Mambas, neural ODEs, and
diffusion models in the domains we studied.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A new versatile family of sequence models that can be used for both
  generative modeling and supervised learning. The codebase will be made
  available upon publication. This paper is dedicated to Thomas Sankara</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provably Scalable Black-Box Variational Inference with Structured
  Variational Families <span class="chip">ICML'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10989v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10989v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joohwan Ko, Kyurae Kim, Woo Chang Kim, Jacob R. Gardner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational families with full-rank covariance approximations are known not
to work well in black-box variational inference (BBVI), both empirically and
theoretically. In fact, recent computational complexity results for BBVI have
established that full-rank variational families scale poorly with the
dimensionality of the problem compared to e.g. mean-field families. This is
particularly critical to hierarchical Bayesian models with local variables;
their dimensionality increases with the size of the datasets. Consequently, one
gets an iteration complexity with an explicit $\mathcal{O}(N^2)$ dependence on
the dataset size $N$. In this paper, we explore a theoretical middle ground
between mean-field variational families and full-rank families: structured
variational families. We rigorously prove that certain scale matrix structures
can achieve a better iteration complexity of $\mathcal{O}\left(N\right)$,
implying better scaling with respect to $N$. We empirically verify our
theoretical results on large-scale hierarchical models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML'24; v3: fixed typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Support-Set Context Matters for Bongard Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Raghuraman, Adam W. Harley, Leonidas Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current machine learning methods struggle to solve Bongard problems, which
are a type of IQ test that requires deriving an abstract "concept" from a set
of positive and negative "support" images, and then classifying whether or not
a new query image depicts the key concept. On Bongard-HOI, a benchmark for
natural-image Bongard problems, most existing methods have reached at best 69%
accuracy (where chance is 50%). Low accuracy is often attributed to neural
nets' lack of ability to find human-like symbolic rules. In this work, we point
out that many existing methods are forfeiting accuracy due to a much simpler
problem: they do not adapt image features given information contained in the
support set as a whole, and rely instead on information extracted from
individual supports. This is a critical issue, because the "key concept" in a
typical Bongard problem can often only be distinguished using multiple
positives and multiple negatives. We explore simple methods to incorporate this
context and show substantial gains over prior works, leading to new
state-of-the-art accuracy on Bongard-LOGO (75.3%) and Bongard-HOI (76.4%)
compared to methods with equivalent vision backbone architectures and strong
performance on the original Bongard problem set (60.8%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR October 2024. Code:
  https://github.com/nraghuraman/bongard-context</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Separate Anything You Describe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05037v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05037v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xubo Liu, Qiuqiang Kong, Yan Zhao, Haohe Liu, Yi Yuan, Yuzhuo Liu, Rui Xia, Yuxuan Wang, Mark D. Plumbley, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-queried audio source separation (LASS) is a new paradigm for
computational auditory scene analysis (CASA). LASS aims to separate a target
sound from an audio mixture given a natural language query, which provides a
natural and scalable interface for digital audio applications. Recent works on
LASS, despite attaining promising separation performance on specific sources
(e.g., musical instruments, limited classes of audio events), are unable to
separate audio concepts in the open domain. In this work, we introduce
AudioSep, a foundation model for open-domain audio source separation with
natural language queries. We train AudioSep on large-scale multimodal datasets
and extensively evaluate its capabilities on numerous tasks including audio
event separation, musical instrument separation, and speech enhancement.
AudioSep demonstrates strong separation performance and impressive zero-shot
generalization ability using audio captions or text labels as queries,
substantially outperforming previous audio-queried and language-queried sound
separation models. For reproducibility of this work, we will release the source
code, evaluation benchmark and pre-trained model at:
https://github.com/Audio-AGI/AudioSep.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, benchmark and pre-trained models:
  https://github.com/Audio-AGI/AudioSep</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SongBsAb: A Dual Prevention Approach against Singing Voice Conversion
  based Illegal Song Covers <span class="chip">NDSS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangke Chen, Yedi Zhang, Fu Song, Ting Wang, Xiaoning Du, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singing voice conversion (SVC) automates song covers by converting a source
singing voice from a source singer into a new singing voice with the same
lyrics and melody as the source, but sounds like being covered by the target
singer of some given target singing voices. However, it raises serious concerns
about copyright and civil right infringements. We propose SongBsAb, the first
proactive approach to tackle SVC-based illegal song covers. SongBsAb adds
perturbations to singing voices before releasing them, so that when they are
used, the process of SVC will be interfered, leading to unexpected singing
voices. Perturbations are carefully crafted to (1) provide a dual prevention,
i.e., preventing the singing voice from being used as the source and target
singing voice in SVC, by proposing a gender-transformation loss and a high/low
hierarchy multi-target loss, respectively; and (2) be harmless, i.e., no
side-effect on the enjoyment of protected songs, by refining a psychoacoustic
model-based loss with the backing track as an additional masker, a unique
accompanying element for singing voices compared to ordinary speech voices. We
also adopt a frame-level interaction reduction-based loss and encoder ensemble
to enhance the transferability of SongBsAb to unknown SVC models. We
demonstrate the prevention effectiveness, harmlessness, and robustness of
SongBsAb on five diverse and promising SVC models, using both English and
Chinese datasets, and both objective and human study-based subjective metrics.
Our work fosters an emerging research direction for mitigating illegal
automated song covers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 32nd Network and Distributed System Security
  (NDSS) Symposium 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-30T00:00:00Z">2024-11-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Review</span> of Prominent Paradigms for LLM-Based Agents: Tool Use
  (Including RAG), Planning, and Feedback Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05804v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05804v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhe Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool use, planning, and feedback learning are currently three prominent
paradigms for developing Large Language Model (LLM)-based agents across various
tasks. Although numerous frameworks have been devised for each paradigm, their
intricate workflows and inconsistent taxonomy create challenges in
understanding and reviewing the frameworks across different paradigms. This
survey introduces a unified taxonomy to systematically review and discuss these
frameworks. Specifically, 1) the taxonomy defines environments/tasks, common
LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),
and universally applicable workflows found in prior work, and 2) it enables a
comparison of key perspectives on the implementations of LMPRs and workflow
designs across different agent paradigms and frameworks. 3) Finally, we
identify three limitations in existing workflow designs and systematically
discuss the future work. Resources have been made publicly available at in our
GitHub repository https://github.com/xinzhel/LLM-Agent-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoLing 2025 Camera Ready (extended to 9 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Large Language Model-empowered Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14165v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14165v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhu, Shiyi Wang, Wenqing Zhong, Nianchen Shen, Yunqi Li, Siqi Wang, Zhiheng Li, Cathy Wu, Zhengbing He, Li Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) plays a crucial role in autonomous driving (AD)
research, propelling its development towards intelligence and efficiency.
Currently, the development of AD technology follows two main technical paths:
modularization and end-to-end. Modularization decompose the driving task into
modules such as perception, prediction, planning, and control, and train them
separately. Due to the inconsistency of training objectives between modules,
the integrated effect suffers from bias. End-to-end attempts to address this
issue by utilizing a single model that directly maps from sensor data to
control signals. This path has limited learning capabilities in a comprehensive
set of features and struggles to handle unpredictable long-tail events and
complex urban traffic scenarios. In the face of challenges encountered in both
paths, many researchers believe that large language models (LLMs) with powerful
reasoning capabilities and extensive knowledge understanding may be the
solution, expecting LLMs to provide AD systems with deeper levels of
understanding and decision-making capabilities. In light of the challenges
faced by both paths, many researchers believe that LLMs, with their powerful
reasoning abilities and extensive knowledge, could offer a solution. To
understand if LLMs could enhance AD, this paper conducts a thorough analysis of
the potential applications of LLMs in AD systems, including exploring their
optimization strategies in both modular and end-to-end approaches, with a
particular focus on how LLMs can tackle the problems and challenges present in
current solutions. Furthermore, we discuss an important question: Can LLM-based
artificial general intelligence (AGI) be a key to achieve high-level AD? We
further analyze the potential limitations and challenges that LLMs may
encounter in promoting the development of AD technology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Pruning and Distillation in Practice: The Minitron Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11796v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11796v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Pavlo Molchanov, Mohammad Shoeybi, Jan Kautz, Ameya Sunil Mahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, Zijia Chen, Yoshi Suhara, Shizhe Diao, Chenhan Yu, Wei-Chun Chen, Hayley Ross, Daniel Korzekwa, Oluwatobi Olabiyi, Ashwath Aithal, Bryan Catanzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive report on compressing the Llama 3.1 8B and Mistral
NeMo 12B models to 4B and 8B parameters, respectively, using pruning and
distillation. We explore two distinct pruning strategies: (1) depth pruning and
(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on
common benchmarks from the LM Evaluation Harness. The models are then aligned
with NeMo Aligner and tested in instruct-tuned versions. This approach produces
a compelling 4B model from Llama 3.1 8B and a state-of-the-art
Mistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo
12B. We found that with no access to the original data, it is beneficial to
slightly fine-tune teacher models on the distillation dataset. We open-source
our base model weights on Hugging Face with a permissive license.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3: Update author list, other changes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpecExec: Massively Parallel Speculative Decoding for Interactive LLM
  Inference on Consumer Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02532v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02532v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models gain widespread adoption, running them efficiently
becomes crucial. Recent works on LLM inference use speculative decoding to
achieve extreme speedups. However, most of these works implicitly design their
algorithms for high-end datacenter hardware. In this work, we ask the opposite
question: how fast can we run LLMs on consumer machines? Consumer GPUs can no
longer fit the largest available models (50B+ parameters) and must offload them
to RAM or SSD. When running with offloaded parameters, the inference engine can
process batches of hundreds or thousands of tokens at the same time as just one
token, making it a natural fit for speculative decoding. We propose SpecExec
(Speculative Execution), a simple parallel decoding method that can generate up
to 20 tokens per target model iteration for popular LLM families. It utilizes
the high spikiness of the token probabilities distribution in modern LLMs and a
high degree of alignment between model output probabilities. SpecExec takes the
most probable tokens continuation from the draft model to build a "cache" tree
for the target model, which then gets validated in a single pass. Using
SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with
RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens
per second with 16-bit weights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models as Interpolated and Extrapolated Event Predictors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libo Zhang, Yue Ning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Salient facts of sociopolitical events are distilled into quadruples
following a format of subject, relation, object, and timestamp. Machine
learning methods, such as graph neural networks (GNNs) and recurrent neural
networks (RNNs), have been built to make predictions and infer relations on the
quadruple-based knowledge graphs (KGs). In many applications, quadruples are
extended to quintuples with auxiliary attributes such as text summaries that
describe the quadruple events. In this paper, we comprehensively investigate
how large language models (LLMs) streamline the design of event prediction
frameworks using quadruple-based or quintuple-based data while maintaining
competitive accuracy. We propose LEAP, a unified framework that leverages large
language models as event predictors. Specifically, we develop multiple prompt
templates to frame the object prediction (OP) task as a standard
question-answering (QA) task, suitable for instruction fine-tuning with an
encoder-decoder LLM. For multi-event forecasting (MEF) task, we design a simple
yet effective prompt template for each event quintuple. This novel approach
removes the need for GNNs and RNNs, instead utilizing an encoder-only LLM to
generate fixed intermediate embeddings, which are processed by a customized
downstream head with a self-attention mechanism to predict potential relation
occurrences in the future. Extensive experiments on multiple real-world
datasets using various evaluation metrics validate the effectiveness of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Mixture Inference: What do BPE Tokenizers Reveal about their
  Training Data? <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16607v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16607v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Hayase, Alisa Liu, Yejin Choi, Sewoong Oh, Noah A. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretraining data of today's strongest language models is opaque; in
particular, little is known about the proportions of various domains or
languages represented. In this work, we tackle a task which we call data
mixture inference, which aims to uncover the distributional make-up of training
data. We introduce a novel attack based on a previously overlooked source of
information: byte-pair encoding (BPE) tokenizers, used by the vast majority of
modern language models. Our key insight is that the ordered list of merge rules
learned by a BPE tokenizer naturally reveals information about the token
frequencies in its training data. Given a tokenizer's merge list along with
example data for each category of interest, we formulate a linear program that
solves for the proportion of each category in the tokenizer's training set. In
controlled experiments, we show that our attack recovers mixture ratios with
high precision for tokenizers trained on known mixtures of natural languages,
programming languages, and data sources. We then apply our approach to
off-the-shelf tokenizers released with recent LMs. We confirm much publicly
disclosed information about these models, and also make several new inferences:
GPT-4o and Mistral NeMo's tokenizers are much more multilingual than their
predecessors, training on 39% and 47% non-English language data, respectively;
Llama 3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use;
GPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). We
hope our work sheds light on current design practices for pretraining data, and
inspires continued research into data mixture inference for LMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS camera-ready, code at
  https://github.com/alisawuffles/tokenizer-attack</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-Term Ad Memorability: Understanding & Generating Memorable Ads <span class="chip">WACV-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00378v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00378v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harini SI, Somesh Singh, Yaman K Singla, Aanisha Bhattacharyya, Veeky Baths, Changyou Chen, Rajiv Ratn Shah, Balaji Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the importance of long-term memory in marketing and brand building,
until now, there has been no large-scale study on the memorability of ads. All
previous memorability studies have been conducted on short-term recall on
specific content types like action videos. On the other hand, long-term
memorability is crucial for the advertising industry, and ads are almost always
highly multimodal. Therefore, we release the first memorability dataset,
LAMBDA, consisting of 1749 participants and 2205 ads covering 276 brands.
Running statistical tests over different participant subpopulations and ad
types, we find many interesting insights into what makes an ad memorable, e.g.,
fast-moving ads are more memorable than those with slower scenes; people who
use ad-blockers remember a lower number of ads than those who don't. Next, we
present a model, Henry, to predict the memorability of a content. Henry
achieves state-of-the-art performance across all prominent literature
memorability datasets. It shows strong generalization performance with better
results in 0-shot on unseen datasets. Finally, with the intent of memorable ad
generation, we present a scalable method to build a high-quality memorable ad
generation model by leveraging automatically annotated data. Our approach, SEED
(Self rEwarding mEmorability Modeling), starts with a language model trained on
LAMBDA as seed data and progressively trains an LLM to generate more memorable
ads. We show that the generated advertisements have 44% higher memorability
scores than the original ads. We release this large-scale ad dataset,
UltraLAMBDA, consisting of 5 million ads. Our code and the datasets, LAMBDA and
UltraLAMBDA, are open-sourced at
https://behavior-in-the-wild.github.io/memorability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in WACV-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReasoningRank: Teaching Student Models to Rank through Reasoning-Based
  Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05168v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05168v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelyu Ji, Zhuochun Li, Rui Meng, Daqing He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reranking documents based on their relevance to a given query is a critical
task in information retrieval. Traditional reranking methods often lack
transparency and rely on proprietary models, hindering reproducibility and
interpretability. We propose Reason-to-Rank (R2R), a novel open-source
reranking approach that enhances transparency by generating two types of
reasoning: direct relevance reasoning, which explains how a document addresses
the query, and comparison reasoning, which justifies the relevance of one
document over another. We leverage large language models (LLMs) as teacher
models to generate these explanations and distill this knowledge into smaller,
openly available student models. Our student models are trained to generate
meaningful reasoning and rerank documents, achieving competitive performance
across multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate
that R2R not only improves reranking accuracy but also provides valuable
insights into the decision-making process. By offering a structured and
interpretable solution with openly accessible resources, R2R aims to bridge the
gap between effectiveness and transparency in information retrieval, fostering
reproducibility and further research in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02224v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02224v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Fan, Guoqiang Ma, Yan Kang, Hanlin Gu, Yuanfeng Song, Lixin Fan, Kai Chen, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research in federated large language models (LLMs) has primarily
focused on enabling clients to fine-tune their locally deployed homogeneous
LLMs collaboratively or on transferring knowledge from server-based LLMs to
small language models (SLMs) at downstream clients. However, a significant gap
remains in the simultaneous mutual enhancement of both the server's LLM and
clients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient
federated mutual knowledge transfer framework for large and small language
models. This framework is designed to adaptively transfer knowledge from the
server's LLM to clients' SLMs while concurrently enriching the LLM with
clients' unique domain insights. We facilitate token alignment using minimum
edit distance (MinED) and then selective mutual knowledge transfer between
client-side SLMs and a server-side LLM, aiming to collectively enhance their
performance. Through extensive experiments across three distinct scenarios, we
evaluate the effectiveness of FedMKT using various public LLMs and SLMs on a
range of NLP text generation tasks. Empirical results demonstrate that FedMKT
simultaneously boosts the performance of both LLMs and SLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM4Mat-Bench: Benchmarking Large Language Models for Materials Property
  Prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00177v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00177v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andre Niyongabo Rubungo, Kangming Li, Jason Hattrick-Simpers, Adji Bousso Dieng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly being used in materials
science. However, little attention has been given to benchmarking and
standardized evaluation for LLM-based materials property prediction, which
hinders progress. We present LLM4Mat-Bench, the largest benchmark to date for
evaluating the performance of LLMs in predicting the properties of crystalline
materials. LLM4Mat-Bench contains about 1.9M crystal structures in total,
collected from 10 publicly available materials data sources, and 45 distinct
properties. LLM4Mat-Bench features different input modalities: crystal
composition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B
tokens in total for each modality, respectively. We use LLM4Mat-Bench to
fine-tune models with different sizes, including LLM-Prop and MatBERT, and
provide zero-shot and few-shot prompts to evaluate the property prediction
capabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. The
results highlight the challenges of general-purpose LLMs in materials science
and the need for task-specific predictive models and task-specific
instruction-tuned LLMs in materials property prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024-AI4Mat Workshop. The Benchmark and code can
  be found at https://github.com/vertaix/LLM4Mat-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking Structured Thinking in Language Models with Cognitive
  <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02953v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02953v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Kramer, Jill Baumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose cognitive prompting as a novel approach to guide problem-solving
in large language models (LLMs) through structured, human-like cognitive
operations, such as goal clarification, decomposition, filtering, abstraction,
and pattern recognition. By employing systematic, step-by-step reasoning,
cognitive prompting enables LLMs to tackle complex, multi-step tasks more
efficiently. We introduce three variants: a deterministic sequence of cognitive
operations, a self-adaptive variant in which the LLM dynamically selects the
sequence of cognitive operations, and a hybrid variant that uses generated
correct solutions as few-shot chain-of-thought prompts. Experiments with LLaMA,
Gemma~2, and Qwen models in each two sizes on the arithmetic reasoning
benchmark GSM8K demonstrate that cognitive prompting significantly improves
performance compared to standard question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, submitted to ESANN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviral Kaintura, Palaniappan R, Shui Song Luar, Indira Iyer Almeida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-source Electronic Design Automation (EDA) tools are rapidly transforming
chip design by addressing key barriers of commercial EDA tools such as
complexity, costs, and access. Recent advancements in Large Language Models
(LLMs) have further enhanced efficiency in chip design by providing user
assistance across a range of tasks like setup, decision-making, and flow
automation. This paper introduces ORAssistant, a conversational assistant for
OpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to
improve the user experience for the OpenROAD flow, from RTL-GDSII by providing
context-specific responses to common user queries, including installation,
command usage, flow setup, and execution, in prose format. Currently,
ORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and
KLayout. The data model is built from publicly available documentation and
GitHub resources. The proposed architecture is scalable, supporting extensions
to other open-source tools, operating modes, and LLM models. We use Google
Gemini as the base LLM model to build and test ORAssistant. Early evaluation
results of the RAG-based model show notable improvements in performance and
accuracy compared to non-fine-tuned LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Concept-Based Explainability Framework for Large Multimodal Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayneel Parekh, Pegah Khayatan, Mustafa Shukor, Alasdair Newson, Matthieu Cord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) combine unimodal encoders and large language
models (LLMs) to perform multimodal tasks. Despite recent advancements towards
the interpretability of these models, understanding internal representations of
LMMs remains largely a mystery. In this paper, we present a novel framework for
the interpretation of LMMs. We propose a dictionary learning based approach,
applied to the representation of tokens. The elements of the learned dictionary
correspond to our proposed concepts. We show that these concepts are well
semantically grounded in both vision and text. Thus we refer to these as
``multi-modal concepts''. We qualitatively and quantitatively evaluate the
results of the learnt concepts. We show that the extracted multimodal concepts
are useful to interpret representations of test samples. Finally, we evaluate
the disentanglement between different concepts and the quality of grounding
concepts visually and textually. Our code is publicly available at
https://github.com/mshukor/xl-vlms
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for
  Filipino 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jann Railey Montalan, Jian Gang Ngui, Wei Qi Leong, Yosephine Susanto, Hamsawardhini Rengarajan, Alham Fikri Aji, William Chandra Tjhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual large language models (LLMs) today may not necessarily provide
culturally appropriate and relevant responses to its Filipino users. We
introduce Kalahi, a cultural LLM evaluation suite collaboratively created by
native Filipino speakers. It is composed of 150 high-quality, handcrafted and
nuanced prompts that test LLMs for generations that are relevant to shared
Filipino cultural knowledge and values. Strong LLM performance in Kalahi
indicates a model's ability to generate responses similar to what an average
Filipino would say or do in a given situation. We conducted experiments on LLMs
with multilingual and Filipino language support. Results show that Kalahi,
while trivial for Filipinos, is challenging for LLMs, with the best model
answering only 46.0% of the questions correctly compared to native Filipino
performance of 89.10%. Thus, Kalahi can be used to accurately and reliably
evaluate Filipino cultural representation in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at Paclic 38, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncovering Safety Risks of Large Language Models through Concept
  Activation Vector <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12038v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12038v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Xu, Ruixuan Huang, Changyu Chen, Xiting Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite careful safety alignment, current large language models (LLMs) remain
vulnerable to various attacks. To further unveil the safety risks of LLMs, we
introduce a Safety Concept Activation Vector (SCAV) framework, which
effectively guides the attacks by accurately interpreting LLMs' safety
mechanisms. We then develop an SCAV-guided attack method that can generate both
attack prompts and embedding-level attacks with automatically selected
perturbation hyperparameters. Both automatic and human evaluations demonstrate
that our attack method significantly improves the attack success rate and
response quality while requiring less training data. Additionally, we find that
our generated attack prompts may be transferable to GPT-4, and the
embedding-level attacks may also be transferred to other white-box LLMs whose
parameters are known. Our experiments further uncover the safety risks present
in current LLMs. For example, in our evaluation of seven open-source LLMs, we
observe an average attack success rate of 99.14%, based on the classic
keyword-matching criterion. Finally, we provide insights into the safety
mechanism of LLMs. The code is available at
https://github.com/SproutNan/AI-Safety_SCAV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Grained Alignment in Vision-and-Language Navigation through
  Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Song, Mario Gianni, Chenguang Yang, Kunyang Lin, Te-Chuan Chiu, Anh Nguyen, Chun-Yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of fine-grained alignment in
Vision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D
environments based on natural language instructions. Current approaches use
contrastive learning to align language with visual trajectory sequences.
Nevertheless, they encounter difficulties with fine-grained vision negatives.
To enhance cross-modal embeddings, we introduce a novel Bayesian
Optimization-based adversarial optimization framework for creating fine-grained
contrastive vision samples. To validate the proposed methodology, we conduct a
series of experiments to assess the effectiveness of the enriched embeddings on
fine-grained vision negatives. We conduct experiments on two common VLN
benchmarks R2R and REVERIE, experiments on the them demonstrate that these
embeddings benefit navigation, and can lead to a promising performance
enhancement. Our source code and trained models are available at:
https://anonymous.4open.science/r/FGVLN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noise-powered Multi-modal Knowledge Graph Representation Framework <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06832v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06832v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Yin Fang, Yichi Zhang, Lingbing Guo, Jiaoyan Che, Jeff Z. Pan, Huajun Chen, Wen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Multi-modal Pre-training highlights the necessity for a unified
Multi-Modal Knowledge Graph (MMKG) representation learning framework. Such a
framework is essential for embedding structured knowledge into multi-modal
Large Language Models effectively, alleviating issues like knowledge
misconceptions and multi-modal hallucinations. In this work, we explore the
efficacy of models in accurately embedding entities within MMKGs through two
pivotal tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal
Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG
method that utilizes a Transformer-based architecture equipped with
modality-level noise masking to robustly integrate multi-modal entity features
in KGs. By incorporating specific training objectives for both MKGC and MMEA,
our approach achieves SOTA performance across a total of ten datasets,
demonstrating its versatility. Moreover, SNAG can not only function as a
standalone model but also enhance other existing methods, providing stable
performance improvements. Code and data are available at
https://github.com/zjukg/SNAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Accpeted, Repo is available at
  https://github.com/zjukg/SNAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 2D Matryoshka Sentence Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14776v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14776v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianming Li, Zongxi Li, Jing Li, Haoran Xie, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Common approaches rely on fixed-length embedding vectors from language models
as sentence embeddings for downstream tasks such as semantic textual similarity
(STS). Such methods are limited in their flexibility due to unknown
computational constraints and budgets across various applications. Matryoshka
Representation Learning (MRL) \cite{aditya2022matryoshka} encodes information
at finer granularities, i.e., with lower embedding dimensions, to adaptively
accommodate \emph{ad hoc} tasks. Similar accuracy can be achieved with a
smaller embedding size, leading to speedups in downstream tasks. Despite its
improved efficiency, MRL still requires traversing all Transformer layers
before obtaining the embedding, which remains the dominant factor in time and
memory consumption. This prompts consideration of whether the fixed number of
Transformer layers affects representation quality and whether using
intermediate layers for sentence representation is feasible. In this paper, we
introduce a novel sentence embedding model called \textit{Two-dimensional
Matryoshka Sentence Embedding} (2DMSE)\footnote{Our code is available at
\url{https://github.com/SeanLee97/AnglE/blob/main/README_2DMSE.md}.}. It
supports elastic settings for both embedding sizes and Transformer layers,
offering greater flexibility and efficiency than MRL. We conduct extensive
experiments on STS tasks and downstream applications. The experimental results
demonstrate the effectiveness of our proposed model in dynamically supporting
different embedding sizes and Transformer layers, allowing it to be highly
adaptable to various scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Decoupled with ESE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs and Finetuning: Benchmarking cross-domain performance for hate
  speech detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18964v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18964v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Nasir, Aadish Sharma, Kokil Jaidka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the evolving landscape of online communication, hate speech detection
remains a formidable challenge, further compounded by the diversity of digital
platforms. This study investigates the effectiveness and adaptability of
pre-trained and fine-tuned Large Language Models (LLMs) in identifying hate
speech, to address two central questions: (1) To what extent does the model
performance depend on the fine-tuning and training parameters?, (2) To what
extent do models generalize to cross-domain hate speech detection? and (3) What
are the specific features of the datasets or models that influence the
generalization potential? The experiment shows that LLMs offer a huge advantage
over the state-of-the-art even without pretraining. Ordinary least squares
analyses suggest that the advantage of training with fine-grained hate speech
labels is washed away with the increase in dataset size. We conclude with a
vision for the future of hate speech detection, emphasizing cross-domain
generalizability and appropriate benchmarking practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws for Precision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanishq Kumar, Zachary Ankner, Benjamin F. Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low precision training and inference affect both the quality and cost of
language models, but current scaling laws do not account for this. In this
work, we devise "precision-aware" scaling laws for both training and inference.
We propose that training in lower precision reduces the model's "effective
parameter count," allowing us to predict the additional loss incurred from
training in low precision and post-train quantization. For inference, we find
that the degradation introduced by post-training quantization increases as
models are trained on more data, eventually making additional pretraining data
actively harmful. For training, our scaling laws allow us to predict the loss
of a model with different parts in different precisions, and suggest that
training larger models in lower precision may be compute optimal. We unify the
scaling laws for post and pretraining quantization to arrive at a single
functional form that predicts degradation from training and inference in varied
precisions. We fit on over 465 pretraining runs and validate our predictions on
model sizes up to 1.7B parameters trained on up to 26B tokens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HEARTS: A Holistic Framework for Explainable, Sustainable and Robust
  Text Stereotype Detection <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11579v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11579v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, Philip Treleaven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereotypes are generalised assumptions about societal groups, and even
state-of-the-art LLMs using in-context learning struggle to identify them
accurately. Due to the subjective nature of stereotypes, where what constitutes
a stereotype can vary widely depending on cultural, social, and individual
perspectives, robust explainability is crucial. Explainable models ensure that
these nuanced judgments can be understood and validated by human users,
promoting trust and accountability. We address these challenges by introducing
HEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text
Stereotype Detection), a framework that enhances model performance, minimises
carbon footprint, and provides transparent, interpretable explanations. We
establish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising
57,201 labelled texts across six groups, including under-represented
demographics like LGBTQ+ and regional stereotypes. Ablation studies confirm
that BERT models fine-tuned on EMGSD outperform those trained on individual
components. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model
using SHAP to generate token-level importance values, ensuring alignment with
human understanding, and calculate explainability confidence scores by
comparing SHAP and LIME outputs...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 SoLaR Workshop and NeurIPS 2024 Safety Gen AI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation
  in Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11353v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11353v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengfei Liang, Archish Arun, Zekun Wu, Cristian Munoz, Jonathan Lutch, Emre Kazim, Adriano Koshiyama, Philip Treleaven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination, the generation of factually incorrect content, is a growing
challenge in Large Language Models (LLMs). Existing detection and mitigation
methods are often isolated and insufficient for domain-specific needs, lacking
a standardized pipeline. This paper introduces THaMES (Tool for Hallucination
Mitigations and EvaluationS), an integrated framework and library addressing
this gap. THaMES offers an end-to-end solution for evaluating and mitigating
hallucinations in LLMs, featuring automated test set generation, multifaceted
benchmarking, and adaptable mitigation strategies. It automates test set
creation from any corpus, ensuring high data quality, diversity, and
cost-efficiency through techniques like batch processing, weighted sampling,
and counterfactual validation. THaMES assesses a model's ability to detect and
reduce hallucinations across various tasks, including text generation and
binary classification, applying optimal mitigation strategies like In-Context
Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient
Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base
of academic papers, political news, and Wikipedia reveal that commercial models
like GPT-4o benefit more from RAG than ICL, while open-weight models like
Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT
significantly enhances the performance of Llama-3.1-8B-Instruct in both
evaluation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 SoLaR (Socially Responsible Language Modelling Research
  ) Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with
  Customisable Fairness Calibration <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11149v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11149v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Guan, Nathaniel Demchak, Saloni Gupta, Ze Wang, Ediz Ertekin Jr., Adriano Koshiyama, Emre Kazim, Zekun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of unbiased large language models is widely recognized as
crucial, yet existing benchmarks fall short in detecting biases due to limited
scope, contamination, and lack of a fairness baseline. SAGED(-Bias) is the
first holistic benchmarking pipeline to address these problems. The pipeline
encompasses five core stages: scraping materials, assembling benchmarks,
generating responses, extracting numeric features, and diagnosing with
disparity metrics. SAGED includes metrics for max disparity, such as impact
ratio, and bias concentration, such as Max Z-scores. Noticing that assessment
tool bias and contextual bias in prompts can distort evaluation, SAGED
implements counterfactual branching and baseline calibration for mitigation.
For demonstration, we use SAGED on G20 Countries with popular 8b-level models
including Gemma2, Llama3.1, Mistral, and Qwen2. With sentiment analysis, we
find that while Mistral and Qwen2 show lower max disparity and higher bias
concentration than Gemma2 and Llama3.1, all models are notably biased against
countries like Russia and (except for Qwen2) China. With further experiments to
have models role-playing U.S. (vice-/former-) presidents, we see bias amplifies
and shifts in heterogeneous directions. Moreover, we see Qwen2 and Mistral not
engage in role-playing, while Llama3.1 and Gemma2 role-play Trump notably more
intensively than Biden and Harris, indicating role-playing performance bias in
these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GADFA: Generator-Assisted Decision-Focused Approach for Opinion
  Expressing Timing Identification <span class="chip">COLING-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chung-Chi Chen, Hiroya Takamura, Ichiro Kobayashi, Yusuke Miyao, Hsin-Hsi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of text generation models has granted us the capability to
produce coherent and convincing text on demand. Yet, in real-life
circumstances, individuals do not continuously generate text or voice their
opinions. For instance, consumers pen product reviews after weighing the merits
and demerits of a product, and professional analysts issue reports following
significant news releases. In essence, opinion expression is typically prompted
by particular reasons or signals. Despite long-standing developments in opinion
mining, the appropriate timing for expressing an opinion remains largely
unexplored. To address this deficit, our study introduces an innovative task -
the identification of news-triggered opinion expressing timing. We ground this
task in the actions of professional stock analysts and develop a novel dataset
for investigation. Our approach is decision-focused, leveraging text generation
models to steer the classification model, thus enhancing overall performance.
Our experimental findings demonstrate that the text generated by our model
contributes fresh insights from various angles, effectively aiding in
identifying the optimal timing for opinion expression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted: COLING-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Perspective for Adapting Generalist AI to Specialized Medical AI
  Applications and Their Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00024v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00024v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifeng Wang, Hanyin Wang, Benjamin Danek, Ying Li, Christina Mack, Hoifung Poon, Yajuan Wang, Pranav Rajpurkar, Jimeng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Large Language Models (LLMs) into medical applications has
sparked widespread interest across the healthcare industry, from drug discovery
and development to clinical decision support, assisting telemedicine, medical
devices, and healthcare insurance applications. This perspective paper aims to
discuss the inner workings of building LLM-powered medical AI applications and
introduces a comprehensive framework for their development. We review existing
literature and outline the unique challenges of applying LLMs in specialized
medical contexts. Additionally, we introduce a three-step framework to organize
medical LLM research activities: 1) Modeling: breaking down complex medical
workflows into manageable steps for developing medical-specific models; 2)
Optimization: optimizing the model performance with crafted prompts and
integrating external knowledge and tools, and 3) System engineering:
decomposing complex tasks into subtasks and leveraging human expertise for
building medical AI applications. Furthermore, we offer a detailed use case
playbook that describes various LLM-powered medical AI applications, such as
optimizing clinical trial design, enhancing clinical decision support, and
advancing medical imaging analysis. Finally, we discuss various challenges and
considerations for building medical AI applications with LLMs, such as handling
hallucination issues, data ownership and compliance, privacy, intellectual
property considerations, compute cost, sustainability issues, and responsible
AI requirements.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">24</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Superpixel Segmentation Methods in the Context of Citizen
  Science and Deforestation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Resende, Isabela Borlido, Victor Sundermann, Eduardo B. Neto, Silvio Jamil F. Guimarães, Fabio Faria, Alvaro Luiz Fazenda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tropical forests play an essential role in the planet's ecosystem, making the
conservation of these biomes a worldwide priority. However, ongoing
deforestation and degradation pose a significant threat to their existence,
necessitating effective monitoring and the proposal of actions to mitigate the
damage caused by these processes. In this regard, initiatives range from
government and private sector monitoring programs to solutions based on citizen
science campaigns, for example. Particularly in the context of citizen science
campaigns, the segmentation of remote sensing images to identify deforested
areas and subsequently submit them to analysis by non-specialized volunteers is
necessary. Thus, segmentation using superpixel-based techniques proves to be a
viable solution for this important task. Therefore, this paper presents an
analysis of 22 superpixel-based segmentation methods applied to remote sensing
images, aiming to identify which of them are more suitable for generating
segments for citizen science campaigns. The results reveal that seven of the
segmentation methods outperformed the baseline method (SLIC) currently employed
in the ForestEyes citizen science project, indicating an opportunity for
improvement in this important stage of campaign development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper was accepted for presentation at SAC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking
  with Motion-Aware Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in
object segmentation tasks but faces challenges in visual object tracking,
particularly when managing crowded scenes with fast-moving or self-occluding
objects. Furthermore, the fixed-window memory approach in the original model
does not consider the quality of memories selected to condition the image
features for the next frame, leading to error propagation in videos. This paper
introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for
visual object tracking. By incorporating temporal motion cues with the proposed
motion-aware memory selection mechanism, SAMURAI effectively predicts object
motion and refines mask selection, achieving robust, accurate tracking without
the need for retraining or fine-tuning. SAMURAI operates in real-time and
demonstrates strong zero-shot performance across diverse benchmark datasets,
showcasing its ability to generalize without fine-tuning. In evaluations,
SAMURAI achieves significant improvements in success rate and precision over
existing trackers, with a 7.1% AUC gain on LaSOT$_{\text{ext}}$ and a 3.5% AO
gain on GOT-10k. Moreover, it achieves competitive results compared to fully
supervised methods on LaSOT, underscoring its robustness in complex tracking
scenarios and its potential for real-world applications in dynamic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at https://yangchris11.github.io/samurai/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-Term Ad Memorability: Understanding & Generating Memorable Ads <span class="chip">WACV-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00378v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00378v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harini SI, Somesh Singh, Yaman K Singla, Aanisha Bhattacharyya, Veeky Baths, Changyou Chen, Rajiv Ratn Shah, Balaji Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the importance of long-term memory in marketing and brand building,
until now, there has been no large-scale study on the memorability of ads. All
previous memorability studies have been conducted on short-term recall on
specific content types like action videos. On the other hand, long-term
memorability is crucial for the advertising industry, and ads are almost always
highly multimodal. Therefore, we release the first memorability dataset,
LAMBDA, consisting of 1749 participants and 2205 ads covering 276 brands.
Running statistical tests over different participant subpopulations and ad
types, we find many interesting insights into what makes an ad memorable, e.g.,
fast-moving ads are more memorable than those with slower scenes; people who
use ad-blockers remember a lower number of ads than those who don't. Next, we
present a model, Henry, to predict the memorability of a content. Henry
achieves state-of-the-art performance across all prominent literature
memorability datasets. It shows strong generalization performance with better
results in 0-shot on unseen datasets. Finally, with the intent of memorable ad
generation, we present a scalable method to build a high-quality memorable ad
generation model by leveraging automatically annotated data. Our approach, SEED
(Self rEwarding mEmorability Modeling), starts with a language model trained on
LAMBDA as seed data and progressively trains an LLM to generate more memorable
ads. We show that the generated advertisements have 44% higher memorability
scores than the original ads. We release this large-scale ad dataset,
UltraLAMBDA, consisting of 5 million ads. Our code and the datasets, LAMBDA and
UltraLAMBDA, are open-sourced at
https://behavior-in-the-wild.github.io/memorability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in WACV-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExpertAF: Expert Actionable Feedback from Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Ashutosh, Tushar Nagarajan, Georgios Pavlakos, Kris Kitani, Kristen Grauman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feedback is essential for learning a new skill or improving one's current
skill-level. However, current methods for skill-assessment from video only
provide scores or compare demonstrations, leaving the burden of knowing what to
do differently on the user. We introduce a novel method to generate actionable
feedback from video of a person doing a physical activity, such as basketball
or soccer. Our method takes a video demonstration and its accompanying 3D body
pose and generates (1) free-form expert commentary describing what the person
is doing well and what they could improve, and (2) a visual expert
demonstration that incorporates the required corrections. We show how to
leverage Ego-Exo4D's videos of skilled activity and expert commentary together
with a strong language model to create a weakly-supervised training dataset for
this task, and we devise a multimodal video-language model to infer coaching
feedback. Our method is able to reason across multi-modal input combinations to
output full-spectrum, actionable coaching -- expert commentary, expert video
retrieval, and expert pose generation -- outperforming strong vision-language
models on both established metrics and human preference studies. Code and data
will be publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial and Spatial-Spectral Morphological Mamba for Hyperspectral Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01372v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01372v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ahmad, Muhammad Hassaan Farooq Butt, Adil Mehmood Khan, Manuel Mazzara, Salvatore Distefano, Muhammad Usama, Swalpa Kumar Roy, Jocelyn Chanussot, Danfeng Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in transformers, specifically self-attention mechanisms,
have significantly improved hyperspectral image (HSI) classification. However,
these models often suffer from inefficiencies, as their computational
complexity scales quadratically with sequence length. To address these
challenges, we propose the morphological spatial mamba (SMM) and morphological
spatial-spectral Mamba (SSMM) model (MorpMamba), which combines the strengths
of morphological operations and the state space model framework, offering a
more computationally efficient alternative to transformers. In MorpMamba, a
novel token generation module first converts HSI patches into spatial-spectral
tokens. These tokens are then processed through morphological operations such
as erosion and dilation, utilizing depthwise separable convolutions to capture
structural and shape information. A token enhancement module refines these
features by dynamically adjusting the spatial and spectral tokens based on
central HSI regions, ensuring effective feature fusion within each block.
Subsequently, multi-head self-attention is applied to further enrich the
feature representations, allowing the model to capture complex relationships
and dependencies within the data. Finally, the enhanced tokens are fed into a
state space module, which efficiently models the temporal evolution of the
features for classification. Experimental results on widely used HSI datasets
demonstrate that MorpMamba achieves superior parametric efficiency compared to
traditional CNN and transformer models while maintaining high accuracy. The
code will be made publicly available at
\url{https://github.com/mahmad000/MorpMamba}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Subject Domain Adaptation for Classifying Working Memory Load with
  Multi-Frame EEG Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.06769v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.06769v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfu Chen, Sirui Li, Dechang Pi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Working memory (WM), denoting the information temporally stored in the mind,
is a fundamental research topic in the field of human cognition.
Electroencephalograph (EEG), which can monitor the electrical activity of the
brain, has been widely used in measuring the level of WM. However, one of the
critical challenges is that individual differences may cause ineffective
results, especially when the established model meets an unfamiliar subject. In
this work, we propose a cross-subject deep adaptation model with spatial
attention (CS-DASA) to generalize the workload classifications across subjects.
First, we transform EEG time series into multi-frame EEG images incorporating
spatial, spectral, and temporal information. First, the Subject-Shared module
in CS-DASA receives multi-frame EEG image data from both source and target
subjects and learns the common feature representations. Then, in the
subject-specific module, the maximum mean discrepancy is implemented to measure
the domain distribution divergence in a reproducing kernel Hilbert space, which
can add an effective penalty loss for domain adaptation. Additionally, the
subject-to-subject spatial attention mechanism is employed to focus on the
discriminative spatial features from the target image data. Experiments
conducted on a public WM EEG dataset containing 13 subjects show that the
proposed model is capable of achieving better performance than existing
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Good Grasps Only: A data engine for <span class="highlight-title">self-supervised</span> fine-tuning of pose
  estimation using grasp poses for verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederik Hagelskjær
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel method for self-supervised fine-tuning of
pose estimation. Leveraging zero-shot pose estimation, our approach enables the
robot to automatically obtain training data without manual labeling. After pose
estimation the object is grasped, and in-hand pose estimation is used for data
validation. Our pipeline allows the system to fine-tune while the process is
running, removing the need for a learning phase. The motivation behind our work
lies in the need for rapid setup of pose estimation solutions. Specifically, we
address the challenging task of bin picking, which plays a pivotal role in
flexible robotic setups. Our method is implemented on a robotics work-cell, and
tested with four different objects. For all objects, our method increases the
performance and outperforms a state-of-the-art method trained on the CAD model
of the objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PassionSR: Post-Training Quantization with Adaptive Scale in One-Step
  Diffusion based Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libo Zhu, Jianze Li, Haotong Qin, Wenbo Li, Yulun Zhang, Yong Guo, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based image super-resolution (SR) models have shown superior
performance at the cost of multiple denoising steps. However, even though the
denoising step has been reduced to one, they require high computational costs
and storage requirements, making it difficult for deployment on hardware
devices. To address these issues, we propose a novel post-training quantization
approach with adaptive scale in one-step diffusion (OSD) image SR, PassionSR.
First, we simplify OSD model to two core components, UNet and Variational
Autoencoder (VAE) by removing the CLIPEncoder. Secondly, we propose Learnable
Boundary Quantizer (LBQ) and Learnable Equivalent Transformation (LET) to
optimize the quantization process and manipulate activation distributions for
better quantization. Finally, we design a Distributed Quantization Calibration
(DQC) strategy that stabilizes the training of quantized parameters for rapid
convergence. Comprehensive experiments demonstrate that PassionSR with 8-bit
and 6-bit obtains comparable visual results with full-precision model.
Moreover, our PassionSR achieves significant advantages over recent leading
low-bit quantization methods for image SR. Our code will be at
https://github.com/libozhu03/PassionSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/libozhu03/PassionSR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MC$^2$: Multi-concept Guidance for Customized Multi-concept Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxiu Jiang, Yabo Zhang, Kailai Feng, Xiaohe Wu, Wenbo Li, Renjing Pei, Fan Li, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customized text-to-image generation, which synthesizes images based on
user-specified concepts, has made significant progress in handling individual
concepts. However, when extended to multiple concepts, existing methods often
struggle with properly integrating different models and avoiding the unintended
blending of characteristics from distinct concepts. In this paper, we propose
MC$^2$, a novel approach for multi-concept customization that enhances
flexibility and fidelity through inference-time optimization. MC$^2$ enables
the integration of multiple single-concept models with heterogeneous
architectures. By adaptively refining attention weights between visual and
textual tokens, our method ensures that image regions accurately correspond to
their associated concepts while minimizing interference between concepts.
Extensive experiments demonstrate that MC$^2$ outperforms training-based
methods in terms of prompt-reference alignment. Furthermore, MC$^2$ can be
seamlessly applied to text-to-image generation, providing robust compositional
capabilities. To facilitate the evaluation of multi-concept customization, we
also introduce a new benchmark, MC++. The code will be publicly available at
https://github.com/JIANGJiaXiu/MC-2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level
  Blending and Spatiotemporal Adapter Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Yan, Yandan Zhao, Shen Chen, Mingyi Guo, Xinghe Fu, Taiping Yao, Shouhong Ding, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three key challenges hinder the development of current deepfake video
detection: (1) Temporal features can be complex and diverse: how can we
identify general temporal artifacts to enhance model generalization? (2)
Spatiotemporal models often lean heavily on one type of artifact and ignore the
other: how can we ensure balanced learning from both? (3) Videos are naturally
resource-intensive: how can we tackle efficiency without compromising accuracy?
This paper attempts to tackle the three challenges jointly. First, inspired by
the notable generality of using image-level blending data for image forgery
detection, we investigate whether and how video-level blending can be effective
in video. We then perform a thorough analysis and identify a previously
underexplored temporal forgery artifact: Facial Feature Drift (FFD), which
commonly exists across different forgeries. To reproduce FFD, we then propose a
novel Video-level Blending data (VB), where VB is implemented by blending the
original image and its warped version frame-by-frame, serving as a hard
negative sample to mine more general artifacts. Second, we carefully design a
lightweight Spatiotemporal Adapter (StA) to equip a pretrained image model
(both ViTs and CNNs) with the ability to capture both spatial and temporal
features jointly and efficiently. StA is designed with two-stream 3D-Conv with
varying kernel sizes, allowing it to process spatial and temporal features
separately. Extensive experiments validate the effectiveness of the proposed
methods; and show our approach can generalize well to previously unseen forgery
videos, even the latest generation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Concept-Based Explainability Framework for Large Multimodal Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayneel Parekh, Pegah Khayatan, Mustafa Shukor, Alasdair Newson, Matthieu Cord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) combine unimodal encoders and large language
models (LLMs) to perform multimodal tasks. Despite recent advancements towards
the interpretability of these models, understanding internal representations of
LMMs remains largely a mystery. In this paper, we present a novel framework for
the interpretation of LMMs. We propose a dictionary learning based approach,
applied to the representation of tokens. The elements of the learned dictionary
correspond to our proposed concepts. We show that these concepts are well
semantically grounded in both vision and text. Thus we refer to these as
``multi-modal concepts''. We qualitatively and quantitatively evaluate the
results of the learnt concepts. We show that the extracted multimodal concepts
are useful to interpret representations of test samples. Finally, we evaluate
the disentanglement between different concepts and the quality of grounding
concepts visually and textually. Our code is publicly available at
https://github.com/mshukor/xl-vlms
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, Yueqi Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in 3D scene reconstruction have transformed 2D images from the
real world into 3D models, producing realistic 3D results from hundreds of
input photos. Despite great success in dense-view reconstruction scenarios,
rendering a detailed scene from insufficient captured views is still an
ill-posed optimization problem, often resulting in artifacts and distortions in
unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction
paradigm that reframes the ambiguous reconstruction challenge as a temporal
generation task. The key insight is to unleash the strong generative prior of
large pre-trained video diffusion models for sparse-view reconstruction.
However, 3D view consistency struggles to be accurately preserved in directly
generated video frames from pre-trained models. To address this, given limited
input views, the proposed ReconX first constructs a global point cloud and
encodes it into a contextual space as the 3D structure condition. Guided by the
condition, the video diffusion model then synthesizes video frames that are
both detail-preserved and exhibit a high degree of 3D consistency, ensuring the
coherence of the scene from various perspectives. Finally, we recover the 3D
scene from the generated video through a confidence-aware 3D Gaussian Splatting
optimization scheme. Extensive experiments on various real-world datasets show
the superiority of our ReconX over state-of-the-art methods in terms of quality
and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://liuff19.github.io/ReconX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Information-Theoretic Regularizer for Lossy Neural Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingwen Zhang, Meng Wang, Xihua Sheng, Peilin Chen, Junru Li, Li Zhang, Shiqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lossy image compression networks aim to minimize the latent entropy of images
while adhering to specific distortion constraints. However, optimizing the
neural network can be challenging due to its nature of learning quantized
latent representations. In this paper, our key finding is that minimizing the
latent entropy is, to some extent, equivalent to maximizing the conditional
source entropy, an insight that is deeply rooted in information-theoretic
equalities. Building on this insight, we propose a novel structural
regularization method for the neural image compression task by incorporating
the negative conditional source entropy into the training objective, such that
both the optimization efficacy and the model's generalization ability can be
promoted. The proposed information-theoretic regularizer is interpretable,
plug-and-play, and imposes no inference overheads. Extensive experiments
demonstrate its superiority in regularizing the models and further squeezing
bits from the latent representation across various compression structures and
unseen domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Grained Alignment in Vision-and-Language Navigation through
  Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Song, Mario Gianni, Chenguang Yang, Kunyang Lin, Te-Chuan Chiu, Anh Nguyen, Chun-Yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of fine-grained alignment in
Vision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D
environments based on natural language instructions. Current approaches use
contrastive learning to align language with visual trajectory sequences.
Nevertheless, they encounter difficulties with fine-grained vision negatives.
To enhance cross-modal embeddings, we introduce a novel Bayesian
Optimization-based adversarial optimization framework for creating fine-grained
contrastive vision samples. To validate the proposed methodology, we conduct a
series of experiments to assess the effectiveness of the enriched embeddings on
fine-grained vision negatives. We conduct experiments on two common VLN
benchmarks R2R and REVERIE, experiments on the them demonstrate that these
embeddings benefit navigation, and can lead to a promising performance
enhancement. Our source code and trained models are available at:
https://anonymous.4open.science/r/FGVLN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MotionCharacter: Identity-Preserving and Motion Controllable Human Video
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haopeng Fang, Di Qiu, Binjie Mao, Pengfei Yan, He Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in personalized Text-to-Video (T2V) generation highlight
the importance of integrating character-specific identities and actions.
However, previous T2V models struggle with identity consistency and
controllable motion dynamics, mainly due to limited fine-grained facial and
action-based textual prompts, and datasets that overlook key human attributes
and actions. To address these challenges, we propose MotionCharacter, an
efficient and high-fidelity human video generation framework designed for
identity preservation and fine-grained motion control. We introduce an
ID-preserving module to maintain identity fidelity while allowing flexible
attribute modifications, and further integrate ID-consistency and region-aware
loss mechanisms, significantly enhancing identity consistency and detail
fidelity. Additionally, our approach incorporates a motion control module that
prioritizes action-related text while maintaining subject consistency, along
with a dataset, Human-Motion, which utilizes large language models to generate
detailed motion descriptions. For simplify user control during inference, we
parameterize motion intensity through a single coefficient, allowing for easy
adjustments. Extensive experiments highlight the effectiveness of
MotionCharacter, demonstrating significant improvements in ID-preserving,
high-quality video generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstantDrag: Improving Interactivity in Drag-based Image Editing <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonghyuk Shin, Daehyeon Choi, Jaesik Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drag-based image editing has recently gained popularity for its interactivity
and precision. However, despite the ability of text-to-image models to generate
samples within a second, drag editing still lags behind due to the challenge of
accurately reflecting user interaction while maintaining image content. Some
existing approaches rely on computationally intensive per-image optimization or
intricate guidance-based methods, requiring additional inputs such as masks for
movable regions and text prompts, thereby compromising the interactivity of the
editing process. We introduce InstantDrag, an optimization-free pipeline that
enhances interactivity and speed, requiring only an image and a drag
instruction as input. InstantDrag consists of two carefully designed networks:
a drag-conditioned optical flow generator (FlowGen) and an optical
flow-conditioned diffusion model (FlowDiffusion). InstantDrag learns motion
dynamics for drag-based image editing in real-world video datasets by
decomposing the task into motion generation and motion-conditioned image
generation. We demonstrate InstantDrag's capability to perform fast,
photo-realistic edits without masks or text prompts through experiments on
facial video datasets and general scenes. These results highlight the
efficiency of our approach in handling drag-based image editing, making it a
promising solution for interactive, real-time applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2024. Project webpage:
  https://joonghyuk.com/instantdrag-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REACT: Real-time Efficiency and Accuracy Compromise for Tradeoffs in
  Scene Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maëlic Neau, Paulo E. Santos, Anne-Gwenn Bosser, Cédric Buche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene Graph Generation (SGG) is a task that encodes visual relationships
between objects in images as graph structures. SGG shows significant promise as
a foundational component for downstream tasks, such as reasoning for embodied
agents. To enable real-time applications, SGG must address the trade-off
between performance and inference speed. However, current methods tend to focus
on one of the following: (1) improving relation prediction accuracy, (2)
enhancing object detection accuracy, or (3) reducing latency, without aiming to
balance all three objectives simultaneously. To address this limitation, we
propose a novel architecture, inference method, and relation prediction model.
Our proposed solution, the REACT model, achieves the highest inference speed
among existing SGG models, improving object detection accuracy without
sacrificing relation prediction performance. Compared to state-of-the-art
approaches, REACT is 2.7 times faster (with a latency of 23 ms) and improves
object detection accuracy by 58.51%. Furthermore, our proposal significantly
reduces model size, with an average of 5.5x fewer parameters. Code is available
at https://github.com/Maelic/SGG-Benchmark
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Croc: <span class="highlight-title">Pretrain</span>ing Large Multimodal Models with Cross-Modal Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Xie, Kaicheng Yang, Ninghua Yang, Weimo Deng, Xiangzi Dai, Tiancheng Gu, Yumeng Wang, Xiang An, Yongle Zhao, Ziyong Feng, Jiankang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have catalyzed the
development of Large Multimodal Models (LMMs). However, existing research
primarily focuses on tuning language and image instructions, ignoring the
critical pretraining phase where models learn to process textual and visual
modalities jointly. In this paper, we propose a new pretraining paradigm for
LMMs to enhance the visual comprehension capabilities of LLMs by introducing a
novel cross-modal comprehension stage. Specifically, we design a dynamically
learnable prompt token pool and employ the Hungarian algorithm to replace part
of the original visual tokens with the most relevant prompt tokens. Then, we
conceptualize visual tokens as analogous to a "foreign language" for the LLMs
and propose a mixed attention mechanism with bidirectional visual attention and
unidirectional textual attention to comprehensively enhance the understanding
of visual tokens. Meanwhile, we integrate a detailed caption generation task,
leveraging rich descriptions to further facilitate LLMs in understanding visual
semantic information. After pretraining on 1.5 million publicly accessible
data, we present a new foundation model called Croc. Experimental results
demonstrate that Croc achieves new state-of-the-art performance on massive
vision-language benchmarks. To support reproducibility and facilitate further
research, we release the training code and pre-trained model weights at
https://github.com/deepglint/Croc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to
  See 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeliang Zhang, Phu Pham, Wentian Zhao, Kun Wan, Yu-Jhe Li, Jianing Zhou, Daniel Miranda, Ajinkya Kale, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By treating visual tokens from visual encoders as text tokens, Multimodal
Large Language Models (MLLMs) have achieved remarkable progress across diverse
visual understanding tasks, leveraging the robust architectures of Large
Language Models (LLMs). However, as token counts grow, the quadratic scaling of
computation in LLMs introduces a significant efficiency bottleneck, impeding
further scalability. Although recent approaches have explored pruning visual
tokens or employing lighter LLM architectures, the computational overhead from
an increasing number of visual tokens remains a substantial challenge.
  In this study, we investigate the redundancy in visual computation at both
the parameter and computational pattern levels within LLaVA, a representative
MLLM, and introduce a suite of streamlined strategies to enhance efficiency.
These include neighbor-aware visual token attention, pruning of inactive visual
attention heads, and selective layer dropping for visual computations. By
implementing these strategies in LLaVA, we achieve a reduction in computational
demands of 88% while maintaining model performance across key benchmarks.
Additionally, we validate the existence of visual computational redundancy in
other MLLMs, such as Qwen2-VL-7B and InternVL-2.0-4B/8B/26B. These results
present a novel pathway for MLLMs to handle dense visual tokens with minimal
computational costs. Code and model checkpoints will be released to support
further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I2VControl: Disentangled and Unified Video Motion Synthesis Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanquan Feng, Tianhao Qi, Jiawei Liu, Mingzhen Sun, Pengqi Tu, Tianxiang Ma, Fei Dai, Songtao Zhao, Siyu Zhou, Qian He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video synthesis techniques are undergoing rapid progress, with
controllability being a significant aspect of practical usability for
end-users. Although text condition is an effective way to guide video
synthesis, capturing the correct joint distribution between text descriptions
and video motion remains a substantial challenge. In this paper, we present a
disentangled and unified framework, namely I2VControl, that unifies multiple
motion control tasks in image-to-video synthesis. Our approach partitions the
video into individual motion units and represents each unit with disentangled
control signals, which allows for various control types to be flexibly combined
within our single system. Furthermore, our methodology seamlessly integrates as
a plug-in for pre-trained models and remains agnostic to specific model
architectures. We conduct extensive experiments, achieving excellent
performance on various control tasks, and our method further facilitates
user-driven creative combinations, enhancing innovation and creativity. The
project page is: https://wanquanf.github.io/I2VControl .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://wanquanf.github.io/I2VControl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Homogeneous and Heterogeneous Consistent Label Associations
  for Unsupervised Visible-Infrared Person ReID 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00672v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00672v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfeng He, De Cheng, Nannan Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised visible-infrared person re-identification (USL-VI-ReID)
endeavors to retrieve pedestrian images of the same identity from different
modalities without annotations. While prior work focuses on establishing
cross-modality pseudo-label associations to bridge the modality-gap, they
ignore maintaining the instance-level homogeneous and heterogeneous consistency
between the feature space and the pseudo-label space, resulting in coarse
associations. In response, we introduce a Modality-Unified Label Transfer
(MULT) module that simultaneously accounts for both homogeneous and
heterogeneous fine-grained instance-level structures, yielding high-quality
cross-modality label associations. It models both homogeneous and heterogeneous
affinities, leveraging them to quantify the inconsistency between the
pseudo-label space and the feature space, subsequently minimizing it. The
proposed MULT ensures that the generated pseudo-labels maintain alignment
across modalities while upholding structural consistency within intra-modality.
Additionally, a straightforward plug-and-play Online Cross-memory Label
Refinement (OCLR) module is proposed to further mitigate the side effects of
noisy pseudo-labels while simultaneously aligning different modalities, coupled
with an Alternative Modality-Invariant Representation Learning (AMIRL)
framework. Experiments demonstrate that our proposed method outperforms
existing state-of-the-art USL-VI-ReID methods, highlighting the superiority of
our MULT in comparison to other cross-modality association methods. Code is
available at https://github.com/FranklinLingfeng/code_for_MULT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Learning Approach to Predict the Fall [of Price] of
  Cryptocurrency Long Before its Actual Fall 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13615v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13615v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anika Tahsin Meem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern times, the cryptocurrency market is one of the world's most rapidly
rising financial markets. The cryptocurrency market is regarded to be more
volatile and illiquid than traditional markets such as equities, foreign
exchange, and commodities. The risk of this market creates an uncertain
condition among the investors. The purpose of this research is to predict the
magnitude of the risk factor of the cryptocurrency market. Risk factor is also
called volatility. Our approach will assist people who invest in the
cryptocurrency market by overcoming the problems and difficulties they
experience. Our approach starts with calculating the risk factor of the
cryptocurrency market from the existing parameters. In twenty elements of the
cryptocurrency market, the risk factor has been predicted using different
machine learning algorithms such as CNN, LSTM, BiLSTM, and GRU. All of the
models have been applied to the calculated risk factor parameter. A new model
has been developed to predict better than the existing models. Our proposed
model gives the highest RMSE value of 1.3229 and the lowest RMSE value of
0.0089. Following our model, it will be easier for investors to trade in
complicated and challenging financial assets like bitcoin, Ethereum, dogecoin,
etc. Where the other existing models, the highest RMSE was 14.5092, and the
lower was 0.02769. So, the proposed model performs much better than models with
proper generalization. Using our approach, it will be easier for investors to
trade in complicated and challenging financial assets like Bitcoin, Ethereum,
and Dogecoin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I am writing to formally request the withdrawal, which is necessary
  due to issues with the author list and the need for improvements to the
  manuscript. We apologize for any inconvenience caused by this request and
  appreciate your understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViBiDSampler: Enhancing Video Interpolation Using Bidirectional
  Diffusion Sampler 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serin Yang, Taesung Kwon, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V)
diffusion models has greatly enhanced video generation, especially in terms of
keyframe interpolation. However, current image-to-video diffusion models, while
powerful in generating videos from a single conditioning frame, need adaptation
for two-frame (start & end) conditioned generation, which is essential for
effective bounded interpolation. Unfortunately, existing approaches that fuse
temporally forward and backward paths in parallel often suffer from
off-manifold issues, leading to artifacts or requiring multiple iterative
re-noising steps. In this work, we introduce a novel, bidirectional sampling
strategy to address these off-manifold issues without requiring extensive
re-noising or fine-tuning. Our method employs sequential sampling along both
forward and backward paths, conditioned on the start and end frames,
respectively, ensuring more coherent and on-manifold generation of intermediate
frames. Additionally, we incorporate advanced guidance techniques, CFG++ and
DDS, to further enhance the interpolation process. By integrating these, our
method achieves state-of-the-art performance, efficiently generating
high-quality, smooth videos between keyframes. On a single 3090 GPU, our method
can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds,
establishing it as a leading solution for keyframe interpolation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vibidsampler.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Video Inverse Problems Using Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taesung Kwon, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, diffusion model-based inverse problem solvers (DIS) have emerged as
state-of-the-art approaches for addressing inverse problems, including image
super-resolution, deblurring, inpainting, etc. However, their application to
video inverse problems arising from spatio-temporal degradation remains largely
unexplored due to the challenges in training video diffusion models. To address
this issue, here we introduce an innovative video inverse solver that leverages
only image diffusion models. Specifically, by drawing inspiration from the
success of the recent decomposed diffusion sampler (DDS), our method treats the
time dimension of a video as the batch dimension of image diffusion models and
solves spatio-temporal optimization problems within denoised spatio-temporal
batches derived from each image diffusion model. Moreover, we introduce a
batch-consistent diffusion sampling strategy that encourages consistency across
batches by synchronizing the stochastic noise components in image diffusion
models. Our approach synergistically combines batch-consistent sampling with
simultaneous optimization of denoised spatio-temporal batches at each reverse
diffusion step, resulting in a novel and efficient diffusion sampling strategy
for video inverse problems. Experimental results demonstrate that our method
effectively addresses various spatio-temporal degradations in video inverse
problems, achieving state-of-the-art reconstructions. Project page:
https://svi-diffusion.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recall, Robustness, and Lexicographic Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11370v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11370v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Diaz, Michael D. Ekstrand, Bhaskar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although originally developed to evaluate sets of items, recall is often used
to evaluate rankings of items, including those produced by recommender,
retrieval, and other machine learning systems. The application of recall
without a formal evaluative motivation has led to criticism of recall as a
vague or inappropriate measure. In light of this debate, we reflect on the
measurement of recall in rankings from a formal perspective. Our analysis is
composed of three tenets: recall, robustness, and lexicographic evaluation.
First, we formally define `recall-orientation' as the sensitivity of a metric
to a user interested in finding every relevant item. Second, we analyze
recall-orientation from the perspective of robustness with respect to possible
content consumers and providers, connecting recall to recent conversations
about fair ranking. Finally, we extend this conceptual and theoretical
treatment of recall by developing a practical preference-based evaluation
method based on lexicographic comparison. Through extensive empirical analysis
across three recommendation tasks and 17 information retrieval tasks, we
establish that our new evaluation method, lexirecall, has convergent validity
(i.e., it is correlated with existing recall metrics) and exhibits
substantially higher sensitivity in terms of discriminative power and stability
in the presence of missing labels. Our conceptual, theoretical, and empirical
analysis substantially deepens our understanding of recall and motivates its
adoption through connections to robustness and fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Cross-Entropy Loss for Sequential Recommendations with Large
  Item Catalogs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Mezentsev, Danil Gusak, Ivan Oseledets, Evgeny Frolov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalability issue plays a crucial role in productionizing modern recommender
systems. Even lightweight architectures may suffer from high computational
overload due to intermediate calculations, limiting their practicality in
real-world applications. Specifically, applying full Cross-Entropy (CE) loss
often yields state-of-the-art performance in terms of recommendations quality.
Still, it suffers from excessive GPU memory utilization when dealing with large
item catalogs. This paper introduces a novel Scalable Cross-Entropy (SCE) loss
function in the sequential learning setup. It approximates the CE loss for
datasets with large-size catalogs, enhancing both time efficiency and memory
usage without compromising recommendations quality. Unlike traditional negative
sampling methods, our approach utilizes a selective GPU-efficient computation
strategy, focusing on the most informative elements of the catalog,
particularly those most likely to be false positives. This is achieved by
approximating the softmax distribution over a subset of the model outputs
through the maximum inner product search. Experimental results on multiple
datasets demonstrate the effectiveness of SCE in reducing peak memory usage by
a factor of up to 100 compared to the alternatives, retaining or even exceeding
their metrics values. The proposed approach also opens new perspectives for
large-scale developments in different domains, such as large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, fixed some typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unraveling Movie Genres through Cross-Attention Fusion of Bi-Modal
  Synergy of Poster 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utsav Kumar Nareti, Chandranath Adak, Soumi Chattopadhyay, Pichao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Movie posters are not just decorative; they are meticulously designed to
capture the essence of a movie, such as its genre, storyline, and tone/vibe.
For decades, movie posters have graced cinema walls, billboards, and now our
digital screens as a form of digital posters. Movie genre classification plays
a pivotal role in film marketing, audience engagement, and recommendation
systems. Previous explorations into movie genre classification have been mostly
examined in plot summaries, subtitles, trailers and movie scenes. Movie posters
provide a pre-release tantalizing glimpse into a film's key aspects, which can
ignite public interest. In this paper, we presented the framework that exploits
movie posters from a visual and textual perspective to address the multilabel
movie genre classification problem. Firstly, we extracted text from movie
posters using an OCR and retrieved the relevant embedding. Next, we introduce a
cross-attention-based fusion module to allocate attention weights to visual and
textual embedding. In validating our framework, we utilized 13882 posters
sourced from the Internet Movie Database (IMDb). The outcomes of the
experiments indicate that our model exhibited promising performance and
outperformed even some prominent contemporary architectures.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Large Language Models Memorizing Bug Benchmarks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Ramos, Claudia Mamede, Kush Jain, Paulo Canelas, Catarina Gamboa, Claire Le Goues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become integral to various software
engineering tasks, including code generation, bug detection, and repair. To
evaluate model performance in these domains, numerous bug benchmarks containing
real-world bugs from software projects have been developed. However, a growing
concern within the software engineering community is that these benchmarks may
not reliably reflect true LLM performance due to the risk of data leakage.
Despite this concern, limited research has been conducted to quantify the
impact of potential leakage. In this paper, we systematically evaluate popular
LLMs to assess their susceptibility to data leakage from widely used bug
benchmarks. To identify potential leakage, we use multiple metrics, including a
study of benchmark membership within commonly used training datasets, as well
as analyses of negative log-likelihood and n-gram accuracy. Our findings show
that certain models, in particular codegen-multi, exhibit significant evidence
of memorization in widely used benchmarks like Defects4J, while newer models
trained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.
These results highlight the need for careful benchmark selection and the
adoption of robust metrics to adequately assess models capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Mixed-State Self-Attention Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02871v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02871v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu Chen, Qinglin Zhao, Li Feng, Chuangtao Chen, Yangbin Lin, Jianhong Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention mechanisms have revolutionized natural language processing.
Combining them with quantum computing aims to further advance this technology.
This paper introduces a novel Quantum Mixed-State Self-Attention Network
(QMSAN) for natural language processing tasks. Our model leverages quantum
computing principles to enhance the effectiveness of self-attention mechanisms.
QMSAN uses a quantum attention mechanism based on mixed state, allowing for
direct similarity estimation between queries and keys in the quantum domain.
This approach leads to more effective attention coefficient calculations. We
also propose an innovative quantum positional encoding scheme, implemented
through fixed quantum gates within the circuit, improving the model's ability
to capture sequence information without additional qubit resources. In
numerical experiments of text classification tasks on public datasets, QMSAN
outperforms Quantum Self-Attention Neural Network (QSANN). Furthermore, we
demonstrate QMSAN's robustness in different quantum noise environments,
highlighting its potential for near-term quantum devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Large Language Model-empowered Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14165v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14165v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhu, Shiyi Wang, Wenqing Zhong, Nianchen Shen, Yunqi Li, Siqi Wang, Zhiheng Li, Cathy Wu, Zhengbing He, Li Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) plays a crucial role in autonomous driving (AD)
research, propelling its development towards intelligence and efficiency.
Currently, the development of AD technology follows two main technical paths:
modularization and end-to-end. Modularization decompose the driving task into
modules such as perception, prediction, planning, and control, and train them
separately. Due to the inconsistency of training objectives between modules,
the integrated effect suffers from bias. End-to-end attempts to address this
issue by utilizing a single model that directly maps from sensor data to
control signals. This path has limited learning capabilities in a comprehensive
set of features and struggles to handle unpredictable long-tail events and
complex urban traffic scenarios. In the face of challenges encountered in both
paths, many researchers believe that large language models (LLMs) with powerful
reasoning capabilities and extensive knowledge understanding may be the
solution, expecting LLMs to provide AD systems with deeper levels of
understanding and decision-making capabilities. In light of the challenges
faced by both paths, many researchers believe that LLMs, with their powerful
reasoning abilities and extensive knowledge, could offer a solution. To
understand if LLMs could enhance AD, this paper conducts a thorough analysis of
the potential applications of LLMs in AD systems, including exploring their
optimization strategies in both modular and end-to-end approaches, with a
particular focus on how LLMs can tackle the problems and challenges present in
current solutions. Furthermore, we discuss an important question: Can LLM-based
artificial general intelligence (AGI) be a key to achieve high-level AD? We
further analyze the potential limitations and challenges that LLMs may
encounter in promoting the development of AD technology.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unraveling Movie Genres through Cross-Attention Fusion of Bi-Modal
  Synergy of Poster 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utsav Kumar Nareti, Chandranath Adak, Soumi Chattopadhyay, Pichao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Movie posters are not just decorative; they are meticulously designed to
capture the essence of a movie, such as its genre, storyline, and tone/vibe.
For decades, movie posters have graced cinema walls, billboards, and now our
digital screens as a form of digital posters. Movie genre classification plays
a pivotal role in film marketing, audience engagement, and recommendation
systems. Previous explorations into movie genre classification have been mostly
examined in plot summaries, subtitles, trailers and movie scenes. Movie posters
provide a pre-release tantalizing glimpse into a film's key aspects, which can
ignite public interest. In this paper, we presented the framework that exploits
movie posters from a visual and textual perspective to address the multilabel
movie genre classification problem. Firstly, we extracted text from movie
posters using an OCR and retrieved the relevant embedding. Next, we introduce a
cross-attention-based fusion module to allocate attention weights to visual and
textual embedding. In validating our framework, we utilized 13882 posters
sourced from the Internet Movie Database (IMDb). The outcomes of the
experiments indicate that our model exhibited promising performance and
outperformed even some prominent contemporary architectures.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-29T00:00:00Z">2024-11-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">66</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Heyward, João Carreira, Dima Damen, Andrew Zisserman, Viorica Pătrăucean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the successful 2023 edition, we organised the Second Perception
Test challenge as a half-day workshop alongside the IEEE/CVF European
Conference on Computer Vision (ECCV) 2024, with the goal of benchmarking
state-of-the-art video models and measuring the progress since last year using
the Perception Test benchmark. This year, the challenge had seven tracks (up
from six last year) and covered low-level and high-level tasks, with language
and non-language interfaces, across video, audio, and text modalities; the
additional track covered hour-long video understanding and introduced a novel
video QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks
were: object tracking, point tracking, temporal action localisation, temporal
sound localisation, multiple-choice video question-answering, grounded video
question-answering, and hour-long video question-answering. We summarise in
this report the challenge tasks and results, and introduce in detail the novel
hour-long video QA benchmark 1h-walk VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2312.13090</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLSBench: Unveiling Visual Leakage in Multimodal Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhao Hu, Dongrui Liu, Hao Li, Xuanjing Huang, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety concerns of Multimodal large language models (MLLMs) have gradually
become an important problem in various applications. Surprisingly, previous
works indicate a counter-intuitive phenomenon that using textual unlearning to
align MLLMs achieves comparable safety performances with MLLMs trained with
image-text pairs. To explain such a counter-intuitive phenomenon, we discover a
visual safety information leakage (VSIL) problem in existing multimodal safety
benchmarks, i.e., the potentially risky and sensitive content in the image has
been revealed in the textual query. In this way, MLLMs can easily refuse these
sensitive text-image queries according to textual queries. However, image-text
pairs without VSIL are common in real-world scenarios and are overlooked by
existing multimodal safety benchmarks. To this end, we construct multimodal
visual leakless safety benchmark (VLSBench) preventing visual safety leakage
from image to textual query with 2.4k image-text pairs. Experimental results
indicate that VLSBench poses a significant challenge to both open-source and
close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.
This study demonstrates that textual alignment is enough for multimodal safety
scenarios with VSIL, while multimodal alignment is a more promising solution
for multimodal safety scenarios without VSIL. Please see our code and data at:
http://hxhcreate.github.io/VLSBench
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Domain-Specific Post-Training for Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daixuan Cheng, Shaohan Huang, Ziyu Zhu, Xintong Zhang, Wayne Xin Zhao, Zhongzhi Luan, Bo Dai, Zhenliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the rapid development of general multimodal large
language models (MLLMs). However, adapting general MLLMs to specific domains,
such as scientific fields and industrial applications, remains less explored.
This paper systematically investigates domain adaptation of MLLMs through
post-training, focusing on data synthesis, training pipelines, and task
evaluation. (1) Data Synthesis: Using open-source models, we develop a visual
instruction synthesizer that effectively generates diverse visual instruction
tasks from domain-specific image-caption pairs. Our synthetic tasks surpass
those generated by manual rules, GPT-4, and GPT-4V in enhancing the
domain-specific performance of MLLMs. (2) Training Pipeline: While the
two-stage training--initially on image-caption pairs followed by visual
instruction tasks--is commonly adopted for developing general MLLMs, we apply a
single-stage training pipeline to enhance task diversity for domain-specific
post-training. (3) Task Evaluation: We conduct experiments in two domains,
biomedicine and food, by post-training MLLMs of different sources and scales
(e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM
performance on various domain-specific tasks. To support further research in
MLLM domain adaptation, we will open-source our implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIMS: Simulating Human-Scene Interactions with Real World Script
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjia Wang, Liang Pan, Zhiyang Dou, Zhouyingcheng Liao, Yuke Lou, Lei Yang, Jingbo Wang, Taku Komura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulating long-term human-scene interaction is a challenging yet fascinating
task. Previous works have not effectively addressed the generation of long-term
human scene interactions with detailed narratives for physics-based animation.
This paper introduces a novel framework for the planning and controlling of
long-horizon physical plausible human-scene interaction. On the one hand, films
and shows with stylish human locomotions or interactions with scenes are
abundantly available on the internet, providing a rich source of data for
script planning. On the other hand, Large Language Models (LLMs) can understand
and generate logical storylines.
  This motivates us to marry the two by using an LLM-based pipeline to extract
scripts from videos, and then employ LLMs to imitate and create new scripts,
capturing complex, time-series human behaviors and interactions with
environments. By leveraging this, we utilize a dual-aware policy that achieves
both language comprehension and scene understanding to guide character motions
within contextual and spatial constraints. To facilitate training and
evaluation, we contribute a comprehensive planning dataset containing diverse
motion sequences extracted from real-world videos and expand them with large
language models. We also collect and re-annotate motion clips from existing
kinematic datasets to enable our policy learn diverse skills. Extensive
experiments demonstrate the effectiveness of our framework in versatile task
execution and its generalization ability to various scenarios, showing
remarkably enhanced performance compared with existing methods. Our code and
data will be publicly available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classical and Quantum Algorithms for the Deterministic L-system
  Inductive Inference Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Lotfi, Ian McQuillan, Steven Rayan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  L-systems can be made to model and create simulations of many biological
processes, such as plant development. Finding an L-system for a given process
is typically solved by hand, by experts, in a hugely time-consuming process. It
would be significant if this could be done automatically from data, such as
from sequences of images. In this paper, we are interested in inferring a
particular type of L-system, deterministic context-free L-system (D0L-system)
from a sequence of strings. We introduce the characteristic graph of a sequence
of strings, which we then utilize to translate our problem (inferring
D0L-system) in polynomial time into the maximum independent set problem (MIS)
and the SAT problem. After that, we offer a classical exact algorithm and an
approximate quantum algorithm for the problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIDetx: a compression-based method for identification of
  machine-learning generated text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Almeida, Pedro Rodrigues, Diogo Magalhães, Armando J. Pinho, Diogo Pratas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces AIDetx, a novel method for detecting machine-generated
text using data compression techniques. Traditional approaches, such as deep
learning classifiers, often suffer from high computational costs and limited
interpretability. To address these limitations, we propose a compression-based
classification framework that leverages finite-context models (FCMs). AIDetx
constructs distinct compression models for human-written and AI-generated text,
classifying new inputs based on which model achieves a higher compression
ratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores
exceeding 97% and 99%, respectively, highlighting its high accuracy. Compared
to current methods, such as large language models (LLMs), AIDetx offers a more
interpretable and computationally efficient solution, significantly reducing
both training time and hardware requirements (e.g., no GPUs needed). The full
implementation is publicly available at https://github.com/AIDetx/AIDetx.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reverse Thinking Makes LLMs Stronger Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Chih-Yao Chen, Zifeng Wang, Hamid Palangi, Rujun Han, Sayna Ebrahimi, Long Le, Vincent Perot, Swaroop Mishra, Mohit Bansal, Chen-Yu Lee, Tomas Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reverse thinking plays a crucial role in human reasoning. Humans can reason
not only from a problem to a solution but also in reverse, i.e., start from the
solution and reason towards the problem. This often enhances overall reasoning
performance as it enables consistency checks between their forward and backward
thinking. To enable Large Language Models (LLMs) to perform reverse thinking,
we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data
augmentation and learning objectives. In RevThink, we augment the dataset by
collecting structured forward-backward reasoning from a teacher model,
consisting of: (1) the original question, (2) forward reasoning, (3) backward
question, and (4) backward reasoning. We then employ three objectives to train
a smaller student model in a multi-task learning fashion: (a) generate forward
reasoning from a question, (b) generate a backward question from a question,
and (c) generate backward reasoning from the backward question. Experiments
across 12 datasets covering commonsense, math, and logical reasoning show an
average 13.53% improvement over the student model's zero-shot performance and a
6.84% improvement over the strongest knowledge distillation baselines.
Moreover, our method demonstrates sample efficiency -- using only 10% of the
correct forward reasoning from the training data, it outperforms a standard
fine-tuning method trained on 10x more forward reasoning. RevThink also
exhibits strong generalization to out-of-distribution held-out datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What fifty-one years of Linguistics and Artificial Intelligence research
  tell us about their correlation: A scientometric <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Q. Shormani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a strong correlation between linguistics and artificial intelligence
(AI), best manifested by deep learning language models. This study provides a
thorough scientometric analysis of this correlation, synthesizing the
intellectual production during 51 years, from 1974 to 2024. It involves 5750
Web of Science-indexed articles published in 2124 journals, which are written
by 20835 authors belonging to 13773 research centers in 794 countries. Two
powerful software, viz., CiteSpace and VOSviewer, were used to generate mapping
visualizations of the intellectual landscape, trending issues and (re)emerging
hotspots. The results indicate that in the 1980s and 1990s, linguistics and AI
research was not robust, characterized by unstable publication over time. It
has, however, witnessed a remarkable increase of publication since then,
reaching 1478 articles in 2023, and 546 articles in January-March timespan in
2024, involving emerging issues and hotspots, addressing new horizons, new
topics, and launching new applications and powerful deep learning language
models including ChatGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensitive Content Classification in Social Media: A Holistic Resource
  and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimosthenis Antypas, Indira Sen, Carla Perez-Almendros, Jose Camacho-Collados, Francesco Barbieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of sensitive content in large datasets is crucial for ensuring
that shared and analysed data is free from harmful material. However, current
moderation tools, such as external APIs, suffer from limitations in
customisation, accuracy across diverse sensitive categories, and privacy
concerns. Additionally, existing datasets and open-source models focus
predominantly on toxic language, leaving gaps in detecting other sensitive
categories such as substance abuse or self-harm. In this paper, we put forward
a unified dataset tailored for social media content moderation across six
sensitive categories: conflictual language, profanity, sexually explicit
material, drug-related content, self-harm, and spam. By collecting and
annotating data with consistent retrieval strategies and guidelines, we address
the shortcomings of previous focalised research. Our analysis demonstrates that
fine-tuning large language models (LLMs) on this novel dataset yields
significant improvements in detection performance compared to open
off-the-shelf models such as LLaMA, and even proprietary OpenAI models, which
underperform by 10-15% overall. This limitation is even more pronounced on
popular moderation APIs, which cannot be easily tailored to specific sensitive
content categories, among others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDR-GNN: Spectral Domain Reconstruction Graph Neural Network for
  Incomplete Multimodal Learning in Conversational Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangze Fu, Wei Ai, Fan Yang, Yuntao Shou, Tao Meng, Keqin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Emotion Recognition in Conversations (MERC) aims to classify
utterance emotions using textual, auditory, and visual modal features. Most
existing MERC methods assume each utterance has complete modalities,
overlooking the common issue of incomplete modalities in real-world scenarios.
Recently, graph neural networks (GNNs) have achieved notable results in
Incomplete Multimodal Emotion Recognition in Conversations (IMERC). However,
traditional GNNs focus on binary relationships between nodes, limiting their
ability to capture more complex, higher-order information. Moreover, repeated
message passing can cause over-smoothing, reducing their capacity to preserve
essential high-frequency details. To address these issues, we propose a
Spectral Domain Reconstruction Graph Neural Network (SDR-GNN) for incomplete
multimodal learning in conversational emotion recognition. SDR-GNN constructs
an utterance semantic interaction graph using a sliding window based on both
speaker and context relationships to model emotional dependencies. To capture
higher-order and high-frequency information, SDR-GNN utilizes weighted
relationship aggregation, ensuring consistent semantic feature extraction
across utterances. Additionally, it performs multi-frequency aggregation in the
spectral domain, enabling efficient recovery of incomplete modalities by
extracting both high- and low-frequency information. Finally, multi-head
attention is applied to fuse and optimize features for emotion recognition.
Extensive experiments on various real-world datasets demonstrate that our
approach is effective in incomplete multimodal learning and outperforms current
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INCLUDE: Evaluating Multilingual Language Understanding with Regional
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzemiński, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee, Sara Hooker, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance differential of large language models (LLM) between languages
hinders their effective deployment in many regions, inhibiting the potential
economic and societal value of generative AI tools in many communities.
However, the development of functional LLMs in many languages (\ie,
multilingual LLMs) is bottlenecked by the lack of high-quality evaluation
resources in languages other than English. Moreover, current practices in
multilingual benchmark construction often translate English resources, ignoring
the regional and cultural knowledge of the environments in which multilingual
systems would be used. In this work, we construct an evaluation suite of
197,243 QA pairs from local exam sources to measure the capabilities of
multilingual LLMs in a variety of regional contexts. Our novel resource,
INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across
44 written languages that evaluates multilingual LLMs for performance in the
actual language environments where they would be deployed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Wu, Wei Ji, Kecheng Zheng, Zicheng Wang, Dong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, human motion analysis has experienced great improvement due to
inspiring generative models such as the denoising diffusion model and large
language model. While the existing approaches mainly focus on generating
motions with textual descriptions and overlook the reciprocal task. In this
paper, we present~\textbf{MoTe}, a unified multi-modal model that could handle
diverse tasks by learning the marginal, conditional, and joint distributions of
motion and text simultaneously. MoTe enables us to handle the paired
text-motion generation, motion captioning, and text-driven motion generation by
simply modifying the input context. Specifically, MoTe is composed of three
components: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), and
Moti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained for
extracting latent embeddings, and subsequently reconstructing the motion
sequences and textual descriptions from the extracted embeddings, respectively.
MTDM, on the other hand, performs an iterative denoising process on the input
context to handle diverse tasks. Experimental results on the benchmark datasets
demonstrate the superior performance of our proposed method on text-to-motion
generation and competitive performance on motion captioning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Five figures, six tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PerLA: Perceptive 3D Language Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guofeng Mei, Wei Lin, Luigi Riz, Yujiao Wu, Fabio Poiesi, Yiming Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling Large Language Models (LLMs) to understand the 3D physical world is
an emerging yet challenging research direction. Current strategies for
processing point clouds typically downsample the scene or divide it into
smaller parts for separate analysis. However, both approaches risk losing key
local details or global contextual information. In this paper, we introduce
PerLA, a 3D language assistant designed to be more perceptive to both details
and context, making visual representations more informative for the LLM. PerLA
captures high-resolution (local) details in parallel from different point cloud
areas and integrates them with (global) context obtained from a
lower-resolution whole point cloud. We present a novel algorithm that preserves
point cloud locality through the Hilbert curve and effectively aggregates
local-to-global information via cross-attention and a graph neural network.
Lastly, we introduce a novel loss for local representation consensus to promote
training stability. PerLA outperforms state-of-the-art 3D language assistants,
with gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 on
ScanRefer and +3.88 on Nr3D for dense
captioning.\url{https://gfmei.github.io/PerLA/}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware
  Omni-Modal Perception of Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite impressive advancements in video understanding, most efforts remain
limited to coarse-grained or visual-only video tasks. However, real-world
videos encompass omni-modal information (vision, audio, and speech) with a
series of events forming a cohesive storyline. The lack of multi-modal video
data with fine-grained event annotations and the high cost of manual labeling
are major obstacles to comprehensive omni-modality video perception. To address
this gap, we propose an automatic pipeline consisting of high-quality
multi-modal video filtering, semantically coherent omni-modal event boundary
detection, and cross-modal correlation-aware event captioning. In this way, we
present LongVALE, the first-ever Vision-Audio-Language Event understanding
benchmark comprising 105K omni-modal events with precise temporal boundaries
and detailed relation-aware captions within 8.4K high-quality long videos.
Further, we build a baseline that leverages LongVALE to enable video large
language models (LLMs) for omni-modality fine-grained temporal video
understanding for the first time. Extensive experiments demonstrate the
effectiveness and great potential of LongVALE in advancing comprehensive
multi-modal video understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noro: A Noise-Robust One-shot Voice Conversion System with Hidden
  Speaker Representation Capabilities <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haorui He, Yuchen Song, Yuancheng Wang, Haoyang Li, Xueyao Zhang, Li Wang, Gongping Huang, Eng Siong Chng, Zhizheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-shot voice conversion (VC) aims to alter the timbre of speech from a
source speaker to match that of a target speaker using just a single reference
speech from the target, while preserving the semantic content of the original
source speech. Despite advancements in one-shot VC, its effectiveness decreases
in real-world scenarios where reference speeches, often sourced from the
internet, contain various disturbances like background noise. To address this
issue, we introduce Noro, a Noise Robust One-shot VC system. Noro features
innovative components tailored for VC using noisy reference speeches, including
a dual-branch reference encoding module and a noise-agnostic contrastive
speaker loss. Experimental results demonstrate that Noro outperforms our
baseline system in both clean and noisy scenarios, highlighting its efficacy
for real-world applications. Additionally, we investigate the hidden speaker
representation capabilities of our baseline system by repurposing its reference
encoder as a speaker encoder. The results shows that it is competitive with
several advanced self-supervised learning models for speaker representation
under the SUPERB settings, highlighting the potential for advancing speaker
representation learning through one-shot VC task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE OJSP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Learning Approach to Language-independent Gender Prediction on
  Twitter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reyhaneh Hashempour, Barbara Plank, Aline Villavicencio, Renato Cordeiro de Amorim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a set of experiments conducted to predict the gender of
Twitter users based on language-independent features extracted from the text of
the users' tweets. The experiments were performed on a version of TwiSty
dataset including tweets written by the users of six different languages:
Portuguese, French, Dutch, English, German, and Italian. Logistic regression
(LR), and feed-forward neural networks (FFNN) with back-propagation were used
to build models in two different settings: Inter-Lingual (IL) and Cross-Lingual
(CL). In the IL setting, the training and testing were performed on the same
language whereas in the CL, Italian and German datasets were set aside and only
used as test sets and the rest were combined to compose training and
development sets. In the IL, the highest accuracy score belongs to LR whereas
in the CL, FFNN with three hidden layers yields the highest score. The results
show that neural network based models underperform traditional models when the
size of the training set is small; however, they beat traditional models by a
non-trivial margin, when they are fed with large enough data. Finally, the
feature analysis confirms that men and women have different writing styles
independent of their language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Santali Linguistic Inclusion: Building the First
  Santali-to-English Translation Model using mT5 <span class="highlight-title">Transformer</span> and Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed Mohammed Mostaque Billah, Ateya Ahmed Subarna, Sudipta Nandi Sarna, Ahmad Shawkat Wasit, Anika Fariha, Asif Sushmit, Arig Yousuf Sadeque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Around seven million individuals in India, Bangladesh, Bhutan, and Nepal
speak Santali, positioning it as nearly the third most commonly used
Austroasiatic language. Despite its prominence among the Austroasiatic language
family's Munda subfamily, Santali lacks global recognition. Currently, no
translation models exist for the Santali language. Our paper aims to include
Santali to the NPL spectrum. We aim to examine the feasibility of building
Santali translation models based on available Santali corpora. The paper
successfully addressed the low-resource problem and, with promising results,
examined the possibility of creating a functional Santali machine translation
model in a low-resource setup. Our study shows that Santali-English parallel
corpus performs better when in transformers like mt5 as opposed to untrained
transformers, proving that transfer learning can be a viable technique that
works with Santali language. Besides the mT5 transformer, Santali-English
performs better than Santali-Bangla parallel corpus as the mT5 has been trained
in way more English data than Bangla data. Lastly, our study shows that with
data augmentation, our model performs better.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TakeLab Retriever: AI-Driven Search Engine for Articles from Croatian
  News Outlets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Dukić, Marin Petričević, Sven Ćurković, Jan Šnajder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TakeLab Retriever is an AI-driven search engine designed to discover,
collect, and semantically analyze news articles from Croatian news outlets. It
offers a unique perspective on the history and current landscape of Croatian
online news media, making it an essential tool for researchers seeking to
uncover trends, patterns, and correlations that general-purpose search engines
cannot provide. TakeLab retriever utilizes cutting-edge natural language
processing (NLP) methods, enabling users to sift through articles using named
entities, phrases, and topics through the web application. This technical
report is divided into two parts: the first explains how TakeLab Retriever is
utilized, while the second provides a detailed account of its design. In the
second part, we also address the software engineering challenges involved and
propose solutions for developing a microservice-based semantic search engine
capable of handling over ten million news articles published over the past two
decades.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating
  Multi-Insight Multi-Document Extraction Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Francis, Saba Esnaashari, Anton Poletaev, Sukankana Chakraborty, Youmna Hashem, Jonathan Bright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities in
text analysis tasks, yet their evaluation on complex, real-world applications
remains challenging. We define a set of tasks, Multi-Insight Multi-Document
Extraction (MIMDE) tasks, which involves extracting an optimal set of insights
from a document corpus and mapping these insights back to their source
documents. This task is fundamental to many practical applications, from
analyzing survey responses to processing medical records, where identifying and
tracing key insights across documents is crucial. We develop an evaluation
framework for MIMDE and introduce a novel set of complementary human and
synthetic datasets to examine the potential of synthetic data for LLM
evaluation. After establishing optimal metrics for comparing extracted
insights, we benchmark 20 state-of-the-art LLMs on both datasets. Our analysis
reveals a strong correlation (0.71) between the ability of LLMs to extracts
insights on our two datasets but synthetic data fails to capture the complexity
of document-level analysis. These findings offer crucial guidance for the use
of synthetic data in evaluating text analysis systems, highlighting both its
potential and limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with
  Multi-dimensional and fine-grained information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanyue Zhang, Ziyong Li, Wen Yang, Chunlin Leng, Yinan Bai, Qianlong Du, Chengqing Zong, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the development of large language models (LLMs), pre-training data
play a critical role in shaping LLMs' capabilities. In recent years several
large-scale and high-quality pre-training datasets have been released to
accelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,
WanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has
increasingly shifted to domain-specific capabilities and safety concerns,
making those previous coarse-grained texts insufficient for meeting training
requirements. Furthermore, fine-grained information, such as quality, domain
and toxicity, is becoming increasingly important in building powerful and
reliable LLMs for various scenarios. To address these challenges, in this paper
we propose a new tool-chain called MDFG-tool for constructing large-scale and
high-quality Chinese datasets with multi-dimensional and fine-grained
information. First, we employ manually crafted rules to discard explicit noisy
texts from raw contents. Second, the quality evaluation model, domain
classifier, and toxicity evaluation model are well-designed to assess the
remaining cleaned data respectively. Finally, we integrate these three types of
fine-grained information for each text. With this approach, we release the
largest, high-quality and fine-grained Chinese text ChineseWebText2.0, which
consists of 3.8TB and each text is associated with a quality score, domain
labels, a toxicity label and a toxicity score, facilitating the LLM researchers
to select data based on various types of fine-grained information. The data,
codes and the tool-chain are available on this website
https://github.com/CASIA-LM/ChineseWebText-2.0
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ChineseWebTex2.0 dataset is available at
  https://github.com/CASIA-LM/ChineseWebText-2.0</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CogACT: A Foundational Vision-Language-Action Model for Synergizing
  Cognition and Action in Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of large Vision-Language-Action (VLA) models has
significantly improved robotic manipulation in terms of language-guided task
execution and generalization to unseen scenarios. While existing VLAs adapted
from pretrained large Vision-Language-Models (VLM) have demonstrated promising
generalizability, their task performance is still unsatisfactory as indicated
by the low tasks success rates in different environments. In this paper, we
present a new advanced VLA architecture derived from VLM. Unlike previous works
that directly repurpose VLM for action prediction by simple action
quantization, we propose a omponentized VLA architecture that has a specialized
action module conditioned on VLM output. We systematically study the design of
the action module and demonstrates the strong performance enhancement with
diffusion action transformers for action sequence modeling, as well as their
favorable scaling behaviors. We also conduct comprehensive experiments and
ablation studies to evaluate the efficacy of our models with varied designs.
The evaluation on 5 robot embodiments in simulation and real work shows that
our model not only significantly surpasses existing VLAs in task performance
and but also exhibits remarkable adaptation to new robots and generalization to
unseen objects and backgrounds. It exceeds the average success rates of OpenVLA
which has similar model size (7B) with ours by over 35% in simulated evaluation
and 55% in real robot experiments. It also outperforms the large RT-2-X model
(55B) by 18% absolute success rates in simulation. Code and models can be found
on our project page (https://cogact.github.io/).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://cogact.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Teacher-Student Framework for Text Classification With No Manually
  Annotated Data: A Case Study in IPTC News Topic Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taja Kuzman, Nikola Ljubešić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the ever-increasing number of news stories available online, classifying
them by topic, regardless of the language they are written in, has become
crucial for enhancing readers' access to relevant content. To address this
challenge, we propose a teacher-student framework based on large language
models (LLMs) for developing multilingual news classification models of
reasonable size with no need for manual data annotation. The framework employs
a Generative Pretrained Transformer (GPT) model as the teacher model to develop
an IPTC Media Topic training dataset through automatic annotation of news
articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits
a high zero-shot performance on all four languages. Its agreement with human
annotators is comparable to that between the human annotators themselves. To
mitigate the computational limitations associated with the requirement of
processing millions of texts daily, smaller BERT-like student models are
fine-tuned on the GPT-annotated dataset. These student models achieve high
performance comparable to the teacher model. Furthermore, we explore the impact
of the training data size on the performance of the student models and
investigate their monolingual, multilingual and zero-shot cross-lingual
capabilities. The findings indicate that student models can achieve high
performance with a relatively small number of training instances, and
demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the
best-performing news topic classifier, enabling multilingual classification
with the top-level categories of the IPTC Media Topic schema.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Multimodal Large Language Models via Dynamic Visual-Token
  Exit and the Empirical Findings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Wu, Wenhao Lin, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The excessive use of visual tokens in existing Multimoal Large Language
Models (MLLMs) often exhibits obvious redundancy and brings in prohibitively
expensive computation. To gain insights into this problem, we first conduct
extensive empirical studies on the attention behaviors of MLLMs, and summarize
three main inference stages in MLLMs: (i) Early fusion between tokens is first
accomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)
Multimodal reasoning} resumes and lasts until the end of inference. In
particular, we reveal that visual tokens will stop contributing to reasoning
when the text tokens receive enough image information, yielding obvious visual
redundancy. Based on these generalized observations, we propose a simple yet
effective method to improve the efficiency of MLLMs, termed dynamic
visual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive
the text token status and decide the removal of all visual tokens after a
certain layer, thereby addressing the observed visual redundancy. To validate
VTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,
and conduct extensive experiments on a bunch of benchmarks. The experiment
results not only show the effectiveness of our VTE in improving MLLMs'
efficiency, but also yield the general modeling patterns of MLLMs, well
facilitating the in-depth understanding of MLLMs. Our code is anonymously
released at https://github.com/DoubtedSteam/DyVTE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Reason about the Region Connection Calculus? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony G Cohn, Robert E Blackwell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Qualitative Spatial Reasoning is a well explored area of Knowledge
Representation and Reasoning and has multiple applications ranging from
Geographical Information Systems to Robotics and Computer Vision. Recently,
many claims have been made for the reasoning capabilities of Large Language
Models (LLMs). Here, we investigate the extent to which a set of representative
LLMs can perform classical qualitative spatial reasoning tasks on the
mereotopological Region Connection Calculus, RCC-8. We conduct three pairs of
experiments (reconstruction of composition tables, alignment to human
composition preferences, conceptual neighbourhood reconstruction) using
state-of-the-art LLMs; in each pair one experiment uses eponymous relations and
one, anonymous relations (to test the extent to which the LLM relies on
knowledge about the relation names obtained during training). All instances are
repeated 30 times to measure the stochasticity of the LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages. arXiv admin note: text overlap with arXiv:2309.15577</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Learning with Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyong Kang, Donghyun Son, Hwanjun Song, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning refers to the emerging ability of large language models
(LLMs) to perform a target task without additional training, utilizing
demonstrations of the task. Recent studies aim to enhance in-context learning
performance by selecting more useful demonstrations. However, they overlook the
presence of inevitable noisy labels in task demonstrations that arise during
the labeling process in the real-world. In this paper, we propose a new task,
in-context learning with noisy labels, which aims to solve real-world problems
for in-context learning where labels in task demonstrations would be corrupted.
Moreover, we propose a new method and baseline methods for the new task,
inspired by studies in learning with noisy labels. Through experiments, we
demonstrate that our proposed method can serve as a safeguard against
performance degradation in in-context learning caused by noisy labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICPR 2024 Competition on Multilingual Claim-Span Identification <span class="chip">ICPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soham Poddar, Biswajit Paul, Moumita Basu, Saptarshi Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A lot of claims are made in social media posts, which may contain
misinformation or fake news. Hence, it is crucial to identify claims as a first
step towards claim verification. Given the huge number of social media posts,
the task of identifying claims needs to be automated. This competition deals
with the task of 'Claim Span Identification' in which, given a text, parts /
spans that correspond to claims are to be identified. This task is more
challenging than the traditional binary classification of text into claim or
not-claim, and requires state-of-the-art methods in Pattern Recognition,
Natural Language Processing and Machine Learning. For this competition, we used
a newly developed dataset called HECSI containing about 8K posts in English and
about 8K posts in Hindi with claim-spans marked by human annotators. This paper
gives an overview of the competition, and the solutions developed by the
participating teams.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KV Shifting Attention Enhances Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Xu, Wei Cheng, Bingning Wang, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current large language models are mainly based on decode-only structure
transformers, which have great in-context learning (ICL) capabilities. It is
generally believed that the important foundation of its ICL capability is the
induction heads mechanism, which requires at least two layers attention. In
order to more efficiently implement the ability of the model's induction, we
revisit the induction heads mechanism and proposed a KV shifting attention. We
theoretically prove that the KV shifting attention reducing the model's
requirements for the depth and width of the induction heads mechanism. Our
experimental results demonstrate that KV shifting attention is beneficial to
learning induction heads and language modeling, which lead to better
performance or faster convergence from toy models to the pre-training models
with more than 10 B parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensemble Watermarks for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Niess, Roman Kern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has made it
increasingly difficult to distinguish between text written by humans and
machines. While watermarks already exist for LLMs, they often lack flexibility,
and struggle with attacks such as paraphrasing. To address these issues, we
propose a multi-feature method for generating watermarks that combines multiple
distinct watermark features into an ensemble watermark. Concretely, we combine
acrostica and sensorimotor norms with the established red-green watermark to
achieve a 98% detection rate. After a paraphrasing attack the performance
remains high with 95% detection rate. The red-green feature alone as baseline
achieves a detection rate of 49%. The evaluation of all feature combinations
reveals that the ensemble of all three consistently has the highest detection
rate across several LLMs and watermark strength settings. Due to the
flexibility of combining features in the ensemble, various requirements and
trade-offs can be addressed. Additionally, for all ensemble configurations the
same detection function can be used without adaptations. This method is
particularly of interest to facilitate accountability and prevent societal
harm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages in the main body. Code is available at
  http://github.com/CommodoreEU/master-generation. arXiv admin note:
  substantial text overlap with arXiv:2405.08400</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initialization using Update Approximation is a Silver Bullet for
  Extremely Efficient Low-Rank Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh Ponkshe, Raghav Singhal, Eduard Gorbunov, Alexey Tumanov, Samuel Horvath, Praneeth Vepakomma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank adapters have become a standard approach for efficiently fine-tuning
large language models (LLMs), but they often fall short of achieving the
performance of full fine-tuning. We propose a method, LoRA Silver Bullet or
LoRA-SB, that approximates full fine-tuning within low-rank subspaces using a
carefully designed initialization strategy. We theoretically demonstrate that
the architecture of LoRA-XS, which inserts a trainable (r x r) matrix between B
and A while keeping other matrices fixed, provides the precise conditions
needed for this approximation. We leverage its constrained update space to
achieve optimal scaling for high-rank gradient updates while removing the need
for hyperparameter tuning. We prove that our initialization offers an optimal
low-rank approximation of the initial gradient and preserves update directions
throughout training. Extensive experiments across mathematical reasoning,
commonsense reasoning, and language understanding tasks demonstrate that our
approach exceeds the performance of standard LoRA while using 27-90x fewer
parameters, and comprehensively outperforms LoRA-XS. Our findings establish
that it is possible to simulate full fine-tuning in low-rank subspaces, and
achieve significant efficiency gains without sacrificing performance. Our code
is publicly available at https://github.com/RaghavSinghal10/lora-sb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Kaustubh Ponkshe and Raghav Singhal contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Agents with Weakly Supervised Feedback from Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dihong Gong, Pu Lu, Zelong Wang, Meng Zhou, Xiuqiang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) offer a promising basis for creating agents that
can tackle complex tasks through iterative environmental interaction. Existing
methods either require these agents to mimic expert-provided trajectories or
rely on definitive environmental feedback for reinforcement learning which
limits their application to specific scenarios like gaming or code generation.
This paper introduces a novel training method for LLM-based agents using weakly
supervised signals from a critic LLM, bypassing the need for expert
trajectories or definitive feedback. Our agents are trained in iterative
manner, where they initially generate trajectories through environmental
interaction. Subsequently, a critic LLM selects a subset of good trajectories,
which are then used to update the agents, enabling them to generate improved
trajectories in the next iteration. Extensive tests on the API-bank dataset
show consistent improvement in our agents' capabilities and comparable
performance to GPT-4, despite using open-source models with much fewer
parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Management for Automobile Failure Analysis Using Graph RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Ojima, Hiroki Sakaji, Tadashi Nakamura, Hiroaki Sakata, Kazuya Seki, Yuu Teshigawara, Masami Yamashita, Kazuhiro Aoyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a knowledge management system for automobile failure
analysis using retrieval-augmented generation (RAG) with large language models
(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a
growing demand for knowledge transfer of failure analysis from experienced
engineers to young engineers. However, failure events are phenomena that occur
in a chain reaction, making them difficult for beginners to analyze them. While
knowledge graphs, which can describe semantic relationships and structure
information is effective in representing failure events, due to their
capability of representing the relationships between components, there is much
information in KGs, so it is challenging for young engineers to extract and
understand sub-graphs from the KG. On the other hand, there is increasing
interest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for
knowledge management. However, when using the current Graph RAG framework with
an existing knowledge graph for automobile failures, several issues arise
because it is difficult to generate executable queries for a knowledge graph
database which is not constructed by LLMs. To address this, we focused on
optimizing the Graph RAG pipeline for existing knowledge graphs. Using an
original Q&A dataset, the ROUGE F1 score of the sentences generated by the
proposed method showed an average improvement of 157.6% compared to the current
method. This highlights the effectiveness of the proposed method for automobile
failure analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, to be published in 2024 IEEE International
  Conference on Bid Data (BigData)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with
  Scalable Context and Symbolic Extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Qiu, You Peng, Guangxin He, Binhang Yuan, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has unlocked great opportunities
in complex data management tasks, particularly in question answering (QA) over
complicated multi-table relational data. Despite significant progress,
systematically evaluating LLMs on multi-table QA remains a critical challenge
due to the inherent complexity of analyzing heterogeneous table structures and
potential large scale of serialized relational data. Existing benchmarks
primarily focus on single-table QA, failing to capture the intricacies of
reasoning across multiple relational tables, as required in real-world domains
such as finance, healthcare, and e-commerce. To address this gap, we present
TQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities
of LLMs in tackling complex QA tasks over relational data. Our benchmark
incorporates diverse relational database instances sourced from real-world
public datasets and introduces a flexible sampling mechanism to create tasks
with varying multi-table context lengths, ranging from 8K to 64K tokens. To
ensure robustness and reliability, we integrate symbolic extensions into the
evaluation framework, enabling the assessment of LLM reasoning capabilities
beyond simple data retrieval or probabilistic pattern matching. We
systematically evaluate a range of LLMs, both open-source and closed-source,
spanning model scales from 7 billion to 70 billion parameters. Our extensive
experiments reveal critical insights into the performance of LLMs in
multi-table QA, highlighting both challenges and opportunities for advancing
their application in complex, data-driven environments. Our benchmark
implementation and results are available at
https://github.com/Relaxed-System-Lab/TQA-Bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COLD: Causal reasOning in cLosed Daily activities <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Joshi, Areeb Ahmad, Ashutosh Modi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown state-of-the-art performance in a
variety of tasks, including arithmetic and reasoning; however, to gauge the
intellectual capabilities of LLMs, causal reasoning has become a reliable proxy
for validating a general understanding of the mechanics and intricacies of the
world similar to humans. Previous works in natural language processing (NLP)
have either focused on open-ended causal reasoning via causal commonsense
reasoning (CCR) or framed a symbolic representation-based question answering
for theoretically backed-up analysis via a causal inference engine. The former
adds an advantage of real-world grounding but lacks theoretically backed-up
analysis/validation, whereas the latter is far from real-world grounding. In
this work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed
Daily activities) framework, which is built upon human understanding of daily
real-world activities to reason about the causal nature of events. We show that
the proposed framework facilitates the creation of enormous causal queries (~ 9
million) and comes close to the mini-turing test, simulating causal reasoning
to evaluate the understanding of a daily real-world task. We evaluate multiple
LLMs on the created causal queries and find that causal reasoning is
challenging even for activities trivial to humans. We further explore (the
causal reasoning abilities of LLMs) using the backdoor criterion to determine
the causal strength between events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at NeurIPS 2024; Total 37 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple and Provable Scaling Law for the Test-Time Compute of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a general two-stage algorithm that enjoys a provable scaling law
for the test-time compute of large language models (LLMs). Given an input
problem, the proposed algorithm first generates $N$ candidate solutions, and
then chooses the best one via a multiple-round knockout tournament where each
pair of candidates are compared for $K$ times and only the winners move on to
the next round. In a minimalistic implementation, both stages can be executed
with a black-box LLM alone and nothing else (e.g., no external verifier or
reward model), and a total of $N \times (K + 1)$ highly parallelizable LLM
calls are needed for solving an input problem. Assuming that a generated
candidate solution is correct with probability $p_{\text{gen}} > 0$ and a
comparison between a pair of correct and incorrect solutions identifies the
right winner with probability $p_{\text{comp}} > 0.5$ (i.e., better than a
random guess), we prove theoretically that the failure probability of the
proposed algorithm decays to zero exponentially with respect to $N$ and $K$:
$$\mathbb{P}(\text{final output is incorrect}) \le (1 - p_{\text{gen}})^N +
\lceil \log_2 N \rceil e^{-2 K (p_{\text{comp}} - 0.5)^2}.$$ Our empirical
results with the challenging MMLU-Pro benchmark validate the technical
assumptions, as well as the efficacy of the proposed algorithm and the gains
from scaling up its test-time compute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension
  Ability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Han, Lei Xu, Sirui Chen, Difan Zou, Chaochao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable capability in natural
language tasks, yet debate persists on whether they truly comprehend deep
structure (i.e., core semantics) or merely rely on surface structure (e.g.,
presentation format). Prior studies observe that LLMs' performance declines
when intervening on surface structure, arguing their success relies on surface
structure recognition. However, surface structure sensitivity does not prevent
deep structure comprehension. Rigorously evaluating LLMs' capability requires
analyzing both, yet deep structure is often overlooked. To this end, we assess
LLMs' comprehension ability using causal mediation analysis, aiming to fully
discover the capability of using both deep and surface structures.
Specifically, we formulate the comprehension of deep structure as direct causal
effect (DCE) and that of surface structure as indirect causal effect (ICE),
respectively. To address the non-estimability of original DCE and ICE --
stemming from the infeasibility of isolating mutual influences of deep and
surface structures, we develop the corresponding quantifiable surrogates,
including approximated DCE (ADCE) and approximated ICE (AICE). We further apply
the ADCE to evaluate a series of mainstream LLMs, showing that most of them
exhibit deep structure comprehension ability, which grows along with the
prediction accuracy. Comparing ADCE and AICE demonstrates closed-source LLMs
rely more on deep structure, while open-source LLMs are more surface-sensitive,
which decreases with model scale. Theoretically, ADCE is a bidirectional
evaluation, which measures both the sufficiency and necessity of deep structure
changes in causing output variations, thus offering a more comprehensive
assessment than accuracy, a common evaluation in LLMs. Our work provides new
insights into LLMs' deep structure comprehension and offers novel methods for
LLMs evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 14 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Yu, Shaolei Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Iterative retrieval refers to the process in which the model continuously
queries the retriever during generation to enhance the relevance of the
retrieved knowledge, thereby improving the performance of Retrieval-Augmented
Generation (RAG). Existing work typically employs few-shot prompting or
manually constructed rules to implement iterative retrieval. This introduces
additional inference overhead and overlooks the remarkable reasoning
capabilities of Large Language Models (LLMs). In this paper, we introduce
Auto-RAG, an autonomous iterative retrieval model centered on the LLM's
powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues
with the retriever, systematically planning retrievals and refining queries to
acquire valuable knowledge. This process continues until sufficient external
information is gathered, at which point the results are presented to the user.
To this end, we develop a method for autonomously synthesizing reasoning-based
decision-making instructions in iterative retrieval and fine-tuned the latest
open-source LLMs. The experimental results indicate that Auto-RAG is capable of
autonomous iterative interaction with the retriever, effectively leveraging the
remarkable reasoning and decision-making abilities of LLMs, which lead to
outstanding performance across six benchmarks. Further analysis reveals that
Auto-RAG can autonomously adjust the number of iterations based on the
difficulty of the questions and the utility of the retrieved knowledge, without
requiring any human intervention. Moreover, Auto-RAG expresses the iterative
retrieval process in natural language, enhancing interpretability while
providing users with a more intuitive experience\footnote{Code is available at
\url{https://github.com/ictnlp/Auto-RAG}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/ictnlp/Auto-RAG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Actions and Objects Pathways for Domain Adaptation in Video Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Safaa Abdullahi Moallim Mohamud, Ho-Young Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce the Actions and Objects Pathways (AOPath) for
out-of-domain generalization in video question answering tasks. AOPath
leverages features from a large pretrained model to enhance generalizability
without the need for explicit training on the unseen domains. Inspired by human
brain, AOPath dissociates the pretrained features into action and object
features, and subsequently processes them through separate reasoning pathways.
It utilizes a novel module which converts out-of-domain features into
domain-agnostic features without introducing any trainable weights. We validate
the proposed approach on the TVQA dataset, which is partitioned into multiple
subsets based on genre to facilitate the assessment of generalizability. The
proposed approach demonstrates 5% and 4% superior performance over conventional
classifiers on out-of-domain and in-domain datasets, respectively. It also
outperforms prior methods that involve training millions of parameters, whereas
the proposed approach trains very few parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Concept Depth: How Large Language Models Acquire Knowledge at
  Different Layers? <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07066v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07066v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performances across a wide
range of tasks. However, the mechanisms by which these models encode tasks of
varying complexities remain poorly understood. In this paper, we explore the
hypothesis that LLMs process concepts of varying complexities in different
layers, introducing the idea of "Concept Depth" to suggest that more complex
concepts are typically acquired in deeper layers. Specifically, we categorize
concepts based on their level of abstraction, defining them in the order of
increasing complexity within factual, emotional, and inferential tasks. We
conduct extensive probing experiments using layer-wise representations across
various LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the
three domains of tasks. Our findings reveal that models could efficiently
conduct probing for simpler tasks in shallow layers, and more complex tasks
typically necessitate deeper layers for accurate understanding. Additionally,
we examine how external factors, such as adding noise to the input and
quantizing the model weights, might affect layer-wise representations. Our
findings suggest that these factors can impede the development of a conceptual
understanding of LLMs until deeper layers are explored. We hope that our
proposed concept and experimental insights will enhance the understanding of
the mechanisms underlying LLMs. Our codes are available at
\url{https://github.com/Luckfort/CD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Prejudice to Parity: A New Approach to Debiasing Large Language
  Model Word Embeddings <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11512v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11512v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embeddings play a pivotal role in the efficacy of Large Language Models. They
are the bedrock on which these models grasp contextual relationships and foster
a more nuanced understanding of language and consequently perform remarkably on
a plethora of complex tasks that require a fundamental understanding of human
language. Given that these embeddings themselves often reflect or exhibit bias,
it stands to reason that these models may also inadvertently learn this bias.
In this work, we build on the seminal previous work and propose DeepSoftDebias,
an algorithm that uses a neural network to perform 'soft debiasing'. We
exhaustively evaluate this algorithm across a variety of SOTA datasets,
accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias
outperforms the current state-of-the-art methods at reducing bias across
gender, race, and religion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SignLLM: Sign Language Production Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Fang, Lei Wang, Ce Zheng, Chunyu Sui, Mingyu Zhao, Yapeng Tian, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose SignLLM, a multilingual Sign Language Production
(SLP) large language model, which includes two novel multilingual SLP modes
MLSF and Prompt2LangGloss that allow sign language gestures generation from
query texts input and question-style prompts input respectively. Both modes can
use a new RL loss based on reinforcement learning and a new RL module named
Priority Learning Channel. These RL components can accelerate the training by
enhancing the model's capability to sample high-quality data. For SignLLM's
training, we introduce Prompt2Sign, a comprehensive multilingual sign language
dataset, which builds from public data, including American Sign Language (ASL)
and seven others. This dataset standardizes information by extracting pose
information from sign language videos into a unified compressed format. We
extensively evaluate SignLLM, demonstrating that our model achieves
state-of-the-art performance on SLP tasks across eight sign languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>website at https://signllm.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sabiá-3 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Abonizio, Thales Sales Almeida, Thiago Laitz, Roseval Malaquias Junior, Giovana Kerche Bonás, Rodrigo Nogueira, Ramon Pires
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report presents Sabi\'a-3, our new flagship language model, and
Sabiazinho-3, a more cost-effective sibling. The models were trained on a large
brazilian-centric corpus. Evaluations across diverse professional and academic
benchmarks show a strong performance on Portuguese and Brazil-related tasks.
Sabi\'a-3 shows large improvements in comparison to our previous best of model,
Sabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi\'a-3's
average performance matches frontier LLMs, while it is offered at a three to
four times lower cost per token, reinforcing the benefits of domain
specialization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-label Sequential Sentence Classification via Large Language Model <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengfei Lan, Lecheng Zheng, Shufan Ming, Halil Kilicoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential sentence classification (SSC) in scientific publications is
crucial for supporting downstream tasks such as fine-grained information
retrieval and extractive summarization. However, current SSC methods are
constrained by model size, sequence length, and single-label setting. To
address these limitations, this paper proposes LLM-SSC, a large language model
(LLM)-based framework for both single- and multi-label SSC tasks. Unlike
previous approaches that employ small- or medium-sized language models, the
proposed framework utilizes LLMs to generate SSC labels through designed
prompts, which enhance task understanding by incorporating demonstrations and a
query to describe the prediction target. We also present a multi-label
contrastive learning loss with auto-weighting scheme, enabling the multi-label
classification task. To support our multi-label SSC analysis, we introduce and
release a new dataset, biorc800, which mainly contains unstructured abstracts
in the biomedical domain with manual annotations. Experiments demonstrate
LLM-SSC's strong performance in SSC under both in-context learning and
task-specific tuning settings. We release biorc800 and our code at:
https://github.com/ScienceNLP-Lab/LLM-SSC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recent Advances of Foundation Language Models-based Continual Learning:
  A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Yang, Jie Zhou, Xuanwen Ding, Tianyu Huai, Shunyu Liu, Qin Chen, Yuan Xie, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, foundation language models (LMs) have marked significant
achievements in the domains of natural language processing (NLP) and computer
vision (CV). Unlike traditional neural network models, foundation LMs obtain a
great ability for transfer learning by acquiring rich commonsense knowledge
through pre-training on extensive unsupervised datasets with a vast number of
parameters. However, they still can not emulate human-like continuous learning
due to catastrophic forgetting. Consequently, various continual learning
(CL)-based methodologies have been developed to refine LMs, enabling them to
adapt to new tasks without forgetting previous knowledge. However, a systematic
taxonomy of existing approaches and a comparison of their performance are still
lacking, which is the gap that our survey aims to fill. We delve into a
comprehensive review, summarization, and classification of the existing
literature on CL-based approaches applied to foundation language models, such
as pre-trained language models (PLMs), large language models (LLMs) and
vision-language models (VLMs). We divide these studies into offline CL and
online CL, which consist of traditional methods, parameter-efficient-based
methods, instruction tuning-based methods and continual pre-training methods.
Offline CL encompasses domain-incremental learning, task-incremental learning,
and class-incremental learning, while online CL is subdivided into hard task
boundary and blurry task boundary settings. Additionally, we outline the
typical datasets and metrics employed in CL research and provide a detailed
analysis of the challenges and future work for LMs-based continual learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Computing Survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think Beyond Size: Adaptive <span class="highlight-title">Prompt</span>ing for More Effective Reasoning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamesh R
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained large language models (LLMs) are increasingly utilized across a
wide range of natural language processing (NLP) tasks due to their impressive
capabilities as few-shot learners. Recent techniques, such as chain-of-thought
(CoT) prompting, have significantly advanced multi-step reasoning by
introducing step-by-step decomposition, achieving state-of-the-art results on
complex reasoning benchmarks. However, these approaches often rely on static
prompting templates that do not adapt to task complexity or errors during the
reasoning process. In this work, we introduce Adaptive Prompting, a dynamic and
iterative framework designed to enhance reasoning by incorporating real-time
adjustments to prompt structures and validation mechanisms.Experimental results
demonstrate that Adaptive Prompting significantly improves performance on
diverse reasoning benchmarks, including arithmetic reasoning (GSM8K,
MultiArith), logical reasoning and commonsense tasks, achieving substantial
accuracy gains compared to static prompting baselines. By integrating guided
prompts, intermediate validation, and self-corrective steps, our approach
enables smaller models to achieve competitive performance with larger
counterparts, such as GPT-4, while maintaining computational efficiency. The
framework achieves this without requiring fine-tuning or task-specific training
data, highlighting the untapped potential of iterative reasoning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICLR 2025. This is a preprint version. Future revisions
  will include additional evaluations and refinements</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13549v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13549v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has
been a new rising research hotspot, which uses powerful Large Language Models
(LLMs) as a brain to perform multimodal tasks. The surprising emergent
capabilities of MLLM, such as writing stories based on images and OCR-free math
reasoning, are rare in traditional multimodal methods, suggesting a potential
path to artificial general intelligence. To this end, both academia and
industry have endeavored to develop MLLMs that can compete with or even better
than GPT-4V, pushing the limit of research at a surprising speed. In this
paper, we aim to trace and summarize the recent progress of MLLMs. First of
all, we present the basic formulation of MLLM and delineate its related
concepts, including architecture, training strategy and data, as well as
evaluation. Then, we introduce research topics about how MLLMs can be extended
to support more granularity, modalities, languages, and scenarios. We continue
with multimodal hallucination and extended techniques, including Multimodal ICL
(M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To
conclude the paper, we discuss existing challenges and point out promising
research directions. In light of the fact that the era of MLLM has only just
begun, we will keep updating this survey and hope it can inspire more research.
An associated GitHub link collecting the latest papers is available at
https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in National Science Review. Project
  page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cherry on Top: Parameter Heterogeneity and Quantization in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanyun Cui, Qianle Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reveals the phenomenon of parameter heterogeneity in large
language models (LLMs). We find that a small subset of "cherry" parameters
exhibit a disproportionately large influence on model performance, while the
vast majority of parameters have minimal impact. This heterogeneity is found to
be prevalent across different model families, scales, and types. Motivated by
this observation, we propose CherryQ, a novel quantization method that unifies
the optimization of mixed-precision parameters. CherryQ identifies and
preserves the critical cherry parameters in high precision while aggressively
quantizing the remaining parameters to low precision. Extensive experiments
demonstrate the effectiveness of CherryQ. CherryQ outperforms existing
quantization approaches in terms of perplexity and downstream task performance.
Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance
compared to their 16-bit counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Singhal, Kaustubh Ponkshe, Praneeth Vepakomma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) is a popular technique for efficient fine-tuning
of foundation models. However, applying LoRA in federated learning
environments, where data is distributed across multiple clients, presents
unique challenges. Existing methods rely on traditional federated averaging of
LoRA adapters, resulting in inexact updates. To address this, we propose
Federated Exact LoRA, or FedExLoRA, which adds a residual error term to the
pretrained frozen weight matrix. Our approach achieves exact updates with
minimal computational and communication overhead, preserving LoRA's efficiency.
We evaluate the method on various models across arithmetic reasoning,
commonsense reasoning, natural language understanding and natural language
generation tasks, showing consistent performance gains over state-of-the-art
methods across multiple settings. Through extensive analysis, we quantify that
the deviations in updates from the ideal solution are significant, highlighting
the need for exact aggregation. Our method's simplicity, efficiency, and broad
applicability position it as a promising solution for accurate and effective
federated fine-tuning of foundation models. Our code is publicly available at
https://github.com/RaghavSinghal10/fedex-lora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Raghav Singhal and Kaustubh Ponkshe contributed equally to this work.
  Another version of the paper accepted at NeurIPS 2024 Workshop on Fine-Tuning
  in Modern Machine Learning: Principles and Scalability</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Data Model Robustness of Text-to-SQL Systems Based on
  Real User Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08349v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08349v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Fürst, Catherine Kosten, Farhad Nooralahzadeh, Yi Zhang, Kurt Stockinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL systems (also known as NL-to-SQL systems) have become an
increasingly popular solution for bridging the gap between user capabilities
and SQL-based data access. These systems translate user requests in natural
language to valid SQL statements for a specific database. Recent Text-to-SQL
systems have benefited from the rapid improvement of transformer-based language
models. However, while Text-to-SQL systems that incorporate such models
continuously reach new high scores on -- often synthetic -- benchmark datasets,
a systematic exploration of their robustness towards different data models in a
real-world, realistic scenario is notably missing. This paper provides the
first in-depth evaluation of the data model robustness of Text-to-SQL systems
in practice based on a multi-year international project focused on Text-to-SQL
interfaces. Our evaluation is based on a real-world deployment of FootballDB, a
system that was deployed over a 9 month period in the context of the FIFA World
Cup 2022, during which about 6K natural language questions were asked and
executed. All of our data is based on real user questions that were asked live
to the system. We manually labeled and translated a subset of these questions
for three different data models. For each data model, we explore the
performance of representative Text-to-SQL systems and language models. We
further quantify the impact of training data size, pre-, and post-processing
steps as well as language model inference time. Our comprehensive evaluation
sheds light on the design choices of real-world Text-to-SQL systems and their
impact on moving from research prototypes to real deployments. Last, we provide
a new benchmark dataset to the community, which is the first to enable the
evaluation of different data models for the same dataset and is substantially
more challenging than most previous datasets in terms of query complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneBit: Towards Extremely Low-bit Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11295v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11295v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model quantification uses low bit-width values to represent the weight
matrices of existing models to be quantized, which is a promising approach to
reduce both storage and computational overheads of deploying highly anticipated
LLMs. However, current quantization methods suffer severe performance
degradation when the bit-width is extremely reduced, and thus focus on
utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes
the weight matrices of LLMs to 1-bit, paving the way for the extremely low
bit-width deployment of LLMs. For this target, we introduce a 1-bit model
compressing framework named OneBit, including a novel 1-bit parameter
representation method to better quantize LLMs as well as an effective parameter
initialization method based on matrix decomposition to improve the convergence
speed of the quantization framework. Sufficient experimental results indicate
that OneBit achieves good performance (at least 81% of the non-quantized
performance on LLaMA models) with robust training processes when only using
1-bit weight matrices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech Translation with Speech Foundation Models and Large Language
  Models: What is There and What is Missing? <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12025v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12025v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Gaido, Sara Papi, Matteo Negri, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of natural language processing (NLP) has recently witnessed a
transformative shift with the emergence of foundation models, particularly
Large Language Models (LLMs) that have revolutionized text-based NLP. This
paradigm has extended to other modalities, including speech, where researchers
are actively exploring the combination of Speech Foundation Models (SFMs) and
LLMs into single, unified models capable of addressing multimodal tasks. Among
such tasks, this paper focuses on speech-to-text translation (ST). By examining
the published papers on the topic, we propose a unified view of the
architectural solutions and training strategies presented so far, highlighting
similarities and differences among them. Based on this examination, we not only
organize the lessons learned but also show how diverse settings and evaluation
approaches hinder the identification of the best-performing solution for each
architectural building block and training choice. Lastly, we outline
recommendations for future works on the topic aimed at better understanding the
strengths and weaknesses of the SFM+LLM solutions for ST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Outstanding paper at the ACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring syntactic information in sentence embeddings through
  multilingual subject-verb agreement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivi Nastase, Chunyang Jiang, Giuseppe Samo, Paola Merlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, our goal is to investigate to what degree multilingual
pretrained language models capture cross-linguistically valid abstract
linguistic representations. We take the approach of developing curated
synthetic data on a large scale, with specific properties, and using them to
study sentence representations built using pretrained language models. We use a
new multiple-choice task and datasets, Blackbird Language Matrices (BLMs), to
focus on a specific grammatical structural phenomenon -- subject-verb agreement
across a variety of sentence structures -- in several languages. Finding a
solution to this task requires a system detecting complex linguistic patterns
and paradigms in text representations. Using a two-level architecture that
solves the problem in two steps -- detect syntactic objects and their
properties in individual sentences, and find patterns across an input sequence
of sentences -- we show that despite having been trained on multilingual texts
in a consistent manner, multilingual pretrained language models have
language-specific differences, and syntactic structure is not shared, even
across closely related languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 tables, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Evaluating Generalist Agents: An Automated Benchmark in Open
  World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08367v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08367v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Zheng, Haowei Lin, Kaichen He, Zihao Wang, Zilong Zheng, Yitao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating generalist agents presents significant challenges due to their
wide-ranging abilities and the limitations of current benchmarks in assessing
true generalization. We introduce the Minecraft Universe (MCU), a fully
automated benchmarking framework set within the open-world game Minecraft. MCU
dynamically generates and evaluates a broad spectrum of tasks, offering three
core components: 1) a task generation mechanism that provides high degrees of
freedom and variability, 2) an ever-expanding set of over 3K composable atomic
tasks, and 3) a general evaluation framework that supports open-ended task
assessment. By integrating large language models (LLMs), MCU dynamically
creates diverse environments for each evaluation, fostering agent
generalization. The framework uses a vision-language model (VLM) to
automatically generate evaluation criteria, achieving over 90% agreement with
human ratings across multi-dimensional assessments, which demonstrates that MCU
is a scalable and explainable solution for evaluating generalist agents.
Additionally, we show that while state-of-the-art foundational models perform
well on specific tasks, they often struggle with increased task diversity and
difficulty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Italian sentence embeddings properties through multi-tasking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivi Nastase, Giuseppe Samo, Chunyang Jiang, Paola Merlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate to what degree existing LLMs encode abstract linguistic
information in Italian in a multi-task setting. We exploit curated synthetic
data on a large scale -- several Blackbird Language Matrices (BLMs) problems in
Italian -- and use them to study how sentence representations built using
pre-trained language models encode specific syntactic and semantic information.
We use a two-level architecture to model separately a compression of the
sentence embeddings into a representation that contains relevant information
for a task, and a BLM task. We then investigate whether we can obtain
compressed sentence representations that encode syntactic and semantic
information relevant to several BLM tasks. While we expected that the sentence
structure -- in terms of sequence of phrases/chunks -- and chunk properties
could be shared across tasks, performance and error analysis show that the
clues for the different tasks are encoded in different manners in the sentence
embeddings, suggesting that abstract linguistic notions such as constituents or
thematic roles does not seem to be present in the pretrained sentence
embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Power of Data Tsunami: A Comprehensive <span class="highlight-title">Survey</span> on Data
  Assessment and Selection for Instruction Tuning of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02085v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02085v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, Xing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning plays a critical role in aligning large language models
(LLMs) with human preference. Despite the vast amount of open instruction
datasets, naively training a LLM on all existing instructions may not be
optimal and practical. To pinpoint the most beneficial datapoints, data
assessment and selection methods have been proposed in the fields of natural
language processing (NLP) and deep learning. However, under the context of
instruction tuning, there still exists a gap in knowledge on what kind of data
evaluation metrics can be employed and how they can be integrated into the
selection mechanism. To bridge this gap, we present a comprehensive review on
existing literature of data assessment and selection especially for instruction
tuning of LLMs. We systematically categorize all applicable methods into
quality-based, diversity-based, and importance-based ones where a unified,
fine-grained taxonomy is structured. For each category, representative methods
are elaborated to describe the landscape of relevant research. In addition,
comparison between the latest methods is conducted on their officially reported
results to provide in-depth discussions on their limitations. Finally, we
summarize the open challenges and propose the promosing avenues for future
studies. All related contents are available at
https://github.com/yuleiqin/fantastic-data-engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>review, survey, 37 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MH-MoE: Multi-Head Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaohan Huang, Xun Wu, Shuming Ma, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by
using the multi-head mechanism to collectively attend to information from
various representation spaces within different experts. In this paper, we
present a novel implementation of MH-MoE that maintains both FLOPs and
parameter parity with sparse Mixture of Experts models. Experimental results on
language models show that the new implementation yields quality improvements
over both vanilla MoE and fine-grained MoE models. Additionally, our
experiments demonstrate that MH-MoE is compatible with 1-bit Large Language
Models (LLMs) such as BitNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 0 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAM Decoding: Speculative Decoding via Suffix Automaton 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Hu, Ke Wang, Xiaokang Zhang, Fanjin Zhang, Cuiping Li, Hong Chen, Jing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized natural language processing
by unifying tasks into text generation, yet their large parameter sizes and
autoregressive nature limit inference speed. SAM-Decoding addresses this by
introducing a novel retrieval-based speculative decoding method that uses a
suffix automaton for efficient and accurate draft generation. Unlike n-gram
matching used by the existing method, SAM-Decoding finds the longest suffix
match in generating text and text corpuss, achieving an average time complexity
of $O(1)$ per generation step. SAM-Decoding constructs static and dynamic
suffix automatons for the text corpus and input prompts, respectively, enabling
fast and precise draft generation. Meanwhile, it is designed as an approach
that can be combined with existing methods, allowing SAM-Decoding to adaptively
select a draft generation strategy based on the matching length, thus
increasing the inference speed of the LLM. When combined with Token Recycling,
evaluations show SAM-Decoding outperforms existing model-free methods,
achieving a speedup of $2.27\times$ over autoregressive decoding on Spec-Bench.
When combined with EAGLE2, it reaches a speedup of $2.49\times$, surpassing all
current approaches. Our code is available at
https://github.com/hyx1999/SAM-Decoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Chat<span class="highlight-title">GPT</span> for Diagnosing Autism-Associated Language Disorders
  and Identifying Distinct Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanbo Hu, Wenqi Li, Mindi Ruan, Xiangxu Yu, Shalaka Deshpande, Lynn K. Paul, Shuo Wang, Xin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diagnosing language disorders associated with autism is a complex challenge,
often hampered by the subjective nature and variability of traditional
assessment methods. Traditional diagnostic methods not only require intensive
human effort but also often result in delayed interventions due to their lack
of speed and precision. In this study, we explored the application of ChatGPT,
a large language model, to overcome these obstacles by enhancing sensitivity
and profiling linguistic features for autism diagnosis. This research utilizes
ChatGPT natural language processing capabilities to simplify and improve the
diagnostic process, focusing on identifying autism related language patterns.
Specifically, we compared ChatGPT performance with that of conventional
supervised learning models, including BERT, a model acclaimed for its
effectiveness in various natural language processing tasks. We showed that
ChatGPT substantially outperformed these models, achieving over 10% improvement
in both sensitivity and positive predictive value, in a zero shot learning
configuration. The findings underscore the model potential as a diagnostic
tool, combining accuracy and applicability. We identified ten key features of
autism associated language disorders across scenarios. Features such as
echolalia, pronoun reversal, and atypical language usage play a critical role
in diagnosing ASD and informing tailored treatment plans. Together, our
findings advocate for adopting sophisticated AI tools like ChatGPT in clinical
settings to assess and diagnose developmental disorders. Our approach promises
enhanced diagnostic precision and supports personalized medicine, potentially
transforming the evaluation landscape for autism and similar neurological
conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ METEOR: Evolutionary Journey of Large Language Models from Guidance to
  Self-Growth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Li, Xiaoang Xu, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model evolution enables learning from feedback to refine experiences and
update skills, transforming models from having no domain knowledge to becoming
domain experts. However, there is currently no unified and effective method for
guiding this evolutionary process. To address this gap, we propose the Meteor
method, which includes three training phases: weak-to-strong data distillation,
iterative training, and self-evolution strategies. Each phase maximizes the
model's inherent domain capabilities, allowing it to autonomously refine its
domain knowledge and enhance performance. Experiments demonstrate that our
approach significantly improves accuracy, completeness, relevance, coherence,
and reliability across domain-specific tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code can be found at https://github.com/DIRECT-BIT/METEOR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Universal Approximation Theory: The Basic Theory for
  <span class="highlight-title">Transformer</span>-based Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00958v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00958v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wang, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have emerged as a critical area of focus in artificial
intelligence, particularly with the introduction of groundbreaking innovations
like ChatGPT. Large-scale Transformer networks have quickly become the leading
approach for advancing natural language processing algorithms. Built on the
Transformer architecture, these models enable interactions that closely mimic
human communication and, equipped with extensive knowledge, can even assist in
guiding human tasks. Despite their impressive capabilities and growing
complexity, a key question remains-the theoretical foundations of large
language models (LLMs). What makes Transformer so effective for powering
intelligent language applications, such as translation and coding? What
underlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme
enhance the fine-tuning of LLMs? And what supports the practicality of pruning
LLMs? To address these critical questions and explore the technological
strategies within LLMs, we leverage the Universal Approximation Theory (UAT) to
offer a theoretical backdrop, shedding light on the mechanisms that underpin
these advancements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Framework for Role-playing: Generation and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00627v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00627v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Liu, Zhengwei Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit impressive proficiency in natural
language generation, understanding user instructions, and emulating human-like
language use, which has led to significant interest in their application to
role-playing scenarios. However, the manual collection of role-specific script
data and the evaluation of model performance are resource-intensive processes.
This project introduces a prompt-based framework designed to leverage GPT's
capabilities for the generation of role-playing dialogue datasets and the
evaluation of role-playing performance. To validate the effectiveness of the
GPT-based generation and evaluation, we further incorporate the recall-oriented
Rouge-L metric, providing an additional quantitative measure of performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IndicLLMSuite: A Blueprint for Creating <span class="highlight-title">Pre-train</span>ing and Fine-Tuning
  <span class="highlight-title">Dataset</span>s for Indian Languages <span class="chip">ACL-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Safi Ur Rahman Khan, Priyam Mehta, Ananth Sankar, Umashankar Kumaravelan, Sumanth Doddapaneni, Suriyaprasaad B, Varun Balan G, Sparsh Jain, Anoop Kunchukuttan, Pratyush Kumar, Raj Dabre, Mitesh M. Khapra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable advancements in English LLMs, the progress in
building comparable models for other languages has been hindered due to the
scarcity of tailored resources. Our work aims to bridge this divide by
introducing an expansive suite of resources specifically designed for the
development of Indic LLMs, covering 22 languages, containing a total of 251B
tokens and 74.8M instruction-response pairs. Recognizing the importance of both
data quality and quantity, our approach combines highly curated manually
verified data, unverified yet valuable data, and synthetic data. We build a
clean, open-source pipeline for curating pre-training data from diverse
sources, including websites, PDFs, and videos, incorporating best practices for
crawling, cleaning, flagging, and deduplication. For instruction-fine tuning,
we amalgamate existing Indic datasets, translate/transliterate English datasets
into Indian languages, and utilize LLaMa2 and Mixtral models to create
conversations grounded in articles from Indian Wikipedia and Wikihow.
Additionally, we address toxicity alignment by generating toxic prompts for
multiple scenarios and then generate non-toxic responses by feeding these toxic
prompts to an aligned LLaMa2 model. We hope that the datasets, tools, and
resources released as a part of this work will not only propel the research and
development of Indic LLMs but also establish an open-source blueprint for
extending such efforts to other languages. The data and other artifacts created
as part of this work are released with permissive licenses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL-2024 Outstanding Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model
  with Frozen LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00774v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00774v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, Long Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapidly developing large language models (LLMs) have brought tremendous
intelligent applications. Especially, the GPT-4o's excellent duplex speech
interaction ability has brought impressive experience to users. Researchers
have recently proposed several multi-modal LLMs in this direction that can
achieve user-agent speech-to-speech conversations. This paper proposes a novel
speech-text multimodal LLM architecture called Freeze-Omni. Our main
contribution is that the speech input and output modalities can be easily
connected to a textual LLM while keeping the LLM's parameters frozen throughout
the training process. We design a three-stage training strategy for modeling
both the speech input and output, enabling Freeze-Omni to obtain
speech-to-speech conversation ability using text-speech paired data (such as
ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.
Moreover, we can effectively ensure that the intelligence of the Freeze-Omni in
the speech modality is at the same level compared with that in the text
modality of its backbone LLM, while achieving low latency end-to-end spoken
response. In addition, we also designed a method to achieve duplex dialogue
ability through multi-task training, giving Freeze-Omni a more natural style of
dialogue ability between users and agents. In summary, Freeze-Omni holds great
potential to conduct speech-to-speech dialogue based on a multimodal LLM under
the condition of a frozen LLM, avoiding the catastrophic forgetting problem
caused by limited data and training resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://freeze-omni.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Liu, Wenxuan Wang, Yihang Su, Jingyuan Huan, Wenting Chen, Yudi Zhang, Cheng-Yi Li, Kao-Jung Chang, Xiaohan Xin, Linlin Shen, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The significant breakthroughs of Medical Multi-Modal Large Language Models
(Med-MLLMs) renovate modern healthcare with robust information synthesis and
medical decision support. However, these models are often evaluated on
benchmarks that are unsuitable for the Med-MLLMs due to the complexity of
real-world diagnostics across diverse specialties. To address this gap, we
introduce Asclepius, a novel Med-MLLM benchmark that comprehensively assesses
Med-MLLMs in terms of: distinct medical specialties (cardiovascular,
gastroenterology, etc.) and different diagnostic capacities (perception,
disease analysis, etc.). Grounded in 3 proposed core principles, Asclepius
ensures a comprehensive evaluation by encompassing 15 medical specialties,
stratifying into 3 main categories and 8 sub-categories of clinical tasks, and
exempting overlap with existing VQA dataset. We further provide an in-depth
analysis of 6 Med-MLLMs and compare them with 3 human specialists, providing
insights into their competencies and limitations in various medical contexts.
Our work not only advances the understanding of Med-MLLMs' capabilities but
also sets a precedent for future evaluations and the safe deployment of these
models in clinical environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Speaking Assessment of Conversation Tests with Novel
  Graph-based Modeling on Spoken Response Coherence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07064v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07064v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiun-Ting Li, Bi-Cheng Yan, Tien-Hong Lo, Yi-Cheng Wang, Yung-Chang Hsu, Berlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated speaking assessment in conversation tests (ASAC) aims to evaluate
the overall speaking proficiency of an L2 (second-language) speaker in a
setting where an interlocutor interacts with one or more candidates. Although
prior ASAC approaches have shown promising performance on their respective
datasets, there is still a dearth of research specifically focused on
incorporating the coherence of the logical flow within a conversation into the
grading model. To address this critical challenge, we propose a hierarchical
graph model that aptly incorporates both broad inter-response interactions
(e.g., discourse relations) and nuanced semantic information (e.g., semantic
words and speaker intents), which is subsequently fused with contextual
information for the final prediction. Extensive experimental results on the
NICT-JLE benchmark dataset suggest that our proposed modeling approach can
yield considerable improvements in prediction accuracy with respect to various
assessment metrics, as compared to some strong baselines. This also sheds light
on the importance of investigating coherence-related facets of spoken responses
in ASAC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conversational Complexity for Assessing Risk in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01247v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01247v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Burden, Manuel Cebrian, Jose Hernandez-Orallo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) present a dual-use dilemma: they enable
beneficial applications while harboring potential for harm, particularly
through conversational interactions. Despite various safeguards, advanced LLMs
remain vulnerable. A watershed case in early 2023 involved journalist Kevin
Roose's extended dialogue with Bing, an LLM-powered search engine, which
revealed harmful outputs after probing questions, highlighting vulnerabilities
in the model's safeguards. This contrasts with simpler early jailbreaks, like
the "Grandma Jailbreak," where users framed requests as innocent help for a
grandmother, easily eliciting similar content. This raises the question: How
much conversational effort is needed to elicit harmful information from LLMs?
We propose two measures to quantify this effort: Conversational Length (CL),
which measures the number of conversational turns needed to obtain a specific
harmful response, and Conversational Complexity (CC), defined as the Kolmogorov
complexity of the user's instruction sequence leading to the harmful response.
To address the incomputability of Kolmogorov complexity, we approximate CC
using a reference LLM to estimate the compressibility of the user instructions.
Applying this approach to a large red-teaming dataset, we perform a
quantitative analysis examining the statistical distribution of harmful and
harmless conversational lengths and complexities. Our empirical findings
suggest that this distributional analysis and the minimization of CC serve as
valuable tools for understanding AI safety, offering insights into the
accessibility of harmful information. This work establishes a foundation for a
new perspective on LLM safety, centered around the algorithmic complexity of
pathways to harm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequential Large Language Model-Based Hyper-Parameter Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanan Mahammadli, Seyda Bolelli Ertekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces SLLMBO, an innovative framework that leverages Large
Language Models (LLMs) for hyperparameter optimization (HPO), incorporating
dynamic search space adaptability, enhanced parameter landscape exploitation,
and a hybrid, novel LLM-Tree-structured Parzen Estimator (LLM-TPE) sampler. By
addressing limitations in recent fully LLM-based methods and traditional
Bayesian Optimization (BO), SLLMBO achieves more robust optimization. This
comprehensive benchmarking evaluates multiple LLMs, including GPT-3.5-turbo,
GPT-4o, Claude-Sonnet-3.5, and Gemini-1.5-flash, extending prior work beyond
GPT-3.5 and GPT-4 and establishing SLLMBO as the first framework to benchmark a
diverse set of LLMs for HPO. By integrating LLMs' established strengths in
parameter initialization with the exploitation abilities demonstrated in this
study, alongside TPE's exploration capabilities, the LLM-TPE sampler achieves a
balanced exploration-exploitation trade-off, reduces API costs, and mitigates
premature early stoppings for more effective parameter searches. Across 14
tabular tasks in classification and regression, the LLM-TPE sampler
outperformed fully LLM-based methods and achieved superior results over BO
methods in 9 tasks. Testing early stopping in budget-constrained scenarios
further demonstrated competitive performance, indicating that LLM-based methods
generally benefit from extended iterations for optimal results. This work lays
the foundation for future research exploring open-source LLMs, reproducibility
of LLM results in HPO, and benchmarking SLLMBO on complex datasets, such as
image classification, segmentation, and machine translation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlphaTablets: A Generic Plane Representation for 3D Planar
  Reconstruction from Monocular Videos <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuze He, Wang Zhao, Shaohui Liu, Yubin Hu, Yushi Bai, Yu-Hui Wen, Yong-Jin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AlphaTablets, a novel and generic representation of 3D planes
that features continuous 3D surface and precise boundary delineation. By
representing 3D planes as rectangles with alpha channels, AlphaTablets combine
the advantages of current 2D and 3D plane representations, enabling accurate,
consistent and flexible modeling of 3D planes. We derive differentiable
rasterization on top of AlphaTablets to efficiently render 3D planes into
images, and propose a novel bottom-up pipeline for 3D planar reconstruction
from monocular videos. Starting with 2D superpixels and geometric cues from
pre-trained models, we initialize 3D planes as AlphaTablets and optimize them
via differentiable rendering. An effective merging scheme is introduced to
facilitate the growth and refinement of AlphaTablets. Through iterative
optimization and merging, we reconstruct complete and accurate 3D planes with
solid surfaces and clear boundaries. Extensive experiments on the ScanNet
dataset demonstrate state-of-the-art performance in 3D planar reconstruction,
underscoring the great potential of AlphaTablets as a generic 3D plane
representation for various applications. Project page is available at:
https://hyzcluster.github.io/alphatablets
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FaVoR: Features via Voxel Rendering for Camera Relocalization <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincenzo Polizzi, Marco Cannici, Davide Scaramuzza, Jonathan Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera relocalization methods range from dense image alignment to direct
camera pose regression from a query image. Among these, sparse feature matching
stands out as an efficient, versatile, and generally lightweight approach with
numerous applications. However, feature-based methods often struggle with
significant viewpoint and appearance changes, leading to matching failures and
inaccurate pose estimates. To overcome this limitation, we propose a novel
approach that leverages a globally sparse yet locally dense 3D representation
of 2D features. By tracking and triangulating landmarks over a sequence of
frames, we construct a sparse voxel map optimized to render image patch
descriptors observed during tracking. Given an initial pose estimate, we first
synthesize descriptors from the voxels using volumetric rendering and then
perform feature matching to estimate the camera pose. This methodology enables
the generation of descriptors for unseen views, enhancing robustness to view
changes. We extensively evaluate our method on the 7-Scenes and Cambridge
Landmarks datasets. Our results show that our method significantly outperforms
existing state-of-the-art feature representation techniques in indoor
environments, achieving up to a 39% improvement in median translation error.
Additionally, our approach yields comparable results to other methods for
outdoor scenarios while maintaining lower memory and computational costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), Tucson, Arizona, US, Feb 28-Mar 4, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEDMamba: Enhancing Selective State Space Modelling with Bottleneck
  Mechanism and Fine-to-Coarse Temporal Fusion for Efficient Error Detection in
  Robot-Assisted Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15920v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15920v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialang Xu, Nazir Sirajudeen, Matthew Boal, Nader Francis, Danail Stoyanov, Evangelos Mazomenos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated detection of surgical errors can improve robotic-assisted surgery.
Despite promising progress, existing methods still face challenges in capturing
rich temporal context to establish long-term dependencies while maintaining
computational efficiency. In this paper, we propose a novel hierarchical model
named SEDMamba, which incorporates the selective state space model (SSM) into
surgical error detection, facilitating efficient long sequence modelling with
linear complexity. SEDMamba enhances selective SSM with a bottleneck mechanism
and fine-to-coarse temporal fusion (FCTF) to detect and temporally localize
surgical errors in long videos. The bottleneck mechanism compresses and
restores features within their spatial dimension, thereby reducing
computational complexity. FCTF utilizes multiple dilated 1D convolutional
layers to merge temporal information across diverse scale ranges, accommodating
errors of varying duration. Our work also contributes the first-of-its-kind,
frame-level, in-vivo surgical error dataset to support error detection in real
surgical cases. Specifically, we deploy the clinically validated observational
clinical human reliability assessment tool (OCHRA) to annotate the errors
during suturing tasks in an open-source radical prostatectomy dataset
(SAR-RARP50). Experimental results demonstrate that our SEDMamba outperforms
state-of-the-art methods with at least 1.82% AUC and 3.80% AP performance gains
with significantly reduced computational complexity. The corresponding error
annotations, code and models are released at
https://github.com/wzjialang/SEDMamba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE RA-L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuro-Symbolic Evaluation of Text-to-Video Models using Formal
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. P. Sharan, Minkyu Choi, Sahil Shah, Harsh Goel, Mohammad Omama, Sandeep Chinchali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-video models such as Sora, Gen-3, MovieGen,
and CogVideoX are pushing the boundaries of synthetic video generation, with
adoption seen in fields like robotics, autonomous driving, and entertainment.
As these models become prevalent, various metrics and benchmarks have emerged
to evaluate the quality of the generated videos. However, these metrics
emphasize visual quality and smoothness, neglecting temporal fidelity and
text-to-video alignment, which are crucial for safety-critical applications. To
address this gap, we introduce NeuS-V, a novel synthetic video evaluation
metric that rigorously assesses text-to-video alignment using neuro-symbolic
formal verification techniques. Our approach first converts the prompt into a
formally defined Temporal Logic (TL) specification and translates the generated
video into an automaton representation. Then, it evaluates the text-to-video
alignment by formally checking the video automaton against the TL
specification. Furthermore, we present a dataset of temporally extended prompts
to evaluate state-of-the-art video generation models against our benchmark. We
find that NeuS-V demonstrates a higher correlation by over 5x with human
evaluations when compared to existing metrics. Our evaluation further reveals
that current video generation models perform poorly on these temporally complex
prompts, highlighting the need for future work in improving text-to-video
generation capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SignLLM: Sign Language Production Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Fang, Lei Wang, Ce Zheng, Chunyu Sui, Mingyu Zhao, Yapeng Tian, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose SignLLM, a multilingual Sign Language Production
(SLP) large language model, which includes two novel multilingual SLP modes
MLSF and Prompt2LangGloss that allow sign language gestures generation from
query texts input and question-style prompts input respectively. Both modes can
use a new RL loss based on reinforcement learning and a new RL module named
Priority Learning Channel. These RL components can accelerate the training by
enhancing the model's capability to sample high-quality data. For SignLLM's
training, we introduce Prompt2Sign, a comprehensive multilingual sign language
dataset, which builds from public data, including American Sign Language (ASL)
and seven others. This dataset standardizes information by extracting pose
information from sign language videos into a unified compressed format. We
extensively evaluate SignLLM, demonstrating that our model achieves
state-of-the-art performance on SLP tasks across eight sign languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>website at https://signllm.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Domain Recommendation Meets Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay Krishna Vajjala, Dipak Meher, Ziwei Zhu, David S. Rosenblum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-domain recommendation (CDR) has emerged as a promising solution to the
cold-start problem, faced by single-domain recommender systems. However,
existing CDR models rely on complex neural architectures, large datasets, and
significant computational resources, making them less effective in data-scarce
scenarios or when simplicity is crucial. In this work, we leverage the
reasoning capabilities of large language models (LLMs) and explore their
performance in the CDR domain across multiple domain pairs. We introduce two
novel prompt designs tailored for CDR and demonstrate that LLMs, when prompted
effectively, outperform state-of-the-art CDR baselines across various metrics
and domain combinations in the rating prediction and ranking tasks. This work
bridges the gap between LLMs and recommendation systems, showcasing their
potential as effective cross-domain recommenders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TakeLab Retriever: AI-Driven Search Engine for Articles from Croatian
  News Outlets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Dukić, Marin Petričević, Sven Ćurković, Jan Šnajder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TakeLab Retriever is an AI-driven search engine designed to discover,
collect, and semantically analyze news articles from Croatian news outlets. It
offers a unique perspective on the history and current landscape of Croatian
online news media, making it an essential tool for researchers seeking to
uncover trends, patterns, and correlations that general-purpose search engines
cannot provide. TakeLab retriever utilizes cutting-edge natural language
processing (NLP) methods, enabling users to sift through articles using named
entities, phrases, and topics through the web application. This technical
report is divided into two parts: the first explains how TakeLab Retriever is
utilized, while the second provides a detailed account of its design. In the
second part, we also address the software engineering challenges involved and
propose solutions for developing a microservice-based semantic search engine
capable of handling over ten million news articles published over the past two
decades.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Know Your RAG: <span class="highlight-title">Dataset</span> Taxonomy and Generation Strategies for Evaluating
  RAG Systems <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Teixeira de Lima, Shubham Gupta, Cesar Berrospi, Lokesh Mishra, Michele Dolfi, Peter Staar, Panagiotis Vagenas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) systems are a widespread application of
Large Language Models (LLMs) in the industry. While many tools exist empowering
developers to build their own systems, measuring their performance locally,
with datasets reflective of the system's use cases, is a technological
challenge. Solutions to this problem range from non-specific and cheap (most
public datasets) to specific and costly (generating data from local documents).
In this paper, we show that using public question and answer (Q&A) datasets to
assess retrieval performance can lead to non-optimal systems design, and that
common tools for RAG dataset generation can lead to unbalanced data. We propose
solutions to these issues based on the characterization of RAG datasets through
labels and through label-targeted data generation. Finally, we show that
fine-tuned small LLMs can efficiently generate Q&A datasets. We believe that
these observations are invaluable to the know-your-data step of RAG systems
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in the 31st International Conference on Computational
  Linguistics (COLING 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Review</span> of LLM-based Explanations in Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Said
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Large Language Models (LLMs), such as LLaMA and ChatGPT, has
opened new opportunities for enhancing recommender systems through improved
explainability. This paper provides a systematic literature review focused on
leveraging LLMs to generate explanations for recommendations -- a critical
aspect for fostering transparency and user trust. We conducted a comprehensive
search within the ACM Guide to Computing Literature, covering publications from
the launch of ChatGPT (November 2022) to the present (November 2024). Our
search yielded 232 articles, but after applying inclusion criteria, only six
were identified as directly addressing the use of LLMs in explaining
recommendations. This scarcity highlights that, despite the rise of LLMs, their
application in explainable recommender systems is still in an early stage. We
analyze these select studies to understand current methodologies, identify
challenges, and suggest directions for future research. Our findings underscore
the potential of LLMs improving explanations of recommender systems and
encourage the development of more transparent and user-centric recommendation
explanation solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Management for Automobile Failure Analysis Using Graph RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Ojima, Hiroki Sakaji, Tadashi Nakamura, Hiroaki Sakata, Kazuya Seki, Yuu Teshigawara, Masami Yamashita, Kazuhiro Aoyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a knowledge management system for automobile failure
analysis using retrieval-augmented generation (RAG) with large language models
(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a
growing demand for knowledge transfer of failure analysis from experienced
engineers to young engineers. However, failure events are phenomena that occur
in a chain reaction, making them difficult for beginners to analyze them. While
knowledge graphs, which can describe semantic relationships and structure
information is effective in representing failure events, due to their
capability of representing the relationships between components, there is much
information in KGs, so it is challenging for young engineers to extract and
understand sub-graphs from the KG. On the other hand, there is increasing
interest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for
knowledge management. However, when using the current Graph RAG framework with
an existing knowledge graph for automobile failures, several issues arise
because it is difficult to generate executable queries for a knowledge graph
database which is not constructed by LLMs. To address this, we focused on
optimizing the Graph RAG pipeline for existing knowledge graphs. Using an
original Q&A dataset, the ROUGE F1 score of the sentences generated by the
proposed method showed an average improvement of 157.6% compared to the current
method. This highlights the effectiveness of the proposed method for automobile
failure analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, to be published in 2024 IEEE International
  Conference on Bid Data (BigData)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContextGNN: Beyond Two-Tower Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Yuan, Zecheng Zhang, Xinwei He, Akihiro Nitta, Weihua Hu, Dong Wang, Manan Shah, Shenyang Huang, Blaž Stojanovič, Alan Krumholz, Jan Eric Lenssen, Jure Leskovec, Matthias Fey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems predominantly utilize two-tower architectures, which
evaluate user-item rankings through the inner product of their respective
embeddings. However, one key limitation of two-tower models is that they learn
a pair-agnostic representation of users and items. In contrast, pair-wise
representations either scale poorly due to their quadratic complexity or are
too restrictive on the candidate pairs to rank. To address these issues, we
introduce Context-based Graph Neural Networks (ContextGNNs), a novel deep
learning architecture for link prediction in recommendation systems. The method
employs a pair-wise representation technique for familiar items situated within
a user's local subgraph, while leveraging two-tower representations to
facilitate the recommendation of exploratory items. A final network then
predicts how to fuse both pair-wise and two-tower recommendations into a single
ranking of items. We demonstrate that ContextGNN is able to adapt to different
data characteristics and outperforms existing methods, both traditional and
GNN-based, on a diverse set of practical recommendation tasks, improving
performance by 20% on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with
  Scalable Context and Symbolic Extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Qiu, You Peng, Guangxin He, Binhang Yuan, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has unlocked great opportunities
in complex data management tasks, particularly in question answering (QA) over
complicated multi-table relational data. Despite significant progress,
systematically evaluating LLMs on multi-table QA remains a critical challenge
due to the inherent complexity of analyzing heterogeneous table structures and
potential large scale of serialized relational data. Existing benchmarks
primarily focus on single-table QA, failing to capture the intricacies of
reasoning across multiple relational tables, as required in real-world domains
such as finance, healthcare, and e-commerce. To address this gap, we present
TQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities
of LLMs in tackling complex QA tasks over relational data. Our benchmark
incorporates diverse relational database instances sourced from real-world
public datasets and introduces a flexible sampling mechanism to create tasks
with varying multi-table context lengths, ranging from 8K to 64K tokens. To
ensure robustness and reliability, we integrate symbolic extensions into the
evaluation framework, enabling the assessment of LLM reasoning capabilities
beyond simple data retrieval or probabilistic pattern matching. We
systematically evaluate a range of LLMs, both open-source and closed-source,
spanning model scales from 7 billion to 70 billion parameters. Our extensive
experiments reveal critical insights into the performance of LLMs in
multi-table QA, highlighting both challenges and opportunities for advancing
their application in complex, data-driven environments. Our benchmark
implementation and results are available at
https://github.com/Relaxed-System-Lab/TQA-Bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Indexing Internet Search Augmented Generation for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangxin He, Zonghong Dai, Jiangcheng Zhu, Binqiang Zhao, Chenyue Li, You Peng, Chen Wang, Binhang Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation has emerged as an effective method to enhance
large language model performance. This approach typically relies on an internal
retrieval module that uses various indexing mechanisms to manage a static
pre-processed corpus. However, such a paradigm often falls short when it is
necessary to integrate the most up-to-date information that has not been
updated into the corpus during generative inference time. In this paper, we
explore an alternative approach that leverages standard search engine APIs to
dynamically integrate the latest online information (without maintaining any
index for any fixed corpus), thereby improving the quality of generated
content. We design a collaborative LLM-based paradigm, where we include: (i) a
parser-LLM that determines if the Internet augmented generation is demanded and
extracts the search keywords if so with a single inference; (ii) a mixed
ranking strategy that re-ranks the retrieved HTML files to eliminate bias
introduced from the search engine API; and (iii) an extractor-LLM that can
accurately and efficiently extract relevant information from the fresh content
in each HTML file. We conduct extensive empirical studies to evaluate the
performance of this Internet search augmented generation paradigm. The
experimental results demonstrate that our method generates content with
significantly improved quality. Our system has been successfully deployed in a
production environment to serve 01.AI's generative inference requests.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware
  Omni-Modal Perception of Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite impressive advancements in video understanding, most efforts remain
limited to coarse-grained or visual-only video tasks. However, real-world
videos encompass omni-modal information (vision, audio, and speech) with a
series of events forming a cohesive storyline. The lack of multi-modal video
data with fine-grained event annotations and the high cost of manual labeling
are major obstacles to comprehensive omni-modality video perception. To address
this gap, we propose an automatic pipeline consisting of high-quality
multi-modal video filtering, semantically coherent omni-modal event boundary
detection, and cross-modal correlation-aware event captioning. In this way, we
present LongVALE, the first-ever Vision-Audio-Language Event understanding
benchmark comprising 105K omni-modal events with precise temporal boundaries
and detailed relation-aware captions within 8.4K high-quality long videos.
Further, we build a baseline that leverages LongVALE to enable video large
language models (LLMs) for omni-modality fine-grained temporal video
understanding for the first time. Extensive experiments demonstrate the
effectiveness and great potential of LongVALE in advancing comprehensive
multi-modal video understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ten Ways in which Virtual Reality Differs from Video Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo de Veciana, Sonia Fahmy, George Kesidis, Voicu Popescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual Reality (VR) applications have a number of unique characteristics
that set them apart from traditional video streaming. These characteristics
have major implications on the design of VR rendering, adaptation, prefetching,
caching, and transport mechanisms. This paper contrasts VR to video streaming,
stored 2D video streaming in particular, and discusses how to rethink system
and network support for VR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Multimodal Large Language Models via Dynamic Visual-Token
  Exit and the Empirical Findings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Wu, Wenhao Lin, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The excessive use of visual tokens in existing Multimoal Large Language
Models (MLLMs) often exhibits obvious redundancy and brings in prohibitively
expensive computation. To gain insights into this problem, we first conduct
extensive empirical studies on the attention behaviors of MLLMs, and summarize
three main inference stages in MLLMs: (i) Early fusion between tokens is first
accomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)
Multimodal reasoning} resumes and lasts until the end of inference. In
particular, we reveal that visual tokens will stop contributing to reasoning
when the text tokens receive enough image information, yielding obvious visual
redundancy. Based on these generalized observations, we propose a simple yet
effective method to improve the efficiency of MLLMs, termed dynamic
visual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive
the text token status and decide the removal of all visual tokens after a
certain layer, thereby addressing the observed visual redundancy. To validate
VTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,
and conduct extensive experiments on a bunch of benchmarks. The experiment
results not only show the effectiveness of our VTE in improving MLLMs'
efficiency, but also yield the general modeling patterns of MLLMs, well
facilitating the in-depth understanding of MLLMs. Our code is anonymously
released at https://github.com/DoubtedSteam/DyVTE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deepfake Media Generation and Detection in the Generative AI Era: A
  <span class="highlight-title">Survey</span> and Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florinel-Alin Croitoru, Andrei-Iulian Hiji, Vlad Hondru, Nicolae Catalin Ristea, Paul Irofti, Marius Popescu, Cristian Rusu, Radu Tudor Ionescu, Fahad Shahbaz Khan, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent advancements in generative modeling, the realism of deepfake
content has been increasing at a steady pace, even reaching the point where
people often fail to detect manipulated media content online, thus being
deceived into various kinds of scams. In this paper, we survey deepfake
generation and detection techniques, including the most recent developments in
the field, such as diffusion models and Neural Radiance Fields. Our literature
review covers all deepfake media types, comprising image, video, audio and
multimodal (audio-visual) content. We identify various kinds of deepfakes,
according to the procedure used to alter or generate the fake content. We
further construct a taxonomy of deepfake generation and detection methods,
illustrating the important groups of methods and the domains where these
methods are applied. Next, we gather datasets used for deepfake detection and
provide updated rankings of the best performing deepfake detectors on the most
popular datasets. In addition, we develop a novel multimodal benchmark to
evaluate deepfake detectors on out-of-distribution content. The results
indicate that state-of-the-art detectors fail to generalize to deepfake content
generated by unseen deepfake generators. Finally, we propose future directions
to obtain robust and powerful deepfake detectors. Our project page and new
benchmark are available at https://github.com/CroitoruAlin/biodeep.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subjective and Objective Quality Assessment Methods of Stereoscopic
  Videos with Visibility Affecting Distortions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sria Biswas, Balasubramanyam Appina, Priyanka Kokil, Sumohana S Channappayya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present two major contributions in this work: 1) we create a full HD
resolution stereoscopic (S3D) video dataset comprised of 12 reference and 360
distorted videos. The test stimuli are produced by simulating the five levels
of fog and haze ambiances on the pristine left and right video sequences. We
perform subjective analysis on the created video dataset with 24 viewers and
compute Difference Mean Opinion Scores (DMOS) as quality representative of the
dataset, 2) an Opinion Unaware (OU) and Distortion Unaware (DU) video quality
assessment model is developed for S3D videos. We construct cyclopean frames
from the individual views of an S3D video and partition them into
nonoverlapping blocks. We analyze the Natural Scene Statistics (NSS) of all
patches of pristine and test videos, and empirically model the NSS features
with Univariate Generalized Gaussian Distribution (UGGD). We compute UGGD model
parameters ({\alpha}, \b{eta}) at multiple spatial scales and multiple
orientations of spherical steerable pyramid decomposition and show that the
UGGD parameters are distortion discriminable. Further, we perform Multivariate
Gaussian (MVG) modeling on the pristine and distorted video feature sets and
compute the corresponding mean vectors and covariance matrices of MVG fits. We
compute the Bhattacharyya distance measure between mean vectors and covariance
matrices to estimate the perceptual deviation of a test video from pristine
video set. Finally, we pool both distance measures to estimate the overall
quality score of an S3D video. The performance of the proposed objective
algorithm is verified on the popular S3D video datasets such as IRCCYN,
LFOVIAS3DPh1, LFOVIAS3DPh2 and the proposed VAD stereo dataset. The algorithm
delivers consistent performance across all datasets and shows competitive
performance against off-the-shelf 2D and 3D image and video quality assessment
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-28T00:00:00Z">2024-11-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaMetrics: Calibrating Metrics For Generation Tasks Using Human
  Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02381v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02381v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WorldCuisines: A Massive-Scale Benchmark for Multilingual and
  Multicultural Visual Question Answering on Global Cuisines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12705v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12705v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Bryan Wilie, Candy Olivia Mawalim, Ching Lam Cheng, Daud Abolade, Emmanuele Chersoni, Enrico Santus, Fariz Ikhwantri, Garry Kuwanto, Hanyang Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Santosa, Peerat Limkonchotiwat, Raj Dabre, Rio Alexander Audino, Samuel Cahyawijaya, Shi-Xiong Zhang, Stephanie Yulia Salim, Yi Zhou, Yinxuan Gui, David Ifeoluwa Adelani, En-Shiun Annie Lee, Shogo Okada, Ayu Purwarianti, Alham Fikri Aji, Taro Watanabe, Derry Tanti Wijaya, Alice Oh, Chong-Wah Ngo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) often struggle with culture-specific knowledge,
particularly in languages other than English and in underrepresented cultural
contexts. To evaluate their understanding of such knowledge, we introduce
WorldCuisines, a massive-scale benchmark for multilingual and multicultural,
visually grounded language understanding. This benchmark includes a visual
question answering (VQA) dataset with text-image pairs across 30 languages and
dialects, spanning 9 language families and featuring over 1 million data
points, making it the largest multicultural VQA benchmark to date. It includes
tasks for identifying dish names and their origins. We provide evaluation
datasets in two sizes (12k and 60k instances) alongside a training dataset (1
million instances). Our findings show that while VLMs perform better with
correct location context, they struggle with adversarial contexts and
predicting specific regional cuisines and languages. To support future
research, we release a knowledge base with annotated food entries and images
along with the VQA data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel and Mini-Batch Stable Matching for Large-Scale Reciprocal
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Nakada, Kazuki Kawamura, Ryosuke Furukawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reciprocal recommender systems (RRSs) are crucial in online two-sided
matching platforms, such as online job or dating markets, as they need to
consider the preferences of both sides of the match. The concentration of
recommendations to a subset of users on these platforms undermines their match
opportunities and reduces the total number of matches. To maximize the total
number of expected matches among market participants, stable matching theory
with transferable utility has been applied to RRSs. However, computational
complexity and memory efficiency quadratically increase with the number of
users, making it difficult to implement stable matching algorithms for several
users. In this study, we propose novel methods using parallel and mini-batch
computations for reciprocal recommendation models to improve the computational
time and space efficiency of the optimization process for stable matching.
Experiments on both real and synthetic data confirmed that our stable matching
theory-based RRS increased the computation speed and enabled tractable
large-scale data processing of up to one million samples with a single graphics
processing unit graphics board, without losing the match count.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Three New Benchmark <span class="highlight-title">Dataset</span>s for Hierarchical Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaco du Toit, Herman Redelinghuys, Marcel Dunaiski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical Text Classification (HTC) is a natural language processing task
with the objective to classify text documents into a set of classes from a
structured class hierarchy. Many HTC approaches have been proposed which
attempt to leverage the class hierarchy information in various ways to improve
classification performance. Machine learning-based classification approaches
require large amounts of training data and are most-commonly compared through
three established benchmark datasets, which include the Web Of Science (WOS),
Reuters Corpus Volume 1 Version 2 (RCV1-V2) and New York Times (NYT) datasets.
However, apart from the RCV1-V2 dataset which is well-documented, these
datasets are not accompanied with detailed description methodologies. In this
paper, we introduce three new HTC benchmark datasets in the domain of research
publications which comprise the titles and abstracts of papers from the Web of
Science publication database. We first create two baseline datasets which use
existing journal-and citation-based classification schemas. Due to the
respective shortcomings of these two existing schemas, we propose an approach
which combines their classifications to improve the reliability and robustness
of the dataset. We evaluate the three created datasets with a clustering-based
analysis and show that our proposed approach results in a higher quality
dataset where documents that belong to the same class are semantically more
similar compared to the other datasets. Finally, we provide the classification
performance of four state-of-the-art HTC approaches on these three new datasets
to provide baselines for future studies on machine learning-based techniques
for scientific publication classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integration of Contextual Descriptors in Ontology Alignment for
  Enrichment of Semantic Correspondence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduard Manziuk, Oleksander Barmak, Pavlo Radiuk, Vladislav Kuznetsov, Iurii Krak, Sergiy Yakovlev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel approach to semantic ontology alignment using
contextual descriptors. A formalization was developed that enables the
integration of essential and contextual descriptors to create a comprehensive
knowledge model. The hierarchical structure of the semantic approach and the
mathematical apparatus for analyzing potential conflicts between concepts,
particularly in the example of "Transparency" and "Privacy" in the context of
artificial intelligence, are demonstrated. Experimental studies showed a
significant improvement in ontology alignment metrics after the implementation
of contextual descriptors, especially in the areas of privacy, responsibility,
and freedom & autonomy. The application of contextual descriptors achieved an
average overall improvement of approximately 4.36%. The results indicate the
effectiveness of the proposed approach for more accurately reflecting the
complexity of knowledge and its contextual dependence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ontology alignment, contextual descriptors, semantic matching,
  knowledge representation, essential descriptors, ontology integration,
  hierarchical structure, semantic heterogeneity, ethical AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Headache to Overstock? Promoting Long-tail Items through Debiased
  Product Bundling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Xu, Haokai Ma, Yunshan Ma, Xiaohao Liu, Lei Meng, Xiangxu Meng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Product bundling aims to organize a set of thematically related items into a
combined bundle for shipment facilitation and item promotion. To increase the
exposure of fresh or overstocked products, sellers typically bundle these items
with popular products for inventory clearance. This specific task can be
formulated as a long-tail product bundling scenario, which leverages the
user-item interactions to define the popularity of each item. The inherent
popularity bias in the pre-extracted user feedback features and the
insufficient utilization of other popularity-independent knowledge may force
the conventional bundling methods to find more popular items, thereby
struggling with this long-tail bundling scenario. Through intuitive and
empirical analysis, we navigate the core solution for this challenge, which is
maximally mining the popularity-free features and effectively incorporating
them into the bundling process. To achieve this, we propose a Distilled
Modality-Oriented Knowledge Transfer framework (DieT) to effectively counter
the popularity bias misintroduced by the user feedback features and adhere to
the original intent behind the real-world bundling behaviors. Specifically,
DieT first proposes the Popularity-free Collaborative Distribution Modeling
module (PCD) to capture the popularity-independent information from the
bundle-item view, which is proven most effective in the long-tail bundling
scenario to enable the directional information transfer. With the tailored
Unbiased Bundle-aware Knowledge Transferring module (UBT), DieT can highlight
the significance of popularity-free features while mitigating the negative
effects of user feedback features in the long-tail scenario via the knowledge
distillation paradigm. Extensive experiments on two real-world datasets
demonstrate the superiority of DieT over a list of SOTA methods in the
long-tail bundling scenario.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICLERB: In-Context Learning Embedding and Reranker Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marie Al Ghossein, Emile Contal, Alexandre Robicquet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Learning (ICL) enables Large Language Models (LLMs) to perform new
tasks by conditioning on prompts with relevant information. Retrieval-Augmented
Generation (RAG) enhances ICL by incorporating retrieved documents into the
LLM's context at query time. However, traditional retrieval methods focus on
semantic relevance, treating retrieval as a search problem. In this paper, we
propose reframing retrieval for ICL as a recommendation problem, aiming to
select documents that maximize utility in ICL tasks. We introduce the
In-Context Learning Embedding and Reranker Benchmark (ICLERB), a novel
evaluation framework that compares retrievers based on their ability to enhance
LLM accuracy in ICL settings. Additionally, we propose a novel Reinforcement
Learning-to-Rank from AI Feedback (RLRAIF) algorithm, designed to fine-tune
retrieval models using minimal feedback from the LLM. Our experimental results
reveal notable differences between ICLERB and existing benchmarks, and
demonstrate that small models fine-tuned with our RLRAIF algorithm outperform
large state-of-the-art retrieval models. These findings highlight the
limitations of existing evaluation methods and the need for specialized
benchmarks and training strategies adapted to ICL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating SPARQL and LLMs for Question Answering over Scholarly Data
  Sources <span class="chip">ISWC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fomubad Borista Fondi, Azanzi Jiomekong Fidel, Gaoussou Camara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Scholarly Hybrid Question Answering over Linked Data (QALD) Challenge at
the International Semantic Web Conference (ISWC) 2024 focuses on Question
Answering (QA) over diverse scholarly sources: DBLP, SemOpenAlex, and
Wikipedia-based texts. This paper describes a methodology that combines SPARQL
queries, divide and conquer algorithms, and a pre-trained extractive question
answering model. It starts with SPARQL queries to gather data, then applies
divide and conquer to manage various question types and sources, and uses the
model to handle personal author questions. The approach, evaluated with Exact
Match and F-score metrics, shows promise for improving QA accuracy and
efficiency in scholarly contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Scholarly Hybrid Question answering challenge from the International
  Semantic Web Conference of 2024(ISWC), 7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A smoothed-Bayesian approach to frequency recovery from sketched data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15408v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15408v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Beraha, Stefano Favaro, Matteo Sesia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a novel statistical perspective on a classical problem at the
intersection of computer science and information theory: recovering the
empirical frequency of a symbol in a large discrete dataset using only a
compressed representation, or sketch, obtained via random hashing. Departing
from traditional algorithmic approaches, recent works have proposed Bayesian
nonparametric (BNP) methods that can provide more informative frequency
estimates by leveraging modeling assumptions about the distribution of the
sketched data. In this paper, we propose a smoothed-Bayesian method, inspired
by existing BNP approaches but designed in a frequentist framework to overcome
the computational limitations of the BNP approaches when dealing with
large-scale data from realistic distributions, including those with power-law
tail behaviors. For sketches obtained with a single hash function, our approach
is supported by rigorous frequentist properties, including unbiasedness and
optimality under a squared error loss function within an intuitive class of
linear estimators. For sketches with multiple hash functions, we introduce an
approach based on multi-view learning to construct computationally efficient
frequency estimators. We validate our method on synthetic and real data,
comparing its performance to that of existing alternatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model, Analyze, and Comprehend User Interactions within a Social Media
  Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Kaykobad Reza, S M Maksudul Alam, Yiran Luo, Youzhe Liu, Md Siam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a novel graph-based approach to model, analyze and
comprehend user interactions within a social media platform based on
post-comment relationship. We construct a user interaction graph from social
media data and analyze it to gain insights into community dynamics, user
behavior, and content preferences. Our investigation reveals that while 56.05%
of the active users are strongly connected within the community, only 0.8% of
them significantly contribute to its dynamics. Moreover, we observe temporal
variations in community activity, with certain periods experiencing heightened
engagement. Additionally, our findings highlight a correlation between user
activity and popularity showing that more active users are generally more
popular. Alongside these, a preference for positive and informative content is
also observed where 82.41% users preferred positive and informative content.
Overall, our study provides a comprehensive framework for understanding and
managing online communities, leveraging graph-based techniques to gain valuable
insights into user behavior and community dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 27th International Conference on Computer and Information
  Technology (ICCIT), 2024. 6 Pages, 6 Figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic <span class="highlight-title">Prompt</span> Generation and Grounding Object Detection for Zero-Shot
  Image Anomaly Detection <span class="chip">SC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsun-Hin Cheung, Ka-Chun Fung, Songjiang Lai, Kwan-Ho Lin, Vincent Ng, Kin-Man Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying defects and anomalies in industrial products is a critical
quality control task. Traditional manual inspection methods are slow,
subjective, and error-prone. In this work, we propose a novel zero-shot
training-free approach for automated industrial image anomaly detection using a
multimodal machine learning pipeline, consisting of three foundation models.
Our method first uses a large language model, i.e., GPT-3. generate text
prompts describing the expected appearances of normal and abnormal products. We
then use a grounding object detection model, called Grounding DINO, to locate
the product in the image. Finally, we compare the cropped product image patches
to the generated prompts using a zero-shot image-text matching model, called
CLIP, to identify any anomalies. Our experiments on two datasets of industrial
product images, namely MVTec-AD and VisA, demonstrate the effectiveness of this
method, achieving high accuracy in detecting various types of defects and
anomalies without the need for model training. Our proposed model enables
efficient, scalable, and objective quality control in industrial manufacturing
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to APSIPA ASC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuperGaussians: Enhancing Gaussian Splatting Using Primitives with
  Spatially Varying Colors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Xu, Wenyue Chen, Jiepeng Wang, Yuan Liu, Peng Wang, Lin Gao, Shiqing Xin, Taku Komura, Xin Li, Wenping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Splattings demonstrate impressive results in multi-view
reconstruction based on Gaussian explicit representations. However, the current
Gaussian primitives only have a single view-dependent color and an opacity to
represent the appearance and geometry of the scene, resulting in a non-compact
representation. In this paper, we introduce a new method called SuperGaussians
that utilizes spatially varying colors and opacity in a single Gaussian
primitive to improve its representation ability. We have implemented bilinear
interpolation, movable kernels, and even tiny neural networks as spatially
varying functions. Quantitative and qualitative experimental results
demonstrate that all three functions outperform the baseline, with the best
movable kernels achieving superior novel view synthesis performance on multiple
datasets, highlighting the strong potential of spatially varying functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Accuracy and Generalization for Efficient Visual Tracking <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Zaveri, Shivang Patel, Yu Gu, Gianfranco Doretto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient visual trackers overfit to their training distributions and lack
generalization abilities, resulting in them performing well on their respective
in-distribution (ID) test sets and not as well on out-of-distribution (OOD)
sequences, imposing limitations to their deployment in-the-wild under
constrained resources. We introduce SiamABC, a highly efficient Siamese tracker
that significantly improves tracking performance, even on OOD sequences.
SiamABC takes advantage of new architectural designs in the way it bridges the
dynamic variability of the target, and of new losses for training. Also, it
directly addresses OOD tracking generalization by including a fast
backward-free dynamic test-time adaptation method that continuously adapts the
model according to the dynamic visual changes of the target. Our extensive
experiments suggest that SiamABC shows remarkable performance gains in OOD sets
while maintaining accurate performance on the ID benchmarks. SiamABC
outperforms MixFormerV2-S by 7.6\% on the OOD AVisT benchmark while being 3x
faster (100 FPS) on a CPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-Guided Foley Sound Generation with Multimodal Controls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Chen, Prem Seetharaman, Bryan Russell, Oriol Nieto, David Bourgin, Andrew Owens, Justin Salamon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating sound effects for videos often requires creating artistic sound
effects that diverge significantly from real-life sources and flexible control
in the sound design. To address this problem, we introduce MultiFoley, a model
designed for video-guided sound generation that supports multimodal
conditioning through text, audio, and video. Given a silent video and a text
prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels
spinning without wind noise) or more whimsical sounds (e.g., making a lion's
roar sound like a cat's meow). MultiFoley also allows users to choose reference
audio from sound effects (SFX) libraries or partial videos for conditioning. A
key novelty of our model lies in its joint training on both internet video
datasets with low-quality audio and professional SFX recordings, enabling
high-quality, full-bandwidth (48kHz) audio generation. Through automated
evaluations and human studies, we demonstrate that MultiFoley successfully
generates synchronized high-quality sounds across varied conditional inputs and
outperforms existing methods. Please see our project page for video results:
https://ificl.github.io/MultiFoley/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project site: https://ificl.github.io/MultiFoley/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General
  Sound <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohe Liu, Xuenan Xu, Yi Yuan, Mengyue Wu, Wenwu Wang, Mark D. Plumbley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly advanced audio processing
through audio codecs that convert audio into discrete tokens, enabling the
application of language modelling techniques to audio data. However,
traditional codecs often operate at high bitrates or within narrow domains such
as speech and lack the semantic clues required for efficient language
modelling. Addressing these challenges, we introduce SemantiCodec, a novel
codec designed to compress audio into fewer than a hundred tokens per second
across diverse audio types, including speech, general sound, and music, without
compromising quality. SemantiCodec features a dual-encoder architecture: a
semantic encoder using a self-supervised pre-trained Audio Masked Autoencoder
(AudioMAE), discretized using k-means clustering on extensive audio data, and
an acoustic encoder to capture the remaining details. The semantic and acoustic
encoder outputs are used to reconstruct audio via a diffusion-model-based
decoder. SemantiCodec is presented in three variants with token rates of 25,
50, and 100 per second, supporting a range of ultra-low bit rates between 0.31
kbps and 1.40 kbps. Experimental results demonstrate that SemantiCodec
significantly outperforms the state-of-the-art Descript codec on reconstruction
quality. Our results also suggest that SemantiCodec contains significantly
richer semantic information than all evaluated state-of-the-art audio codecs,
even at significantly lower bitrates. Our code and demos are available at
https://haoheliu.github.io/SemantiCodec/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Journal of Selected Topics in Signal Processing (JSTSP).
  Demo and code: https://haoheliu.github.io/SemantiCodec/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dance Any Beat: Blending Beats with Visuals in Dance Video Generation <span class="chip">WACV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09266v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09266v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanchen Wang, Heng Wang, Dongnan Liu, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating dance from music is crucial for advancing automated choreography.
Current methods typically produce skeleton keypoint sequences instead of dance
videos and lack the capability to make specific individuals dance, which
reduces their real-world applicability. These methods also require precise
keypoint annotations, complicating data collection and limiting the use of
self-collected video datasets. To overcome these challenges, we introduce a
novel task: generating dance videos directly from images of individuals guided
by music. This task enables the dance generation of specific individuals
without requiring keypoint annotations, making it more versatile and applicable
to various situations. Our solution, the Dance Any Beat Diffusion model
(DabFusion), utilizes a reference image and a music piece to generate dance
videos featuring various dance types and choreographies. The music is analyzed
by our specially designed music encoder, which identifies essential features
including dance style, movement, and rhythm. DabFusion excels in generating
dance videos not only for individuals in the training dataset but also for any
previously unseen person. This versatility stems from its approach of
generating latent optical flow, which contains all necessary motion information
to animate any person in the image. We evaluate DabFusion's performance using
the AIST++ dataset, focusing on video quality, audio-video synchronization, and
motion-music alignment. We propose a 2D Motion-Music Alignment Score (2D-MM
Align), which builds on the Beat Alignment Score to more effectively evaluate
motion-music alignment for this new task. Experiments show that our DabFusion
establishes a solid baseline for this innovative task. Video results can be
found on our project page: https://DabFusion.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV2025, 11 pages, 7 figures, demo page: https://DabFusion.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Training Boosted Multi-Factor Matching Network for Composed Image
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Wen, Xuemeng Song, Jianhua Yin, Jianlong Wu, Weili Guan, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The composed image retrieval (CIR) task aims to retrieve the desired target
image for a given multimodal query, i.e., a reference image with its
corresponding modification text. The key limitations encountered by existing
efforts are two aspects: 1) ignoring the multi-faceted query-target matching
factors; 2) ignoring the potential unlabeled reference-target image pairs in
existing benchmark datasets. To address these two limitations is non-trivial
due to the following challenges: 1) how to effectively model the multi-faceted
matching factors in a latent way without direct supervision signals; 2) how to
fully utilize the potential unlabeled reference-target image pairs to improve
the generalization ability of the CIR model. To address these challenges, in
this work, we first propose a muLtI-faceted Matching Network (LIMN), which
consists of three key modules: multi-grained image/text encoder, latent
factor-oriented feature aggregation, and query-target matching modeling.
Thereafter, we design an iterative dual self-training paradigm to further
enhance the performance of LIMN by fully utilizing the potential unlabeled
reference-target image pairs in a semi-supervised manner. Specifically, we
denote the iterative dual self-training paradigm enhanced LIMN as LIMN+.
Extensive experiments on three real-world datasets, FashionIQ, Shoes, and
Birds-to-Words, show that our proposed method significantly surpasses the
state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-27T00:00:00Z">2024-11-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Generative and Dense Retrieval for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Yang, Fabian Paischer, Kaveh Hassani, Jiacheng Li, Shuai Shao, Zhang Gabriel Li, Yun He, Xue Feng, Nima Noorshams, Sem Park, Bo Long, Robert D Nowak, Xiaoli Gao, Hamid Eghbalzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential dense retrieval models utilize advanced sequence learning
techniques to compute item and user representations, which are then used to
rank relevant items for a user through inner product computation between the
user and all item representations. However, this approach requires storing a
unique representation for each item, resulting in significant memory
requirements as the number of items grow. In contrast, the recently proposed
generative retrieval paradigm offers a promising alternative by directly
predicting item indices using a generative model trained on semantic IDs that
encapsulate items' semantic information. Despite its potential for large-scale
applications, a comprehensive comparison between generative retrieval and
sequential dense retrieval under fair conditions is still lacking, leaving open
questions regarding performance, and computation trade-offs. To address this,
we compare these two approaches under controlled conditions on academic
benchmarks and propose LIGER (LeveragIng dense retrieval for GEnerative
Retrieval), a hybrid model that combines the strengths of these two widely used
methods. LIGER integrates sequential dense retrieval into generative retrieval,
mitigating performance differences and enhancing cold-start item recommendation
in the datasets evaluated. This hybrid approach provides insights into the
trade-offs between these approaches and demonstrates improvements in efficiency
and effectiveness for recommendation systems in small-scale benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Literature <span class="highlight-title">Review</span> Using NLP Techniques and LLM-Based
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurshat Fateh Ali, Md. Mahdi Mohtasim, Shakil Mosharrof, T. Gopi Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents and compares multiple approaches to automate the
generation of literature reviews using several Natural Language Processing
(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language
Model (LLM). The ever-increasing number of research articles provides a huge
challenge for manual literature review. It has resulted in an increased demand
for automation. Developing a system capable of automatically generating the
literature reviews from only the PDF files as input is the primary objective of
this research work. The effectiveness of several Natural Language Processing
(NLP) strategies, such as the frequency-based method (spaCy), the transformer
model (Simple T5), and retrieval-augmented generation (RAG) with Large Language
Model (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR
dataset is chosen for this research experiment and three distinct techniques
are utilized to implement three different systems for auto-generating the
literature reviews. The ROUGE scores are used for the evaluation of all three
systems. Based on the evaluation, the Large Language Model GPT-3.5-turbo
achieved the highest ROUGE-1 score, 0.364. The transformer model comes in
second place and spaCy is at the last position. Finally, a graphical user
interface is created for the best system based on the large language model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Key Words : T5, SpaCy, Large Language Model, GPT, ROUGE, Literature
  Review, Natural Language Processing, Retrieval-augmented generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Isometry pursuit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samson Koelle, Marina Meila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Isometry pursuit is a convex algorithm for identifying orthonormal
column-submatrices of wide matrices. It consists of a novel normalization
method followed by multitask basis pursuit. Applied to Jacobians of putative
coordinate functions, it helps identity isometric embeddings from within
interpretable dictionaries. We provide theoretical and experimental results
justifying this method. For problems involving coordinate selection and
diversification, it offers a synergistic alternative to greedy and brute force
search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delineating Feminist Studies through bibliometric analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natsumi S. Shokida, Diego Kozlowski, Vincent Larivière
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multidisciplinary and socially anchored nature of Feminist Studies
presents unique challenges for bibliometric analysis, as this research area
transcends traditional disciplinary boundaries and reflects discussions from
feminist and LGBTQIA+ social movements. This paper proposes a novel approach
for identifying gender/sex related publications scattered across diverse
scientific disciplines. Using the Dimensions database, we employ bibliometric
techniques, natural language processing (NLP) and manual curation to compile a
dataset of scientific publications that allows for the analysis of Gender
Studies and its influence across different disciplines.
  This is achieved through a methodology that combines a core of specialized
journals with a comprehensive keyword search over titles. These keywords are
obtained by applying Topic Modeling (BERTopic) to the corpus of titles and
abstracts from the core. This methodological strategy, divided into two stages,
reflects the dynamic interaction between Gender Studies and its dialogue with
different disciplines. This hybrid system surpasses basic keyword search by
mitigating potential biases introduced through manual keyword enumeration.
  The resulting dataset comprises over 1.9 million scientific documents
published between 1668 and 2023, spanning four languages. This dataset enables
a characterization of Gender Studies in terms of addressed topics, citation and
collaboration dynamics, and institutional and regional participation. By
addressing the methodological challenges of studying "more-than-disciplinary"
research areas, this approach could also be adapted to delineate other
conversations where disciplinary boundaries are difficult to disentangle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Break the ID-Language Barrier: An Adaption Framework for Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Yu, Li Zhang, Xin Zhao, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent breakthrough of large language models (LLMs) in natural language
processing has sparked exploration in recommendation systems, however, their
limited domain-specific knowledge remains a critical bottleneck. Specifically,
LLMs lack key pieces of information crucial for sequential recommendations,
such as user behavior patterns. To address this critical gap, we propose
IDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich
in domain-specific knowledge, into LLMs to improve recommendation accuracy.
IDLE-Adapter acts as a bridge, transforming sparse user-item interaction data
into dense, LLM-compatible representations through a Pre-trained ID Sequential
Model, Dimensionality Alignment, Layer-wise Embedding Refinement, and
Layer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates
remarkable flexibility by seamlessly integrating ID embeddings from diverse
ID-based sequential models and LLM architectures. Extensive experiments across
various datasets demonstrate the superiority of IDLE-Adapter, achieving over
10\% and 20\% improvements in HitRate@5 and NDCG@5 metrics, respectively,
compared to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Rn-index: a more accurate variant of the Rk-index 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alonso Rodriguez-Navarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The contribution to pushing the boundaries of knowledge is a critical metric
for evaluating the research performance of countries and institutions, which in
many cases is not revealed by common bibliometric indicators. The Rk-index was
specifically designed to assess such contributions, and the Rn-index is a
variant that corrects the weakness of the Rk-index, particularly in the
evaluation of countries that produce a high proportion of global advancements.
This is the case of the USA and China in many technological fields.
Additionally, the Rn-index is simple to calculate and understand, as it
involves only summing the ratios between the local and global ranks of papers,
ordered by their citation count. Moreover, the Rn-index may also be
fractionally counted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages; 2 figures; 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DuMapper: Towards Automatic Verification of Large-Scale POIs with Street
  Views at Baidu Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Fan, Jizhou Huang, Haifeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increased popularity of mobile devices, Web mapping services have
become an indispensable tool in our daily lives. To provide user-satisfied
services, such as location searches, the point of interest (POI) database is
the fundamental infrastructure, as it archives multimodal information on
billions of geographic locations closely related to people's lives, such as a
shop or a bank. Therefore, verifying the correctness of a large-scale POI
database is vital. To achieve this goal, many industrial companies adopt
volunteered geographic information (VGI) platforms that enable thousands of
crowdworkers and expert mappers to verify POIs seamlessly; but to do so, they
have to spend millions of dollars every year. To save the tremendous labor
costs, we devised DuMapper, an automatic system for large-scale POI
verification with the multimodal street-view data at Baidu Maps. DuMapper takes
the signboard image and the coordinates of a real-world place as input to
generate a low-dimensional vector, which can be leveraged by ANN algorithms to
conduct a more accurate search through billions of archived POIs in the
database for verification within milliseconds. It can significantly increase
the throughput of POI verification by $50$ times. DuMapper has already been
deployed in production since \DuMPOnline, which dramatically improves the
productivity and efficiency of POI verification at Baidu Maps. As of December
31, 2021, it has enacted over $405$ million iterations of POI verification
within a 3.5-year period, representing an approximate workload of $800$
high-performance expert mappers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Overview</span> of TREC 2024 Biomedical Generative Retrieval (BioGen) Track 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Gupta, Dina Demner-Fushman, William Hersh, Steven Bedrick, Kirk Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of large language models (LLMs), the biomedical domain
has seen significant progress and improvement in multiple tasks such as
biomedical question answering, lay language summarization of the biomedical
literature, clinical note summarization, etc. However, hallucinations or
confabulations remain one of the key challenges when using LLMs in the
biomedical and other domains. Inaccuracies may be particularly harmful in
high-risk situations, such as making clinical decisions or appraising
biomedical research. Studies on the evaluation of the LLMs' abilities to ground
generated statements in verifiable sources have shown that models perform
significantly worse on lay-user generated questions, and often fail to
reference relevant sources. This can be problematic when those seeking
information want evidence from studies to back up the claims from LLMs[3].
Unsupported statements are a major barrier to using LLMs in any applications
that may affect health. Methods for grounding generated statements in reliable
sources along with practical evaluation approaches are needed to overcome this
barrier. Towards this, in our pilot task organized at TREC 2024, we introduced
the task of reference attribution as a means to mitigate the generation of
false statements by LLMs answering biomedical questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lusifer: LLM-based User SImulated Feedback Environment for online
  Recommender systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danial Ebrat, Eli Paradalis, Luis Rueda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training reinforcement learning-based recommender systems is often hindered
by the lack of dynamic and realistic user interactions. To address this
limitation, we introduce Lusifer, a novel environment leveraging Large Language
Models (LLMs) to generate simulated user feedback. Lusifer synthesizes user
profiles and interaction histories to simulate responses and behaviors toward
recommended items, with profiles updated after each rating to reflect evolving
user characteristics. Utilizing the MovieLens dataset as a proof of concept, we
limited our implementation to the last 40 interactions for each user,
representing approximately 39% and 22% of the training sets, to focus on recent
user behavior. For consistency and to gain insights into the performance of
traditional methods with limited data, we implemented baseline approaches using
the same data subset. Our results demonstrate that Lusifer accurately emulates
user behavior and preferences, even with reduced training data having an RMSE
of 1.3 across various test sets. This paper presents Lusifer's operational
pipeline, including prompt generation and iterative user profile updates, and
compares its performance against baseline methods. The findings validate
Lusifer's ability to produce realistic dynamic feedback and suggest that it
offers a scalable and adjustable framework for user simulation in online
reinforcement learning recommender systems for future studies, particularly
when training data is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating the Post-API Dilemma | Search Engine Results Pages Present a
  Biased View of Social Media Data <span class="chip">WWW '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15479v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15479v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrit Poudel, Tim Weninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent decisions to discontinue access to social media APIs are having
detrimental effects on Internet research and the field of computational social
science as a whole. This lack of access to data has been dubbed the Post-API
era of Internet research. Fortunately, popular search engines have the means to
crawl, capture, and surface social media data on their Search Engine Results
Pages (SERP) if provided the proper search query, and may provide a solution to
this dilemma. In the present work we ask: does SERP provide a complete and
unbiased sample of social media data? Is SERP a viable alternative to direct
API-access? To answer these questions, we perform a comparative analysis
between (Google) SERP results and nonsampled data from Reddit and Twitter/X. We
find that SERP results are highly biased in favor of popular posts; against
political, pornographic, and vulgar posts; are more positive in their
sentiment; and have large topical gaps. Overall, we conclude that SERP is not a
viable alternative to social media API access.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the ACM Web Conference 2024 (WWW '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A
  Benchmark and Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Yu, Mingyue Cheng, Jiqian Yang, Jie Ouyang, Yucong Luo, Chenyi Lei, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is increasingly recognized as an
effective approach for mitigating the hallucination of large language models
(LLMs) through the integration of external knowledge. While numerous efforts,
most studies focus on a single type of externeal knowledge source. However, in
real-world applications, most situations involve diverse knowledge from various
sources, yet this area has been less explored. The main dilemma is the lack of
a suitable dataset containing multiple knowledge sources and pre-exploration of
the associated issues. To address these challenges, we standardize a benchmark
dataset that combines structured and unstructured knowledge across diverse and
complementary domains. Based on this dataset, we further develop a
plug-and-play RAG framework, PruningRAG, whose main characteristic is to employ
multi-granularity pruning strategies for optimizing the integration of relevant
information and minimizing misleading context. Building upon the standardized
dataset and PruningRAG, we also report a series of experimental results, as
well as insightful findings. Our dataset and code are publicly
available\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim of
advancing future research in the RAG community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CELA: Cost-Efficient Language Model Alignment for CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10596v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10596v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingmei Wang, Weiwen Liu, Xiaolong Chen, Qi Liu, Xu Huang, Yichao Wang, Xiangyang Li, Yasheng Wang, Zhenhua Dong, Defu Lian, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-Through Rate (CTR) prediction holds a paramount position in recommender
systems. The prevailing ID-based paradigm underperforms in cold-start scenarios
due to the skewed distribution of feature frequency. Additionally, the
utilization of a single modality fails to exploit the knowledge contained
within textual features. Recent efforts have sought to mitigate these
challenges by integrating Pre-trained Language Models (PLMs). They design hard
prompts to structure raw features into text for each interaction and then apply
PLMs for text processing. With external knowledge and reasoning capabilities,
PLMs extract valuable information even in cases of sparse interactions.
Nevertheless, compared to ID-based models, pure text modeling degrades the
efficacy of collaborative filtering, as well as feature scalability and
efficiency during both training and inference. To address these issues, we
propose \textbf{C}ost-\textbf{E}fficient \textbf{L}anguage Model
\textbf{A}lignment (\textbf{CELA}) for CTR prediction. CELA incorporates
textual features and language models while preserving the collaborative
filtering capabilities of ID-based models. This model-agnostic framework can be
equipped with plug-and-play textual features, with item-level alignment
enhancing the utilization of external information while maintaining training
and inference efficiency. Through extensive offline experiments, CELA
demonstrates superior performance compared to state-of-the-art methods.
Furthermore, an online A/B test conducted on an industrial App recommender
system showcases its practical effectiveness, solidifying the potential for
real-world applications of CELA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Citywide Electric Vehicle Charging Demand Prediction Approach
  Considering Urban Region and Dynamic Influences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18766v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18766v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan Kuang, Kunxiang Deng, Linlin You, Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electric vehicle charging demand prediction is important for vacant charging
pile recommendation and charging infrastructure planning, thus facilitating
vehicle electrification and green energy development. The performance of
previous spatio-temporal studies is still far from satisfactory nowadays
because urban region attributes and multivariate temporal influences are not
adequately taken into account. To tackle these issues, we propose a learning
approach for citywide electric vehicle charging demand prediction, named
CityEVCP. To learn non-pairwise relationships in urban areas, we cluster
service areas by the types and numbers of points of interest in the areas and
develop attentive hypergraph networks accordingly. Graph attention mechanisms
are employed for information propagation between neighboring areas.
Additionally, we propose a variable selection network to adaptively learn
dynamic auxiliary information and improve the Transformer encoder utilizing
gated mechanisms for fluctuating charging time-series data. Experiments on a
citywide electric vehicle charging dataset demonstrate the performances of our
proposed approach compared with a broad range of competing baselines.
Furthermore, we demonstrate the impact of dynamic influences on prediction
results in different areas of the city and the effectiveness of our area
clustering method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Chat<span class="highlight-title">GPT</span>-Like Large-Scale Language Models with Local Knowledge
  Base for Industrial Prognostics and Health Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14945v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14945v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Wang, Yan-Fu Li, Min Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prognostics and health management (PHM) is essential for industrial operation
and maintenance, focusing on predicting, diagnosing, and managing the health
status of industrial systems. The emergence of the ChatGPT-Like large-scale
language model (LLM) has begun to lead a new round of innovation in the AI
field. It has extensively promoted the level of intelligence in various fields.
Therefore, it is also expected further to change the application paradigm in
industrial PHM and promote PHM to become intelligent. Although ChatGPT-Like
LLMs have rich knowledge reserves and powerful language understanding and
generation capabilities, they lack domain-specific expertise, significantly
limiting their practicability in PHM applications. To this end, this study
explores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in
industrial PHM to solve the above limitations. In addition, we introduce the
method and steps of combining the LKB with LLMs, including LKB preparation, LKB
vectorization, prompt engineering, etc. Experimental analysis of real cases
shows that combining the LKB with ChatGPT-Like LLM can significantly improve
its performance and make ChatGPT-Like LLMs more accurate, relevant, and able to
provide more insightful information. This can promote the development of
ChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMSearch: Benchmarking the Potential of Large Models as Multi-modal
  Search Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has paved the way for AI search
engines, e.g., SearchGPT, showcasing a new paradigm in human-internet
interaction. However, most current AI search engines are limited to text-only
settings, neglecting the multimodal user queries and the text-image interleaved
nature of website information. Recently, Large Multimodal Models (LMMs) have
made impressive strides. Yet, whether they can function as AI search engines
remains under-explored, leaving the potential of LMMs in multimodal search an
open question. To this end, we first design a delicate pipeline,
MMSearch-Engine, to empower any LMMs with multimodal search capabilities. On
top of this, we introduce MMSearch, a comprehensive evaluation benchmark to
assess the multimodal search performance of LMMs. The curated dataset contains
300 manually collected instances spanning 14 subfields, which involves no
overlap with the current LMMs' training data, ensuring the correct answer can
only be obtained within searching. By using MMSearch-Engine, the LMMs are
evaluated by performing three individual tasks (requery, rerank, and
summarization), and one challenging end-to-end task with a complete searching
process. We conduct extensive experiments on closed-source and open-source
LMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best
results, which surpasses the commercial product, Perplexity Pro, in the
end-to-end task, demonstrating the effectiveness of our proposed pipeline. We
further present error analysis to unveil current LMMs still struggle to fully
grasp the multimodal search tasks, and conduct ablation study to indicate the
potential of scaling test-time computation for AI search engine. We hope
MMSearch may provide unique insights to guide the future development of
multimodal AI search engine. Project Page: https://mmsearch.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://mmsearch.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling Adoption of Regenerative Agriculture through Soil Carbon
  Copilots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margaret Capetz, Swati Sharma, Rafael Padilha, Peder Olsen, Jessica Wolk, Emre Kiciman, Ranveer Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating climate change requires transforming agriculture to minimize
environ mental impact and build climate resilience. Regenerative agricultural
practices enhance soil organic carbon (SOC) levels, thus improving soil health
and sequestering carbon. A challenge to increasing regenerative agriculture
practices is cheaply measuring SOC over time and understanding how SOC is
affected by regenerative agricultural practices and other environmental factors
and farm management practices. To address this challenge, we introduce an
AI-driven Soil Organic Carbon Copilot that automates the ingestion of complex
multi-resolution, multi-modal data to provide large-scale insights into soil
health and regenerative practices. Our data includes extreme weather event data
(e.g., drought and wildfire incidents), farm management data (e.g., cropland
information and tillage predictions), and SOC predictions. We find that
integrating public data and specialized models enables large-scale, localized
analysis for sustainable agriculture. In comparisons of agricultural practices
across California counties, we find evidence that diverse agricultural activity
may mitigate the negative effects of tillage; and that while extreme weather
conditions heavily affect SOC, composting may mitigate SOC loss. Finally,
implementing role-specific personas empowers agronomists, farm consultants,
policymakers, and other stakeholders to implement evidence-based strategies
that promote sustainable agriculture and build climate resilience.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-driven Sampling for Efficient Pairwise Comparison Subjective
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shima Mohammadi, João Ascenso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing image quality is crucial in image processing tasks such as
compression, super-resolution, and denoising. While subjective assessments
involving human evaluators provide the most accurate quality scores, they are
impractical for large-scale or continuous evaluations due to their high cost
and time requirements. Pairwise comparison subjective assessment tests, which
rank image pairs instead of assigning scores, offer more reliability and
accuracy but require numerous comparisons, leading to high costs. Although
objective quality metrics are more efficient, they lack the precision of
subjective tests, which are essential for benchmarking and training
learning-based quality metrics. This paper proposes an uncertainty-based
sampling method to optimize the pairwise comparison subjective assessment
process. By utilizing deep learning models to estimate human preferences and
identify pairs that need human labeling, the approach reduces the number of
required comparisons while maintaining high accuracy. The key contributions
include modeling uncertainty for accurate preference predictions and for
pairwise sampling. The experimental results demonstrate superior performance of
the proposed approach compared to traditional active sampling methods. Software
is publicly available at: shimamohammadi/LBPS-EIC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, 7 Figures, Submitted to IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery
  from Videos <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13397v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13397v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zheng, Xianpeng Liu, Qucheng Peng, Tianfu Wu, Pu Wang, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mesh recovery (HMR) provides rich human body information for various
real-world applications. While image-based HMR methods have achieved impressive
results, they often struggle to recover humans in dynamic scenarios, leading to
temporal inconsistencies and non-smooth 3D motion predictions due to the
absence of human motion. In contrast, video-based approaches leverage temporal
information to mitigate this issue. In this paper, we present DiffMesh, an
innovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh
establishes a bridge between diffusion models and human motion, efficiently
generating accurate and smooth output mesh sequences by incorporating human
motion within the forward process and reverse process in the diffusion model.
Extensive experiments are conducted on the widely used datasets (Human3.6M
\cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness
and efficiency of our DiffMesh. Visual comparisons in real-world scenarios
further highlight DiffMesh's suitability for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Over-the-Air Learning-based Geometry Point Cloud Transmission 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghong Bian, Yulin Shao, Deniz Gunduz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents novel solutions for the efficient and reliable
transmission of 3D point clouds over wireless channels. We first propose SEPT
for the transmission of small-scale point clouds, which encodes the point cloud
via an iterative downsampling and feature extraction process. At the receiver,
SEPT decoder reconstructs the point cloud with latent reconstruction and
offset-based upsampling. A novel channel-adaptive module is proposed to allow
SEPT to operate effectively over a wide range of channel conditions. Next, we
propose OTA-NeRF, a scheme inspired by neural radiance fields. OTA-NeRF
performs voxelization to the point cloud input and learns to encode the
voxelized point cloud into a neural network. Instead of transmitting the
extracted feature vectors as in the SEPT scheme, it transmits the learned
neural network weights over the air in an analog fashion along with few
hyperparameters that are transmitted digitally. At the receiver, the OTA-NeRF
decoder reconstructs the original point cloud using the received noisy neural
network weights. To further increase the bandwidth efficiency of the OTA-NeRF
scheme, a fine-tuning algorithm is developed, where only a fraction of the
neural network weights are retrained and transmitted. Extensive numerical
experiments confirm that both the SEPT and the OTA-NeRF schemes achieve
superior or comparable performance over the conventional approaches, where an
octree-based or a learning-based point cloud compression scheme is concatenated
with a channel code. As an additional advantage, both schemes mitigate the
cliff and leveling effects making them particularly attractive for highly
mobile scenarios, where accurate channel estimation is challenging if not
impossible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, submitted to IEEE journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CHORDONOMICON: A <span class="highlight-title">Dataset</span> of 666,000 Songs and their Chord Progressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spyridon Kantarelis, Konstantinos Thomas, Vassilis Lyberatos, Edmund Dervakos, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chord progressions encapsulate important information about music, pertaining
to its structure and conveyed emotions. They serve as the backbone of musical
composition, and in many cases, they are the sole information required for a
musician to play along and follow the music. Despite their importance, chord
progressions as a data domain remain underexplored. There is a lack of
large-scale datasets suitable for deep learning applications, and limited
research exploring chord progressions as an input modality. In this work, we
present Chordonomicon, a dataset of over 666,000 songs and their chord
progressions, annotated with structural parts, genre, and release date -
created by scraping various sources of user-generated progressions and
associated metadata. We demonstrate the practical utility of the Chordonomicon
dataset for classification and generation tasks, and discuss its potential to
provide valuable insights to the research community. Chord progressions are
unique in their ability to be represented in multiple formats (e.g. text,
graph) and the wealth of information chords convey in given contexts, such as
their harmonic function . These characteristics make the Chordonomicon an ideal
testbed for exploring advanced machine learning techniques, including
transformers, graph machine learning, and hybrid systems that combine knowledge
representation and machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMU: Your Swiss Army Knife for Music Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15573v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15573v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjie Zhao, Zhi Zhong, Zhuoyuan Mao, Shiqi Yang, Wei-Hsiang Liao, Shusuke Takahashi, Hiromi Wakaki, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present OpenMU-Bench, a large-scale benchmark suite for addressing the
data scarcity issue in training multimodal language models to understand music.
To construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new
annotations. OpenMU-Bench also broadens the scope of music understanding by
including lyrics understanding and music tool usage. Using OpenMU-Bench, we
trained our music understanding model, OpenMU, with extensive ablations,
demonstrating that OpenMU outperforms baseline models such as MU-Llama. Both
OpenMU and OpenMU-Bench are open-sourced to facilitate future research in music
understanding and to enhance creative music production efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Resources: https://github.com/sony/openmu</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-26T00:00:00Z">2024-11-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongKey: Keyphrase Extraction for Long Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeovane Honorio Alves, Radu State, Cinthia Obladen de Almendra Freitas, Jean Paul Barddal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era of information overload, manually annotating the vast and growing
corpus of documents and scholarly papers is increasingly impractical. Automated
keyphrase extraction addresses this challenge by identifying representative
terms within texts. However, most existing methods focus on short documents (up
to 512 tokens), leaving a gap in processing long-context documents. In this
paper, we introduce LongKey, a novel framework for extracting keyphrases from
lengthy documents, which uses an encoder-based language model to capture
extended text intricacies. LongKey uses a max-pooling embedder to enhance
keyphrase candidate representation. Validated on the comprehensive LDKP
datasets and six diverse, unseen datasets, LongKey consistently outperforms
existing unsupervised and language model-based keyphrase extraction methods.
Our findings demonstrate LongKey's versatility and superior performance,
marking an advancement in keyphrase extraction for varied text lengths and
domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the 2024 IEEE International Conference
  on Big Data (IEEE BigData 2024). Code available at
  https://github.com/jeohalves/longkey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automating Chapter-Level Classification for Electronic Theses and
  Dissertations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bipasha Banerjee, William A. Ingram, Edward A. Fox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional archival practices for describing electronic theses and
dissertations (ETDs) rely on broad, high-level metadata schemes that fail to
capture the depth, complexity, and interdisciplinary nature of these long
scholarly works. The lack of detailed, chapter-level content descriptions
impedes researchers' ability to locate specific sections or themes, thereby
reducing discoverability and overall accessibility. By providing chapter-level
metadata information, we improve the effectiveness of ETDs as research
resources. This makes it easier for scholars to navigate them efficiently and
extract valuable insights. The absence of such metadata further obstructs
interdisciplinary research by obscuring connections across fields, hindering
new academic discoveries and collaboration. In this paper, we propose a machine
learning and AI-driven solution to automatically categorize ETD chapters. This
solution is intended to improve discoverability and promote understanding of
chapters. Our approach enriches traditional archival practices by providing
context-rich descriptions that facilitate targeted navigation and improved
access. We aim to support interdisciplinary research and make ETDs more
accessible. By providing chapter-level classification labels and using them to
index in our developed prototype system, we make content in ETD chapters more
discoverable and usable for a diverse range of scholarly needs. Implementing
this AI-enhanced approach allows archives to serve researchers better, enabling
efficient access to relevant information and supporting deeper engagement with
ETDs. This will increase the impact of ETDs as research tools, foster
interdisciplinary exploration, and reinforce the role of archives in scholarly
communication within the data-intensive academic landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making History Readable 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bipasha Banerjee, Jennifer Goyne, William A. Ingram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Virginia Tech University Libraries (VTUL) Digital Library Platform (DLP)
hosts digital collections that offer our users access to a wide variety of
documents of historical and cultural importance. These collections are not only
of academic importance but also provide our users with a glance at local
historical events. Our DLP contains collections comprising digital objects
featuring complex layouts, faded imagery, and hard-to-read handwritten text,
which makes providing online access to these materials challenging. To address
these issues, we integrate AI into our DLP workflow and convert the text in the
digital objects into a machine-readable format. To enhance the user experience
with our historical collections, we use custom AI agents for handwriting
recognition, text extraction, and large language models (LLMs) for
summarization. This poster highlights three collections focusing on handwritten
letters, newspapers, and digitized topographic maps. We discuss the challenges
with each collection and detail our approaches to address them. Our proposed
methods aim to enhance the user experience by making the contents in these
collections easier to search and navigate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic AI for Improving Precision in Identifying Contributions to
  Sustainable Development Goals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William A. Ingram, Bipasha Banerjee, Edward A. Fox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As research institutions increasingly commit to supporting the United
Nations' Sustainable Development Goals (SDGs), there is a pressing need to
accurately assess their research output against these goals. Current
approaches, primarily reliant on keyword-based Boolean search queries, conflate
incidental keyword matches with genuine contributions, reducing retrieval
precision and complicating benchmarking efforts. This study investigates the
application of autoregressive Large Language Models (LLMs) as evaluation agents
to identify relevant scholarly contributions to SDG targets in scholarly
publications. Using a dataset of academic abstracts retrieved via SDG-specific
keyword queries, we demonstrate that small, locally-hosted LLMs can
differentiate semantically relevant contributions to SDG targets from documents
retrieved due to incidental keyword matches, addressing the limitations of
traditional methods. By leveraging the contextual understanding of LLMs, this
approach provides a scalable framework for improving SDG-related research
metrics and informing institutional reporting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Structural Dynamics in Retracted and Non-Retracted Author's
  Collaboration Networks: A Quantitative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Sharma, Aanchal Sharma, Jazlyn Jose, Vansh Saini, Raghavraj Sobti, Ziya Uddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retractions undermine the reliability of scientific literature and the
foundation of future research. Analyzing collaboration networks in retracted
papers can identify risk factors, such as recurring co-authors or institutions.
This study compared the network structures of retracted and non-retracted
papers, using data from Retraction Watch and Scopus for 30 authors with
significant retractions. Collaboration networks were constructed, and network
properties analyzed. Retracted networks showed hierarchical and centralized
structures, while non-retracted networks exhibited distributed collaboration
with stronger clustering and connectivity. Statistical tests, including
$t$-tests and Cohen's $d$, revealed significant differences in metrics like
Degree Centrality and Weighted Degree, highlighting distinct structural
dynamics. These insights into retraction-prone collaborations can guide
policies to improve research integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness And Performance In Harmony: Data Debiasing Is All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhua Liu, Wendy Wan Yee Hui, Roy Ka-Wei Lee, Kwan Hui Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness in both machine learning (ML) predictions and human decisions is
critical, with ML models prone to algorithmic and data bias, and human
decisions affected by subjectivity and cognitive bias. This study investigates
fairness using a real-world university admission dataset with 870 profiles,
leveraging three ML models, namely XGB, Bi-LSTM, and KNN. Textual features are
encoded with BERT embeddings. For individual fairness, we assess decision
consistency among experts with varied backgrounds and ML models, using a
consistency score. Results show ML models outperform humans in fairness by
14.08% to 18.79%. For group fairness, we propose a gender-debiasing pipeline
and demonstrate its efficacy in removing gender-specific language without
compromising prediction performance. Post-debiasing, all models maintain or
improve their classification accuracy, validating the hypothesis that fairness
and performance can coexist. Our findings highlight ML's potential to enhance
fairness in admissions while maintaining high accuracy, advocating a hybrid
approach combining human judgement and ML models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Cross-Domain Recommendation with Joint Identifiability of
  User Preference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Du, Zesheng Ye, Bin Guo, Zhiwen Yu, Jia Wu, Jian Yang, Michael Sheng, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent cross-domain recommendation (CDR) studies assume that disentangled
domain-shared and domain-specific user representations can mitigate domain gaps
and facilitate effective knowledge transfer. However, achieving perfect
disentanglement is challenging in practice, because user behaviors in CDR are
highly complex, and the true underlying user preferences cannot be fully
captured through observed user-item interactions alone. Given this
impracticability, we instead propose to model {\it joint identifiability} that
establishes unique correspondence of user representations across domains,
ensuring consistent preference modeling even when user behaviors exhibit shifts
in different domains. To achieve this, we introduce a hierarchical user
preference modeling framework that organizes user representations by the neural
network encoder's depth, allowing separate treatment of shallow and deeper
subspaces. In the shallow subspace, our framework models the interest centroids
for each user within each domain, probabilistically determining the users'
interest belongings and selectively aligning these centroids across domains to
ensure fine-grained consistency in domain-irrelevant features. For deeper
subspace representations, we enforce joint identifiability by decomposing it
into a shared cross-domain stable component and domain-variant components,
linked by a bijective transformation for unique correspondence. Empirical
studies on real-world CDR tasks with varying domain correlations demonstrate
that our method consistently surpasses state-of-the-art, even with weakly
correlated tasks, highlighting the importance of joint identifiability in
achieving robust CDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2D Matryoshka Training for Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Wang, Shengyao Zhuang, Bevan Koopman, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D Matryoshka Training is an advanced embedding representation training
approach designed to train an encoder model simultaneously across various
layer-dimension setups. This method has demonstrated higher effectiveness in
Semantic Text Similarity (STS) tasks over traditional training approaches when
using sub-layers for embeddings. Despite its success, discrepancies exist
between two published implementations, leading to varied comparative results
with baseline models. In this reproducibility study, we implement and evaluate
both versions of 2D Matryoshka Training on STS tasks and extend our analysis to
retrieval tasks. Our findings indicate that while both versions achieve higher
effectiveness than traditional Matryoshka training on sub-dimensions, and
traditional full-sized model training approaches, they do not outperform models
trained separately on specific sub-layer and sub-dimension setups. Moreover,
these results generalize well to retrieval tasks, both in supervised (MSMARCO)
and zero-shot (BEIR) settings. Further explorations of different loss
computations reveals more suitable implementations for retrieval tasks, such as
incorporating full-dimension loss and training on a broader range of target
dimensions. Conversely, some intuitive approaches, such as fixing document
encoders to full model outputs, do not yield improvements. Our reproduction
code is available at https://github.com/ielab/2DMSE-Reproduce.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scholar Name Disambiguation with Search-enhanced LLM Across Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renyu Zhao, Yunxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of scholar name disambiguation is crucial in various real-world
scenarios, including bibliometric-based candidate evaluation for awards,
application material anti-fraud measures, and more. Despite significant
advancements, current methods face limitations due to the complexity of
heterogeneous data, often necessitating extensive human intervention. This
paper proposes a novel approach by leveraging search-enhanced language models
across multiple languages to improve name disambiguation. By utilizing the
powerful query rewriting, intent recognition, and data indexing capabilities of
search engines, our method can gather richer information for distinguishing
between entities and extracting profiles, resulting in a more comprehensive
data dimension. Given the strong cross-language capabilities of large language
models(LLMs), optimizing enhanced retrieval methods with this technology offers
substantial potential for high-efficiency information retrieval and
utilization. Our experiments demonstrate that incorporating local languages
significantly enhances disambiguation performance, particularly for scholars
from diverse geographic regions. This multi-lingual, search-enhanced
methodology offers a promising direction for more efficient and accurate active
scholar name disambiguation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Do Recommendation Models Amplify Popularity Bias? An Analysis from
  the Spectral Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12008v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12008v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyi Lin, Chongming Gao, Jiawei Chen, Sheng Zhou, Binbin Hu, Yan Feng, Chun Chen, Can Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation Systems (RS) are often plagued by popularity bias. When
training a recommendation model on a typically long-tailed dataset, the model
tends to not only inherit this bias but often exacerbate it, resulting in
over-representation of popular items in the recommendation lists. This study
conducts comprehensive empirical and theoretical analyses to expose the root
causes of this phenomenon, yielding two core insights: 1) Item popularity is
memorized in the principal spectrum of the score matrix predicted by the
recommendation model; 2) The dimension collapse phenomenon amplifies the
relative prominence of the principal spectrum, thereby intensifying the
popularity bias. Building on these insights, we propose a novel debiasing
strategy that leverages a spectral norm regularizer to penalize the magnitude
of the principal singular value. We have developed an efficient algorithm to
expedite the calculation of the spectral norm by exploiting the spectral
property of the score matrix. Extensive experiments across seven real-world
datasets and three testing paradigms have been conducted to validate the
superiority of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A multi-language toolkit for the semi-automated checking of research
  outputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02935v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02935v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard J. Preen, Maha Albashir, Simon Davy, Jim Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a free and open source toolkit that supports the
semi-automated checking of research outputs (SACRO) for privacy disclosure
within secure data environments. SACRO is a framework that applies
best-practice principles-based statistical disclosure control (SDC) techniques
on-the-fly as researchers conduct their analyses. SACRO is designed to assist
human checkers rather than seeking to replace them as with current automated
rules-based approaches. The toolkit is composed of a lightweight Python package
that sits over well-known analysis tools that produce outputs such as tables,
plots, and statistical models. This package adds functionality to (i)
automatically identify potentially disclosive outputs against a range of
commonly used disclosure tests; (ii) apply optional disclosure mitigation
strategies as requested; (iii) report reasons for applying SDC; and (iv)
produce simple summary documents trusted research environment staff can use to
streamline their workflow and maintain auditable records. This creates an
explicit change in the dynamics so that SDC is something done with researchers
rather than to them, and enables more efficient communication with checkers. A
graphical user interface supports human checkers by displaying the requested
output and results of the checks in an immediately accessible format,
highlighting identified issues, potential mitigation options, and tracking
decisions made. The major analytical programming languages used by researchers
(Python, R, and Stata) are supported by providing front-end packages that
interface with the core Python back-end. Source code, packages, and
documentation are available under MIT license at https://github.com/AI-SDC/ACRO
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zeng, Ojas Tendolkar, Raymond Baartmans, Qingyun Wu, Lizhong Chen, Huazheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ranking passages by prompting a large language model (LLM) can achieve
promising performance in modern information retrieval (IR) systems. A common
approach to sort the ranking list is by prompting LLMs for a pairwise or
setwise comparison which often relies on sorting algorithms. However,
sorting-based methods require consistent comparisons to correctly sort the
passages, which we show that LLMs often violate. We identify two kinds of
intrinsic inconsistency in LLM-based pairwise comparisons: order inconsistency
which leads to conflicting results when switching the passage order, and
transitive inconsistency which leads to non-transitive triads among all
preference pairs. Our study of these inconsistencies is relevant for
understanding and improving the stability of any ranking scheme based on
relative preferences. In this paper, we propose LLM-RankFusion, an LLM-based
ranking framework that mitigates these inconsistencies and produces a robust
ranking list. LLM-RankFusion mitigates order inconsistency using in-context
learning (ICL) to demonstrate order-agnostic comparisons and calibration to
estimate the underlying preference probability between two passages. We then
address transitive inconsistency by aggregating the ranking results from
multiple rankers. In our experiments, we empirically show that LLM-RankFusion
can significantly reduce inconsistent comparison results, improving the ranking
quality by making the final ranking list more robust. Our code is available at
\href{https://github.com/XHMY/LLM-RankFusion}{https://github.com/XHMY/LLM-RankFusion}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM4DSR: Leveraing Large Language Model for Denoising Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08208v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08208v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Wang, Feng Liu, Changwang Zhang, Jiawei Chen, Yudi Wu, Sheng Zhou, Xingyu Lou, Jun Wang, Yan Feng, Chun Chen, Can Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommenders generate recommendations based on users' historical
interaction sequences. However, in practice, these collected sequences are
often contaminated by noisy interactions, which significantly impairs
recommendation performance. Accurately identifying such noisy interactions
without additional information is particularly challenging due to the absence
of explicit supervisory signals indicating noise. Large Language Models (LLMs),
equipped with extensive open knowledge and semantic reasoning abilities, offer
a promising avenue to bridge this information gap. However, employing LLMs for
denoising in sequential recommendation presents notable challenges: 1) Direct
application of pretrained LLMs may not be competent for the denoising task,
frequently generating nonsensical responses; 2) Even after fine-tuning, the
reliability of LLM outputs remains questionable, especially given the
complexity of the denoising task and the inherent hallucinatory issue of LLMs.
  To tackle these challenges, we propose LLM4DSR, a tailored approach for
denoising sequential recommendation using LLMs. We constructed a
self-supervised fine-tuning task to activate LLMs' capabilities to identify
noisy items and suggest replacements. Furthermore, we developed an uncertainty
estimation module that ensures only high-confidence responses are utilized for
sequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing corrected
sequences to be flexibly applied across various recommendation models.
Extensive experiments validate the superiority of LLM4DSR over existing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Driven Guided Response for Security Operation Centers with Microsoft
  Copilot for Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09017v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09017v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scott Freitas, Jovan Kalajdjieski, Amir Gharib, Robert McCann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Security operation centers contend with a constant stream of security
incidents, ranging from straightforward to highly complex. To address this, we
developed Microsoft Copilot for Security Guided Response (CGR), an
industry-scale ML architecture that guides security analysts across three key
tasks -- (1) investigation, providing essential historical context by
identifying similar incidents; (2) triaging to ascertain the nature of the
incident -- whether it is a true positive, false positive, or benign positive;
and (3) remediation, recommending tailored containment actions. CGR is
integrated into the Microsoft Defender XDR product and deployed worldwide,
generating millions of recommendations across thousands of customers. Our
extensive evaluation, incorporating internal evaluation, collaboration with
security experts, and customer feedback, demonstrates that CGR delivers
high-quality recommendations across all three tasks. We provide a comprehensive
overview of the CGR architecture, setting a precedent as the first
cybersecurity company to openly discuss these capabilities in such depth.
Additionally, we release GUIDE, the largest public collection of real-world
security incidents, spanning 13M evidences across 1M incidents annotated with
ground-truth triage labels by customer security analysts. This dataset
represents the first large-scale cybersecurity resource of its kind, supporting
the development and evaluation of guided response systems and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Positional Attention for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02793v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02793v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Luo, Haibo He, Juan Zhang, Shenghui Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-attention-based networks have achieved remarkable performance in
sequential recommendation tasks. A crucial component of these models is
positional encoding. In this study, we delve into the learned positional
embedding, demonstrating that it often captures the distance between tokens.
Building on this insight, we introduce novel attention models that directly
learn positional relations. Extensive experiments reveal that our proposed
models, \textbf{PARec} and \textbf{FPARec} outperform previous
self-attention-based approaches. The code can be found here:
https://github.com/NetEase-Media/FPARec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterogeneous Graph-based Framework with Disentangled Representations
  Learning for Multi-target Cross Domain Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaopeng Liu, Juan Zhang, Chongqi Ren, Shenghui Xu, Zhaoming Pan, Zhimin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CDR (Cross-Domain Recommendation), i.e., leveraging information from multiple
domains, is a critical solution to data sparsity problem in recommendation
system. The majority of previous research either focused on single-target CDR
(STCDR) by utilizing data from the source domains to improve the model's
performance on the target domain, or applied dual-target CDR (DTCDR) by
integrating data from the source and target domains. In addition, multi-target
CDR (MTCDR) is a generalization of DTCDR, which is able to capture the link
among different domains. In this paper we present HGDR (Heterogeneous
Graph-based Framework with Disentangled Representations Learning), an
end-to-end heterogeneous network architecture where graph convolutional layers
are applied to model relations among different domains, meanwhile utilizes the
idea of disentangling representation for domain-shared and domain-specifc
information. First, a shared heterogeneous graph is generated by gathering
users and items from several domains without any further side information.
Second, we use HGDR to compute disentangled representations for users and items
in all domains. Experiments on real-world datasets and online A/B tests prove
that our proposed model can transmit information among domains effectively and
reach the SOTA performance. The code can be found here:
https://github.com/NetEase-Media/HGCDR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display
  Advertisement Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengxin Li, Yi Li, Yue Liu, Chao Zhou, Yuan Wang, Xiaoxiang Deng, Wei Xue, Dapeng Liu, Lei Xiao, Haijie Gu, Jie Jiang, Hongyan Liu, Biao Qin, Jun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Display advertising provides significant value to advertisers, publishers,
and users. Traditional display advertising systems utilize a multi-stage
architecture consisting of retrieval, coarse ranking, and final ranking.
However, conventional retrieval methods rely on ID-based learning to rank
mechanisms and fail to adequately utilize the content information of ads, which
hampers their ability to provide diverse recommendation lists.
  To address this limitation, we propose leveraging the extensive world
knowledge of LLMs. However, three key challenges arise when attempting to
maximize the effectiveness of LLMs: "How to capture user interests", "How to
bridge the knowledge gap between LLMs and advertising system", and "How to
efficiently deploy LLMs". To overcome these challenges, we introduce a novel
LLM-based framework called LLM Empowered Display ADvertisement REcommender
system (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware
Prompt Engineering introduces multi-faceted knowledge and designs intent-aware
<Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users'
personal interests. (2) The Advertising-Specific Knowledge Alignment
incorporates auxiliary fine-tuning tasks and Direct Preference Optimization
(DPO) to align LLMs with ad semantic and business value. (3) The Efficient
System Deployment deploys LEADRE in an online environment by integrating both
latency-tolerant and latency-sensitive service. Extensive offline experiments
demonstrate the effectiveness of LEADRE and validate the contributions of
individual modules. Online A/B test shows that LEADRE leads to a 1.57% and
1.17% GMV lift for serviced users on WeChat Channels and Moments separately.
LEADRE has been deployed on both platforms, serving tens of billions of
requests each day.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshita Gupta, Tatiana Likhomanenko, Karren Dai Yang, Richard He Bai, Zakaria Aldeneh, Navdeep Jaitly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new task -- generating speech from videos of
people and their transcripts (VTTS) -- to motivate new techniques for
multimodal speech generation. This task generalizes the task of generating
speech from cropped lip videos, and is also more complicated than the task of
generating generic audio clips (e.g., dog barking) from videos and text.
Multilingual versions of the task could lead to new techniques for
cross-lingual dubbing. We also present a decoder-only multimodal model for this
task, which we call Visatronic. This model embeds vision, text and speech
directly into the common subspace of a transformer model and uses an
autoregressive loss to learn a generative model of discretized mel-spectrograms
conditioned on speaker videos and transcripts of their speech. By embedding all
modalities into a common subspace, Visatronic can achieve improved results over
models that use only text or video as input. Further, it presents a much
simpler approach for multimodal speech generation compared to prevailing
approaches which rely on lip-detectors and complicated architectures to fuse
modalities while producing better results. Since the model is flexible enough
to accommodate different ways of ordering inputs as a sequence, we carefully
explore different strategies to better understand the best way to propagate
information to the generative steps. To facilitate further research on VTTS, we
will release (i) our code, (ii) clean transcriptions for the large-scale
VoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTS
incorporating both objective and subjective metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identity-Preserving Text-to-Video Generation by Frequency Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identity-preserving text-to-video (IPT2V) generation aims to create
high-fidelity videos with consistent human identity. It is an important task in
video generation but remains an open problem for generative models. This paper
pushes the technical frontier of IPT2V in two directions that have not been
resolved in literature: (1) A tuning-free pipeline without tedious case-by-case
finetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based
control scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V
model to keep human identity consistent in the generated video. Inspired by
prior findings in frequency analysis of diffusion transformers, it employs
identity-control signals in the frequency domain, where facial features can be
decomposed into low-frequency global features and high-frequency intrinsic
features. First, from a low-frequency perspective, we introduce a global facial
extractor, which encodes reference images and facial key points into a latent
space, generating features enriched with low-frequency information. These
features are then integrated into shallow layers of the network to alleviate
training challenges associated with DiT. Second, from a high-frequency
perspective, we design a local facial extractor to capture high-frequency
details and inject them into transformer blocks, enhancing the model's ability
to preserve fine-grained features. We propose a hierarchical training strategy
to leverage frequency information for identity preservation, transforming a
vanilla pre-trained video generation model into an IPT2V model. Extensive
experiments demonstrate that our frequency-aware heuristic scheme provides an
optimal control solution for DiT-based models. Thanks to this scheme, our
ConsisID generates high-quality, identity-preserving videos, making strides
towards more effective IPT2V.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Walking: A Large-Scale Image-Text Benchmark for Text-based Person
  Anomaly Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyu Yang, Yaxiong Wang, Li Zhu, Zhedong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based person search aims to retrieve specific individuals across camera
networks using natural language descriptions. However, current benchmarks often
exhibit biases towards common actions like walking or standing, neglecting the
critical need for identifying abnormal behaviors in real-world scenarios. To
meet such demands, we propose a new task, text-based person anomaly search,
locating pedestrians engaged in both routine or anomalous activities via text.
To enable the training and evaluation of this new task, we construct a
large-scale image-text Pedestrian Anomaly Behavior (PAB) benchmark, featuring a
broad spectrum of actions, e.g., running, performing, playing soccer, and the
corresponding anomalies, e.g., lying, being hit, and falling of the same
identity. The training set of PAB comprises 1,013,605 synthesized image-text
pairs of both normalities and anomalies, while the test set includes 1,978
real-world image-text pairs. To validate the potential of PAB, we introduce a
cross-modal pose-aware framework, which integrates human pose patterns with
identity-based hard negative pair sampling. Extensive experiments on the
proposed benchmark show that synthetic training data facilitates the
fine-grained behavior retrieval in the real-world test set, while the proposed
pose-aware method further improves the recall@1 by 2.88%. We will release the
dataset, code, and checkpoints to facilitate further research and ensure the
reproducibility of our results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Health AI Developer Foundations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atilla P. Kiraly, Sebastien Baur, Kenneth Philbrick, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Nick George, Fayaz Jamil, Jing Tang, Kai Bailey, Faruk Ahmed, Akshay Goel, Abbi Ward, Lin Yang, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Shravya Shetty, Daniel Golden, Shekoofeh Azizi, David F. Steiner, Yun Liu, Tim Thelin, Rory Pilgrim, Can Kirmizibayrak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust medical Machine Learning (ML) models have the potential to
revolutionize healthcare by accelerating clinical research, improving workflows
and outcomes, and producing novel insights or capabilities. Developing such ML
models from scratch is cost prohibitive and requires substantial compute, data,
and time (e.g., expert labeling). To address these challenges, we introduce
Health AI Developer Foundations (HAI-DEF), a suite of pre-trained,
domain-specific foundation models, tools, and recipes to accelerate building ML
for health applications. The models cover various modalities and domains,
including radiology (X-rays and computed tomography), histopathology,
dermatological imaging, and audio. These models provide domain specific
embeddings that facilitate AI development with less labeled data, shorter
training times, and reduced computational costs compared to traditional
approaches. In addition, we utilize a common interface and style across these
models, and prioritize usability to enable developers to integrate HAI-DEF
efficiently. We present model evaluations across various tasks and conclude
with a discussion of their application and evaluation, covering the importance
of ensuring efficacy, fairness, and equity. Finally, while HAI-DEF and
specifically the foundation models lower the barrier to entry for ML in
healthcare, we emphasize the importance of validation with problem- and
population-specific data for each desired usage setting. This technical report
will be updated over time as more modalities and features are added.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Album Sequencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Herrmann, Dylan R. Ashley, Jürgen Schmidhuber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Album sequencing is a critical part of the album production process.
Recently, a data-driven approach was proposed that sequences general
collections of independent media by extracting the narrative essence of the
items in the collections. While this approach implies an album sequencing
technique, it is not widely accessible to a less technical audience, requiring
advanced knowledge of machine learning techniques to use. To address this, we
introduce a new user-friendly web-based tool that allows a less technical
audience to upload music tracks, execute this technique in one click, and
subsequently presents the result in a clean visualization to the user. To both
increase the number of templates available to the user and address shortcomings
of previous work, we also introduce a new direct transformer-based album
sequencing method. We find that our more direct method outperforms a random
baseline but does not reach the same performance as the narrative essence
approach. Both methods are included in our web-based user interface, and this
-- alongside a full copy of our implementation -- is publicly available at
https://github.com/dylanashley/automatic-album-sequencing
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>presented as a late breaking demo in the 25th International Society
  for Music Information Retrieval Conference; 3 pages in main text + 1 page of
  references, 3 figures in main text; source code available at
  https://github.com/dylanashley/automatic-album-sequencing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WavChat: A <span class="highlight-title">Survey</span> of Spoken Dialogue Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengpeng Ji, Yifu Chen, Minghui Fang, Jialong Zuo, Jingyu Lu, Hanting Wang, Ziyue Jiang, Long Zhou, Shujie Liu, Xize Cheng, Xiaoda Yang, Zehan Wang, Qian Yang, Jian Li, Yidi Jiang, Jingzhen He, Yunfei Chu, Jin Xu, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in spoken dialogue models, exemplified by systems like
GPT-4o, have captured significant attention in the speech domain. Compared to
traditional three-tier cascaded spoken dialogue models that comprise speech
recognition (ASR), large language models (LLMs), and text-to-speech (TTS),
modern spoken dialogue models exhibit greater intelligence. These advanced
spoken dialogue models not only comprehend audio, music, and other
speech-related features, but also capture stylistic and timbral characteristics
in speech. Moreover, they generate high-quality, multi-turn speech responses
with low latency, enabling real-time interaction through simultaneous listening
and speaking capability. Despite the progress in spoken dialogue systems, there
is a lack of comprehensive surveys that systematically organize and analyze
these systems and the underlying technologies. To address this, we have first
compiled existing spoken dialogue systems in the chronological order and
categorized them into the cascaded and end-to-end paradigms. We then provide an
in-depth overview of the core technologies in spoken dialogue models, covering
aspects such as speech representation, training paradigm, streaming, duplex,
and interaction capabilities. Each section discusses the limitations of these
technologies and outlines considerations for future research. Additionally, we
present a thorough review of relevant datasets, evaluation metrics, and
benchmarks from the perspectives of training and evaluating spoken dialogue
systems. We hope this survey will contribute to advancing both academic
research and industrial applications in the field of spoken dialogue systems.
The related material is available at https://github.com/jishengpeng/WavChat.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 papes, working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gotta Hear Them All: Sound Source Aware Vision to Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Guo, Heng Wang, Jianbo Ma, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-to-audio (V2A) synthesis has broad applications in multimedia. Recent
advancements of V2A methods have made it possible to generate relevant audios
from inputs of videos or still images. However, the immersiveness and
expressiveness of the generation are limited. One possible problem is that
existing methods solely rely on the global scene and overlook details of local
sounding objects (i.e., sound sources). To address this issue, we propose a
Sound Source-Aware V2A (SSV2A) generator. SSV2A is able to locally perceive
multimodal sound sources from a scene with visual detection and cross-modality
translation. It then contrastively learns a Cross-Modal Sound Source (CMSS)
Manifold to semantically disambiguate each source. Finally, we attentively mix
their CMSS semantics into a rich audio representation, from which a pretrained
audio generator outputs the sound. To model the CMSS manifold, we curate a
novel single-sound-source visual-audio dataset VGGS3 from VGGSound. We also
design a Sound Source Matching Score to measure localized audio relevance. This
is to our knowledge the first work to address V2A generation at the
sound-source level. Extensive experiments show that SSV2A surpasses
state-of-the-art methods in both generation fidelity and relevance. We further
demonstrate SSV2A's ability to achieve intuitive V2A control by compositing
vision, text, and audio conditions. Our SSV2A generation can be tried and heard
at https://ssv2a.github.io/SSV2A-demo .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures, source code released at
  https://github.com/wguo86/SSV2A</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-25T00:00:00Z">2024-11-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommender Systems for Good (RS4Good): <span class="highlight-title">Survey</span> of Use Cases and a Call
  to Action for Research that Matters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dietmar Jannach, Alan Said, Marko Tkalčič, Markus Zanker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the area of recommender systems, the vast majority of research efforts is
spent on developing increasingly sophisticated recommendation models, also
using increasingly more computational resources. Unfortunately, most of these
research efforts target a very small set of application domains, mostly
e-commerce and media recommendation. Furthermore, many of these models are
never evaluated with users, let alone put into practice. The scientific,
economic and societal value of much of these efforts by scholars therefore
remains largely unclear. To achieve a stronger positive impact resulting from
these efforts, we posit that we as a research community should more often
address use cases where recommender systems contribute to societal good
(RS4Good). In this opinion piece, we first discuss a number of examples where
the use of recommender systems for problems of societal concern has been
successfully explored in the literature. We then proceed by outlining a
paradigmatic shift that is needed to conduct successful RS4Good research, where
the key ingredients are interdisciplinary collaborations and longitudinal
evaluation approaches with humans in the loop.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Data Classification of Historical Music Manuscripts: A Few-Shot
  Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elona Shatri, Daniel Raymond, George Fazekas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the intersection of technology and cultural
preservation by developing a self-supervised learning framework for the
classification of musical symbols in historical manuscripts. Optical Music
Recognition (OMR) plays a vital role in digitising and preserving musical
heritage, but historical documents often lack the labelled data required by
traditional methods. We overcome this challenge by training a neural-based
feature extractor on unlabelled data, enabling effective classification with
minimal samples. Key contributions include optimising crop preprocessing for a
self-supervised Convolutional Neural Network and evaluating classification
methods, including SVM, multilayer perceptrons, and prototypical networks. Our
experiments yield an accuracy of 87.66\%, showcasing the potential of AI-driven
methods to ensure the survival of historical music for future generations
through advanced digital archiving techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, The Sixth IEEE international conference on Image Processing
  Applications and Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What can LLM tell us about cities? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoheng Li, Yaochen Wang, Zhixue Song, Yuqi Huang, Rui Bao, Guanjie Zheng, Zhenhui Jessie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the capabilities of large language models (LLMs) in
providing knowledge about cities and regions on a global scale. We employ two
methods: directly querying the LLM for target variable values and extracting
explicit and implicit features from the LLM correlated with the target
variable. Our experiments reveal that LLMs embed a broad but varying degree of
knowledge across global cities, with ML models trained on LLM-derived features
consistently leading to improved predictive accuracy. Additionally, we observe
that LLMs demonstrate a certain level of knowledge across global cities on all
continents, but it is evident when they lack knowledge, as they tend to
generate generic or random outputs for unfamiliar tasks. These findings suggest
that LLMs can offer new opportunities for data-driven decision-making in the
study of cities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Multi-graph Learning with Neighbor Hierarchical Sifting for
  Semi-supervised Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Ai, Jianbin Li, Ze Wang, Yingying Wei, Tao Meng, Yuntao Shou, Keqin Lib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph contrastive learning has been successfully applied in text
classification due to its remarkable ability for self-supervised node
representation learning. However, explicit graph augmentations may lead to a
loss of semantics in the contrastive views. Secondly, existing methods tend to
overlook edge features and the varying significance of node features during
multi-graph learning. Moreover, the contrastive loss suffer from false
negatives. To address these limitations, we propose a novel method of
contrastive multi-graph learning with neighbor hierarchical sifting for
semi-supervised text classification, namely ConNHS. Specifically, we exploit
core features to form a multi-relational text graph, enhancing semantic
connections among texts. By separating text graphs, we provide diverse views
for contrastive learning. Our approach ensures optimal preservation of the
graph information, minimizing data loss and distortion. Then, we separately
execute relation-aware propagation and cross-graph attention propagation, which
effectively leverages the varying correlations between nodes and edge features
while harmonising the information fusion across graphs. Subsequently, we
present the neighbor hierarchical sifting loss (NHS) to refine the negative
selection. For one thing, following the homophily assumption, NHS masks
first-order neighbors of the anchor and positives from being negatives. For
another, NHS excludes the high-order neighbors analogous to the anchor based on
their similarities. Consequently, it effectively reduces the occurrence of
false negatives, preventing the expansion of the distance between similar
samples in the embedding space. Our experiments on ThuCNews, SogouNews, 20
Newsgroups, and Ohsumed datasets achieved 95.86\%, 97.52\%, 87.43\%, and
70.65\%, which demonstrates competitive results in semi-supervised text
classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stop Playing the Guessing Game! Target-free User Simulation for
  Evaluating Conversational Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunghwan Kim, Tongyoung Kim, Kwangwook Seo, Jinyoung Yeo, Dongha Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent approaches in Conversational Recommender Systems (CRSs) have tried to
simulate real-world users engaging in conversations with CRSs to create more
realistic testing environments that reflect the complexity of human-agent
dialogue. Despite the significant advancements, reliably evaluating the
capability of CRSs to elicit user preferences still faces a significant
challenge. Existing evaluation metrics often rely on target-biased user
simulators that assume users have predefined preferences, leading to
interactions that devolve into simplistic guessing game. These simulators
typically guide the CRS toward specific target items based on fixed attributes,
limiting the dynamic exploration of user preferences and struggling to capture
the evolving nature of real-user interactions. Additionally, current evaluation
metrics are predominantly focused on single-turn recall of target items,
neglecting the intermediate processes of preference elicitation. To address
this, we introduce PEPPER, a novel CRS evaluation protocol with target-free
user simulators constructed from real-user interaction histories and reviews.
PEPPER enables realistic user-CRS dialogues without falling into simplistic
guessing games, allowing users to gradually discover their preferences through
enriched interactions, thereby providing a more accurate and reliable
assessment of the CRS's ability to elicit personal preferences. Furthermore,
PEPPER presents detailed measures for comprehensively evaluating the preference
elicitation capabilities of CRSs, encompassing both quantitative and
qualitative measures that capture four distinct aspects of the preference
elicitation process. Through extensive experiments, we demonstrate the validity
of PEPPER as a simulation environment and conduct a thorough analysis of how
effectively existing CRSs perform in preference elicitation and recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Awareness Gate For Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Hassan Heydari, Arshia Hemmat, Erfan Naman, Afsaneh Fatemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach
to mitigate the limitations of large language models (LLMs) in answering
domain-specific questions. Previous research has predominantly focused on
improving the accuracy and quality of retrieved data chunks to enhance the
overall performance of the generation pipeline. However, despite ongoing
advancements, the critical issue of retrieving irrelevant information -- which
can impair the ability of the model to utilize its internal knowledge
effectively -- has received minimal attention. In this work, we investigate the
impact of retrieving irrelevant information in open-domain question answering,
highlighting its significant detrimental effect on the quality of LLM outputs.
To address this challenge, we propose the Context Awareness Gate (CAG)
architecture, a novel mechanism that dynamically adjusts the LLMs' input prompt
based on whether the user query necessitates external context retrieval.
Additionally, we introduce the Vector Candidates method, a core mathematical
component of CAG that is statistical, LLM-independent, and highly scalable. We
further examine the distributions of relationships between contexts and
questions, presenting a statistical analysis of these distributions. This
analysis can be leveraged to enhance the context retrieval process in Retrieval
Augmented Generation (RAG) systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensemble Learning via Knowledge Transfer for CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honghao Li, Yiwen Zhang, Yi Zhang, Lei Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction plays a critical role in recommender
systems and web searches. While many existing methods utilize ensemble learning
to improve model performance, they typically limit the ensemble to two or three
sub-networks, with little exploration of larger ensembles. In this paper, we
investigate larger ensemble networks and find three inherent limitations in
commonly used ensemble learning method: (1) performance degradation with more
networks; (2) sharp decline and high variance in sub-network performance; (3)
large discrepancies between sub-network and ensemble predictions.
  To simultaneously address the above limitations, this paper investigates
potential solutions from the perspectives of Knowledge Distillation (KD) and
Deep Mutual Learning (DML). Based on the empirical performance of these
methods, we combine them to propose a novel model-agnostic Ensemble Knowledge
Transfer Framework (EKTF). Specifically, we employ the collective
decision-making of the students as an abstract teacher to guide each student
(sub-network) towards more effective learning. Additionally, we encourage
mutual learning among students to enable knowledge acquisition from different
views. To address the issue of balancing the loss hyperparameters, we design a
novel examination mechanism to ensure tailored teaching from teacher-to-student
and selective learning in peer-to-peer. Experimental results on five real-world
datasets demonstrate the effectiveness and compatibility of EKTF. The code,
running logs, and detailed hyperparameter configurations are available at:
https://github.com/salmon1802/EKTF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept Drift Adaptation in Text Stream Mining Settings: A Systematic
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristiano Mesquita Garcia, Ramon Simoes Abilio, Alessandro Lameiras Koerich, Alceu de Souza Britto Jr., Jean Paul Barddal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The society produces textual data online in several ways, e.g., via reviews
and social media posts. Therefore, numerous researchers have been working on
discovering patterns in textual data that can indicate peoples' opinions,
interests, etc. Most tasks regarding natural language processing are addressed
using traditional machine learning methods and static datasets. This setting
can lead to several problems, e.g., outdated datasets and models, which degrade
in performance over time. This is particularly true regarding concept drift, in
which the data distribution changes over time. Furthermore, text streaming
scenarios also exhibit further challenges, such as the high speed at which data
arrives over time. Models for stream scenarios must adhere to the
aforementioned constraints while learning from the stream, thus storing texts
for limited periods and consuming low memory. This study presents a systematic
literature review regarding concept drift adaptation in text stream scenarios.
Considering well-defined criteria, we selected 48 papers published between 2018
and August 2024 to unravel aspects such as text drift categories, detection
types, model update mechanisms, stream mining tasks addressed, and text
representation methods and their update mechanisms. Furthermore, we discussed
drift visualization and simulation and listed real-world datasets used in the
selected papers. Finally, we brought forward a discussion on existing works in
the area, also highlighting open challenges and future research directions for
the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>69 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word4Per: Zero-shot Composed Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16515v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16515v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Fei Su, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for specific person has great social benefits and security value,
and it often involves a combination of visual and textual information.
Conventional person retrieval methods, whether image-based or text-based,
usually fall short in effectively harnessing both types of information, leading
to the loss of accuracy. In this paper, a whole new task called Composed Person
Retrieval (CPR) is proposed to jointly utilize both image and text information
for target person retrieval. However, the supervised CPR requires very costly
manual annotation dataset, while there are currently no available resources. To
mitigate this issue, we firstly introduce the Zero-shot Composed Person
Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the
CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we
propose a two-stage learning framework, Word4Per, where a lightweight Textual
Inversion Network (TINet) and a text-based person retrieval model based on
fine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned
without utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed
Person Retrieval (ITCPR) dataset is built as the benchmark to assess the
performance of the proposed Word4Per framework. Extensive experiments under
both Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR
task, surpassing the comparative methods by over 10\%. The code and ITCPR
dataset will be publicly available at
https://github.com/Delong-liu-bupt/Word4Per.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Any2Any: Incomplete Multimodal Retrieval with Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-han Li, Yunhao Yang, Mohammad Omama, Sandeep Chinchali, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents perceive and interpret their surroundings by integrating
multimodal inputs, such as vision, audio, and LiDAR. These perceptual
modalities support retrieval tasks, such as place recognition in robotics.
However, current multimodal retrieval systems encounter difficulties when parts
of the data are missing due to sensor failures or inaccessibility, such as
silent videos or LiDAR scans lacking RGB information. We propose Any2Any-a
novel retrieval framework that addresses scenarios where both query and
reference instances have incomplete modalities. Unlike previous methods limited
to the imputation of two modalities, Any2Any handles any number of modalities
without training generative models. It calculates pairwise similarities with
cross-modal encoders and employs a two-stage calibration process with conformal
prediction to align the similarities. Any2Any enables effective retrieval
across multimodal datasets, e.g., text-LiDAR and text-time series. It achieves
a Recall@5 of 35% on the KITTI dataset, which is on par with baseline models
with complete modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-han Li, Sandeep P. Chinchali, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal encoders like CLIP excel in tasks such as zero-shot image
classification and cross-modal retrieval. However, they require excessive
training data. We propose canonical similarity analysis (CSA), which uses two
unimodal encoders to replicate multimodal encoders using limited data. CSA maps
unimodal features into a multimodal space, using a new similarity score to
retain only the multimodal information. CSA only involves the inference of
unimodal encoders and a cubic-complexity matrix decomposition, eliminating the
need for extensive GPU-based model training. Experiments show that CSA
outperforms CLIP while requiring $300,000\times$ fewer multimodal data pairs
and $6\times$ fewer unimodal data for ImageNet classification and
misinformative news captions detection. CSA surpasses the state-of-the-art
method to map unimodal features to multimodal features. We also demonstrate the
ability of CSA with modalities beyond image and text, paving the way for future
modality pairs with limited paired multimodal data but abundant unpaired
unimodal data, such as lidar and text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Stance Detection on Social Media: New Directions and
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Genan Dai, Fuqiang Niu, Nan Yin, Xiaomao Fan, Senzhang Wang, Xiaochun Cao, Hu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern digital environments, users frequently express opinions on
contentious topics, providing a wealth of information on prevailing attitudes.
The systematic analysis of these opinions offers valuable insights for
decision-making in various sectors, including marketing and politics. As a
result, stance detection has emerged as a crucial subfield within affective
computing, enabling the automatic detection of user stances in social media
conversations and providing a nuanced understanding of public sentiment on
complex issues. Recent years have seen a surge of research interest in
developing effective stance detection methods, with contributions from multiple
communities, including natural language processing, web science, and social
computing. This paper provides a comprehensive survey of stance detection
techniques on social media, covering task definitions, datasets, approaches,
and future works. We review traditional stance detection models, as well as
state-of-the-art methods based on large language models, and discuss their
strengths and limitations. Our survey highlights the importance of stance
detection in understanding public opinion and sentiment, and identifies gaps in
current research. We conclude by outlining potential future directions for
stance detection on social media, including the need for more robust and
generalizable models, and the importance of addressing emerging challenges such
as multi-modal stance detection and stance detection in low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Item Cold-Start Recommendation with Popularity-Aware
  Meta-Learning <span class="chip">KDD '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Luo, Yuezihan Jiang, Yinjie Jiang, Gaode Chen, Jingchi Wang, Kaigui Bian, Peiyi Li, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of e-commerce and short videos, online recommender systems that
can capture users' interests and update new items in real-time play an
increasingly important role. In both online and offline recommendation, the
cold-start problem due to interaction sparsity has been affecting the
recommendation effect of cold-start items, which is also known as the long-tail
problem of item distribution. Many cold-start scheme based on fine-tuning or
knowledge transferring shows excellent performance on offline recommendation.
Yet, these schemes are infeasible for online recommendation on streaming data
pipelines due to different training method, computational overhead and time
constraints. Inspired by the above questions, we propose a model-agnostic
recommendation algorithm called Popularity-Aware Meta-learning (PAM), to
address the item cold-start problem under streaming data settings. PAM divides
the incoming data into different meta-learning tasks by predefined item
popularity thresholds. The model can distinguish and reweight behavior-related
and content-related features in each task based on their different roles in
different popularity levels, thus adapting to recommendations for cold-start
samples. These task-fixing design significantly reduces additional computation
and storage costs compared to offline methods. Furthermore, PAM also introduced
data augmentation and an additional self-supervised loss specifically designed
for low-popularity tasks, leveraging insights from high-popularity samples.
This approach effectively mitigates the issue of inadequate supervision due to
the scarcity of cold-start samples. Experimental results across multiple public
datasets demonstrate the superiority of our approach over other baseline
methods in addressing cold-start challenges in online streaming data scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, to be published in KDD '25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lens Distortion Encoding System Version 1.0 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Maksymilian Fober
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lens Distortion Encoding System (LDES) allows for a distortion-accurate
workflow, with a seamless interchange of high quality motion picture images
regardless of the lens source. This system is similar in a concept to the
Academy Color Encoding System (ACES), but for distortion. Presented solution is
fully compatible with existing software/plug-in tools for STMapping found in
popular production software like Adobe After Effects or DaVinci Resolve. LDES
utilizes common distortion space and produces single high-quality, animatable
STMap used for direct transformation of one view to another, neglecting the
need of lens-swapping for each shoot. The LDES profile of a lens consist of two
elements; View Map texture, and Footage Map texture, each labeled with the FOV
value. Direct distortion mapping is produced by sampling of the Footage Map
through the View Map. The result; animatable mapping texture, is then used to
sample the footage to a desired distortion. While the Footage Map is specific
to a footage, View Maps can be freely combined/transitioned and animated,
allowing for effects like smooth shift from anamorphic to spherical distortion,
previously impossible to achieve in practice. Presented LDES Version 1.0 uses
common 32-bit STMap format for encoding, supported by most compositing
software, directly or via plug-ins. The difference between standard STMap
workflow and LDES is that it encodes absolute pixel position in the spherical
image model. The main benefit of this approach is the ability to achieve a
similar look of a highly expensive lens using some less expensive equipment in
terms of distortion. It also provides greater artistic control and never seen
before manipulation of footage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully Automatic Deep Learning Pipeline for Whole Slide Image Quality
  Assessment <span class="chip">ALT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Falah Jabar, Lill-Tove Rasmussen Busund, Biagio Ricciuti, Masoud Tafavvoghi, Mette Pøhl, Sigve Andersen, Tom Donnem, David J. Kwiatkowski, Mehrdad Rakaee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the use of deep learning (DL) methods, including
convolutional neural networks (CNNs) and vision transformers (ViTs), has
significantly advanced computational pathology, enhancing both diagnostic
accuracy and efficiency. Hematoxylin and Eosin (H&E) Whole Slide Images (WSI)
plays a crucial role by providing detailed tissue samples for the analysis and
training of DL models. However, WSIs often contain regions with artifacts such
as tissue folds, blurring, as well as non-tissue regions (background), which
can negatively impact DL model performance. These artifacts are diagnostically
irrelevant and can lead to inaccurate results. This paper proposes a fully
automatic supervised DL pipeline for WSI Quality Assessment (WSI-QA) that uses
a fused model combining CNNs and ViTs to detect and exclude WSI regions with
artifacts, ensuring that only qualified WSI regions are used to build DL-based
computational pathology applications. The proposed pipeline employs a
pixel-based segmentation model to classify WSI regions as either qualified or
non-qualified based on the presence of artifacts. The proposed model was
trained on a large and diverse dataset and validated with internal and external
data from various human organs, scanners, and H&E staining procedures.
Quantitative and qualitative evaluations demonstrate the superiority of the
proposed model, which outperforms state-of-the-art methods in WSI artifact
detection. The proposed model consistently achieved over 95% accuracy,
precision, recall, and F1 score across all artifact types. Furthermore, the
WSI-QA pipeline shows strong generalization across different tissue types and
scanning conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS,
  November 25, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting Multimodal LLMs with Self-Reflective Tokens for
  Knowledge-based Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Cocchi, Nicholas Moratelli, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal LLMs (MLLMs) are the natural extension of large language models to
handle multimodal inputs, combining text and image data. They have recently
garnered attention due to their capability to address complex tasks involving
both modalities. However, their effectiveness is limited to the knowledge
acquired during training, which restricts their practical utility. In this
work, we introduce a novel method to enhance the adaptability of MLLMs by
integrating external knowledge sources. Our proposed model, Reflective LLaVA
(ReflectiVA), utilizes reflective tokens to dynamically determine the need for
external knowledge and predict the relevance of information retrieved from an
external database. Tokens are trained following a two-stage two-model training
recipe. This ultimately enables the MLLM to manage external knowledge while
preserving fluency and performance on tasks where external knowledge is not
needed. Through our experiments, we demonstrate the efficacy of ReflectiVA for
knowledge-based visual question answering, highlighting its superior
performance compared to existing methods. Source code and trained models are
publicly available at https://github.com/aimagelab/ReflectiVA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sonic: Shifting Focus to Global Audio Perception in Portrait Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, Qinglin Lu, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of talking face generation mainly explores the intricacies of
synchronizing facial movements and crafting visually appealing,
temporally-coherent animations. However, due to the limited exploration of
global audio perception, current approaches predominantly employ auxiliary
visual and spatial knowledge to stabilize the movements, which often results in
the deterioration of the naturalness and temporal inconsistencies.Considering
the essence of audio-driven animation, the audio signal serves as the ideal and
unique priors to adjust facial expressions and lip movements, without resorting
to interference of any visual signals. Based on this motivation, we propose a
novel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of
global audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,
we disentangle it into intra- and inter-clip audio perception and collaborate
with both aspects to enhance overall perception.For the intra-clip audio
perception, 1). \textbf{Context-enhanced audio learning}, in which long-range
intra-clip temporal audio knowledge is extracted to provide facial expression
and lip motion priors implicitly expressed as the tone and speed of speech. 2).
\textbf{Motion-decoupled controller}, in which the motion of the head and
expression movement are disentangled and independently controlled by
intra-audio clips. Most importantly, for inter-clip audio perception, as a
bridge to connect the intra-clips to achieve the global perception,
\textbf{Time-aware position shift fusion}, in which the global inter-clip audio
information is considered and fused for long-audio inference via through
consecutively time-aware shifted windows. Extensive experiments demonstrate
that the novel audio-driven paradigm outperform existing SOTA methodologies in
terms of video quality, temporally consistency, lip synchronization precision,
and motion diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>refer to our main-page \url{https://jixiaozhong.github.io/Sonic/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ENCLIP: Ensembling and Clustering-Based Contrastive Language-Image
  <span class="highlight-title">Pretrain</span>ing for Fashion Multimodal Search with Limited Data and Low-Quality
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prithviraj Purushottam Naik, Rohit Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal search has revolutionized the fashion industry, providing a
seamless and intuitive way for users to discover and explore fashion items.
Based on their preferences, style, or specific attributes, users can search for
products by combining text and image information. Text-to-image searches enable
users to find visually similar items or describe products using natural
language. This paper presents an innovative approach called ENCLIP, for
enhancing the performance of the Contrastive Language-Image Pretraining (CLIP)
model, specifically in Multimodal Search targeted towards the domain of fashion
intelligence. This method focuses on addressing the challenges posed by limited
data availability and low-quality images. This paper proposes an algorithm that
involves training and ensembling multiple instances of the CLIP model, and
leveraging clustering techniques to group similar images together. The
experimental findings presented in this study provide evidence of the
effectiveness of the methodology. This approach unlocks the potential of CLIP
in the domain of fashion intelligence, where data scarcity and image quality
issues are prevalent. Overall, the ENCLIP method represents a valuable
contribution to the field of fashion intelligence and provides a practical
solution for optimizing the CLIP model in scenarios with limited data and
low-quality images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embedded Blockchains: A Synthesis of Blockchains, Spread Spectrum
  Watermarking, Perceptual Hashing & Digital Signatures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.00951v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.00951v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Blake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce a scheme for detecting manipulated audio and
video. The scheme is a synthesis of blockchains, encrypted spread spectrum
watermarks, perceptual hashing and digital signatures, which we call an
Embedded Blockchain. Within this scheme, we use the blockchain for its data
structure of a cryptographically linked list, cryptographic hashing for
absolute comparisons, perceptual hashing for flexible comparisons, digital
signatures for proof of ownership, and encrypted spread spectrum watermarking
to embed the blockchain into the background noise of the media. So each media
recording has its own unique blockchain, with each block holding information
describing the media segment. The problem of verifying the integrity of the
media is recast to traversing the blockchain, block-by-block, and
segment-by-segment of the media. If any chain is broken, the difference in the
computed and extracted perceptual hash is used to estimate the level of
manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Going in a different direction with this research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Any2Any: Incomplete Multimodal Retrieval with Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-han Li, Yunhao Yang, Mohammad Omama, Sandeep Chinchali, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents perceive and interpret their surroundings by integrating
multimodal inputs, such as vision, audio, and LiDAR. These perceptual
modalities support retrieval tasks, such as place recognition in robotics.
However, current multimodal retrieval systems encounter difficulties when parts
of the data are missing due to sensor failures or inaccessibility, such as
silent videos or LiDAR scans lacking RGB information. We propose Any2Any-a
novel retrieval framework that addresses scenarios where both query and
reference instances have incomplete modalities. Unlike previous methods limited
to the imputation of two modalities, Any2Any handles any number of modalities
without training generative models. It calculates pairwise similarities with
cross-modal encoders and employs a two-stage calibration process with conformal
prediction to align the similarities. Any2Any enables effective retrieval
across multimodal datasets, e.g., text-LiDAR and text-time series. It achieves
a Recall@5 of 35% on the KITTI dataset, which is on par with baseline models
with complete modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Fish Feeding Intensity Assessment in Aquaculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05058v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05058v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Cui, Xubo Liu, Haohe Liu, Zhuangzhuang Du, Tao Chen, Guoping Lian, Daoliang Li, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fish feeding intensity assessment (FFIA) aims to evaluate fish appetite
changes during feeding, which is crucial in industrial aquaculture
applications. Existing FFIA methods are limited by their robustness to noise,
computational complexity, and the lack of public datasets for developing the
models. To address these issues, we first introduce AV-FFIA, a new dataset
containing 27,000 labeled audio and video clips that capture different levels
of fish feeding intensity. Then, we introduce multi-modal approaches for FFIA
by leveraging the models pre-trained on individual modalities and fused with
data fusion methods. We perform benchmark studies of these methods on AV-FFIA,
and demonstrate the advantages of the multi-modal approach over the
single-modality based approach, especially in noisy environments. However,
compared to the methods developed for individual modalities, the multimodal
approaches may involve higher computational costs due to the need for
independent encoders for each modality. To overcome this issue, we further
present a novel unified mixed-modality based method for FFIA, termed as U-FFIA.
U-FFIA is a single model capable of processing audio, visual, or audio-visual
modalities, by leveraging modality dropout during training and knowledge
distillation using the models pre-trained with data from single modality. We
demonstrate that U-FFIA can achieve performance better than or on par with the
state-of-the-art modality-specific FFIA models, with significantly lower
computational overhead, enabling robust and efficient FFIA for improved
aquaculture management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured Multi-Track Accompaniment Arrangement via Style Prior
  Modelling <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16334v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16334v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Zhao, Gus Xia, Ziyu Wang, Ye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of music AI, arranging rich and structured multi-track
accompaniments from a simple lead sheet presents significant challenges. Such
challenges include maintaining track cohesion, ensuring long-term coherence,
and optimizing computational efficiency. In this paper, we introduce a novel
system that leverages prior modelling over disentangled style factors to
address these challenges. Our method presents a two-stage process: initially, a
piano arrangement is derived from the lead sheet by retrieving piano texture
styles; subsequently, a multi-track orchestration is generated by infusing
orchestral function styles into the piano arrangement. Our key design is the
use of vector quantization and a unique multi-stream Transformer to model the
long-term flow of the orchestration style, which enables flexible,
controllable, and structured music generation. Experiments show that by
factorizing the arrangement task into interpretable sub-stages, our approach
enhances generative capacity while improving efficiency. Additionally, our
system supports a variety of music genres and provides style control at
different composition hierarchies. We further show that our system achieves
superior coherence, structure, and overall arrangement quality compared to
existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024; typos addressed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-CrossNet: A complex spectral mapping approach to target speaker
  extraction with cross attention speaker embedding fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Sun, Bo Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Target speaker extraction (TSE) is a technique for isolating a target
speaker's voice from mixed speech using auxiliary features associated with the
target speaker. It is another attempt at addressing the cocktail party problem
and is generally considered to have more practical application prospects than
traditional speech separation methods. Although academic research in this area
has achieved high performance and evaluation scores on public datasets, most
models exhibit significantly reduced performance in real-world noisy or
reverberant conditions. To address this limitation, we propose a novel TSE
model, X-CrossNet, which leverages CrossNet as its backbone. CrossNet is a
speech separation network specifically optimized for challenging noisy and
reverberant environments, achieving state-of-the-art performance in tasks such
as speaker separation under these conditions. Additionally, to enhance the
network's ability to capture and utilize auxiliary features of the target
speaker, we integrate a Cross-Attention mechanism into the global multi-head
self-attention (GMHSA) module within each CrossNet block. This facilitates more
effective integration of target speaker features with mixed speech features.
Experimental results show that our method performs superior separation on the
WSJ0-2mix and WHAMR! datasets, demonstrating strong robustness and stability.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-12-03T05:28:24.409709990Z">
            2024-12-03 05:28:24 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
