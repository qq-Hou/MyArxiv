{"2024-12-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.16208v3","updated":"2024-12-02T18:59:28Z","published":"2024-10-21T17:11:21Z","title":"Compute-Constrained Data Selection","summary":"  Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective. For compute-optimal training, we find that perplexity\nand gradient data selection require training-to-selection model size ratios of\n5x and 10x, respectively.\n","authors":["Junjie Oscar Yin","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.16208v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05677v2","updated":"2024-12-02T18:13:28Z","published":"2024-09-09T14:44:19Z","title":"RIRAG: Regulatory Information Retrieval and Answer Generation","summary":"  Regulatory documents, issued by governmental regulatory bodies, establish\nrules, guidelines, and standards that organizations must adhere to for legal\ncompliance. These documents, characterized by their length, complexity and\nfrequent updates, are challenging to interpret, requiring significant\nallocation of time and expertise on the part of organizations to ensure ongoing\ncompliance. Regulatory Natural Language Processing (RegNLP) is a\nmultidisciplinary field aimed at simplifying access to and interpretation of\nregulatory rules and obligations. We introduce a task of generating\nquestion-passages pairs, where questions are automatically created and paired\nwith relevant regulatory passages, facilitating the development of regulatory\nquestion-answering systems. We create the ObliQA dataset, containing 27,869\nquestions derived from the collection of Abu Dhabi Global Markets (ADGM)\nfinancial regulation documents, design a baseline Regulatory Information\nRetrieval and Answer Generation (RIRAG) system and evaluate it with RePASs, a\nnovel evaluation metric that tests whether generated answers accurately capture\nall relevant obligations while avoiding contradictions.\n","authors":["Tuba Gokhan","Kexin Wang","Iryna Gurevych","Ted Briscoe"],"pdf_url":"https://arxiv.org/pdf/2409.05677v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17593v3","updated":"2024-12-02T17:43:20Z","published":"2024-11-26T17:01:27Z","title":"What Differentiates Educational Literature? A Multimodal Fusion Approach\n  of Transformers and Computational Linguistics","summary":"  The integration of new literature into the English curriculum remains a\nchallenge since educators often lack scalable tools to rapidly evaluate\nreadability and adapt texts for diverse classroom needs. This study proposes to\naddress this gap through a multimodal approach that combines transformer-based\ntext classification with linguistic feature analysis to align texts with UK Key\nStages. Eight state-of-the-art Transformers were fine-tuned on segmented text\ndata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,\n500 deep neural network topologies were searched for the classification of\nlinguistic characteristics, achieving an F1 score of 0.392. The fusion of these\nmodalities shows a significant improvement, with every multimodal approach\noutperforming all unimodal models. In particular, the ELECTRA Transformer fused\nwith the neural network achieved an F1 score of 0.996. Unimodal and multimodal\napproaches are shown to have statistically significant differences in all\nvalidation metrics (accuracy, precision, recall, F1 score) except for inference\ntime. The proposed approach is finally encapsulated in a stakeholder-facing web\napplication, providing non-technical stakeholder access to real-time insights\non text complexity, reading difficulty, curriculum alignment, and\nrecommendations for learning age range. The application empowers data-driven\ndecision making and reduces manual workload by integrating AI-based\nrecommendations into lesson planning for English literature.\n","authors":["Jordan J. Bird"],"pdf_url":"https://arxiv.org/pdf/2411.17593v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19855v2","updated":"2024-12-02T16:39:26Z","published":"2024-11-29T17:10:33Z","title":"Artificial intelligence contribution to translation industry: looking\n  back and forward","summary":"  This study provides a comprehensive analysis of artificial intelligence (AI)\ncontribution to translation industry (ACTI) research, synthesizing it over\nforty-one years from 1980-2024. 13220 articles were retrieved from three\nsources, namely WoS, Scopus, and Lens. We provided two types of analysis, viz.,\nscientometric and thematic, focusing on cluster, subject categories, keywords,\nburstness, centrality and research centers as for the former. For the latter,\nwe thematically review 18 articles, selected purposefully from the articles\ninvolved, centering on purpose, approach, findings, and contribution to ACTI\nfuture directions. The findings reveal that in the past AI contribution to\ntranslation industry was not rigorous, resulting in rule-based machine\ntranslation and statistical machine translation whose output was not\nsatisfactory. However, the more AI develops, the more machine translation\ndevelops, incorporating Neural Networking Algorithms and (Deep) Language\nLearning Models like ChatGPT whose translation output has developed\nconsiderably. However, much rigorous research is still needed to overcome\nseveral problems encountering translation industry, specifically concerning\nlow-source languages, multi-dialectical and free word order languages, and\ncultural and religious registers.\n","authors":["Mohammed Q. Shormani"],"pdf_url":"https://arxiv.org/pdf/2411.19855v2.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.19839v3","updated":"2024-12-02T16:27:16Z","published":"2024-09-30T00:41:51Z","title":"ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities","summary":"  Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$<0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.\n","authors":["Ezra Karger","Houtan Bastani","Chen Yueh-Han","Zachary Jacobs","Danny Halawi","Fred Zhang","Philip E. Tetlock"],"pdf_url":"https://arxiv.org/pdf/2409.19839v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17607v2","updated":"2024-12-02T16:13:24Z","published":"2024-11-26T17:19:09Z","title":"Scaling Speech-Text Pre-training with Synthetic Interleaved Data","summary":"  Speech language models (SpeechLMs) accept speech input and produce speech\noutput, allowing for more natural human-computer interaction compared to\ntext-based large language models (LLMs). Traditional approaches for developing\nSpeechLMs are constrained by the limited availability of unsupervised speech\ndata and parallel speech-text data, which are significantly less abundant than\ntext pre-training data, thereby limiting their scalability as LLMs. We propose\na novel approach to scaling speech-text pre-training by leveraging large-scale\nsynthetic interleaved data derived from text corpora, eliminating the need for\nparallel speech-text datasets. Our method efficiently constructs speech-text\ninterleaved data by sampling text spans from existing text corpora and\nsynthesizing corresponding speech spans using a text-to-token model, bypassing\nthe need to generate actual speech. We also employ a supervised speech\ntokenizer derived from an automatic speech recognition (ASR) model by\nincorporating a vector-quantized bottleneck into the encoder. This supervised\ntraining approach results in discrete speech tokens with strong semantic\npreservation even at lower frame rates (e.g. 12.5Hz), while still maintaining\nspeech reconstruction quality. Starting from a pre-trained language model and\nscaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved\nspeech-text data), we achieve state-of-the-art performance in speech language\nmodeling and spoken question answering, improving performance on spoken\nquestions tasks from the previous SOTA of 13% (Moshi) to 31%. We further\ndemonstrate that by fine-tuning the pre-trained model with speech dialogue\ndata, we can develop an end-to-end spoken chatbot that achieves competitive\nperformance comparable to existing baselines in both conversational abilities\nand speech quality, even operating exclusively in the speech domain.\n","authors":["Aohan Zeng","Zhengxiao Du","Mingdao Liu","Lei Zhang","Shengmin Jiang","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2411.17607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12850v2","updated":"2024-12-02T15:46:35Z","published":"2024-07-08T09:50:49Z","title":"Limits to Predicting Online Speech Using Large Language Models","summary":"  We study the predictability of online speech on social media, and whether\npredictability improves with information outside a user's own posts. Recent\ntheoretical results suggest that posts from a user's social circle are as\npredictive of the user's future posts as that of the user's past posts.\nMotivated by the success of large language models, we empirically test this\nhypothesis. We define predictability as a measure of the model's uncertainty,\ni.e., its negative log-likelihood on future tokens given context. As the basis\nof our study, we collect 10M tweets for ``tweet-tuning'' base models and a\nfurther 6.25M posts from more than five thousand X (previously Twitter) users\nand their peers. Across four large language models ranging in size from 1.5\nbillion to 70 billion parameters, we find that predicting a user's posts from\ntheir peers' posts performs poorly. Moreover, the value of the user's own posts\nfor prediction is consistently higher than that of their peers'. We extend our\ninvestigation with a detailed analysis on what's learned in-context and the\nrobustness of our findings. From context, base models learn to correctly\npredict @-mentions and hashtags. Moreover, our results replicate if instead of\nprompting the model with additional context, we finetune on it. Across the\nboard, we find that predicting the posts of individual users remains hard.\n","authors":["Mina Remeli","Moritz Hardt","Robert C. Williamson"],"pdf_url":"https://arxiv.org/pdf/2407.12850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10886v2","updated":"2024-12-02T15:40:45Z","published":"2024-02-16T18:43:10Z","title":"Reviewer2: Optimizing Review Generation Through Prompt Generation","summary":"  Recent developments in LLMs offer new opportunities for assisting authors in\nimproving their work. In this paper, we envision a use case where authors can\nreceive LLM-generated reviews that uncover weak points in the current draft.\nWhile initial methods for automated review generation already exist, these\nmethods tend to produce reviews that lack detail, and they do not cover the\nrange of opinions that human reviewers produce. To address this shortcoming, we\npropose an efficient two-stage review generation framework called Reviewer2.\nUnlike prior work, this approach explicitly models the distribution of possible\naspects that the review may address. We show that this leads to more detailed\nreviews that better cover the range of aspects that human reviewers identify in\nthe draft. As part of the research, we generate a large-scale review dataset of\n27k papers and 99k reviews that we annotate with aspect prompts, which we make\navailable as a resource for future research.\n","authors":["Zhaolin Gao","Kianté Brantley","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2402.10886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06562v2","updated":"2024-12-02T15:32:41Z","published":"2023-12-11T17:46:44Z","title":"On Meta-Prompting","summary":"  Modern generative language models are capable of interpreting input strings\nas instructions, or prompts, and carry out tasks based on them. Many approaches\nto prompting and pre-training these models involve the automated generation of\nthese prompts: meta-prompting, or prompting to obtain prompts. We propose a\ntheoretical framework based on category theory to generalize and describe them.\nThis framework is flexible enough to account for stochasticity, and allows us\nto obtain formal results around task agnosticity and equivalence of various\nmeta-prompting approaches. Experimentally, we test our framework in two active\nareas of model research: creativity and ideation. We find that user preference\nstrongly favors (p < 0.01) the prompts generated under meta-prompting, as well\nas their corresponding outputs, over a series of hardcoded baseline prompts\nthat include the original task definition. Using our framework, we argue that\nmeta-prompting is more effective than basic prompting at generating desirable\noutputs.\n","authors":["Adrian de Wynter","Xun Wang","Qilong Gu","Si-Qing Chen"],"pdf_url":"https://arxiv.org/pdf/2312.06562v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.13730v2","updated":"2024-12-02T15:11:23Z","published":"2024-09-10T01:20:26Z","title":"VisScience: An Extensive Benchmark for Evaluating K12 Educational\n  Multi-modal Scientific Reasoning","summary":"  Multi-modal large language models (MLLMs) have demonstrated promising\ncapabilities across various tasks by integrating textual and visual information\nto achieve visual understanding in complex scenarios. Despite the availability\nof several benchmarks aims to evaluating MLLMs in tasks from visual question\nanswering to complex problem-solving, most focus predominantly on mathematics\nor general visual understanding tasks. This reveals a critical gap in current\nbenchmarks, which often overlook the inclusion of other key scientific\ndisciplines such as physics and chemistry. To address this gap, we meticulously\nconstruct a comprehensive benchmark, named VisScience, which is utilized to\nassess the multi-modal scientific reasoning across the three disciplines of\nmathematics, physics, and chemistry. This benchmark comprises 3,000 questions\ndrawn from K12 education - spanning elementary school through high school -\nequally distributed across three disciplines, with 1,000 questions per\ndiscipline. The questions within VisScience span 21 distinct subjects and are\ncategorized into five difficulty levels, offering a broad spectrum of topics\nwithin each discipline. With VisScience, we present a detailed evaluation of\nthe performance of 25 representative MLLMs in scientific reasoning.\nExperimental results demonstrate that closed-source MLLMs generally outperform\nopen-source models. The best performance observed include a 53.4\\% accuracy in\nmathematics by Claude3.5-Sonnet, 38.2\\% in physics by GPT-4o, and 47.0\\% in\nchemistry by Gemini-1.5-Pro. These results underscore the strengths and\nlimitations of MLLMs, suggesting areas for future improvement and highlighting\nthe importance of developing models that can effectively handle the diverse\ndemands of multi-modal scientific reasoning.\n","authors":["Zhihuan Jiang","Zhen Yang","Jinhao Chen","Zhengxiao Du","Weihan Wang","Bin Xu","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2409.13730v2.pdf","comment":"89 pages, 70 figures"},{"id":"http://arxiv.org/abs/2409.13729v2","updated":"2024-12-02T14:59:08Z","published":"2024-09-10T01:20:22Z","title":"MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large\n  Language Model","summary":"  Large language models (LLMs) have demonstrated significant capabilities in\nmathematical reasoning, particularly with text-based mathematical problems.\nHowever, current multi-modal large language models (MLLMs), especially those\nspecialized in mathematics, tend to focus predominantly on solving geometric\nproblems but ignore the diversity of visual information available in other\nareas of mathematics. Moreover, the geometric information for these specialized\nmathematical MLLMs is derived from several public datasets, which are typically\nlimited in diversity and complexity. To address these limitations, we aim to\nconstruct a fine-tuning dataset named MathVL, and develop a series of\nspecialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised\nFine-Tuning (SFT) on MathVL with various parameter-scale backbones. To\nextensively evaluate the effectiveness of MathGLM-Vision, we conduct\nexperiments on several public benchmarks and our curated MathVL-test consisting\nof 2,000 problems. Experimental results demonstrate that MathGLM-Vision\nachieves significant improvements compared with some existing models, including\nbackbone models and open-source mathematical MLLMs. These findings indicate the\nimportance of diversity dataset in enhancing the mathematical reasoning\nabilities of MLLMs.\n","authors":["Zhen Yang","Jinhao Chen","Zhengxiao Du","Wenmeng Yu","Weihan Wang","Wenyi Hong","Zhihuan Jiang","Bin Xu","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2409.13729v2.pdf","comment":"30 pages,19 figures"},{"id":"http://arxiv.org/abs/2411.19655v2","updated":"2024-12-02T14:28:07Z","published":"2024-11-29T12:21:15Z","title":"Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis","summary":"  After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field.\n","authors":["Alessandro Scirè","Andrei Stefan Bejgu","Simone Tedeschi","Karim Ghonim","Federico Martelli","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2411.19655v2.pdf","comment":"15 pages. To be submitted to CL journal"},{"id":"http://arxiv.org/abs/2308.00802v4","updated":"2024-12-02T13:33:17Z","published":"2023-08-01T19:34:18Z","title":"GRDD: A Dataset for Greek Dialectal NLP","summary":"  In this paper, we present a dataset for the computational study of a number\nof Modern Greek dialects. It consists of raw text data from four dialects of\nModern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset is\nof considerable size, albeit imbalanced, and presents the first attempt to\ncreate large scale dialectal resources of this type for Modern Greek dialects.\nWe then use the dataset to perform dialect idefntification. We experiment with\ntraditional ML algorithms, as well as simple DL architectures. The results show\nvery good performance on the task, potentially revealing that the dialects in\nquestion have distinct enough characteristics allowing even simple ML models to\nperform well on the task. Error analysis is performed for the top performing\nalgorithms showing that in a number of cases the errors are due to insufficient\ndataset cleaning.\n","authors":["Stergios Chatzikyriakidis","Chatrine Qwaider","Ilias Kolokousis","Christina Koula","Dimitris Papadakis","Efthymia Sakellariou"],"pdf_url":"https://arxiv.org/pdf/2308.00802v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07123v2","updated":"2024-12-02T13:04:18Z","published":"2024-09-11T09:21:20Z","title":"Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem","summary":"  Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German.\n","authors":["Qianli Wang","Tatiana Anikina","Nils Feldhus","Simon Ostermann","Sebastian Möller","Vera Schmitt"],"pdf_url":"https://arxiv.org/pdf/2409.07123v2.pdf","comment":"Accepted at COLING 2025; long paper"},{"id":"http://arxiv.org/abs/2411.02272v4","updated":"2024-12-02T12:36:30Z","published":"2024-11-04T17:03:55Z","title":"Combining Induction and Transduction for Abstract Reasoning","summary":"  When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC by training neural models for induction (inferring latent\nfunctions) and transduction (directly predicting the test output for a given\ntest input). We train on synthetically generated variations of Python programs\nthat solve ARC training tasks. We find inductive and transductive models solve\ndifferent kinds of test problems, despite having the same training problems and\nsharing the same neural architecture: Inductive program synthesis excels at\nprecise computations, and at composing multiple concepts, while transduction\nsucceeds on fuzzier perceptual concepts. Ensembling them approaches human-level\nperformance on ARC.\n","authors":["Wen-Ding Li","Keya Hu","Carter Larsen","Yuqing Wu","Simon Alford","Caleb Woo","Spencer M. Dunn","Hao Tang","Michelangelo Naim","Dat Nguyen","Wei-Long Zheng","Zenna Tavares","Yewen Pu","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2411.02272v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07818v5","updated":"2024-12-02T12:29:47Z","published":"2024-02-12T17:24:15Z","title":"Differentially Private Zeroth-Order Methods for Scalable Large Language\n  Model Finetuning","summary":"  Fine-tuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy\nconcerns, differentially private (DP) fine-tuning of pretrained LLMs has been\nwidely used to safeguarding the privacy of task-specific datasets. Lying at the\ndesign core of DP LLM fine-tuning methods is the satisfactory tradeoff among\nprivacy, utility, and scalability. Most existing methods build upon the seminal\nwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,\nDP-SGD-based fine-tuning methods are unfortunately limited by the inherent\ninefficiency of SGD.\n  In this paper, we investigate the potential of DP zeroth-order methods for\nLLM pretraining, which avoids the scalability bottleneck of SGD by\napproximating the gradient with the more efficient zeroth-order gradient.\nRather than treating the zeroth-order method as a drop-in replacement for SGD,\nthis paper presents a comprehensive study both theoretically and empirically.\nFirst, we propose the stagewise DP zeroth-order method (DP-ZOSO) that\ndynamically schedules key hyperparameters. This design is grounded on the\nsynergy between DP random perturbation and the gradient approximation error of\nthe zeroth-order method, and its effect on fine-tuning trajectory.\n  We provide theoretical analysis for both proposed methods. We conduct\nextensive empirical analysis on both encoder-only masked language model and\ndecoder-only autoregressive language model, achieving impressive results in\nterms of scalability and utility regardless of the class of tasks (compared\nwith DPZero, DP-ZOPO improves $4.5\\%$ on SST-5, $5.5\\%$ on MNLI with\nRoBERTa-Large and 9.2\\% on CB, 3.9\\% on BoolQ with OPT-2.7b when $\\epsilon=4$,\ndemonstrates more significant enhancement in performance on more complicated\ntasks).\n","authors":["Z Liu","J Lou","W Bao","Y Hu","B Li","Z Qin","K Ren"],"pdf_url":"https://arxiv.org/pdf/2402.07818v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06892v2","updated":"2024-12-02T11:24:20Z","published":"2024-03-11T16:48:25Z","title":"Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head","summary":"  End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}\n","authors":["Tiancheng Zhao","Peng Liu","Xuan He","Lu Zhang","Kyusong Lee"],"pdf_url":"https://arxiv.org/pdf/2403.06892v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2411.14708v2","updated":"2024-12-02T10:52:21Z","published":"2024-11-22T03:33:51Z","title":"Understanding LLM Embeddings for Regression","summary":"  With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.\n","authors":["Eric Tang","Bangding Yang","Xingyou Song"],"pdf_url":"https://arxiv.org/pdf/2411.14708v2.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.01432v5","updated":"2024-12-02T10:48:36Z","published":"2024-03-03T08:07:55Z","title":"Fine Tuning vs. Retrieval Augmented Generation for Less Popular\n  Knowledge","summary":"  Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting\nstrong performance across diverse tasks and domains. However, it has been\nobserved that the performance diminishes when dealing with less-popular or\nlow-frequency concepts and entities, for example in domain specific\napplications. The two prominent approaches to enhance the performance of LMs on\nlow-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning\n(FT) over synthetic data. This paper explores and evaluates the impact of RAG\nand FT on customizing LMs in handling low-frequency entities on question\nanswering tasks. We conduct extensive experiments on twelve LMs of varying size\nand type and different fine tuning, data augmentation, and retrieval models.\nOur findings indicate that while FT boosts the performance across entities of\nvarying popularity, RAG surpasses FT by a large margin particularly for least\npopular factual knowledge. Additionally, the success of both RAG and FT\napproaches is amplified by improving retrieval and data augmentation\ntechniques. Fine tuning, while beneficial for small LMs, requires extensive\nresources. To address this issue, we propose the new Stimulus RAG approach that\nsurpasses the effectiveness of fine tuning based approaches, thereby\neliminating the need for the costly data augmentation and fine tuning step for\nenriching LMs with less popular factual knowledge. The code is available at\n\\url{https://github.com/informagi/RAGvsFT}.\n","authors":["Heydar Soudani","Evangelos Kanoulas","Faegheh Hasibi"],"pdf_url":"https://arxiv.org/pdf/2403.01432v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19211v2","updated":"2024-12-02T10:44:08Z","published":"2024-03-28T08:19:33Z","title":"Dual-Personalizing Adapter for Federated Foundation Models","summary":"  Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\ndiverse instruction data. Notably, federated foundation models (FedFM) emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to FedFM for better user preferences alignment.\nHowever, a critical gap in existing research is the neglect of test-time\ndistribution shifts in real-world applications, and conventional methods for\ntest-time distribution shifts in personalized FL are less effective for FedFM\ndue to their failure to adapt to complex distribution shift scenarios and the\nrequirement to train all parameters. To bridge this gap, we refine the setting\nin FedFM, termed test-time personalization, which aims to learn personalized\nfederated foundation models on clients while effectively handling test-time\ndistribution shifts simultaneously. To address challenges in this setting, we\nexplore a simple yet effective solution, a Federated Dual-Personalizing Adapter\n(FedDPA) architecture. By co-working with a foundation model, a global adapter\nand a local adapter jointly tackle the test-time distribution shifts and\nclient-specific personalization. Additionally, we introduce an instance-wise\ndynamic weighting mechanism that dynamically integrates the global and local\nadapters for each test instance during inference, facilitating effective\ntest-time personalization. The effectiveness of the proposed method has been\nevaluated on benchmark datasets across different NLP tasks.\n","authors":["Yiyuan Yang","Guodong Long","Tao Shen","Jing Jiang","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2403.19211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14212v3","updated":"2024-12-02T10:30:50Z","published":"2024-01-25T14:53:30Z","title":"Explicitly Representing Syntax Improves Sentence-to-layout Prediction of\n  Unexpected Situations","summary":"  Recognizing visual entities in a natural language sentence and arranging them\nin a 2D spatial layout require a compositional understanding of language and\nspace. This task of layout prediction is valuable in text-to-image synthesis as\nit allows localized and controlled in-painting of the image. In this\ncomparative study it is shown that we can predict layouts from language\nrepresentations that implicitly or explicitly encode sentence syntax, if the\nsentences mention similar entity-relationships to the ones seen during\ntraining. To test compositional understanding, we collect a test set of\ngrammatically correct sentences and layouts describing compositions of entities\nand relations that unlikely have been seen during training. Performance on this\ntest set substantially drops, showing that current models rely on correlations\nin the training data and have difficulties in understanding the structure of\nthe input sentences. We propose a novel structural loss function that better\nenforces the syntactic structure of the input sentence and show large\nperformance gains in the task of 2D spatial layout prediction conditioned on\ntext. The loss has the potential to be used in other generation tasks where a\ntree-like structure underlies the conditioning modality. Code, trained models\nand the USCOCO evaluation set are available via github.\n","authors":["Wolf Nuyts","Ruben Cartuyvels","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2401.14212v3.pdf","comment":"Published in TACL"},{"id":"http://arxiv.org/abs/2409.06067v2","updated":"2024-12-02T10:18:38Z","published":"2024-09-09T21:04:16Z","title":"MLLM-LLaVA-FL: Multimodal Large Language Model Assisted Federated\n  Learning","summary":"  Previous studies on federated learning (FL) often encounter performance\ndegradation due to data heterogeneity among different clients. In light of the\nrecent advances in multimodal large language models (MLLMs), such as GPT-4v and\nLLaVA, which demonstrate their exceptional proficiency in multimodal tasks,\nsuch as image captioning and multimodal question answering. We introduce a\nnovel federated learning framework, named Multimodal Large Language Model\nAssisted Federated Learning (MLLM-LLaVA-FL), which employs powerful MLLMs at\nthe server end to address the heterogeneous and long-tailed challenges. Owing\nto the advanced cross-modality representation capabilities and the extensive\nopen-vocabulary prior knowledge of MLLMs, our framework is adept at harnessing\nthe extensive, yet previously underexploited, open-source data accessible from\nwebsites and powerful server-side computational resources. Hence, the\nMLLM-LLaVA-FL not only enhances the performance but also avoids increasing the\nrisk of privacy leakage and the computational burden on local devices,\ndistinguishing it from prior methodologies. Our framework has three key stages.\nInitially, we conduct global visual-text pretraining of the model. This\npretraining is facilitated by utilizing the extensive open-source data\navailable online, with the assistance of MLLMs. Subsequently, the pretrained\nmodel is distributed among various clients for local training. Finally, once\nthe locally trained models are transmitted back to the server, a global\nalignment is carried out under the supervision of MLLMs to further enhance the\nperformance. Experimental evaluations on established benchmarks, show that our\nframework delivers promising performance in the typical scenarios with data\nheterogeneity and long-tail distribution across different clients in FL.\n","authors":["Jianyi Zhang","Hao Frank Yang","Ang Li","Xin Guo","Pu Wang","Haiming Wang","Yiran Chen","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2409.06067v2.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2407.04125v2","updated":"2024-12-02T09:42:24Z","published":"2024-07-04T18:54:30Z","title":"Query-Guided Self-Supervised Summarization of Nursing Notes","summary":"  Nursing notes, an important part of Electronic Health Records (EHRs), track a\npatient's health during a care episode. Summarizing key information in nursing\nnotes can help clinicians quickly understand patients' conditions. However,\nexisting summarization methods in the clinical setting, especially abstractive\nmethods, have overlooked nursing notes and require reference summaries for\ntraining. We introduce QGSumm, a novel query-guided self-supervised domain\nadaptation approach for abstractive nursing note summarization. The method uses\npatient-related clinical queries for guidance, and hence does not need\nreference summaries for training. Through automatic experiments and manual\nevaluation by an expert clinician, we study our approach and other\nstate-of-the-art Large Language Models (LLMs) for nursing note summarization.\nOur experiments show: 1) GPT-4 is competitive in maintaining information in the\noriginal nursing notes, 2) QGSumm can generate high-quality summaries with a\ngood balance between recall of the original content and hallucination rate\nlower than other top methods. Ultimately, our work offers a new perspective on\nconditional text summarization, tailored to clinical applications.\n","authors":["Ya Gao","Hans Moen","Saila Koivusalo","Miika Koskinen","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2407.04125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19916v3","updated":"2024-12-02T08:56:26Z","published":"2024-09-30T03:37:10Z","title":"Deep Learning and Machine Learning, Advancing Big Data Analytics and\n  Management: Object-Oriented Programming","summary":"  Object-Oriented Programming (OOP) has become a crucial paradigm for managing\nthe growing complexity of modern software systems, particularly in fields like\nmachine learning, deep learning, large language models (LLM), and data\nanalytics. This work provides a comprehensive introduction to the integration\nof OOP techniques within these domains, with a focus on improving code\nmodularity, maintainability, and scalability. We begin by outlining the\nevolution of computing and the rise of OOP, followed by an in-depth discussion\nof key OOP principles such as encapsulation, inheritance, polymorphism, and\nabstraction. The practical application of these principles is demonstrated\nusing Python, a widely adopted language in AI and data science. Furthermore, we\nexamine how design patterns and modular programming can be employed to enhance\nthe structure and efficiency of machine learning systems. In subsequent\nsections, we apply these OOP concepts to real-world AI tasks, including the\nencapsulation of preprocessing workflows, machine learning model training, and\nevaluation. Detailed examples illustrate how OOP can be used to build reusable,\nscalable machine learning systems while maintaining code clarity and reducing\nredundancy.This work is intended to serve as a bridge for both beginners and\nexperienced developers, equipping them with the necessary knowledge to apply\nOOP methodologies in AI-driven projects, ultimately fostering the development\nof more robust and maintainable systems.\n","authors":["Tianyang Wang","Ziqian Bi","Keyu Chen","Jiawei Xu","Qian Niu","Junyu Liu","Benji Peng","Ming Li","Sen Zhang","Xuanhe Pan","Jinlang Wang","Pohsun Feng","Caitlyn Heqi Yin","Yizhu Wen","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2409.19916v3.pdf","comment":"49pages"},{"id":"http://arxiv.org/abs/2409.09318v2","updated":"2024-12-02T08:51:09Z","published":"2024-09-14T05:31:29Z","title":"ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language\n  Models","summary":"  Hallucination poses a persistent challenge for multimodal large language\nmodels (MLLMs). However, existing benchmarks for evaluating hallucinations are\ngenerally static, which may overlook the potential risk of data contamination.\nTo address this issue, we propose ODE, an open-set, dynamic protocol designed\nto evaluate object hallucinations in MLLMs at both the existence and attribute\nlevels. ODE employs a graph-based structure to represent real-world object\nconcepts, their attributes, and the distributional associations between them.\nThis structure facilitates the extraction of concept combinations based on\ndiverse distributional criteria, generating varied samples for structured\nqueries that evaluate hallucinations in both generative and discriminative\ntasks. Through the generation of new samples, dynamic concept combinations, and\nvaried distribution frequencies, ODE mitigates the risk of data contamination\nand broadens the scope of evaluation. This protocol is applicable to both\ngeneral and specialized scenarios, including those with limited data.\nExperimental results demonstrate the effectiveness of our protocol, revealing\nthat MLLMs exhibit higher hallucination rates when evaluated with ODE-generated\nsamples, which indicates potential data contamination. Furthermore, these\ngenerated samples aid in analyzing hallucination patterns and fine-tuning\nmodels, offering an effective approach to mitigating hallucinations in MLLMs.\n","authors":["Yahan Tu","Rui Hu","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2409.09318v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01077v2","updated":"2024-12-02T08:47:24Z","published":"2024-04-01T12:19:08Z","title":"Efficient Prompting Methods for Large Language Models: A Survey","summary":"  Prompting is a mainstream paradigm for adapting large language models to\nspecific natural language processing tasks without modifying internal\nparameters. Therefore, detailed supplementary knowledge needs to be integrated\ninto external prompts, which inevitably brings extra human efforts and\ncomputational burdens for practical applications. As an effective solution to\nmitigate resource consumption, Efficient Prompting Methods have attracted a\nwide range of attention. We provide mathematical expressions at a high level to\ndeeply discuss Automatic Prompt Engineering for different prompt components and\nPrompt Compression in continuous and discrete spaces. Finally, we highlight\npromising future directions to inspire researchers interested in this field.\n","authors":["Kaiyan Chang","Songcheng Xu","Chenglong Wang","Yingfeng Luo","Xiaoqian Liu","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.01077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01380v2","updated":"2024-12-02T08:43:16Z","published":"2024-10-02T09:49:45Z","title":"Knowledge Entropy Decay during Language Model Pretraining Hinders New\n  Knowledge Acquisition","summary":"  In this work, we investigate how a model's tendency to broadly integrate its\nparametric knowledge evolves throughout pretraining, and how this behavior\naffects overall performance, particularly in terms of knowledge acquisition and\nforgetting. We introduce the concept of knowledge entropy, which quantifies the\nrange of memory sources the model engages with; high knowledge entropy\nindicates that the model utilizes a wide range of memory sources, while low\nknowledge entropy suggests reliance on specific sources with greater certainty.\nOur analysis reveals a consistent decline in knowledge entropy as pretraining\nadvances. We also find that the decline is closely associated with a reduction\nin the model's ability to acquire and retain knowledge, leading us to conclude\nthat diminishing knowledge entropy (smaller number of active memory sources)\nimpairs the model's knowledge acquisition and retention capabilities. We find\nfurther support for this by demonstrating that increasing the activity of\ninactive memory sources enhances the model's capacity for knowledge acquisition\nand retention.\n","authors":["Jiyeon Kim","Hyunji Lee","Hyowon Cho","Joel Jang","Hyeonbin Hwang","Seungpil Won","Youbin Ahn","Dohaeng Lee","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2410.01380v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02408v2","updated":"2024-12-02T07:47:00Z","published":"2024-02-04T08:57:54Z","title":"GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large\n  Language Model","summary":"  Despite the rapid progress of large language models (LLMs), their task\nperformance remains sensitive to prompt design. Recent studies have explored\nleveraging the LLM itself as an optimizer to identify optimal prompts that\nmaximize task accuracy. However, when evaluating prompts, such approaches\nheavily rely on elusive manually annotated gold labels to calculate task\naccuracy for each candidate prompt, which hinders the widespread implementation\nand generality. To overcome the limitation, this work proposes a gold\nlabel-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold\nlabels. Motivated by the observed correlation between self-consistency and the\naccuracy of the answer, we adopt self-consistency as the initial evaluation\nscore. Subsequently, we refine the scores of prompts producing identical\nanswers to be mutually consistent. Experimental results show that GLaPE\nprovides reliable evaluations uniform with accuracy, even in the absence of\ngold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt\noptimization yields effective prompts comparable to accuracy-based ones. The\ncode is publicly available at https://github.com/thunderous77/GLaPE.\n","authors":["Xuanchang Zhang","Zhuosheng Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.02408v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.12027v3","updated":"2024-12-02T07:22:40Z","published":"2024-03-18T17:57:09Z","title":"From Pixels to Insights: A Survey on Automatic Chart Understanding in\n  the Era of Large Foundation Models","summary":"  Data visualization in the form of charts plays a pivotal role in data\nanalysis, offering critical insights and aiding in informed decision-making.\nAutomatic chart understanding has witnessed significant advancements with the\nrise of large foundation models in recent years. Foundation models, such as\nlarge language models, have revolutionized various natural language processing\ntasks and are increasingly being applied to chart understanding tasks. This\nsurvey paper provides a comprehensive overview of the recent developments,\nchallenges, and future directions in chart understanding within the context of\nthese foundation models. We review fundamental building blocks crucial for\nstudying chart understanding tasks. Additionally, we explore various tasks and\ntheir evaluation metrics and sources of both charts and textual inputs. Various\nmodeling strategies are then examined, encompassing both classification-based\nand generation-based approaches, along with tool augmentation techniques that\nenhance chart understanding performance. Furthermore, we discuss the\nstate-of-the-art performance of each task and discuss how we can improve the\nperformance. Challenges and future directions are addressed, highlighting the\nimportance of several topics, such as domain-specific charts, lack of efforts\nin developing evaluation metrics, and agent-oriented settings. This survey\npaper serves as a comprehensive resource for researchers and practitioners in\nthe fields of natural language processing, computer vision, and data analysis,\nproviding valuable insights and directions for future research in chart\nunderstanding leveraging large foundation models. The studies mentioned in this\npaper, along with emerging new research, will be continually updated at:\nhttps://github.com/khuangaf/Awesome-Chart-Understanding.\n","authors":["Kung-Hsiang Huang","Hou Pong Chan","Yi R. Fung","Haoyi Qiu","Mingyang Zhou","Shafiq Joty","Shih-Fu Chang","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.12027v3.pdf","comment":"IEEE Transactions on Knowledge and Data Engineering (TKDE)"},{"id":"http://arxiv.org/abs/2411.19951v2","updated":"2024-12-02T06:54:47Z","published":"2024-11-29T18:59:54Z","title":"T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs","summary":"  The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid.\n","authors":["Shukang Yin","Chaoyou Fu","Sirui Zhao","Yunhang Shen","Chunjiang Ge","Yan Yang","Zuwei Long","Yuhan Dai","Tong Xu","Xing Sun","Ran He","Caifeng Shan","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.19951v2.pdf","comment":"Project page: https://github.com/xjtupanda/T2Vid"},{"id":"http://arxiv.org/abs/2410.13025v2","updated":"2024-12-02T06:40:50Z","published":"2024-10-16T20:33:06Z","title":"LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks","summary":"  Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient\nfine-tuning of Large Language Models (LLMs). We study how different LoRA\nmodules can be merged to achieve skill composition -- testing the performance\nof the merged model on a target task that involves combining multiple skills,\neach skill coming from a single LoRA. This setup is favorable when it is\ndifficult to obtain training data for the target task and when it can be\ndecomposed into multiple skills. First, we identify practically occurring\nuse-cases that can be studied under the realm of skill composition, e.g.\nsolving hard math-word problems with code, creating a bot to answer questions\non proprietary manuals or about domain-specialized corpora. Our main\ncontribution is to show that concatenation of LoRAs (CAT), which optimally\nweights LoRAs that were individually trained on different skills, outperforms\nexisting model- and data- merging techniques; for instance on math-word\nproblems, CAT beats these methods by an average of 43% and 12% respectively.\nThus, this paper advocates model merging as an efficient way to solve\ncompositional tasks and underscores CAT as a simple, compute-friendly and\neffective procedure. To our knowledge, this is the first work demonstrating the\nsuperiority of model merging over data mixing for binary skill composition\ntasks. Code and data are available at https://github.com/aksh555/LoRA-Soups\n","authors":["Akshara Prabhakar","Yuanzhi Li","Karthik Narasimhan","Sham Kakade","Eran Malach","Samy Jelassi"],"pdf_url":"https://arxiv.org/pdf/2410.13025v2.pdf","comment":"COLING 2025 Industry track; 9 pages plus references and appendices"},{"id":"http://arxiv.org/abs/2411.19943v2","updated":"2024-12-02T06:26:38Z","published":"2024-11-29T18:58:22Z","title":"Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability","summary":"  Large Language Models (LLMs) have exhibited remarkable performance on\nreasoning tasks. They utilize autoregressive token generation to construct\nreasoning trajectories, enabling the development of a coherent chain of\nthought. In this work, we explore the impact of individual tokens on the final\noutcomes of reasoning tasks. We identify the existence of ``critical tokens''\nthat lead to incorrect reasoning trajectories in LLMs. Specifically, we find\nthat LLMs tend to produce positive outcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this observation, we propose a novel\napproach - cDPO - designed to automatically recognize and conduct token-level\nrewards for the critical tokens during the alignment process. Specifically, we\ndevelop a contrastive estimation approach to automatically identify critical\ntokens. It is achieved by comparing the generation likelihood of positive and\nnegative models. To achieve this, we separately fine-tune the positive and\nnegative models on various reasoning trajectories, consequently, they are\ncapable of identifying identify critical tokens within incorrect trajectories\nthat contribute to erroneous outcomes. Moreover, to further align the model\nwith the critical token information during the alignment process, we extend the\nconventional DPO algorithms to token-level DPO and utilize the differential\nlikelihood from the aforementioned positive and negative model as important\nweight for token-level DPO learning.Experimental results on GSM8K and MATH500\nbenchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math\n(7B) demonstrate the effectiveness of the propsoed approach cDPO.\n","authors":["Zicheng Lin","Tian Liang","Jiahao Xu","Xing Wang","Ruilin Luo","Chufan Shi","Siheng Li","Yujiu Yang","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2411.19943v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2406.11285v2","updated":"2024-12-02T05:22:01Z","published":"2024-06-17T07:46:45Z","title":"Self and Cross-Model Distillation for LLMs: Effective Methods for\n  Refusal Pattern Alignment","summary":"  Large Language Models (LLMs) like OpenAI's GPT series, Anthropic's Claude,\nand Meta's LLaMa have shown remarkable capabilities in text generation.\nHowever, their susceptibility to toxic prompts presents significant security\nchallenges. This paper investigates alignment techniques, including Supervised\nFine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), to\nmitigate these risks. We conduct an empirical study on refusal patterns across\nnine LLMs, revealing that models with uniform refusal patterns, such as\nClaude3, exhibit higher security. Based on these findings, we propose\nself-distilling and cross-model distilling methods to enhance LLM security. Our\nresults show that these methods significantly improve refusal rates and reduce\nunsafe content, with cross-model distilling achieving refusal rates close to\nClaude3's 94.51%. These findings underscore the potential of distillation-based\nalignment in securing LLMs against toxic prompts.\n","authors":["Jie Li","Yi Liu","Chongyang Liu","Xiaoning Ren","Ling Shi","Weisong Sun","Yinxing Xue"],"pdf_url":"https://arxiv.org/pdf/2406.11285v2.pdf","comment":"The method used in the paper has obvious problems and ambiguities.\n  The security enhancement method we used cannot be considered distillation,\n  but it is described as distillation in the paper, and the experiment lacks\n  comparison and baseline, which has been criticized by many peers. In order to\n  avoid further dissemination, we have decided to withdraw the paper"},{"id":"http://arxiv.org/abs/2411.18203v2","updated":"2024-12-02T05:00:19Z","published":"2024-11-27T10:28:57Z","title":"Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning","summary":"  Vision-language models (VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.\n","authors":["Di Zhang","Junxian Li","Jingdi Lei","Xunzhi Wang","Yujie Liu","Zonglin Yang","Jiatong Li","Weida Wang","Suorong Yang","Jianbo Wu","Peng Ye","Wanli Ouyang","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.18203v2.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.07656v2","updated":"2024-12-02T04:36:45Z","published":"2024-11-12T09:14:16Z","title":"Mitigating Bias in Queer Representation within Large Language Models: A\n  Collaborative Agent Approach","summary":"  Large Language Models (LLMs) often perpetuate biases in pronoun usage,\nleading to misrepresentation or exclusion of queer individuals. This paper\naddresses the specific problem of biased pronoun usage in LLM outputs,\nparticularly the inappropriate use of traditionally gendered pronouns (\"he,\"\n\"she\") when inclusive language is needed to accurately represent all\nidentities. We introduce a collaborative agent pipeline designed to mitigate\nthese biases by analyzing and optimizing pronoun usage for inclusivity. Our\nmulti-agent framework includes specialized agents for both bias detection and\ncorrection. Experimental evaluations using the Tango dataset-a benchmark\nfocused on gender pronoun usage-demonstrate that our approach significantly\nimproves inclusive pronoun classification, achieving a 32.6 percentage point\nincrease over GPT-4o in correctly disagreeing with inappropriate traditionally\ngendered pronouns $(\\chi^2 = 38.57, p < 0.0001)$. These results accentuate the\npotential of agent-driven frameworks in enhancing fairness and inclusivity in\nAI-generated content, demonstrating their efficacy in reducing biases and\npromoting socially responsible AI.\n","authors":["Tianyi Huang","Arya Somasundaram"],"pdf_url":"https://arxiv.org/pdf/2411.07656v2.pdf","comment":"NeurIPS 2024 Queer in AI Workshop"},{"id":"http://arxiv.org/abs/2406.12336v2","updated":"2024-12-02T04:08:49Z","published":"2024-06-18T07:03:34Z","title":"Towards Understanding Domain Adapted Sentence Embeddings for Document\n  Retrieval","summary":"  A plethora of sentence embedding models makes it challenging to choose one,\nespecially for technical domains rich with specialized vocabulary. In this\nwork, we domain adapt embeddings using telecom, health and science datasets for\nquestion answering. We evaluate embeddings obtained from publicly available\nmodels and their domain-adapted variants, on both point retrieval accuracies,\nas well as their (95\\%) confidence intervals. We establish a systematic method\nto obtain thresholds for similarity scores for different embeddings. As\nexpected, we observe that fine-tuning improves mean bootstrapped accuracies. We\nalso observe that it results in tighter confidence intervals, which further\nimprove when pre-training is preceded by fine-tuning. We introduce metrics\nwhich measure the distributional overlaps of top-$K$, correct and random\ndocument similarities with the question. Further, we show that these metrics\nare correlated with retrieval accuracy and similarity thresholds. Recent\nliterature shows conflicting effects of isotropy on retrieval accuracies. Our\nexperiments establish that the isotropy of embeddings (as measured by two\nindependent state-of-the-art isotropy metric definitions) is poorly correlated\nwith retrieval performance. We show that embeddings for domain-specific\nsentences have little overlap with those for domain-agnostic ones, and\nfine-tuning moves them further apart. Based on our results, we provide\nrecommendations for use of our methodology and metrics by researchers and\npractitioners.\n","authors":["Sujoy Roychowdhury","Sumit Soman","H. G. Ranjani","Vansh Chhabra","Neeraj Gunda","Shashank Gautam","Subhadip Bandyopadhyay","Sai Krishna Bala"],"pdf_url":"https://arxiv.org/pdf/2406.12336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12707v3","updated":"2024-12-02T03:45:42Z","published":"2024-07-17T16:30:27Z","title":"TTSDS -- Text-to-Speech Distribution Score","summary":"  Many recently published Text-to-Speech (TTS) systems produce audio close to\nreal speech. However, TTS evaluation needs to be revisited to make sense of the\nresults obtained with the new architectures, approaches and datasets. We\npropose evaluating the quality of synthetic speech as a combination of multiple\nfactors such as prosody, speaker identity, and intelligibility. Our approach\nassesses how well synthetic speech mirrors real speech by obtaining correlates\nof each factor and measuring their distance from both real speech datasets and\nnoise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and\nshow that our score computed as an unweighted average of factors strongly\ncorrelates with the human evaluations from each time period.\n","authors":["Christoph Minixhofer","Ondřej Klejch","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2407.12707v3.pdf","comment":"SLT 2024"},{"id":"http://arxiv.org/abs/2404.01245v3","updated":"2024-12-02T03:27:10Z","published":"2024-04-01T17:03:41Z","title":"A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules","summary":"  Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.\n","authors":["Xiang Li","Feng Ruan","Huiyuan Wang","Qi Long","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2404.01245v3.pdf","comment":"To appear in the Annals of Statistics"},{"id":"http://arxiv.org/abs/2409.01345v3","updated":"2024-12-02T03:10:37Z","published":"2024-09-02T15:58:27Z","title":"Language Models Benefit from Preparation with Elicited Knowledge","summary":"  The zero-shot chain of thought (CoT) approach is often used in question\nanswering (QA) by language models (LMs) for tasks that require multiple\nreasoning steps. However, some QA tasks hinge more on accessing relevant\nknowledge than on chaining reasoning steps. We introduce a simple prompting\ntechnique, called PREP, that involves using two instances of LMs: the first\n(LM1) generates relevant information, and the second (LM2) receives the\ninformation from the user and answers the question. This design is intended to\nmake better use of the LM's instruction-following capability. PREP is\napplicable across various QA tasks without domain-specific prompt engineering.\nPREP is developed on a dataset of 100 QA questions, derived from an extensive\nschematic dataset specifying artifact parts and material composition. These\nquestions ask which of two artifacts is less likely to share materials with\nanother artifact. Such questions probe the LM's knowledge of shared materials\nin the part structure of different artifacts. We test our method on our\nparts-and-materials dataset and three published commonsense reasoning datasets.\nThe average accuracy of our method is consistently higher than that of all the\nother tested methods across all the tested datasets.\n","authors":["Jiacan Yu","Hannah An","Lenhart K. Schubert"],"pdf_url":"https://arxiv.org/pdf/2409.01345v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11266v3","updated":"2024-12-02T02:27:17Z","published":"2024-11-18T03:45:34Z","title":"VersaTune: An Efficient Data Composition Framework for Training\n  Multi-Capability LLMs","summary":"  Large-scale pretrained models, particularly Large Language Models (LLMs),\nhave exhibited remarkable capabilities in handling multiple tasks across\ndomains due to their emergent properties. These capabilities are further\naugmented during the Supervised Fine-Tuning (SFT) phase. Despite their\npotential, existing work mainly focuses on domain-specific enhancements during\nfine-tuning, the challenge of which lies in catastrophic forgetting of\nknowledge across other domains. In this study, we introduce VersaTune, a novel\ndata composition framework designed for enhancing LLMs' overall multi-ability\nperformances during training. We categorize knowledge into distinct domains\nincluding law, medicine, finance, science, code, etc. We begin with detecting\nthe distribution of domain-specific knowledge within the base model, followed\nby the training data composition that aligns with the model's existing\nknowledge distribution. During the training process, domain weights are\ndynamically adjusted based on their learnable potential and forgetting degree.\nExperimental results demonstrate that VersaTune achieves significant\nimprovements in multi-domain performance, with an 35.21% enhancement in\ncomprehensive multi-domain tasks. Additionally, in scenarios where specific\ndomain optimization is required, VersaTune reduces the degradation of\nperformance in other domains by 38.77%, without compromising the target\ndomain's training efficacy.\n","authors":["Keer Lu","Keshi Zhao","Zheng Liang","Da Pan","Shusen Zhang","Xin Wu","Weipeng Chen","Zenan Zhou","Guosheng Dong","Bin Cui","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.11266v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02326v2","updated":"2024-12-02T01:59:30Z","published":"2024-04-23T18:55:49Z","title":"Evaluating LLMs for Hardware Design and Test","summary":"  Large Language Models (LLMs) have demonstrated capabilities for producing\ncode in Hardware Description Languages (HDLs). However, most of the focus\nremains on their abilities to write functional code, not test code. The\nhardware design process consists of both design and test, and so eschewing\nvalidation and verification leaves considerable potential benefit unexplored,\ngiven that a design and test framework may allow for progress towards full\nautomation of the digital design pipeline. In this work, we perform one of the\nfirst studies exploring how a LLM can both design and test hardware modules\nfrom provided specifications. Using a suite of 8 representative benchmarks, we\nexamined the capabilities and limitations of the state-of-the-art\nconversational LLMs when producing Verilog for functional and verification\npurposes. We taped out the benchmarks on a Skywater 130nm shuttle and received\nthe functional chip.\n","authors":["Jason Blocklove","Siddharth Garg","Ramesh Karri","Hammond Pearce"],"pdf_url":"https://arxiv.org/pdf/2405.02326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05356v2","updated":"2024-12-02T00:57:32Z","published":"2022-12-10T19:54:53Z","title":"Punctuation Restoration for Singaporean Spoken Languages: English,\n  Malay, and Mandarin","summary":"  This paper presents the work of restoring punctuation for ASR transcripts\ngenerated by multilingual ASR systems. The focus languages are English,\nMandarin, and Malay which are three of the most popular languages in Singapore.\nTo the best of our knowledge, this is the first system that can tackle\npunctuation restoration for these three languages simultaneously. Traditional\napproaches usually treat the task as a sequential labeling task, however, this\nwork adopts a slot-filling approach that predicts the presence and type of\npunctuation marks at each word boundary. The approach is similar to the\nMasked-Language Model approach employed during the pre-training stages of BERT,\nbut instead of predicting the masked word, our model predicts masked\npunctuation. Additionally, we find that using Jieba1 instead of only using the\nbuilt-in SentencePiece tokenizer of XLM-R can significantly improve the\nperformance of punctuating Mandarin transcripts. Experimental results on\nEnglish and Mandarin IWSLT2022 datasets and Malay News show that the proposed\napproach achieved state-of-the-art results for Mandarin with 73.8% F1-score\nwhile maintaining a reasonable F1-score for English and Malay, i.e. 74.7% and\n78% respectively. Our source code that allows reproducing the results and\nbuilding a simple web-based application for demonstration purposes is available\non Github.\n","authors":["Abhinav Rao","Ho Thi-Nga","Chng Eng-Siong"],"pdf_url":"https://arxiv.org/pdf/2212.05356v2.pdf","comment":"Accepted at APSIPA 2022, Chiang-Mai, Thailand"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.16921v2","updated":"2024-12-02T18:01:53Z","published":"2024-09-25T13:27:29Z","title":"Moner: Motion Correction in Undersampled Radial MRI with Unsupervised\n  Neural Representation","summary":"  Motion correction (MoCo) in radial MRI is a challenging problem due to the\nunpredictability of subject's motion. Current state-of-the-art (SOTA) MoCo\nalgorithms often use extensive high-quality MR images to pre-train neural\nnetworks, obtaining excellent reconstructions. However, the need for\nlarge-scale datasets significantly increases costs and limits model\ngeneralization. In this work, we propose Moner, an unsupervised MoCo method\nthat jointly solves artifact-free MR images and accurate motion from\nundersampled, rigid motion-corrupted k-space data, without requiring training\ndata. Our core idea is to leverage the continuous prior of implicit neural\nrepresentation (INR) to constrain this ill-posed inverse problem, enabling\nideal solutions. Specifically, we incorporate a quasi-static motion model into\nthe INR, granting its ability to correct subject's motion. To stabilize model\noptimization, we reformulate radial MRI as a back-projection problem using the\nFourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash\nencoding strategy, significantly enhancing MoCo accuracy. Experiments on\nmultiple MRI datasets show our Moner achieves performance comparable to SOTA\nMoCo techniques on in-domain data, while demonstrating significant improvements\non out-of-domain data.\n","authors":["Qing Wu","Chenhe Du","XuanYu Tian","Jingyi Yu","Yuyao Zhang","Hongjiang Wei"],"pdf_url":"https://arxiv.org/pdf/2409.16921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24060v5","updated":"2024-12-02T18:00:18Z","published":"2024-10-31T15:57:04Z","title":"Understanding Generalizability of Diffusion Models Requires Rethinking\n  the Hidden Gaussian Structure","summary":"  In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models.\n","authors":["Xiang Li","Yixiang Dai","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2410.24060v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15098v3","updated":"2024-12-02T17:59:40Z","published":"2024-11-22T17:55:15Z","title":"OminiControl: Minimal and Universal Control for Diffusion Transformer","summary":"  In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.\n","authors":["Zhenxiong Tan","Songhua Liu","Xingyi Yang","Qiaochu Xue","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15098v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19895v2","updated":"2024-12-02T17:44:52Z","published":"2024-11-29T17:59:03Z","title":"GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) has recently created impressive assets for\nvarious applications. However, the copyright of these assets is not well\nprotected as existing watermarking methods are not suited for 3DGS considering\nsecurity, capacity, and invisibility. Besides, these methods often require\nhours or even days for optimization, limiting the application scenarios. In\nthis paper, we propose GuardSplat, an innovative and efficient framework that\neffectively protects the copyright of 3DGS assets. Specifically, 1) We first\npropose a CLIP-guided Message Decoupling Optimization module for training the\nmessage decoder, leveraging CLIP's aligning capability and rich representations\nto achieve a high extraction accuracy with minimal optimization costs,\npresenting exceptional capability and efficiency. 2) Then, we propose a\nSpherical-harmonic-aware (SH-aware) Message Embedding module tailored for 3DGS,\nwhich employs a set of SH offsets to seamlessly embed the message into the SH\nfeatures of each 3D Gaussian while maintaining the original 3D structure. It\nenables the 3DGS assets to be watermarked with minimal fidelity trade-offs and\nprevents malicious users from removing the messages from the model files,\nmeeting the demands for invisibility and security. 3) We further propose an\nAnti-distortion Message Extraction module to improve robustness against various\nvisual distortions. Extensive experiments demonstrate that GuardSplat\noutperforms the state-of-the-art methods and achieves fast optimization speed.\n","authors":["Zixuan Chen","Guangcong Wang","Jiahao Zhu","Jianhuang Lai","Xiaohua Xie"],"pdf_url":"https://arxiv.org/pdf/2411.19895v2.pdf","comment":"Project page: https://narcissusex.github.io/GuardSplat and Code:\n  https://github.com/NarcissusEx/GuardSplat"},{"id":"http://arxiv.org/abs/2411.07118v3","updated":"2024-12-02T17:11:07Z","published":"2024-11-11T16:45:18Z","title":"ConvMixFormer- A Resource-efficient Convolution Mixer for\n  Transformer-based Dynamic Hand Gesture Recognition","summary":"  Transformer models have demonstrated remarkable success in many domains such\nas natural language processing (NLP) and computer vision. With the growing\ninterest in transformer-based architectures, they are now utilized for gesture\nrecognition. So, we also explore and devise a novel ConvMixFormer architecture\nfor dynamic hand gestures. The transformers use quadratic scaling of the\nattention features with the sequential data, due to which these models are\ncomputationally complex and heavy. We have considered this drawback of the\ntransformer and designed a resource-efficient model that replaces the\nself-attention in the transformer with the simple convolutional layer-based\ntoken mixer. The computational cost and the parameters used for the\nconvolution-based mixer are comparatively less than the quadratic\nself-attention. Convolution-mixer helps the model capture the local spatial\nfeatures that self-attention struggles to capture due to their sequential\nprocessing nature. Further, an efficient gate mechanism is employed instead of\na conventional feed-forward network in the transformer to help the model\ncontrol the flow of features within different stages of the proposed model.\nThis design uses fewer learnable parameters which is nearly half the vanilla\ntransformer that helps in fast and efficient training. The proposed method is\nevaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has\nachieved state-of-the-art results on single and multimodal inputs. We have also\nshown the parameter efficiency of the proposed ConvMixFormer model compared to\nother methods. The source code is available at\nhttps://github.com/mallikagarg/ConvMixFormer.\n","authors":["Mallika Garg","Debashis Ghosh","Pyari Mohan Pradhan"],"pdf_url":"https://arxiv.org/pdf/2411.07118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01493v3","updated":"2024-12-02T17:10:34Z","published":"2024-06-03T16:20:24Z","title":"Learning Temporally Consistent Video Depth from Video Diffusion Priors","summary":"  This work addresses the challenge of streamed video depth estimation, which\nexpects not only per-frame accuracy but, more importantly, cross-frame\nconsistency. We argue that sharing contextual information between frames or\nclips is pivotal in fostering temporal consistency. Thus, instead of directly\ndeveloping a depth estimator from scratch, we reformulate this predictive task\ninto a conditional generation problem to provide contextual information within\na clip and across clips. Specifically, we propose a consistent context-aware\ntraining and inference strategy for arbitrarily long videos to provide\ncross-clip context. We sample independent noise levels for each frame within a\nclip during training while using a sliding window strategy and initializing\noverlapping frames with previously predicted frames without adding noise.\nMoreover, we design an effective training strategy to provide context within a\nclip. Extensive experimental results validate our design choices and\ndemonstrate the superiority of our approach, dubbed ChronoDepth. Project page:\nhttps://xdimlab.github.io/ChronoDepth/.\n","authors":["Jiahao Shao","Yuanbo Yang","Hongyu Zhou","Youmin Zhang","Yujun Shen","Vitor Guizilini","Yue Wang","Matteo Poggi","Yiyi Liao"],"pdf_url":"https://arxiv.org/pdf/2406.01493v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10789v2","updated":"2024-12-02T17:04:07Z","published":"2024-08-20T12:30:37Z","title":"PartGS:Learning Part-aware 3D Representations by Fusing 2D Gaussians and\n  Superquadrics","summary":"  Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3D\nGaussians, are commonly used to represent 3D objects or scenes. However, human\nperception typically understands 3D objects at a higher level as a composition\nof parts or structures rather than points or voxels. Representing 3D objects or\nscenes as semantic parts can benefit further understanding and applications. In\nthis paper, we introduce $\\textbf{PartGS}$, $\\textbf{part}$-aware 3D\nreconstruction by a hybrid representation of 2D $\\textbf{G}$aussians and\n$\\textbf{S}$uperquadrics, which parses objects or scenes into semantic parts,\ndigging 3D structural clues from multi-view image inputs. Accurate structured\ngeometry reconstruction and high-quality rendering are achieved at the same\ntime. Our method simultaneously optimizes superquadric meshes and Gaussians by\ncoupling their parameters within our hybrid representation. On one hand, this\nhybrid representation inherits the advantage of superquadrics to represent\ndifferent shape primitives, supporting flexible part decomposition of scenes.\nOn the other hand, 2D Gaussians capture complex texture and geometry details,\nensuring high-quality appearance and geometry reconstruction. Our method is\nfully unsupervised and outperforms existing state-of-the-art approaches in\nextensive experiments on DTU, ShapeNet, and real-life datasets.\n","authors":["Zhirui Gao","Renjiao Yi","Yuhang Huang","Wei Chen","Chenyang Zhu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2408.10789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17251v2","updated":"2024-12-02T16:37:41Z","published":"2024-11-26T09:29:27Z","title":"DGNN-YOLO: Dynamic Graph Neural Networks with YOLO11 for Small Object\n  Detection and Tracking in Traffic Surveillance","summary":"  Accurate detection and tracking of small objects such as pedestrians,\ncyclists, and motorbikes are critical for traffic surveillance systems, which\nare crucial in improving road safety and decision-making in intelligent\ntransportation systems. However, traditional methods struggle with challenges\nsuch as occlusion, low resolution, and dynamic traffic conditions,\nnecessitating innovative approaches to address these limitations. This paper\nintroduces DGNN-YOLO, a novel framework integrating dynamic graph neural\nnetworks (DGNN) with YOLO11 to enhance small object detection and tracking in\ntraffic surveillance systems. The framework leverages YOLO11's advanced spatial\nfeature extraction capabilities for precise object detection and incorporates\nDGNN to model spatial-temporal relationships for robust real-time tracking\ndynamically. By constructing and updating graph structures, DGNN-YOLO\neffectively represents objects as nodes and their interactions as edges,\nensuring adaptive and accurate tracking in complex and dynamic environments.\nExtensive experiments demonstrate that DGNN-YOLO consistently outperforms\nstate-of-the-art methods in detecting and tracking small objects under diverse\ntraffic conditions, achieving the highest precision (0.8382), recall (0.6875),\nand mAP@0.5:0.95 (0.6476), showcasing its robustness and scalability,\nparticularly in challenging scenarios involving small and occluded objects.\nThis work provides a scalable, real-time traffic surveillance and analysis\nsolution, significantly contributing to intelligent transportation systems.\n","authors":["Shahriar Soudeep","M. F. Mridha","Md Abrar Jahin","Nilanjan Dey"],"pdf_url":"https://arxiv.org/pdf/2411.17251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13983v2","updated":"2024-12-02T16:29:46Z","published":"2024-04-22T08:44:10Z","title":"Structure-Aware Human Body Reshaping with Adaptive Affinity-Graph\n  Network","summary":"  Given a source portrait, the automatic human body reshaping task aims at\nediting it to an aesthetic body shape. As the technology has been widely used\nin media, several methods have been proposed mainly focusing on generating\noptical flow to warp the body shape. However, those previous works only\nconsider the local transformation of different body parts (arms, torso, and\nlegs), ignoring the global affinity, and limiting the capacity to ensure\nconsistency and quality across the entire body. In this paper, we propose a\nnovel Adaptive Affinity-Graph Network (AAGN), which extracts the global\naffinity between different body parts to enhance the quality of the generated\noptical flow. Specifically, our AAGN primarily introduces the following\ndesigns: (1) we propose an Adaptive Affinity-Graph (AAG) Block that leverages\nthe characteristic of a fully connected graph. AAG represents different body\nparts as nodes in an adaptive fully connected graph and captures all the\naffinities between nodes to obtain a global affinity map. The design could\nbetter improve the consistency between body parts. (2) Besides, for\nhigh-frequency details are crucial for photo aesthetics, a Body Shape\nDiscriminator (BSD) is designed to extract information from both high-frequency\nand spatial domain. Particularly, an SRM filter is utilized to extract\nhigh-frequency details, which are combined with spatial features as input to\nthe BSD. With this design, BSD guides the Flow Generator (FG) to pay attention\nto various fine details rather than rigid pixel-level fitting. Extensive\nexperiments conducted on the BR-5K dataset demonstrate that our framework\nsignificantly enhances the aesthetic appeal of reshaped photos, surpassing all\nprevious work to achieve state-of-the-art in all evaluation metrics.\n","authors":["Qiwen Deng","Yangcen Liu","Wen Li","Guoqing Wang"],"pdf_url":"https://arxiv.org/pdf/2404.13983v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2406.14539v3","updated":"2024-12-02T16:26:57Z","published":"2024-06-20T17:49:11Z","title":"Invertible Consistency Distillation for Text-Guided Image Editing in\n  Around 7 Steps","summary":"  Diffusion distillation represents a highly promising direction for achieving\nfaithful text-to-image generation in a few sampling steps. However, despite\nrecent successes, existing distilled models still do not provide the full\nspectrum of diffusion abilities, such as real image inversion, which enables\nmany precise image manipulation methods. This work aims to enrich distilled\ntext-to-image diffusion models with the ability to effectively encode real\nimages into their latent space. To this end, we introduce invertible\nConsistency Distillation (iCD), a generalized consistency distillation\nframework that facilitates both high-quality image synthesis and accurate image\nencoding in only 3-4 inference steps. Though the inversion problem for\ntext-to-image diffusion models gets exacerbated by high classifier-free\nguidance scales, we notice that dynamic guidance significantly reduces\nreconstruction errors without noticeable degradation in generation performance.\nAs a result, we demonstrate that iCD equipped with dynamic guidance may serve\nas a highly effective tool for zero-shot text-guided image editing, competing\nwith more expensive state-of-the-art alternatives.\n","authors":["Nikita Starodubcev","Mikhail Khoroshikh","Artem Babenko","Dmitry Baranchuk"],"pdf_url":"https://arxiv.org/pdf/2406.14539v3.pdf","comment":"Project page: https://yandex-research.github.io/invertible-cd/"},{"id":"http://arxiv.org/abs/2411.04630v2","updated":"2024-12-02T15:47:17Z","published":"2024-11-07T11:29:55Z","title":"Brain Tumour Removing and Missing Modality Generation using 3D WDM","summary":"  This paper presents the second-placed solution for task 8 and the\nparticipation solution for task 7 of BraTS 2024. The adoption of automated\nbrain analysis algorithms to support clinical practice is increasing. However,\nmany of these algorithms struggle with the presence of brain lesions or the\nabsence of certain MRI modalities. The alterations in the brain's morphology\nleads to high variability and thus poor performance of predictive models that\nwere trained only on healthy brains. The lack of information that is usually\nprovided by some of the missing MRI modalities also reduces the reliability of\nthe prediction models trained with all modalities. In order to improve the\nperformance of these models, we propose the use of conditional 3D wavelet\ndiffusion models. The wavelet transform enabled full-resolution image training\nand prediction on a GPU with 48 GB VRAM, without patching or downsampling,\npreserving all information for prediction. The code for these tasks is\navailable at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.\n","authors":["André Ferreira","Gijs Luijten","Behrus Puladi","Jens Kleesiek","Victor Alves","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2411.04630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08085v2","updated":"2024-12-02T15:20:08Z","published":"2024-11-12T16:52:51Z","title":"Deep Learning 2.0: Artificial Neurons That Matter -- Reject Correlation,\n  Embrace Orthogonality","summary":"  We introduce a yat-product-powered neural network, the Neural Matter Network\n(NMN), a breakthrough in deep learning that achieves non-linear pattern\nrecognition without activation functions. Our key innovation relies on the\nyat-product and yat-product, which naturally induces non-linearity by\nprojecting inputs into a pseudo-metric space, eliminating the need for\ntraditional activation functions while maintaining only a softmax layer for\nfinal class probability distribution. This approach simplifies network\narchitecture and provides unprecedented transparency into the network's\ndecision-making process. Our comprehensive empirical evaluation across\ndifferent datasets demonstrates that NMN consistently outperforms traditional\nMLPs. The results challenge the assumption that separate activation functions\nare necessary for effective deep-learning models. The implications of this work\nextend beyond immediate architectural benefits, by eliminating intermediate\nactivation functions while preserving non-linear capabilities, yat-MLP\nestablishes a new paradigm for neural network design that combines simplicity\nwith effectiveness. Most importantly, our approach provides unprecedented\ninsights into the traditionally opaque \"black-box\" nature of neural networks,\noffering a clearer understanding of how these models process and classify\ninformation.\n","authors":["Taha Bouhsine"],"pdf_url":"https://arxiv.org/pdf/2411.08085v2.pdf","comment":"fixed proof, added softermax"},{"id":"http://arxiv.org/abs/2402.06390v2","updated":"2024-12-02T15:04:25Z","published":"2024-02-09T13:11:57Z","title":"Deepfake for the Good: Generating Avatars through Face-Swapping with\n  Implicit Deepfake Generation","summary":"  Numerous emerging deep-learning techniques have had a substantial impact on\ncomputer graphics. Among the most promising breakthroughs are the rise of\nNeural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the\nobject's shape and color in neural network weights using a handful of images\nwith known camera positions to generate novel views. In contrast, GS provides\naccelerated training and inference without a decrease in rendering quality by\nencoding the object's characteristics in a collection of Gaussian\ndistributions. These two techniques have found many use cases in spatial\ncomputing and other domains. On the other hand, the emergence of deepfake\nmethods has sparked considerable controversy. Deepfakes refers to artificial\nintelligence-generated videos that closely mimic authentic footage. Using\ngenerative models, they can modify facial features, enabling the creation of\naltered identities or expressions that exhibit a remarkably realistic\nappearance to a real person. Despite these controversies, deepfake can offer a\nnext-generation solution for avatar creation and gaming when of desirable\nquality. To that end, we show how to combine all these emerging technologies to\nobtain a more plausible outcome. Our ImplicitDeepfake uses the classical\ndeepfake algorithm to modify all training images separately and then train NeRF\nand GS on modified faces. Such simple strategies can produce plausible 3D\ndeepfake-based avatars.\n","authors":["Georgii Stanishevskii","Jakub Steczkiewicz","Tomasz Szczepanik","Sławomir Tadeja","Jacek Tabor","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2402.06390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17773v2","updated":"2024-12-02T14:55:49Z","published":"2024-11-26T09:36:02Z","title":"Efficient Multi-modal Large Language Models via Visual Token Grouping","summary":"  The development of Multi-modal Large Language Models (MLLMs) enhances Large\nLanguage Models (LLMs) with the ability to perceive data formats beyond text,\nsignificantly advancing a range of downstream applications, such as visual\nquestion answering and image captioning. However, the substantial computational\ncosts associated with processing high-resolution images and videos pose a\nbarrier to their broader adoption. To address this challenge, compressing\nvision tokens in MLLMs has emerged as a promising approach to reduce inference\ncosts. While existing methods conduct token reduction in the feature alignment\nphase. In this paper, we introduce VisToG, a novel grouping mechanism that\nleverages the capabilities of pre-trained vision encoders to group similar\nimage segments without the need for segmentation masks. Specifically, we\nconcatenate semantic tokens to represent image semantic segments after the\nlinear projection layer before feeding into the vision encoder. Besides, with\nthe isolated attention we adopt, VisToG can identify and eliminate redundant\nvisual tokens utilizing the prior knowledge in the pre-trained vision encoder,\nwhich effectively reduces computational demands. Extensive experiments\ndemonstrate the effectiveness of VisToG, maintaining 98.1% of the original\nperformance while achieving a reduction of over 27\\% inference time.\n","authors":["Minbin Huang","Runhui Huang","Han Shi","Yimeng Chen","Chuanyang Zheng","Xiangguo Sun","Xin Jiang","Zhenguo Li","Hong Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.17773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06154v2","updated":"2024-12-02T14:49:55Z","published":"2024-10-08T15:55:40Z","title":"GLOV: Guided Large Language Models as Implicit Optimizers for Vision\n  Language Models","summary":"  In this work, we propose a novel method (GLOV) enabling Large Language Models\n(LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to\nenhance downstream vision tasks. Our GLOV meta-prompts an LLM with the\ndownstream task description, querying it for suitable VLM prompts (e.g., for\nzero-shot classification with CLIP). These prompts are ranked according to a\npurity measure obtained through a fitness function. In each respective\noptimization step, the ranked prompts are fed as in-context examples (with\ntheir accuracies) to equip the LLM with the knowledge of the type of text\nprompts preferred by the downstream VLM. Furthermore, we also explicitly steer\nthe LLM generation process in each optimization step by specifically adding an\noffset difference vector of the embeddings from the positive and negative\nsolutions found by the LLM, in previous optimization steps, to the intermediate\nlayer of the network for the next generation step. This offset vector steers\nthe LLM generation toward the type of language preferred by the downstream VLM,\nresulting in enhanced performance on the downstream vision tasks. We\ncomprehensively evaluate our GLOV on 16 diverse datasets using two families of\nVLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models\n-- showing that the discovered solutions can enhance the recognition\nperformance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these\nmodels.\n","authors":["M. Jehanzeb Mirza","Mengjie Zhao","Zhuoyuan Mao","Sivan Doveh","Wei Lin","Paul Gavrikov","Michael Dorkenwald","Shiqi Yang","Saurav Jha","Hiromi Wakaki","Yuki Mitsufuji","Horst Possegger","Rogerio Feris","Leonid Karlinsky","James Glass"],"pdf_url":"https://arxiv.org/pdf/2410.06154v2.pdf","comment":"Code: https://github.com/jmiemirza/GLOV"},{"id":"http://arxiv.org/abs/2411.01819v2","updated":"2024-12-02T14:42:09Z","published":"2024-11-04T05:39:01Z","title":"Free-Mask: A Novel Paradigm of Integration Between the Segmentation\n  Diffusion Model and Image Editing to Improve Segmentation Ability","summary":"  Current semantic segmentation models typically require a substantial amount\nof manually annotated data, a process that is both time-consuming and\nresource-intensive. Alternatively, leveraging advanced text-to-image models\nsuch as Midjourney and Stable Diffusion has emerged as an efficient strategy,\nenabling the automatic generation of synthetic data in place of manual\nannotations. However, previous methods have been limited to generating\nsingle-instance images, as the generation of multiple instances with Stable\nDiffusion has proven unstable. To address this limitation and expand the scope\nand diversity of synthetic datasets, we propose a framework \\textbf{Free-Mask}\nthat combines a Diffusion Model for segmentation with advanced image editing\ncapabilities, allowing for the integration of multiple objects into images via\ntext-to-image models. Our method facilitates the creation of highly realistic\ndatasets that closely emulate open-world environments while generating accurate\nsegmentation masks. It reduces the labor associated with manual annotation and\nalso ensures precise mask generation. Experimental results demonstrate that\nsynthetic data generated by \\textbf{Free-Mask} enables segmentation models to\noutperform those trained on real data, especially in zero-shot settings.\nNotably, \\textbf{Free-Mask} achieves new state-of-the-art results on previously\nunseen classes in the VOC 2012 benchmark.\n","authors":["Bo Gao","Fangxu Xing","Daniel Tang"],"pdf_url":"https://arxiv.org/pdf/2411.01819v2.pdf","comment":"16 pages,5 figures,5 tables"},{"id":"http://arxiv.org/abs/2108.11986v2","updated":"2024-12-02T14:25:58Z","published":"2021-08-25T11:45:40Z","title":"Anomaly Detection in Medical Imaging -- A Mini Review","summary":"  The increasing digitization of medical imaging enables machine learning based\nimprovements in detecting, visualizing and segmenting lesions, easing the\nworkload for medical experts. However, supervised machine learning requires\nreliable labelled data, which is is often difficult or impossible to collect or\nat least time consuming and thereby costly. Therefore methods requiring only\npartly labeled data (semi-supervised) or no labeling at all (unsupervised\nmethods) have been applied more regularly. Anomaly detection is one possible\nmethodology that is able to leverage semi-supervised and unsupervised methods\nto handle medical imaging tasks like classification and segmentation. This\npaper uses a semi-exhaustive literature review of relevant anomaly detection\npapers in medical imaging to cluster into applications, highlight important\nresults, establish lessons learned and give further advice on how to approach\nanomaly detection in medical imaging. The qualitative analysis is based on\ngoogle scholar and 4 different search terms, resulting in 120 different\nanalysed papers. The main results showed that the current research is mostly\nmotivated by reducing the need for labelled data. Also, the successful and\nsubstantial amount of research in the brain MRI domain shows the potential for\napplications in further domains like OCT and chest X-ray.\n","authors":["Maximilian E. Tschuchnig","Michael Gadermayr"],"pdf_url":"https://arxiv.org/pdf/2108.11986v2.pdf","comment":"Accepted and presented at iDSC2021 edit: During work on this\n  publication Maximilian Ernst Tschuchnig was affiliated with Salzburg\n  University of Applied Sciences and University of Salzburg"},{"id":"http://arxiv.org/abs/2204.10942v2","updated":"2024-12-02T14:12:18Z","published":"2022-04-22T21:48:56Z","title":"Evaluation of Multi-Scale Multiple Instance Learning to Improve Thyroid\n  Cancer Classification","summary":"  Thyroid cancer is currently the fifth most common malignancy diagnosed in\nwomen. Since differentiation of cancer sub-types is important for treatment and\ncurrent, manual methods are time consuming and subjective, automatic\ncomputer-aided differentiation of cancer types is crucial. Manual\ndifferentiation of thyroid cancer is based on tissue sections, analysed by\npathologists using histological features. Due to the enormous size of gigapixel\nwhole slide images, holistic classification using deep learning methods is not\nfeasible. Patch based multiple instance learning approaches, combined with\naggregations such as bag-of-words, is a common approach. This work's\ncontribution is to extend a patch based state-of-the-art method by generating\nand combining feature vectors of three different patch resolutions and\nanalysing three distinct ways of combining them. The results showed\nimprovements in one of the three multi-scale approaches, while the others led\nto decreased scores. This provides motivation for analysis and discussion of\nthe individual approaches.\n","authors":["Maximilian E. Tschuchnig","Philipp Grubmüller","Lea M. Stangassinger","Christina Kreutzer","Sébastien Couillard-Després","Gertie J. Oostingh","Anton Hittmair","Michael Gadermayr"],"pdf_url":"https://arxiv.org/pdf/2204.10942v2.pdf","comment":"Accepted and presented at IPTA 2022 (Best Paper) edit: During work on\n  this publication Maximilian Ernst Tschuchnig was affiliated with Salzburg\n  University of Applied Sciences and University of Salzburg"},{"id":"http://arxiv.org/abs/2411.12440v3","updated":"2024-12-02T13:44:39Z","published":"2024-11-19T11:59:54Z","title":"Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear\n  Kernels","summary":"  Recent advancements in 3D Gaussian Splatting (3DGS) have substantially\nimproved novel view synthesis, enabling high-quality reconstruction and\nreal-time rendering. However, blurring artifacts, such as floating primitives\nand over-reconstruction, remain challenging. Current methods address these\nissues by refining scene structure, enhancing geometric representations,\naddressing blur in training images, improving rendering consistency, and\noptimizing density control, yet the role of kernel design remains\nunderexplored. We identify the soft boundaries of Gaussian ellipsoids as one of\nthe causes of these artifacts, limiting detail capture in high-frequency\nregions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), which\nreplaces Gaussian kernels with linear kernels to achieve sharper and more\nprecise results, particularly in high-frequency regions. Through evaluations on\nthree datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, along\nwith a 30% FPS improvement over baseline 3DGS. The implementation will be made\npublicly available upon acceptance.\n","authors":["Haodong Chen","Runnan Chen","Qiang Qu","Zhaoqing Wang","Tongliang Liu","Xiaoming Chen","Yuk Ying Chung"],"pdf_url":"https://arxiv.org/pdf/2411.12440v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12990v2","updated":"2024-12-02T13:40:59Z","published":"2023-12-20T12:48:18Z","title":"Multi-task Learning To Improve Semantic Segmentation Of CBCT Scans Using\n  Image Reconstruction","summary":"  Semantic segmentation is a crucial task in medical image processing,\nessential for segmenting organs or lesions such as tumors. In this study we aim\nto improve automated segmentation in CBCTs through multi-task learning. To\nevaluate effects on different volume qualities, a CBCT dataset is synthesised\nfrom the CT Liver Tumor Segmentation Benchmark (LiTS) dataset. To improve\nsegmentation, two approaches are investigated. First, we perform multi-task\nlearning to add morphology based regularization through a volume reconstruction\ntask. Second, we use this reconstruction task to reconstruct the best quality\nCBCT (most similar to the original CT), facilitating denoising effects. We\nexplore both holistic and patch-based approaches. Our findings reveal that,\nespecially using a patch-based approach, multi-task learning improves\nsegmentation in most cases and that these results can further be improved by\nour denoising approach.\n","authors":["Maximilian Ernst Tschuchnig","Julia Coste-Marin","Philipp Steininger","Michael Gadermayr"],"pdf_url":"https://arxiv.org/pdf/2312.12990v2.pdf","comment":"Accepted and presented at German Conference on Medical Image\n  Computing (BVM) 2024 edit: During work on this publication Maximilian Ernst\n  Tschuchnig was affiliated with Salzburg University of Applied Sciences and\n  University of Salzburg"},{"id":"http://arxiv.org/abs/2406.11536v2","updated":"2024-12-02T13:20:23Z","published":"2024-06-17T13:38:57Z","title":"RO-SVD: A Reconfigurable Hardware Copyright Protection Framework for\n  AIGC Applications","summary":"  The dramatic surge in the utilisation of generative artificial intelligence\n(GenAI) underscores the need for a secure and efficient mechanism to\nresponsibly manage, use and disseminate multi-dimensional data generated by\nartificial intelligence (AI). In this paper, we propose a blockchain-based\ncopyright traceability framework called ring oscillator-singular value\ndecomposition (RO-SVD), which introduces decomposition computing to approximate\nlow-rank matrices generated from hardware entropy sources and establishes an\nAI-generated content (AIGC) copyright traceability mechanism at the device\nlevel. By leveraging the parallelism and reconfigurability of\nfield-programmable gate arrays (FPGAs), our framework can be easily constructed\non existing AI-accelerated devices and provide a low-cost solution to emerging\ncopyright issues of AIGC. We developed a hardware-software (HW/SW) co-design\nprototype based on comprehensive analysis and on-board experiments with\nmultiple AI-applicable FPGAs. Using AI-generated images as a case study, our\nframework demonstrated effectiveness and emphasised customisation,\nunpredictability, efficiency, management and reconfigurability. To the best of\nour knowledge, this is the first practical hardware study discussing and\nimplementing copyright traceability specifically for AI-generated content.\n","authors":["Zhuoheng Ran","Muhammad A. A. Abdelgawad","Zekai Zhang","Ray C. C. Cheung","Hong Yan"],"pdf_url":"https://arxiv.org/pdf/2406.11536v2.pdf","comment":"Accepted on 20 May 2024 as a full paper at ASAP 2024"},{"id":"http://arxiv.org/abs/2410.07926v2","updated":"2024-12-02T13:17:25Z","published":"2024-10-10T13:53:42Z","title":"Multimodal Perception System for Real Open Environment","summary":"  This paper presents a novel multimodal perception system for a real open\nenvironment. The proposed system includes an embedded computation platform,\ncameras, ultrasonic sensors, GPS, and IMU devices. Unlike the traditional\nframeworks, our system integrates multiple sensors with advanced computer\nvision algorithms to help users walk outside reliably. The system can\nefficiently complete various tasks, including navigating to specific locations,\npassing through obstacle regions, and crossing intersections. Specifically, we\nalso use ultrasonic sensors and depth cameras to enhance obstacle avoidance\nperformance. The path planning module is designed to find the locally optimal\nroute based on various feedback and the user's current state. To evaluate the\nperformance of the proposed system, we design several experiments under\ndifferent scenarios. The results show that the system can help users walk\nefficiently and independently in complex situations.\n","authors":["Yuyang Sha"],"pdf_url":"https://arxiv.org/pdf/2410.07926v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18328v3","updated":"2024-12-02T13:04:47Z","published":"2023-11-30T07:58:54Z","title":"Advances in 3D Neural Stylization: A Survey","summary":"  Modern artificial intelligence offers a novel and transformative approach to\ncreating digital art across diverse styles and modalities like images, videos\nand 3D data, unleashing the power of creativity and revolutionizing the way\nthat we perceive and interact with visual content. This paper reports on recent\nadvances in stylized 3D asset creation and manipulation with the expressive\npower of neural networks. We establish a taxonomy for neural stylization,\nconsidering crucial design choices such as scene representation, guidance data,\noptimization strategies, and output styles. Building on such taxonomy, our\nsurvey first revisits the background of neural stylization on 2D images, and\nthen presents in-depth discussions on recent neural stylization methods for 3D\ndata, accompanied by a benchmark evaluating selected mesh and neural field\nstylization methods. Based on the insights gained from the survey, we highlight\nthe practical significance, open challenges, future research, and potential\nimpacts of neural stylization, which facilitates researchers and practitioners\nto navigate the rapidly evolving landscape of 3D content creation using modern\nartificial intelligence.\n","authors":["Yingshu Chen","Guocheng Shao","Ka Chun Shum","Binh-Son Hua","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2311.18328v3.pdf","comment":"curated list of papers:\n  https://github.com/chenyingshu/advances_3d_neural_stylization"},{"id":"http://arxiv.org/abs/2312.14132v3","updated":"2024-12-02T13:00:56Z","published":"2023-12-21T18:52:14Z","title":"DUSt3R: Geometric 3D Vision Made Easy","summary":"  Multi-view stereo reconstruction (MVS) in the wild requires to first estimate\nthe camera parameters e.g. intrinsic and extrinsic parameters. These are\nusually tedious and cumbersome to obtain, yet they are mandatory to triangulate\ncorresponding pixels in 3D space, which is the core of all best performing MVS\nalgorithms. In this work, we take an opposite stance and introduce DUSt3R, a\nradically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction\nof arbitrary image collections, i.e. operating without prior information about\ncamera calibration nor viewpoint poses. We cast the pairwise reconstruction\nproblem as a regression of pointmaps, relaxing the hard constraints of usual\nprojective camera models. We show that this formulation smoothly unifies the\nmonocular and binocular reconstruction cases. In the case where more than two\nimages are provided, we further propose a simple yet effective global alignment\nstrategy that expresses all pairwise pointmaps in a common reference frame. We\nbase our network architecture on standard Transformer encoders and decoders,\nallowing us to leverage powerful pretrained models. Our formulation directly\nprovides a 3D model of the scene as well as depth information, but\ninterestingly, we can seamlessly recover from it, pixel matches, relative and\nabsolute camera. Exhaustive experiments on all these tasks showcase that the\nproposed DUSt3R can unify various 3D vision tasks and set new SoTAs on\nmonocular/multi-view depth estimation as well as relative pose estimation. In\nsummary, DUSt3R makes many geometric 3D vision tasks easy.\n","authors":["Shuzhe Wang","Vincent Leroy","Yohann Cabon","Boris Chidlovskii","Jerome Revaud"],"pdf_url":"https://arxiv.org/pdf/2312.14132v3.pdf","comment":"fixing the ref for StaticThings3D dataset"},{"id":"http://arxiv.org/abs/2411.18025v2","updated":"2024-12-02T12:42:28Z","published":"2024-11-27T03:44:21Z","title":"Pixel-aligned RGB-NIR Stereo Imaging and Dataset for Robot Vision","summary":"  Integrating RGB and NIR stereo imaging provides complementary spectral\ninformation, potentially enhancing robotic 3D vision in challenging lighting\nconditions. However, existing datasets and imaging systems lack pixel-level\nalignment between RGB and NIR images, posing challenges for downstream vision\ntasks. In this paper, we introduce a robotic vision system equipped with\npixel-aligned RGB-NIR stereo cameras and a LiDAR sensor mounted on a mobile\nrobot. The system simultaneously captures pixel-aligned pairs of RGB stereo\nimages, NIR stereo images, and temporally synchronized LiDAR points. Utilizing\nthe mobility of the robot, we present a dataset containing continuous video\nframes under diverse lighting conditions. We then introduce two methods that\nutilize the pixel-aligned RGB-NIR images: an RGB-NIR image fusion method and a\nfeature fusion method. The first approach enables existing RGB-pretrained\nvision models to directly utilize RGB-NIR information without fine-tuning. The\nsecond approach fine-tunes existing vision models to more effectively utilize\nRGB-NIR information. Experimental results demonstrate the effectiveness of\nusing pixel-aligned RGB-NIR images across diverse lighting conditions.\n","authors":["Jinnyeong Kim","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2411.18025v2.pdf","comment":"8 pages for main article, 32 pages for supplemental document. Fix\n  typos"},{"id":"http://arxiv.org/abs/2408.16886v3","updated":"2024-12-02T12:39:07Z","published":"2024-08-29T20:19:10Z","title":"LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation","summary":"  While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at \\url{https://github.com/juntaoJianggavin/LV-UNet}.\n","authors":["Juntao Jiang","Mengmeng Wang","Huizhong Tian","Lingbo Cheng","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.16886v3.pdf","comment":"Accepted by IEEE BIBM2024 ML4BMI workshop"},{"id":"http://arxiv.org/abs/2411.14951v2","updated":"2024-12-02T12:38:39Z","published":"2024-11-22T14:09:56Z","title":"Morph: A Motion-free Physics Optimization Framework for Human Motion\n  Generation","summary":"  Human motion generation plays a vital role in applications such as digital\nhumans and humanoid robot control. However, most existing approaches disregard\nphysics constraints, leading to the frequent production of physically\nimplausible motions with pronounced artifacts such as floating and foot\nsliding. In this paper, we propose \\textbf{Morph}, a\n\\textbf{Mo}tion-f\\textbf{r}ee \\textbf{ph}ysics optimization framework,\ncomprising a Motion Generator and a Motion Physics Refinement module, for\nenhancing physical plausibility without relying on costly real-world motion\ndata. Specifically, the Motion Generator is responsible for providing\nlarge-scale synthetic motion data, while the Motion Physics Refinement Module\nutilizes these synthetic data to train a motion imitator within a physics\nsimulator, enforcing physical constraints to project the noisy motions into a\nphysically-plausible space. These physically refined motions, in turn, are used\nto fine-tune the Motion Generator, further enhancing its capability.\nExperiments on both text-to-motion and music-to-dance generation tasks\ndemonstrate that our framework achieves state-of-the-art motion generation\nquality while improving physical plausibility drastically.\n","authors":["Zhuo Li","Mingshuang Luo","Ruibing Hou","Xin Zhao","Hao Liu","Hong Chang","Zimo Liu","Chen Li"],"pdf_url":"https://arxiv.org/pdf/2411.14951v2.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.07648v2","updated":"2024-12-02T12:23:47Z","published":"2024-06-11T18:29:13Z","title":"Multi-View Large Reconstruction Model via Geometry-Aware Positional\n  Encoding and Attention","summary":"  Despite recent advancements in the Large Reconstruction Model (LRM)\ndemonstrating impressive results, when extending its input from single image to\nmultiple images, it exhibits inefficiencies, subpar geometric and texture\nquality, as well as slower convergence speed than expected. It is attributed to\nthat, LRM formulates 3D reconstruction as a naive images-to-3D translation\nproblem, ignoring the strong 3D coherence among the input images. In this\npaper, we propose a Multi-view Large Reconstruction Model (M-LRM) designed to\nreconstruct high-quality 3D shapes from multi-views in a 3D-aware manner.\nSpecifically, we introduce a multi-view consistent cross-attention scheme to\nenable M-LRM to accurately query information from the input images. Moreover,\nwe employ the 3D priors of the input multi-view images to initialize the\ntriplane tokens. Compared to previous methods, the proposed M-LRM can generate\n3D shapes of high fidelity. Experimental studies demonstrate that our model\nachieves a significant performance gain and faster training convergence.\nProject page: \\url{https://murphylmf.github.io/M-LRM/}.\n","authors":["Mengfei Li","Xiaoxiao Long","Yixun Liang","Weiyu Li","Yuan Liu","Peng Li","Wenhan Luo","Wenping Wang","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2406.07648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13272v2","updated":"2024-12-02T12:18:12Z","published":"2024-06-19T07:08:48Z","title":"AniFaceDiff: Animating Stylized Avatars via Parametric Conditioned\n  Diffusion Models","summary":"  Animating stylized avatars with dynamic poses and expressions has attracted\nincreasing attention for its broad range of applications. Previous research has\nmade significant progress by training controllable generative models to\nsynthesize animations based on reference characteristics, pose, and expression\nconditions. However, the mechanisms used in these methods to control pose and\nexpression often inadvertently introduce unintended features from the target\nmotion, while also causing a loss of expression-related details, particularly\nwhen applied to stylized animation. This paper proposes a new method based on\nStable Diffusion, called AniFaceDiff, incorporating a new conditioning module\nfor animating stylized avatars. First, we propose a refined spatial\nconditioning approach by Facial Alignment to prevent the inclusion of identity\ncharacteristics from the target motion. Then, we introduce an Expression\nAdapter that incorporates additional cross-attention layers to address the\npotential loss of expression-related information. Our approach effectively\npreserves pose and expression from the target video while maintaining input\nimage consistency. Extensive experiments demonstrate that our method achieves\nstate-of-the-art results, showcasing superior image quality, preservation of\nreference features, and expression accuracy, particularly for out-of-domain\nanimation across diverse styles, highlighting its versatility and strong\ngeneralization capabilities. This work aims to enhance the quality of virtual\nstylized animation for positive applications. To promote responsible use in\nvirtual environments, we contribute to the advancement of detection for\ngenerative content by evaluating state-of-the-art detectors, highlighting\npotential areas for improvement, and suggesting solutions.\n","authors":["Ken Chen","Sachith Seneviratne","Wei Wang","Dongting Hu","Sanjay Saha","Md. Tarek Hasan","Sanka Rasnayaka","Tamasha Malepathirana","Mingming Gong","Saman Halgamuge"],"pdf_url":"https://arxiv.org/pdf/2406.13272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13656v4","updated":"2024-12-02T12:11:13Z","published":"2023-01-31T14:18:19Z","title":"A Survey and Benchmark of Automatic Surface Reconstruction from Point\n  Clouds","summary":"  We present a comprehensive survey and benchmark of both traditional and\nlearning-based methods for surface reconstruction from point clouds. This task\nis particularly challenging for real-world acquisitions due to factors such as\nnoise, outliers, non-uniform sampling, and missing data. Traditional approaches\noften simplify the problem by imposing handcrafted priors on either the input\npoint clouds or the resulting surface, a process that can require tedious\nhyperparameter tuning. In contrast, deep learning models have the capability to\ndirectly learn the properties of input point clouds and desired surfaces from\ndata. We study the influence of handcrafted and learned priors on the precision\nand robustness of surface reconstruction techniques. We evaluate various\ntime-tested and contemporary methods in a standardized manner. When both\ntrained and evaluated on point clouds with identical characteristics, the\nlearning-based models consistently produce higher-quality surfaces compared to\ntheir traditional counterparts -- even in scenarios involving novel shape\ncategories. However, traditional methods demonstrate greater resilience to the\ndiverse anomalies commonly found in real-world 3D acquisitions. For the benefit\nof the research community, we make our code and datasets available, inviting\nfurther enhancements to learning-based surface reconstruction. This can be\naccessed at https://github.com/raphaelsulzer/dsr-benchmark .\n","authors":["Raphael Sulzer","Renaud Marlet","Bruno Vallet","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2301.13656v4.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2411.03795v4","updated":"2024-12-02T12:09:39Z","published":"2024-11-06T09:39:52Z","title":"VQA$^2$: Visual Question Answering for Video Quality Assessment","summary":"  The advent and proliferation of large multi-modal models (LMMs) have\nintroduced new paradigms to computer vision, transforming various tasks into a\nunified visual question answering framework. Video Quality Assessment (VQA), a\nclassic field in low-level visual perception, focused initially on quantitative\nvideo quality scoring. However, driven by advances in LMMs, it is now\nprogressing toward more holistic visual quality understanding tasks. Recent\nstudies in the image domain have demonstrated that Visual Question Answering\n(VQA) can markedly enhance low-level visual quality evaluation. Nevertheless,\nrelated work has not been explored in the video domain, leaving substantial\nroom for improvement. To address this gap, we introduce the VQA2 Instruction\nDataset - the first visual question answering instruction dataset that focuses\non video quality assessment. This dataset consists of 3 subsets and covers\nvarious video types, containing 157,755 instruction question-answer pairs.\nThen, leveraging this foundation, we present the VQA2 series models. The VQA2\nseries models interleave visual and motion tokens to enhance the perception of\nspatial-temporal quality details in videos. We conduct extensive experiments on\nvideo quality scoring and understanding tasks, and results demonstrate that the\nVQA2series models achieve excellent performance in both tasks. Notably, our\nfinal model, the VQA2-Assistant, exceeds the renowned GPT-4o in visual quality\nunderstanding tasks while maintaining strong competitiveness in quality scoring\ntasks. Our work provides a foundation and feasible approach for integrating\nlow-level video quality assessment and understanding with LMMs.\n","authors":["Ziheng Jia","Zicheng Zhang","Jiaying Qian","Haoning Wu","Wei Sun","Chunyi Li","Xiaohong Liu","Weisi Lin","Guangtao Zhai","Xiongkuo Min"],"pdf_url":"https://arxiv.org/pdf/2411.03795v4.pdf","comment":"23 pages 12 figures"},{"id":"http://arxiv.org/abs/2410.23132v2","updated":"2024-12-02T12:05:29Z","published":"2024-10-30T15:42:59Z","title":"Revisiting MAE pre-training for 3D medical image segmentation","summary":"  Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the\npotential of vast, untapped clinical datasets, for various downstream\napplications that suffer from the scarcity of labeled data. While SSL has\nrevolutionized fields like natural language processing and computer vision, its\nadoption in 3D medical image computing has been limited by three key pitfalls:\nSmall pre-training dataset sizes, architectures inadequate for 3D medical image\nanalysis, and insufficient evaluation practices. In this paper, we address\nthese issues by i) leveraging a large-scale dataset of 39k 3D brain MRI volumes\nand ii) using a Residual Encoder U-Net architecture within the state-of-the-art\nnnU-Net framework. iii) A robust development framework, incorporating 5\ndevelopment and 8 testing brain MRI segmentation datasets, allowed\nperformance-driven design decisions to optimize the simple concept of Masked\nAuto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses\nprevious SSL methods but also outperforms the strong nnU-Net baseline by an\naverage of approximately 3 Dice points setting a new state-of-the-art. Our code\nand models are made available here.\n","authors":["Tassilo Wald","Constantin Ulrich","Stanislav Lukyanenko","Andrei Goncharov","Alberto Paderno","Leander Maerkisch","Paul F. Jäger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2410.23132v2.pdf","comment":"Arxiv Preprint. Revised and under review"},{"id":"http://arxiv.org/abs/2411.06098v3","updated":"2024-12-02T11:49:05Z","published":"2024-11-09T07:19:56Z","title":"An Architectural Approach to Enhance Deep Long-Tailed Learning","summary":"  Deep long-tailed recognition has been widely studied to address the issue of\nimbalanced data distributions in real-world scenarios. However, there has been\ninsufficient focus on the design of neural architectures, despite empirical\nevidence suggesting that architecture can significantly impact performance. In\nthis paper, we attempt to mitigate long-tailed issues through architectural\nimprovements. To simplify the design process, we utilize Differential\nArchitecture Search (DARTS) to achieve this goal. Unfortunately, existing DARTS\nmethods struggle to perform well in long-tailed scenarios. To tackle this\nchallenge, we introduce Long-Tailed Differential Architecture Search (LTDAS).\nSpecifically, we conduct extensive experiments to explore architectural\ncomponents that demonstrate better performance on long-tailed data and propose\na new search space based on our observations. This ensures that the\narchitecture obtained through our search process incorporates superior\ncomponents. Additionally, we propose replacing the learnable linear classifier\nwith an Equiangular Tight Frame (ETF) classifier to further enhance our method.\nThis classifier effectively alleviates the biased search process and prevents\nperformance collapse. Extensive experimental evaluations demonstrate that our\napproach consistently improves upon existing methods from an orthogonal\nperspective and achieves state-of-the-art results with simple enhancements.\n","authors":["Yuhan Pan","Yanan Sun","Wei Gong"],"pdf_url":"https://arxiv.org/pdf/2411.06098v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06892v2","updated":"2024-12-02T11:24:20Z","published":"2024-03-11T16:48:25Z","title":"Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head","summary":"  End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}\n","authors":["Tiancheng Zhao","Peng Liu","Xuan He","Lu Zhang","Kyusong Lee"],"pdf_url":"https://arxiv.org/pdf/2403.06892v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2405.12789v3","updated":"2024-12-02T11:16:09Z","published":"2024-05-21T13:40:30Z","title":"Anticipating Object State Changes in Long Procedural Videos","summary":"  In this work, we introduce (a) the new problem of anticipating object state\nchanges in images and videos during procedural activities, (b) new curated\nannotation data for object state change classification based on the Ego4D\ndataset, and (c) the first method for addressing this challenging problem.\nSolutions to this new task have important implications in vision-based scene\nunderstanding, automated monitoring systems, and action planning. The proposed\nnovel framework predicts object state changes that will occur in the near\nfuture due to yet unseen human actions by integrating learned visual features\nthat represent recent visual information with natural language (NLP) features\nthat represent past object state changes and actions. Leveraging the extensive\nand challenging Ego4D dataset which provides a large-scale collection of\nfirst-person perspective videos across numerous interaction scenarios, we\nintroduce an extension noted Ego4D-OSCA that provides new curated annotation\ndata for the object state change anticipation task (OSCA). An extensive\nexperimental evaluation is presented demonstrating the proposed method's\nefficacy in predicting object state changes in dynamic scenarios. The\nperformance of the proposed approach also underscores the potential of\nintegrating video and linguistic cues to enhance the predictive performance of\nvideo understanding systems and lays the groundwork for future research on the\nnew task of object state change anticipation. The source code and the new\nannotation data (Ego4D-OSCA) will be made publicly available.\n","authors":["Victoria Manousaki","Konstantinos Bacharidis","Filippos Gouidis","Konstantinos Papoutsakis","Dimitris Plexousakis","Antonis Argyros"],"pdf_url":"https://arxiv.org/pdf/2405.12789v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04632v2","updated":"2024-12-02T10:48:28Z","published":"2024-11-07T11:35:31Z","title":"Improved Multi-Task Brain Tumour Segmentation with Synthetic Data\n  Augmentation","summary":"  This paper presents the winning solution of task 1 and the third-placed\nsolution of task 3 of the BraTS challenge. The use of automated tools in\nclinical practice has increased due to the development of more and more\nsophisticated and reliable algorithms. However, achieving clinical standards\nand developing tools for real-life scenarios is a major challenge. To this end,\nBraTS has organised tasks to find the most advanced solutions for specific\npurposes. In this paper, we propose the use of synthetic data to train\nstate-of-the-art frameworks in order to improve the segmentation of adult\ngliomas in a post-treatment scenario, and the segmentation of meningioma for\nradiotherapy planning. Our results suggest that the use of synthetic data leads\nto more robust algorithms, although the synthetic data generation pipeline is\nnot directly suited to the meningioma task. In task 1, we achieved a DSC of\n0.7900, 0.8076, 0.7760, 0.8926, 0.7874, 0.8938 and a HD95 of 35.63, 30.35,\n44.58, 16.87, 38.19, 17.95 for ET, NETC, RC, SNFH, TC and WT, respectively and,\nin task 3, we achieved a DSC of 0.801 and HD95 of 38.26, in the testing phase.\nThe code for these tasks is available at\nhttps://github.com/ShadowTwin41/BraTS_2023_2024_solutions.\n","authors":["André Ferreira","Tiago Jesus","Behrus Puladi","Jens Kleesiek","Victor Alves","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2411.04632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09874v4","updated":"2024-12-02T10:33:19Z","published":"2023-03-17T10:38:27Z","title":"Image Statistics Predict the Sensitivity of Perceptual Quality Metrics","summary":"  Previously, Barlow and Attneave hypothesised a link between biological vision\nand information maximisation. Following Shannon, information was defined using\nthe probability of natural images. Several physiological and psychophysical\nphenomena have been derived from principles like info-max, efficient coding, or\noptimal denoising. However, it remains unclear how this link is expressed in\nmathematical terms from image probability. Classical derivations were subjected\nto strong assumptions on the probability models and on the behaviour of the\nsensors. Moreover, the direct evaluation of the hypothesis was limited by the\ninability of classical image models to deliver accurate estimates of the\nprobability. Here, we directly evaluate image probabilities using a generative\nmodel for natural images, and analyse how probability-related factors can be\ncombined to predict the sensitivity of state-of-the-art subjective image\nquality metrics, a proxy for human perception. We use information theory and\nregression analysis to find a simple model that when combining just two\nprobability-related factors achieves 0.77 correlation with subjective metrics.\nThis probability-based model is validated in two ways: through direct\ncomparison with the opinion of real observers in a subjective quality\nexperiment, and by reproducing basic trends of classical psychophysical facts\nsuch as the Contrast Sensitivity Function, the Weber-law, and contrast masking.\n","authors":["Alexander Hepburn","Valero Laparra","Raúl Santos-Rodriguez","Jesús Malo"],"pdf_url":"https://arxiv.org/pdf/2303.09874v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01574v4","updated":"2024-12-02T10:26:39Z","published":"2023-09-04T12:53:54Z","title":"Object-Size-Driven Design of Convolutional Neural Networks: Virtual Axle\n  Detection based on Raw Data","summary":"  As infrastructure ages, the need for efficient monitoring methods becomes\nincreasingly critical. Bridge Weigh-In-Motion (BWIM) systems are crucial for\ncost-effective determination of loads and, consequently, the residual service\nlife of road and railway infrastructure. However, conventional BWIM systems\nrequire additional sensors for axle detection, which must be installed in\npotentially inaccessible locations or places that interfere with bridge\noperation.\n  This study presents a novel approach for real-time detection of train axles\nusing sensors arbitrarily placed on bridges, providing an alternative to\ndedicated axle detectors. The developed Virtual Axle Detector with Enhanced\nReceptive Field (VADER) has been validated on a single-track railway bridge\nusing only acceleration measurements, detecting 99.9% of axles with a spatial\nerror of 3.69cm. Using raw data as input outperformed the state-of-the-art\nspectrogram-based method in both speed and memory usage by 99%, thereby making\nreal-time application feasible for the first time.\n  Additionally, we introduce the Maximum Receptive Field (MRF) rule, a novel\napproach to optimise hyperparameters of Convolutional Neural Networks (CNNs)\nbased on the size of objects. In this context, the object size relates to the\nfundamental frequency of a bridge. The MRF rule effectively narrows the\nhyperparameter search space, overcoming the need for extensive hyperparameter\ntuning. Since the MRF rule can theoretically be applied to all unstructured\ndata, it could have implications for a wide range of deep learning problems,\nfrom earthquake prediction to object recognition.\n","authors":["Henik Riedel","Robert Steven Lorenzen","Clemens Hübler"],"pdf_url":"https://arxiv.org/pdf/2309.01574v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12044v3","updated":"2024-12-02T10:17:48Z","published":"2024-06-17T19:31:24Z","title":"ARTIST: Improving the Generation of Text-rich Images with Disentangled\n  Diffusion Models and Large Language Models","summary":"  Diffusion models have demonstrated exceptional capabilities in generating a\nbroad spectrum of visual content, yet their proficiency in rendering text is\nstill limited: they often generate inaccurate characters or words that fail to\nblend well with the underlying image. To address these shortcomings, we\nintroduce a novel framework named, ARTIST, which incorporates a dedicated\ntextual diffusion model to focus on the learning of text structures\nspecifically. Initially, we pretrain this textual model to capture the\nintricacies of text representation. Subsequently, we finetune a visual\ndiffusion model, enabling it to assimilate textual structure information from\nthe pretrained textual model. This disentangled architecture design and\ntraining strategy significantly enhance the text rendering ability of the\ndiffusion models for text-rich image generation. Additionally, we leverage the\ncapabilities of pretrained large language models to interpret user intentions\nbetter, contributing to improved generation quality. Empirical results on the\nMARIO-Eval benchmark underscore the effectiveness of the proposed method,\nshowing an improvement of up to 15% in various metrics.\n","authors":["Jianyi Zhang","Yufan Zhou","Jiuxiang Gu","Curtis Wigington","Tong Yu","Yiran Chen","Tong Sun","Ruiyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.12044v3.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2303.17550v6","updated":"2024-12-02T10:06:28Z","published":"2023-03-30T17:18:31Z","title":"DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with\n  Diffusion Autoencoder","summary":"  While recent research has made significant progress in speech-driven talking\nface generation, the quality of the generated video still lags behind that of\nreal recordings. One reason for this is the use of handcrafted intermediate\nrepresentations like facial landmarks and 3DMM coefficients, which are designed\nbased on human knowledge and are insufficient to precisely describe facial\nmovements. Additionally, these methods require an external pretrained model for\nextracting these representations, whose performance sets an upper bound on\ntalking face generation. To address these limitations, we propose a novel\nmethod called DAE-Talker that leverages data-driven latent representations\nobtained from a diffusion autoencoder (DAE). DAE contains an image encoder that\nencodes an image into a latent vector and a DDIM image decoder that\nreconstructs the image from it. We train our DAE on talking face video frames\nand then extract their latent representations as the training target for a\nConformer-based speech2latent model. This allows DAE-Talker to synthesize full\nvideo frames and produce natural head movements that align with the content of\nspeech, rather than relying on a predetermined head pose from a template video.\nWe also introduce pose modelling in speech2latent for pose controllability.\nAdditionally, we propose a novel method for generating continuous video frames\nwith the DDIM image decoder trained on individual frames, eliminating the need\nfor modelling the joint distribution of consecutive frames directly. Our\nexperiments show that DAE-Talker outperforms existing popular methods in\nlip-sync, video fidelity, and pose naturalness. We also conduct ablation\nstudies to analyze the effectiveness of the proposed techniques and demonstrate\nthe pose controllability of DAE-Talker.\n","authors":["Chenpeng Du","Qi Chen","Tianyu He","Xu Tan","Xie Chen","Kai Yu","Sheng Zhao","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2303.17550v6.pdf","comment":"Accepted to ACM Multimedia 2023"},{"id":"http://arxiv.org/abs/2409.15344v2","updated":"2024-12-02T09:45:07Z","published":"2024-09-10T07:04:48Z","title":"Video-Driven Graph Network-Based Simulators","summary":"  Lifelike visualizations in design, cinematography, and gaming rely on precise\nphysics simulations, typically requiring extensive computational resources and\ndetailed physical input. This paper presents a method that can infer a system's\nphysical properties from a short video, eliminating the need for explicit\nparameter input, provided it is close to the training condition. The learned\nrepresentation is then used within a Graph Network-based Simulator to emulate\nthe trajectories of physical systems. We demonstrate that the video-derived\nencodings effectively capture the physical properties of the system and\nshowcase a linear dependence between some of the encodings and the system's\nmotion.\n","authors":["Franciszek Szewczyk","Gilles Louppe","Matthia Sabatelli"],"pdf_url":"https://arxiv.org/pdf/2409.15344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18810v2","updated":"2024-12-02T09:10:34Z","published":"2024-11-27T23:32:54Z","title":"Enhancing Compositional Text-to-Image Generation with Reliable Random\n  Seeds","summary":"  Text-to-image diffusion models have demonstrated remarkable capability in\ngenerating realistic images from arbitrary text prompts. However, they often\nproduce inconsistent results for compositional prompts such as \"two dogs\" or \"a\npenguin on the right of a bowl\". Understanding these inconsistencies is crucial\nfor reliable image generation. In this paper, we highlight the significant role\nof initial noise in these inconsistencies, where certain noise patterns are\nmore reliable for compositional prompts than others. Our analyses reveal that\ndifferent initial random seeds tend to guide the model to place objects in\ndistinct image areas, potentially adhering to specific patterns of camera\nangles and image composition associated with the seed. To improve the model's\ncompositional ability, we propose a method for mining these reliable cases,\nresulting in a curated training set of generated images without requiring any\nmanual annotation. By fine-tuning text-to-image models on these generated\nimages, we significantly enhance their compositional capabilities. For\nnumerical composition, we observe relative increases of 29.3% and 19.5% for\nStable Diffusion and PixArt-{\\alpha}, respectively. Spatial composition sees\neven larger gains, with 60.7% for Stable Diffusion and 21.1% for\nPixArt-{\\alpha}.\n","authors":["Shuangqi Li","Hieu Le","Jingyi Xu","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2411.18810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15778v2","updated":"2024-12-02T09:06:32Z","published":"2024-11-24T10:58:48Z","title":"Enhancing the automatic segmentation and analysis of 3D liver\n  vasculature models","summary":"  Surgical assessment of liver cancer patients requires identification of the\nvessel trees from medical images. Specifically, the venous trees - the portal\n(perfusing) and the hepatic (draining) trees are important for understanding\nthe liver anatomy and disease state, and perform surgery planning. This\nresearch aims to improve the 3D segmentation, skeletonization, and subsequent\nanalysis of vessel trees, by creating an automatic pipeline based on deep\nlearning and image processing techniques.\n  The first part of this work explores the impact of differentiable\nskeletonization methods such as ClDice and morphological skeletonization loss,\non the overall liver vessel segmentation performance. To this aim, it studies\nhow to improve vessel tree connectivity.\n  The second part of this study converts a single class vessel segmentation\ninto multi-class ones, separating the two venous trees. It builds on the\nprevious two-class vessel segmentation model, which vessel tree outputs might\nbe entangled, and on connected components and skeleton analyses of the trees.\n  After providing sub-labeling of the specific anatomical branches of each\nvenous tree, these algorithms also enable a morphometric analysis of the vessel\ntrees by extracting various geometrical markers.\n  In conclusion, we propose a method that successfully improves current\nskeletonization methods, for extensive vascular trees that contain vessels of\ndifferent calibers. The separation algorithm creates a clean multi-class\nsegmentation of the vessels, validated by surgeons to provide low error. A new,\npublicly shared high-quality liver vessel dataset of 77 cases is thus created.\nFinally a method to annotate vessel trees according to anatomy is provided,\nenabling a unique liver vessel morphometry analysis.\n","authors":["Yassine Machta","Omar Ali","Kevin Hakkakian","Ana Vlasceanu","Amaury Facque","Nicolas Golse","Irene Vignon-Clementel"],"pdf_url":"https://arxiv.org/pdf/2411.15778v2.pdf","comment":"Internship at Simbiotx"},{"id":"http://arxiv.org/abs/2411.17772v2","updated":"2024-12-02T09:04:20Z","published":"2024-11-26T08:55:20Z","title":"MVBoost: Boost 3D Reconstruction with Multi-View Refinement","summary":"  Recent advancements in 3D object reconstruction have been remarkable, yet\nmost current 3D models rely heavily on existing 3D datasets. The scarcity of\ndiverse 3D datasets results in limited generalization capabilities of 3D\nreconstruction models. In this paper, we propose a novel framework for boosting\n3D reconstruction with multi-view refinement (MVBoost) by generating pseudo-GT\ndata. The key of MVBoost is combining the advantages of the high accuracy of\nthe multi-view generation model and the consistency of the 3D reconstruction\nmodel to create a reliable data source. Specifically, given a single-view input\nimage, we employ a multi-view diffusion model to generate multiple views,\nfollowed by a large 3D reconstruction model to produce consistent 3D data.\nMVBoost then adaptively refines these multi-view images, rendered from the\nconsistent 3D data, to build a large-scale multi-view dataset for training a\nfeed-forward 3D reconstruction model. Additionally, the input view optimization\nis designed to optimize the corresponding viewpoints based on the user's input\nimage, ensuring that the most important viewpoint is accurately tailored to the\nuser's needs. Extensive evaluations demonstrate that our method achieves\nsuperior reconstruction results and robust generalization compared to prior\nworks.\n","authors":["Xiangyu Liu","Xiaomei Zhang","Zhiyuan Ma","Xiangyu Zhu","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2411.17772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17213v2","updated":"2024-12-02T08:59:20Z","published":"2024-11-26T08:29:24Z","title":"Scaling nnU-Net for CBCT Segmentation","summary":"  This paper presents our approach to scaling the nnU-Net framework for\nmulti-structure segmentation on Cone Beam Computed Tomography (CBCT) images,\nspecifically in the scope of the ToothFairy2 Challenge. We leveraged the\nnnU-Net ResEnc L model, introducing key modifications to patch size, network\ntopology, and data augmentation strategies to address the unique challenges of\ndental CBCT imaging. Our method achieved a mean Dice coefficient of 0.9253 and\nHD95 of 18.472 on the test set, securing a mean rank of 4.6 and with it the\nfirst place in the ToothFairy2 challenge. The source code is publicly\navailable, encouraging further research and development in the field.\n","authors":["Fabian Isensee","Yannick Kirchhoff","Lars Kraemer","Maximilian Rokuss","Constantin Ulrich","Klaus H. Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2411.17213v2.pdf","comment":"Fabian Isensee and Yannick Kirchhoff contributed equally"},{"id":"http://arxiv.org/abs/2409.09318v2","updated":"2024-12-02T08:51:09Z","published":"2024-09-14T05:31:29Z","title":"ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language\n  Models","summary":"  Hallucination poses a persistent challenge for multimodal large language\nmodels (MLLMs). However, existing benchmarks for evaluating hallucinations are\ngenerally static, which may overlook the potential risk of data contamination.\nTo address this issue, we propose ODE, an open-set, dynamic protocol designed\nto evaluate object hallucinations in MLLMs at both the existence and attribute\nlevels. ODE employs a graph-based structure to represent real-world object\nconcepts, their attributes, and the distributional associations between them.\nThis structure facilitates the extraction of concept combinations based on\ndiverse distributional criteria, generating varied samples for structured\nqueries that evaluate hallucinations in both generative and discriminative\ntasks. Through the generation of new samples, dynamic concept combinations, and\nvaried distribution frequencies, ODE mitigates the risk of data contamination\nand broadens the scope of evaluation. This protocol is applicable to both\ngeneral and specialized scenarios, including those with limited data.\nExperimental results demonstrate the effectiveness of our protocol, revealing\nthat MLLMs exhibit higher hallucination rates when evaluated with ODE-generated\nsamples, which indicates potential data contamination. Furthermore, these\ngenerated samples aid in analyzing hallucination patterns and fine-tuning\nmodels, offering an effective approach to mitigating hallucinations in MLLMs.\n","authors":["Yahan Tu","Rui Hu","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2409.09318v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11522v2","updated":"2024-12-02T08:43:40Z","published":"2024-07-16T09:00:45Z","title":"FIRE: A Dataset for Feedback Integration and Refinement Evaluation of\n  Multimodal Models","summary":"  Vision language models (VLMs) have achieved impressive progress in diverse\napplications, becoming a prevalent research direction. In this paper, we build\nFIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn\nconversations that are derived from 27 source datasets, empowering VLMs to\nspontaneously refine their responses based on user feedback across diverse\ntasks. To scale up the data collection, FIRE is collected in two components:\nFIRE-100K and FIRE-1M, where FIRE-100K is generated by GPT-4V, and FIRE-1M is\nfreely generated via models trained on FIRE-100K. Then, we build FIRE-Bench, a\nbenchmark to comprehensively evaluate the feedback-refining capability of VLMs,\nwhich contains 11K feedback-refinement conversations as the test data, two\nevaluation settings, and a model to provide feedback for VLMs. We develop the\nFIRE-LLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, which shows\nremarkable feedback-refining capability on FIRE-Bench and outperforms untrained\nVLMs by 50%, making more efficient user-agent interactions and underscoring the\nsignificance of the FIRE dataset.\n","authors":["Pengxiang Li","Zhi Gao","Bofei Zhang","Tao Yuan","Yuwei Wu","Mehrtash Harandi","Yunde Jia","Song-Chun Zhu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2407.11522v2.pdf","comment":"NeurIPS 2024 Dataset & Benchmark Track"},{"id":"http://arxiv.org/abs/2410.07278v2","updated":"2024-12-02T08:43:33Z","published":"2024-10-09T07:13:22Z","title":"PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal\n  Models","summary":"  Multimodal large language models (MLLMs) demonstrate strong performance\nacross visual tasks, but their efficiency is hindered by significant\ncomputational and memory demands from processing long contexts in multimodal\ninputs. To address this, we introduce PAR (Prompt-Aware Token Reduction), a\nnovel and plug-and-play approach that reduces visual tokens efficiently without\ncompromising model performance. Unlike previous methods that rely heavily on\nattention mechanisms and overlooking cross-modal interactions , we uses a\nprompt-aware strategy to adpative identify and cluster essential visual tokens.\nPAR categorizes visual context redundancy into two types: external and\ninternal. External redundancy is minimized through semantic retrieval, while\ninternal redundancy is addressed using a token routing mechanism. This method\nsubstantially reduces computational load without requiring additional training\nor complex architectural modifications. \\textbf{Experimental results\ndemonstrate that across various visual question answering tasks, PAR reduces\nFLOPs by 83\\% with a compression ratio of 89\\%, while retaining 97\\% of\nbaseline accuracy.} The adaptive design of PAR achieves a 2x token reduction\nratio compared to prior approaches, enabling a better balance between\nperformance and efficiency.\n","authors":["Yingen Liu","Fan Wu","Ruihui Li","Zhuo Tang","Kenli Li"],"pdf_url":"https://arxiv.org/pdf/2410.07278v2.pdf","comment":"10 pages, 5 figures,3 tables"},{"id":"http://arxiv.org/abs/2405.16605v2","updated":"2024-12-02T08:41:46Z","published":"2024-05-26T15:31:09Z","title":"Demystify Mamba in Vision: A Linear Attention Perspective","summary":"  Mamba is an effective state space model with linear computation complexity.\nIt has recently shown impressive efficiency in dealing with high-resolution\ninputs across various vision tasks. In this paper, we reveal that the powerful\nMamba model shares surprising similarities with linear attention Transformer,\nwhich typically underperform conventional Transformer in practice. By exploring\nthe similarities and disparities between the effective Mamba and subpar linear\nattention Transformer, we provide comprehensive analyses to demystify the key\nfactors behind Mamba's success. Specifically, we reformulate the selective\nstate space model and linear attention within a unified formulation, rephrasing\nMamba as a variant of linear attention Transformer with six major distinctions:\ninput gate, forget gate, shortcut, no attention normalization, single-head, and\nmodified block design. For each design, we meticulously analyze its pros and\ncons, and empirically evaluate its impact on model performance in vision tasks.\nInterestingly, the results highlight the forget gate and block design as the\ncore contributors to Mamba's success, while the other four designs are less\ncrucial. Based on these findings, we propose a Mamba-Inspired Linear Attention\n(MILA) model by incorporating the merits of these two key designs into linear\nattention. The resulting model outperforms various vision Mamba models in both\nimage classification and high-resolution dense prediction tasks, while enjoying\nparallelizable computation and fast inference speed. Code is available at\nhttps://github.com/LeapLabTHU/MLLA.\n","authors":["Dongchen Han","Ziyi Wang","Zhuofan Xia","Yizeng Han","Yifan Pu","Chunjiang Ge","Jun Song","Shiji Song","Bo Zheng","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2405.16605v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2303.10211v5","updated":"2024-12-02T08:37:24Z","published":"2023-03-17T19:00:24Z","title":"SITReg: Multi-resolution architecture for symmetric, inverse consistent,\n  and topology preserving image registration","summary":"  Deep learning has emerged as a strong alternative for classical iterative\nmethods for deformable medical image registration, where the goal is to find a\nmapping between the coordinate systems of two images. Popular classical image\nregistration methods enforce the useful inductive biases of symmetricity,\ninverse consistency, and topology preservation by construction. However, while\nmany deep learning registration methods encourage these properties via loss\nfunctions, no earlier methods enforce all of them by construction. Here, we\npropose a novel registration architecture based on extracting multi-resolution\nfeature representations which is by construction symmetric, inverse consistent,\nand topology preserving. We also develop an implicit layer for memory efficient\ninversion of the deformation fields. Our method achieves state-of-the-art\nregistration accuracy on three datasets. The code is available at\nhttps://github.com/honkamj/SITReg.\n","authors":["Joel Honkamaa","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2303.10211v5.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:026"},{"id":"http://arxiv.org/abs/2408.10845v2","updated":"2024-12-02T08:31:40Z","published":"2024-08-19T09:53:49Z","title":"CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous\n  Driving","summary":"  Autonomous driving, particularly navigating complex and unanticipated\nscenarios, demands sophisticated reasoning and planning capabilities. While\nMulti-modal Large Language Models (MLLMs) offer a promising avenue for this,\ntheir use has been largely confined to understanding complex environmental\ncontexts or generating high-level driving commands, with few studies extending\ntheir application to end-to-end path planning. A major research bottleneck is\nthe lack of large-scale annotated datasets encompassing vision, language, and\naction. To address this issue, we propose CoVLA (Comprehensive\nVision-Language-Action) Dataset, an extensive dataset comprising real-world\ndriving videos spanning more than 80 hours. This dataset leverages a novel,\nscalable approach based on automated data processing and a caption generation\npipeline to generate accurate driving trajectories paired with detailed natural\nlanguage descriptions of driving environments and maneuvers. This approach\nutilizes raw in-vehicle sensor data, allowing it to surpass existing datasets\nin scale and annotation richness. Using CoVLA, we investigate the driving\ncapabilities of MLLMs that can handle vision, language, and action in a variety\nof driving scenarios. Our results illustrate the strong proficiency of our\nmodel in generating coherent language and action outputs, emphasizing the\npotential of Vision-Language-Action (VLA) models in the field of autonomous\ndriving. This dataset establishes a framework for robust, interpretable, and\ndata-driven autonomous driving systems by providing a comprehensive platform\nfor training and evaluating VLA models, contributing to safer and more reliable\nself-driving vehicles. The dataset is released for academic purpose.\n","authors":["Hidehisa Arai","Keita Miwa","Kento Sasaki","Yu Yamaguchi","Kohei Watanabe","Shunsuke Aoki","Issei Yamamoto"],"pdf_url":"https://arxiv.org/pdf/2408.10845v2.pdf","comment":"WACV 2025, Project Page: https://turingmotors.github.io/covla-ad/"},{"id":"http://arxiv.org/abs/2411.12787v2","updated":"2024-12-02T07:41:38Z","published":"2024-11-19T11:03:09Z","title":"Visual Cue Enhancement and Dual Low-Rank Adaptation for Efficient Visual\n  Instruction Fine-Tuning","summary":"  Parameter-efficient fine-tuning multimodal large language models (MLLMs)\npresents significant challenges, including reliance on high-level visual\nfeatures that limit fine-grained detail comprehension, and data conflicts that\narise from task complexity. To address these issues, we propose an efficient\nfine-tuning framework with two novel approaches: Vision Cue Enhancement (VCE)\nand Dual Low-Rank Adaptation (Dual-LoRA). VCE enhances the vision projector by\nintegrating multi-level visual cues, improving the model's ability to capture\nfine-grained visual features. Dual-LoRA introduces a dual low-rank structure\nfor instruction tuning, decoupling learning into skill and task spaces to\nenable precise control and efficient adaptation across diverse tasks. Our\nmethod simplifies implementation, enhances visual comprehension, and improves\nadaptability. Experiments on both downstream tasks and general benchmarks\ndemonstrate the effectiveness of our proposed approach.\n","authors":["Pengkun Jiao","Bin Zhu","Jingjing Chen","Chong-Wah Ngo","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.12787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12027v3","updated":"2024-12-02T07:22:40Z","published":"2024-03-18T17:57:09Z","title":"From Pixels to Insights: A Survey on Automatic Chart Understanding in\n  the Era of Large Foundation Models","summary":"  Data visualization in the form of charts plays a pivotal role in data\nanalysis, offering critical insights and aiding in informed decision-making.\nAutomatic chart understanding has witnessed significant advancements with the\nrise of large foundation models in recent years. Foundation models, such as\nlarge language models, have revolutionized various natural language processing\ntasks and are increasingly being applied to chart understanding tasks. This\nsurvey paper provides a comprehensive overview of the recent developments,\nchallenges, and future directions in chart understanding within the context of\nthese foundation models. We review fundamental building blocks crucial for\nstudying chart understanding tasks. Additionally, we explore various tasks and\ntheir evaluation metrics and sources of both charts and textual inputs. Various\nmodeling strategies are then examined, encompassing both classification-based\nand generation-based approaches, along with tool augmentation techniques that\nenhance chart understanding performance. Furthermore, we discuss the\nstate-of-the-art performance of each task and discuss how we can improve the\nperformance. Challenges and future directions are addressed, highlighting the\nimportance of several topics, such as domain-specific charts, lack of efforts\nin developing evaluation metrics, and agent-oriented settings. This survey\npaper serves as a comprehensive resource for researchers and practitioners in\nthe fields of natural language processing, computer vision, and data analysis,\nproviding valuable insights and directions for future research in chart\nunderstanding leveraging large foundation models. The studies mentioned in this\npaper, along with emerging new research, will be continually updated at:\nhttps://github.com/khuangaf/Awesome-Chart-Understanding.\n","authors":["Kung-Hsiang Huang","Hou Pong Chan","Yi R. Fung","Haoyi Qiu","Mingyang Zhou","Shafiq Joty","Shih-Fu Chang","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.12027v3.pdf","comment":"IEEE Transactions on Knowledge and Data Engineering (TKDE)"},{"id":"http://arxiv.org/abs/2411.16316v3","updated":"2024-12-02T07:05:29Z","published":"2024-11-25T12:09:43Z","title":"Monocular Lane Detection Based on Deep Learning: A Survey","summary":"  Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.\n","authors":["Xin He","Haiyun Guo","Kuan Zhu","Bingke Zhu","Xu Zhao","Jianwu Fang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16316v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18363v2","updated":"2024-12-02T07:04:40Z","published":"2024-11-27T14:11:10Z","title":"ChatRex: Taming Multimodal LLM for Joint Perception and Understanding","summary":"  Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After standard two-stage training,\nChatRex demonstrates strong perception capabilities while preserving multimodal\nunderstanding performance. The combination of these two capabilities\nsimultaneously unlocks many attractive applications, demonstrating the\ncomplementary roles of both perception and understanding in MLLM. Code is\navailable at \\url{https://github.com/IDEA-Research/ChatRex}.\n","authors":["Qing Jiang","Gen Luo","Yuqin Yang","Yuda Xiong","Yihao Chen","Zhaoyang Zeng","Tianhe Ren","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.18363v2.pdf","comment":"35 pages, 19 figures"},{"id":"http://arxiv.org/abs/2411.19951v2","updated":"2024-12-02T06:54:47Z","published":"2024-11-29T18:59:54Z","title":"T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs","summary":"  The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid.\n","authors":["Shukang Yin","Chaoyou Fu","Sirui Zhao","Yunhang Shen","Chunjiang Ge","Yan Yang","Zuwei Long","Yuhan Dai","Tong Xu","Xing Sun","Ran He","Caifeng Shan","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.19951v2.pdf","comment":"Project page: https://github.com/xjtupanda/T2Vid"},{"id":"http://arxiv.org/abs/2311.06394v3","updated":"2024-12-02T06:22:54Z","published":"2023-11-10T20:50:36Z","title":"A design of Convolutional Neural Network model for the Diagnosis of the\n  COVID-19","summary":"  With the spread of COVID-19 around the globe over the past year, the usage of\nartificial intelligence (AI) algorithms and image processing methods to analyze\nthe X-ray images of patients' chest with COVID-19 has become essential. The\nCOVID-19 virus recognition in the lung area of a patient is one of the basic\nand essential needs of clicical centers and hospitals. Most research in this\nfield has been devoted to papers on the basis of deep learning methods\nutilizing CNNs (Convolutional Neural Network), which mainly deal with the\nscreening of sick and healthy people.In this study, a new structure of a\n19-layer CNN has been recommended for accurately recognition of the COVID-19\nfrom the X-ray pictures of chest. The offered CNN is developed to serve as a\nprecise diagnosis system for a three class (viral pneumonia, Normal, COVID) and\na four classclassification (Lung opacity, Normal, COVID-19, and pneumonia). A\ncomparison is conducted among the outcomes of the offered procedure and some\npopular pretrained networks, including Inception, Alexnet, ResNet50,\nSqueezenet, and VGG19 and based on Specificity, Accuracy, Precision,\nSensitivity, Confusion Matrix, and F1-score. The experimental results of the\noffered CNN method specify its dominance over the existing published\nprocedures. This method can be a useful tool for clinicians in deciding\nproperly about COVID-19.\n","authors":["Xinyuan Song"],"pdf_url":"https://arxiv.org/pdf/2311.06394v3.pdf","comment":"Important mistakes. Also, another author has contributed some to the\n  revised version. So it is not appropriate for it to be with only my name"},{"id":"http://arxiv.org/abs/2411.11477v2","updated":"2024-12-02T05:58:49Z","published":"2024-11-18T11:26:11Z","title":"SL-YOLO: A Stronger and Lighter Drone Target Detection Model","summary":"  Detecting small objects in complex scenes, such as those captured by drones,\nis a daunting challenge due to the difficulty in capturing the complex features\nof small targets. While the YOLO family has achieved great success in large\ntarget detection, its performance is less than satisfactory when faced with\nsmall targets. Because of this, this paper proposes a revolutionary model\nSL-YOLO (Stronger and Lighter YOLO) that aims to break the bottleneck of small\ntarget detection. We propose the Hierarchical Extended Path Aggregation Network\n(HEPAN), a pioneering cross-scale feature fusion method that can ensure\nunparalleled detection accuracy even in the most challenging environments. At\nthe same time, without sacrificing detection capabilities, we design the C2fDCB\nlightweight module and add the SCDown downsampling module to greatly reduce the\nmodel's parameters and computational complexity. Our experimental results on\nthe VisDrone2019 dataset reveal a significant improvement in performance, with\nmAP@0.5 jumping from 43.0% to 46.9% and mAP@0.5:0.95 increasing from 26.0% to\n28.9%. At the same time, the model parameters are reduced from 11.1M to 9.6M,\nand the FPS can reach 132, making it an ideal solution for real-time small\nobject detection in resource-constrained environments.\n","authors":["Defan Chen","Luchan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.11477v2.pdf","comment":"We are withdrawing this submission to incorporate substantial updates\n  and improvements to the manuscript, including additional data and analysis"},{"id":"http://arxiv.org/abs/2411.19454v2","updated":"2024-12-02T05:47:15Z","published":"2024-11-29T03:54:54Z","title":"GausSurf: Geometry-Guided 3D Gaussian Splatting for Surface\n  Reconstruction","summary":"  3D Gaussian Splatting has achieved impressive performance in novel view\nsynthesis with real-time rendering capabilities. However, reconstructing\nhigh-quality surfaces with fine details using 3D Gaussians remains a\nchallenging task. In this work, we introduce GausSurf, a novel approach to\nhigh-quality surface reconstruction by employing geometry guidance from\nmulti-view consistency in texture-rich areas and normal priors in texture-less\nareas of a scene. We observe that a scene can be mainly divided into two\nprimary regions: 1) texture-rich and 2) texture-less areas. To enforce\nmulti-view consistency at texture-rich areas, we enhance the reconstruction\nquality by incorporating a traditional patch-match based Multi-View Stereo\n(MVS) approach to guide the geometry optimization in an iterative scheme. This\nscheme allows for mutual reinforcement between the optimization of Gaussians\nand patch-match refinement, which significantly improves the reconstruction\nresults and accelerates the training process. Meanwhile, for the texture-less\nareas, we leverage normal priors from a pre-trained normal estimation model to\nguide optimization. Extensive experiments on the DTU and Tanks and Temples\ndatasets demonstrate that our method surpasses state-of-the-art methods in\nterms of reconstruction quality and computation time.\n","authors":["Jiepeng Wang","Yuan Liu","Peng Wang","Cheng Lin","Junhui Hou","Xin Li","Taku Komura","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2411.19454v2.pdf","comment":"Project page: https://jiepengwang.github.io/GausSurf/"},{"id":"http://arxiv.org/abs/2411.10831v2","updated":"2024-12-02T05:36:39Z","published":"2024-11-16T16:24:28Z","title":"Neighboring Slice Noise2Noise: Self-Supervised Medical Image Denoising\n  from Single Noisy Image Volume","summary":"  In the last few years, with the rapid development of deep learning\ntechnologies, supervised methods based on convolutional neural networks have\ngreatly enhanced the performance of medical image denoising. However, these\nmethods require large quantities of noisy-clean image pairs for training, which\ngreatly limits their practicality. Although some researchers have attempted to\ntrain denoising networks using only single noisy images, existing\nself-supervised methods, including blind-spot-based and data-splitting-based\nmethods, heavily rely on the assumption that noise is pixel-wise independent.\nHowever, this assumption often does not hold in real-world medical images.\nTherefore, in the field of medical imaging, there remains a lack of simple and\npractical denoising methods that can achieve high-quality denoising performance\nusing only single noisy images. In this paper, we propose a novel\nself-supervised medical image denoising method, Neighboring Slice Noise2Noise\n(NS-N2N). The proposed method utilizes neighboring slices within a single noisy\nimage volume to construct weighted training data, and then trains the denoising\nnetwork using a self-supervised scheme with regional consistency loss and\ninter-slice continuity loss. NS-N2N only requires a single noisy image volume\nobtained from one medical imaging procedure to achieve high-quality denoising\nof the image volume itself. Extensive experiments demonstrate that the proposed\nmethod outperforms state-of-the-art self-supervised denoising methods in both\ndenoising performance and processing efficiency. Furthermore, since NS-N2N\noperates solely in the image domain, it is free from device-specific issues\nsuch as reconstruction geometry, making it easier to apply in various clinical\npractices.\n","authors":["Langrui Zhou","Ziteng Zhou","Xinyu Huang","Xiangyu Zhang","Huiru Wang","Guang Li"],"pdf_url":"https://arxiv.org/pdf/2411.10831v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09806v2","updated":"2024-12-02T05:18:11Z","published":"2024-07-13T08:52:44Z","title":"Asynchronous Feedback Network for Perceptual Point Cloud Quality\n  Assessment","summary":"  Recent years have witnessed the success of the deep learning-based technique\nin research of no-reference point cloud quality assessment (NR-PCQA). For a\nmore accurate quality prediction, many previous studies have attempted to\ncapture global and local features in a bottom-up manner, but ignored the\ninteraction and promotion between them. To solve this problem, we propose a\nnovel asynchronous feedback quality prediction network (AFQ-Net). Motivated by\nhuman visual perception mechanisms, AFQ-Net employs a dual-branch structure to\ndeal with global and local features, simulating the left and right hemispheres\nof the human brain, and constructs a feedback module between them.\nSpecifically, the input point clouds are first fed into a transformer-based\nglobal encoder to generate the attention maps that highlight these semantically\nrich regions, followed by being merged into the global feature. Then, we\nutilize the generated attention maps to perform dynamic convolution for\ndifferent semantic regions and obtain the local feature. Finally, a\ncoarse-to-fine strategy is adopted to merge the two features into the final\nquality score. We conduct comprehensive experiments on three datasets and\nachieve superior performance over the state-of-the-art approaches on all of\nthese datasets. The code will be available at The code will be available at\nhttps://github.com/zhangyujie-1998/AFQ-Net.\n","authors":["Yujie Zhang","Qi Yang","Ziyu Shan","Yiling Xu"],"pdf_url":"https://arxiv.org/pdf/2407.09806v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15523v4","updated":"2024-12-02T05:09:06Z","published":"2023-05-24T19:20:59Z","title":"Task-aware Distributed Source Coding under Dynamic Bandwidth","summary":"  Efficient compression of correlated data is essential to minimize\ncommunication overload in multi-sensor networks. In such networks, each sensor\nindependently compresses the data and transmits them to a central node due to\nlimited communication bandwidth. A decoder at the central node decompresses and\npasses the data to a pre-trained machine learning-based task to generate the\nfinal output. Thus, it is important to compress the features that are relevant\nto the task. Additionally, the final performance depends heavily on the total\navailable bandwidth. In practice, it is common to encounter varying\navailability in bandwidth, and higher bandwidth results in better performance\nof the task. We design a novel distributed compression framework composed of\nindependent encoders and a joint decoder, which we call neural distributed\nprincipal component analysis (NDPCA). NDPCA flexibly compresses data from\nmultiple sources to any available bandwidth with a single model, reducing\ncomputing and storage overhead. NDPCA achieves this by learning low-rank task\nrepresentations and efficiently distributing bandwidth among sensors, thus\nproviding a graceful trade-off between performance and bandwidth. Experiments\nshow that NDPCA improves the success rate of multi-view robotic arm\nmanipulation by 9% and the accuracy of object detection tasks on satellite\nimagery by 14% compared to an autoencoder with uniform bandwidth allocation.\n","authors":["Po-han Li","Sravan Kumar Ankireddy","Ruihan Zhao","Hossein Nourkhiz Mahjoub","Ehsan Moradi-Pari","Ufuk Topcu","Sandeep Chinchali","Hyeji Kim"],"pdf_url":"https://arxiv.org/pdf/2305.15523v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18203v2","updated":"2024-12-02T05:00:19Z","published":"2024-11-27T10:28:57Z","title":"Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning","summary":"  Vision-language models (VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.\n","authors":["Di Zhang","Junxian Li","Jingdi Lei","Xunzhi Wang","Yujie Liu","Zonglin Yang","Jiatong Li","Weida Wang","Suorong Yang","Jianbo Wu","Peng Ye","Wanli Ouyang","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.18203v2.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.18673v2","updated":"2024-12-02T04:43:30Z","published":"2024-11-27T18:49:13Z","title":"AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion\n  Transformers","summary":"  Numerous works have recently integrated 3D camera control into foundational\ntext-to-video models, but the resulting camera control is often imprecise, and\nvideo generation quality suffers. In this work, we analyze camera motion from a\nfirst principles perspective, uncovering insights that enable precise 3D camera\nmanipulation without compromising synthesis quality. First, we determine that\nmotion induced by camera movements in videos is low-frequency in nature. This\nmotivates us to adjust train and test pose conditioning schedules, accelerating\ntraining convergence while improving visual and motion quality. Then, by\nprobing the representations of an unconditional video diffusion transformer, we\nobserve that they implicitly perform camera pose estimation under the hood, and\nonly a sub-portion of their layers contain the camera information. This\nsuggested us to limit the injection of camera conditioning to a subset of the\narchitecture to prevent interference with other video features, leading to 4x\nreduction of training parameters, improved training speed and 10% higher visual\nquality. Finally, we complement the typical dataset for camera control learning\nwith a curated dataset of 20K diverse dynamic videos with stationary cameras.\nThis helps the model disambiguate the difference between camera and scene\nmotion, and improves the dynamics of generated pose-conditioned videos. We\ncompound these findings to design the Advanced 3D Camera Control (AC3D)\narchitecture, the new state-of-the-art model for generative video modeling with\ncamera control.\n","authors":["Sherwin Bahmani","Ivan Skorokhodov","Guocheng Qian","Aliaksandr Siarohin","Willi Menapace","Andrea Tagliasacchi","David B. Lindell","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2411.18673v2.pdf","comment":"Project Page: https://snap-research.github.io/ac3d/"},{"id":"http://arxiv.org/abs/2410.10269v2","updated":"2024-12-02T04:27:39Z","published":"2024-10-14T08:21:08Z","title":"Two-Stage Approach for Brain MR Image Synthesis: 2D Image Synthesis and\n  3D Refinement","summary":"  Despite significant advancements in automatic brain tumor segmentation\nmethods, their performance is not guaranteed when certain MR sequences are\nmissing. Addressing this issue, it is crucial to synthesize the missing MR\nimages that reflect the unique characteristics of the absent modality with\nprecise tumor representation. Typically, MRI synthesis methods generate partial\nimages rather than full-sized volumes due to computational constraints. This\nlimitation can lead to a lack of comprehensive 3D volumetric information and\nresult in image artifacts during the merging process. In this paper, we propose\na two-stage approach that first synthesizes MR images from 2D slices using a\nnovel intensity encoding method and then refines the synthesized MRI. The\nproposed intensity encoding reduces artifacts when synthesizing MRI on a 2D\nslice basis. Then, the \\textit{Refiner}, which leverages complete 3D volume\ninformation, further improves the quality of the synthesized images and\nenhances their applicability to segmentation methods. Experimental results\ndemonstrate that the intensity encoding effectively minimizes artifacts in the\nsynthesized MRI and improves perceptual quality. Furthermore, using the\n\\textit{Refiner} on synthesized MRI significantly improves brain tumor\nsegmentation results, highlighting the potential of our approach in practical\napplications.\n","authors":["Jihoon Cho","Seunghyuck Park","Jinah Park"],"pdf_url":"https://arxiv.org/pdf/2410.10269v2.pdf","comment":"MICCAI 2024 BraSyn Challenge 1st place"},{"id":"http://arxiv.org/abs/2411.16752v2","updated":"2024-12-02T04:12:35Z","published":"2024-11-24T05:27:21Z","title":"Imagine and Seek: Improving Composed Image Retrieval with an Imagined\n  Proxy","summary":"  The Zero-shot Composed Image Retrieval (ZSCIR) requires retrieving images\nthat match the query image and the relative captions. Current methods focus on\nprojecting the query image into the text feature space, subsequently combining\nthem with features of query texts for retrieval. However, retrieving images\nonly with the text features cannot guarantee detailed alignment due to the\nnatural gap between images and text. In this paper, we introduce Imagined Proxy\nfor CIR (IP-CIR), a training-free method that creates a proxy image aligned\nwith the query image and text description, enhancing query representation in\nthe retrieval process. We first leverage the large language model's\ngeneralization capability to generate an image layout, and then apply both the\nquery text and image for conditional generation. The robust query features are\nenhanced by merging the proxy image, query image, and text semantic\nperturbation. Our newly proposed balancing metric integrates text-based and\nproxy retrieval similarities, allowing for more accurate retrieval of the\ntarget image while incorporating image-side information into the process.\nExperiments on three public datasets demonstrate that our method significantly\nimproves retrieval performances. We achieve state-of-the-art (SOTA) results on\nthe CIRR dataset with a Recall@K of 70.07 at K=10. Additionally, we achieved an\nimprovement in Recall@10 on the FashionIQ dataset, rising from 45.11 to 45.74,\nand improved the baseline performance in CIRCO with a mAPK@10 score, increasing\nfrom 32.24 to 34.26.\n","authors":["You Li","Fan Ma","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2411.16752v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2309.10625v4","updated":"2024-12-02T04:10:31Z","published":"2023-09-19T14:04:04Z","title":"NoisyNN: Exploring the Impact of Information Entropy Change in Learning\n  Systems","summary":"  We investigate the impact of entropy change in deep learning systems by noise\ninjection at different levels, including the embedding space and the image. The\nseries of models that employ our methodology are collectively known as Noisy\nNeural Networks (NoisyNN), with examples such as NoisyViT and NoisyCNN. Noise\nis conventionally viewed as a harmful perturbation in various deep learning\narchitectures, such as convolutional neural networks (CNNs) and vision\ntransformers (ViTs), as well as different learning tasks like image\nclassification and transfer learning. However, this work shows noise can be an\neffective way to change the entropy of the learning system. We demonstrate that\nspecific noise can boost the performance of various deep models under certain\nconditions. We theoretically prove the enhancement gained from positive noise\nby reducing the task complexity defined by information entropy and\nexperimentally show the significant performance gain in large image datasets,\nsuch as the ImageNet. Herein, we use the information entropy to define the\ncomplexity of the task. We categorize the noise into two types, positive noise\n(PN) and harmful noise (HN), based on whether the noise can help reduce the\ntask complexity. Extensive experiments of CNNs and ViTs have shown performance\nimprovements by proactively injecting positive noise, where we achieved an\nunprecedented top 1 accuracy of 95$\\%$ on ImageNet. Both theoretical analysis\nand empirical evidence have confirmed that the presence of positive noise, can\nbenefit the learning process, while the traditionally perceived harmful noise\nindeed impairs deep learning models. The different roles of noise offer new\nexplanations for deep models on specific tasks and provide a new paradigm for\nimproving model performance. Moreover, it reminds us that we can influence the\nperformance of learning systems via information entropy change.\n","authors":["Xiaowei Yu","Zhe Huang","Minheng Chen","Yao Xue","Tianming Liu","Dajiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2309.10625v4.pdf","comment":"Task Entropy, NoisyViT, NoisyCNN"},{"id":"http://arxiv.org/abs/2404.13873v4","updated":"2024-12-02T04:06:47Z","published":"2024-04-22T04:47:52Z","title":"Texture, Shape and Order Matter: A New Transformer Design for Sequential\n  DeepFake Detection","summary":"  Sequential DeepFake detection is an emerging task that predicts the\nmanipulation sequence in order. Existing methods typically formulate it as an\nimage-to-sequence problem, employing conventional Transformer architectures.\nHowever, these methods lack dedicated design and consequently result in limited\nperformance. As such, this paper describes a new Transformer design, called\nTSOM, by exploring three perspectives: Texture, Shape, and Order of\nManipulations. Our method features four major improvements: \\ding{182} we\ndescribe a new texture-aware branch that effectively captures subtle\nmanipulation traces with a Diversiform Pixel Difference Attention module.\n\\ding{183} Then we introduce a Multi-source Cross-attention module to seek deep\ncorrelations among spatial and sequential features, enabling effective modeling\nof complex manipulation traces. \\ding{184} To further enhance the\ncross-attention, we describe a Shape-guided Gaussian mapping strategy,\nproviding initial priors of the manipulation shape. \\ding{185} Finally,\nobserving that the subsequent manipulation in a sequence may influence traces\nleft in the preceding one, we intriguingly invert the prediction order from\nforward to backward, leading to notable gains as expected. Extensive\nexperimental results demonstrate that our method outperforms others by a large\nmargin, highlighting the superiority of our method.\n","authors":["Yunfei Li","Yuezun Li","Xin Wang","Baoyuan Wu","Jiaran Zhou","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2404.13873v4.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2411.16749v2","updated":"2024-12-02T04:05:34Z","published":"2024-11-24T04:49:07Z","title":"AnySynth: Harnessing the Power of Image Synthetic Data Generation for\n  Generalized Vision-Language Tasks","summary":"  Diffusion models have recently been employed to generate high-quality images,\nreducing the need for manual data collection and improving model generalization\nin tasks such as object detection, instance segmentation, and image perception.\nHowever, the synthetic framework is usually designed with meticulous human\neffort for each task due to various requirements on image layout, content, and\nannotation formats, restricting the application of synthetic data on more\ngeneral scenarios. In this paper, we propose AnySynth, a unified framework\nintegrating adaptable, comprehensive, and highly controllable components\ncapable of generating an arbitrary type of synthetic data given diverse\nrequirements. Specifically, the Task-Specific Layout Generation Module is first\nintroduced to produce reasonable layouts for different tasks by leveraging the\ngeneration ability of large language models and layout priors of real-world\nimages. A Uni-Controlled Image Generation Module is then developed to create\nhigh-quality synthetic images that are controllable and based on the generated\nlayouts. In addition, user specific reference images, and style images can be\nincorporated into the generation to task requirements. Finally, the\nTask-Oriented Annotation Module offers precise and detailed annotations for the\ngenerated images across different tasks. We have validated our framework's\nperformance across various tasks, including Few-shot Object Detection,\nCross-domain Object Detection, Zero-shot Composed Image Retrieval, and\nMulti-modal Image Perception and Grounding. The specific data synthesized by\nour framework significantly improves model performance in these tasks,\ndemonstrating the generality and effectiveness of our framework.\n","authors":["You Li","Fan Ma","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2411.16749v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13800v5","updated":"2024-12-02T03:55:49Z","published":"2023-03-24T04:45:45Z","title":"Aligning Step-by-Step Instructional Diagrams to Video Demonstrations","summary":"  Multimodal alignment facilitates the retrieval of instances from one modality\nwhen queried using another. In this paper, we consider a novel setting where\nsuch an alignment is between (i) instruction steps that are depicted as\nassembly diagrams (commonly seen in Ikea assembly manuals) and (ii) video\nsegments from in-the-wild videos; these videos comprising an enactment of the\nassembly actions in the real world. To learn this alignment, we introduce a\nnovel supervised contrastive learning method that learns to align videos with\nthe subtle details in the assembly diagrams, guided by a set of novel losses.\nTo study this problem and demonstrate the effectiveness of our method, we\nintroduce a novel dataset: IAW for Ikea assembly in the wild consisting of 183\nhours of videos from diverse furniture assembly collections and nearly 8,300\nillustrations from their associated instruction manuals and annotated for their\nground truth alignments. We define two tasks on this dataset: First, nearest\nneighbor retrieval between video segments and illustrations, and, second,\nalignment of instruction steps and the segments for each video. Extensive\nexperiments on IAW demonstrate superior performances of our approach against\nalternatives.\n","authors":["Jiahao Zhang","Anoop Cherian","Yanbin Liu","Yizhak Ben-Shabat","Cristian Rodriguez","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2303.13800v5.pdf","comment":"Accepted to CVPR'23"},{"id":"http://arxiv.org/abs/2411.00769v2","updated":"2024-12-02T03:54:23Z","published":"2024-11-01T17:59:17Z","title":"GameGen-X: Interactive Open-world Game Video Generation","summary":"  We introduce GameGen-X, the first diffusion transformer model specifically\ndesigned for both generating and interactively controlling open-world game\nvideos. This model facilitates high-quality, open-domain generation by\nsimulating an extensive array of game engine features, such as innovative\ncharacters, dynamic environments, complex actions, and diverse events.\nAdditionally, it provides interactive controllability, predicting and altering\nfuture content based on the current clip, thus allowing for gameplay\nsimulation. To realize this vision, we first collected and built an Open-World\nVideo Game Dataset from scratch. It is the first and largest dataset for\nopen-world game video generation and control, which comprises over a million\ndiverse gameplay video clips sampling from over 150 games with informative\ncaptions from GPT-4o. GameGen-X undergoes a two-stage training process,\nconsisting of foundation model pre-training and instruction tuning. Firstly,\nthe model was pre-trained via text-to-video generation and video continuation,\nendowing it with the capability for long-sequence, high-quality open-domain\ngame video generation. Further, to achieve interactive controllability, we\ndesigned InstructNet to incorporate game-related multi-modal control signal\nexperts. This allows the model to adjust latent representations based on user\ninputs, unifying character interaction and scene content control for the first\ntime in video generation. During instruction tuning, only the InstructNet is\nupdated while the pre-trained foundation model is frozen, enabling the\nintegration of interactive controllability without loss of diversity and\nquality of generated video content.\n","authors":["Haoxuan Che","Xuanhua He","Quande Liu","Cheng Jin","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2411.00769v2.pdf","comment":"Homepage: https://gamegen-x.github.io/ Github:\n  https://github.com/GameGen-X/GameGen-X"},{"id":"http://arxiv.org/abs/2407.12066v4","updated":"2024-12-02T03:39:10Z","published":"2024-07-16T05:44:30Z","title":"Temporally Grounding Instructional Diagrams in Unconstrained Videos","summary":"  We study the challenging problem of simultaneously localizing a sequence of\nqueries in the form of instructional diagrams in a video. This requires\nunderstanding not only the individual queries but also their\ninterrelationships. However, most existing methods focus on grounding one query\nat a time, ignoring the inherent structures among queries such as the general\nmutual exclusiveness and the temporal order. Consequently, the predicted\ntimespans of different step diagrams may overlap considerably or violate the\ntemporal order, thus harming the accuracy. In this paper, we tackle this issue\nby simultaneously grounding a sequence of step diagrams. Specifically, we\npropose composite queries, constructed by exhaustively pairing up the visual\ncontent features of the step diagrams and a fixed number of learnable\npositional embeddings. Our insight is that self-attention among composite\nqueries carrying different content features suppress each other to reduce\ntimespan overlaps in predictions, while the cross-attention corrects the\ntemporal misalignment via content and position joint guidance. We demonstrate\nthe effectiveness of our approach on the IAW dataset for grounding step\ndiagrams and the YouCook2 benchmark for grounding natural language queries,\nsignificantly outperforming existing methods while simultaneously grounding\nmultiple queries.\n","authors":["Jiahao Zhang","Frederic Z. Zhang","Cristian Rodriguez","Yizhak Ben-Shabat","Anoop Cherian","Stephen Gould"],"pdf_url":"https://arxiv.org/pdf/2407.12066v4.pdf","comment":"Accepted to WACV'25"},{"id":"http://arxiv.org/abs/2411.19527v2","updated":"2024-12-02T03:34:45Z","published":"2024-11-29T07:54:56Z","title":"DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow\n  Decoding","summary":"  Human motion, inherently continuous and dynamic, presents significant\nchallenges for generative models. Despite their dominance, discrete\nquantization methods, such as VQ-VAEs, suffer from inherent limitations,\nincluding restricted expressiveness and frame-wise noise artifacts. Continuous\napproaches, while producing smoother and more natural motions, often falter due\nto high-dimensional complexity and limited training data. To resolve this\n\"discord\" between discrete and continuous representations, we introduce\nDisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a\nnovel method that decodes discrete motion tokens into continuous motion through\nrectified flow. By employing an iterative refinement process in the continuous\nspace, DisCoRD captures fine-grained dynamics and ensures smoother and more\nnatural motions. Compatible with any discrete-based framework, our method\nenhances naturalness without compromising faithfulness to the conditioning\nsignals. Extensive evaluations demonstrate that DisCoRD achieves\nstate-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on\nKIT-ML. These results solidify DisCoRD as a robust solution for bridging the\ndivide between discrete efficiency and continuous realism. Our project page is\navailable at: https://whwjdqls.github.io/discord.github.io/.\n","authors":["Jungbin Cho","Junwan Kim","Jisoo Kim","Minseo Kim","Mingu Kang","Sungeun Hong","Tae-Hyun Oh","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2411.19527v2.pdf","comment":"20 pages 18 figures"},{"id":"http://arxiv.org/abs/2411.17606v2","updated":"2024-12-02T03:19:04Z","published":"2024-11-26T17:18:20Z","title":"HyperSeg: Towards Universal Visual Segmentation with Large Language\n  Model","summary":"  This paper aims to address universal segmentation for image and video\nperception with the strong reasoning ability empowered by Visual Large Language\nModels (VLLMs). Despite significant progress in current unified segmentation\nmethods, limitations in adaptation to both image and video scenarios, as well\nas the complex reasoning segmentation, make it difficult for them to handle\nvarious challenging instructions and achieve an accurate understanding of\nfine-grained vision-language correlations. We propose HyperSeg, the first\nVLLM-based universal segmentation model for pixel-level image and video\nperception, encompassing generic segmentation tasks and more complex reasoning\nperception tasks requiring powerful reasoning abilities and world knowledge.\nBesides, to fully leverage the recognition capabilities of VLLMs and the\nfine-grained visual information, HyperSeg incorporates hybrid entity\nrecognition and fine-grained visual perceiver modules for various segmentation\ntasks. Combined with the temporal adapter, HyperSeg achieves a comprehensive\nunderstanding of temporal information. Experimental results validate the\neffectiveness of our insights in resolving universal image and video\nsegmentation tasks, including the more complex reasoning perception tasks. Our\ncode is available.\n","authors":["Cong Wei","Yujie Zhong","Haoxian Tan","Yong Liu","Zheng Zhao","Jie Hu","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2411.17606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12307v7","updated":"2024-12-02T03:18:41Z","published":"2023-03-22T04:49:23Z","title":"Predicting and Enhancing the Fairness of DNNs with the Curvature of\n  Perceptual Manifolds","summary":"  To address the challenges of long-tailed classification, researchers have\nproposed several approaches to reduce model bias, most of which assume that\nclasses with few samples are weak classes. However, recent studies have shown\nthat tail classes are not always hard to learn, and model bias has been\nobserved on sample-balanced datasets, suggesting the existence of other factors\nthat affect model bias. In this work, we first establish a geometric\nperspective for analyzing model fairness and then systematically propose a\nseries of geometric measurements for perceptual manifolds in deep neural\nnetworks. Subsequently, we comprehensively explore the effect of the geometric\ncharacteristics of perceptual manifolds on classification difficulty and how\nlearning shapes the geometric characteristics of perceptual manifolds. An\nunanticipated finding is that the correlation between the class accuracy and\nthe separation degree of perceptual manifolds gradually decreases during\ntraining, while the negative correlation with the curvature gradually\nincreases, implying that curvature imbalance leads to model bias.Building upon\nthese observations, we propose curvature regularization to facilitate the model\nto learn curvature-balanced and flatter perceptual manifolds. Evaluations on\nmultiple long-tailed and non-long-tailed datasets show the excellent\nperformance and exciting generality of our approach, especially in achieving\nsignificant performance improvements based on current state-of-the-art\ntechniques. Our work opens up a geometric analysis perspective on model bias\nand reminds researchers to pay attention to model bias on non-long-tailed and\neven sample-balanced datasets.\n","authors":["Yanbiao Ma","Licheng Jiao","Fang Liu","Maoji Wen","Lingling Li","Wenping Ma","Shuyuan Yang","Xu Liu","Puhua Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12307v7.pdf","comment":"17pages, Accepted by CVPR 2023, Submitted to TPAMI"},{"id":"http://arxiv.org/abs/2404.16323v2","updated":"2024-12-02T03:11:06Z","published":"2024-04-25T04:18:59Z","title":"LeanGaussian: Breaking Pixel or Point Cloud Correspondence in Modeling\n  3D Gaussians","summary":"  Rencently, Gaussian splatting has demonstrated significant success in novel\nview synthesis. Current methods often regress Gaussians with pixel or point\ncloud correspondence, linking each Gaussian with a pixel or a 3D point. This\nleads to the redundancy of Gaussians being used to overfit the correspondence\nrather than the objects represented by the 3D Gaussians themselves,\nconsequently wasting resources and lacking accurate geometries or textures. In\nthis paper, we introduce LeanGaussian, a novel approach that treats each query\nin deformable Transformer as one 3D Gaussian ellipsoid, breaking the pixel or\npoint cloud correspondence constraints. We leverage deformable decoder to\niteratively refine the Gaussians layer-by-layer with the image features as keys\nand values. Notably, the center of each 3D Gaussian is defined as 3D reference\npoints, which are then projected onto the image for deformable attention in 2D\nspace. On both the ShapeNet SRN dataset (category level) and the Google Scanned\nObjects dataset (open-category level, trained with the Objaverse dataset), our\napproach, outperforms prior methods by approximately 6.1\\%, achieving a PSNR of\n25.44 and 22.36, respectively. Additionally, our method achieves a 3D\nreconstruction speed of 7.2 FPS and rendering speed 500 FPS. The code will be\nreleased at https://github.com/jwubz123/DIG3D.\n","authors":["Jiamin Wu","Kenkun Liu","Han Gao","Xiaoke Jiang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.16323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13174v2","updated":"2024-12-02T02:55:29Z","published":"2024-01-24T01:41:26Z","title":"Towards Complementary Knowledge Distillation for Efficient Dense Image\n  Prediction","summary":"  It has been revealed that small efficient dense image prediction (EDIP)\nmodels, trained using the knowledge distillation (KD) framework, encounter two\nkey challenges, including maintaining boundary region completeness and\npreserving target region connectivity, despite their favorable capacity to\nrecognize main object regions. In this work, we propose a complementary\nboundary and context distillation (BCD) method within the KD framework for\nEDIPs, which facilitates the targeted knowledge transfer from large accurate\nteacher models to compact efficient student models. Specifically, the boundary\ndistillation component focuses on extracting explicit object-level semantic\nboundaries from the hierarchical feature maps of the backbone network to\nenhance the student model's mask quality in boundary regions. Concurrently, the\ncontext distillation component leverages self-relations as a bridge to transfer\nimplicit pixel-level contexts from the teacher model to the student model,\nensuring strong connectivity in target regions. Our proposed BCD method is\nspecifically designed for EDIP tasks and is characterized by its simplicity and\nefficiency. Extensive experimental results across semantic segmentation, object\ndetection, and instance segmentation on various representative datasets\ndemonstrate that our method can outperform existing methods without requiring\nextra supervisions or incurring increased inference costs, resulting in\nwell-defined object boundaries and smooth connecting regions.\n","authors":["Dong Zhang","Pingcheng Dong","Xinting Hu","Long Chen","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.13174v2.pdf","comment":"under submission"},{"id":"http://arxiv.org/abs/2407.12735v4","updated":"2024-12-02T02:34:47Z","published":"2024-07-17T16:55:42Z","title":"EchoSight: Advancing Visual-Language Models with Wiki Knowledge","summary":"  Knowledge-based Visual Question Answering (KVQA) tasks require answering\nquestions about images using extensive background knowledge. Despite\nsignificant advancements, generative models often struggle with these tasks due\nto the limited integration of external knowledge. In this paper, we introduce\nEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) framework\nthat enables large language models (LLMs) to answer visual questions requiring\nfine-grained encyclopedic knowledge. To strive for high-performing retrieval,\nEchoSight first searches wiki articles by using visual-only information,\nsubsequently, these candidate articles are further reranked according to their\nrelevance to the combined text-image query. This approach significantly\nimproves the integration of multimodal knowledge, leading to enhanced retrieval\noutcomes and more accurate VQA responses. Our experimental results on the\nEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishes\nnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of\n41.8% on Encyclopedic VQA and 31.3% on InfoSeek.\n","authors":["Yibin Yan","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2407.12735v4.pdf","comment":"Accepted by EMNLP 2024 findings; Project Page:\n  https://go2heart.github.io/echosight"},{"id":"http://arxiv.org/abs/2411.06071v2","updated":"2024-12-02T02:24:08Z","published":"2024-11-09T05:22:13Z","title":"GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot\n  Anomaly Detection","summary":"  Zero-shot anomaly detection (ZSAD) is crucial for detecting anomalous\npatterns in target datasets without using training samples, specifically in\nscenarios where there are distributional differences between the target domain\nand training data or where data scarcity arises because of restricted access.\nAlthough recently pretrained vision-language models demonstrate strong\nzero-shot performance across various visual tasks, they focus on learning class\nsemantics, which makes their direct application to ZSAD challenging. To address\nthis scenario, we propose GlocalCLIP, which uniquely separates global and local\nprompts and jointly optimizes them. This approach enables the object-agnostic\nglocal semantic prompt to effectively capture general normal and anomalous\npatterns without dependency on specific objects in the image. We refine the\ntext prompts for more precise adjustments by utilizing deep-text prompt tuning\nin the text encoder. In the vision encoder, we apply V-V attention layers to\ncapture detailed local image features. Finally, we introduce glocal contrastive\nlearning to improve the complementary learning of global and local prompts,\neffectively detecting anomalous patterns across various domains. The\ngeneralization performance of GlocalCLIP in ZSAD was demonstrated on 15\nreal-world datasets from both the industrial and medical domains, achieving\nsuperior performance compared to existing methods. Code will be made available\nat https://github.com/YUL-git/GlocalCLIP.\n","authors":["Jiyul Ham","Yonggon Jung","Jun-Geol Baek"],"pdf_url":"https://arxiv.org/pdf/2411.06071v2.pdf","comment":"29 pages, 36 figures"},{"id":"http://arxiv.org/abs/2411.19224v2","updated":"2024-12-02T02:04:53Z","published":"2024-11-28T15:49:08Z","title":"Differentiable Voxel-based X-ray Rendering Improves Sparse-View 3D CBCT\n  Reconstruction","summary":"  We present DiffVox, a self-supervised framework for Cone-Beam Computed\nTomography (CBCT) reconstruction by directly optimizing a voxelgrid\nrepresentation using physics-based differentiable X-ray rendering. Further, we\ninvestigate how the different implementations of the X-ray image formation\nmodel in the renderer affect the quality of 3D reconstruction and novel view\nsynthesis. When combined with our regularized voxel-based learning framework,\nwe find that using an exact implementation of the discrete Beer-Lambert law for\nX-ray attenuation in the renderer outperforms both widely used iterative CBCT\nreconstruction algorithms and modern neural field approaches, particularly when\ngiven only a few input views. As a result, we reconstruct high-fidelity 3D CBCT\nvolumes from fewer X-rays, potentially reducing ionizing radiation exposure and\nimproving diagnostic utility. Our implementation is available at\nhttps://github.com/hossein-momeni/DiffVox.\n","authors":["Mohammadhossein Momeni","Vivek Gopalakrishnan","Neel Dey","Polina Golland","Sarah Frisken"],"pdf_url":"https://arxiv.org/pdf/2411.19224v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18977v2","updated":"2024-12-02T02:01:05Z","published":"2024-11-28T07:58:30Z","title":"Det-SAM2:Technical Report on the Self-Prompting Segmentation Framework\n  Based on Segment Anything Model 2","summary":"  Segment Anything Model 2 (SAM2) demonstrates exceptional performance in video\nsegmentation and refinement of segmentation results. We anticipate that it can\nfurther evolve to achieve higher levels of automation for practical\napplications. Building upon SAM2, we conducted a series of practices that\nultimately led to the development of a fully automated pipeline, termed\nDet-SAM2, in which object prompts are automatically generated by a detection\nmodel to facilitate inference and refinement by SAM2. This pipeline enables\ninference on infinitely long video streams with constant VRAM and RAM usage,\nall while preserving the same efficiency and accuracy as the original SAM2.\n  This technical report focuses on the construction of the overall Det-SAM2\nframework and the subsequent engineering optimization applied to SAM2. We\npresent a case demonstrating an application built on the Det-SAM2 framework: AI\nrefereeing in a billiards scenario, derived from our business context. The\nproject at \\url{https://github.com/motern88/Det-SAM2}.\n","authors":["Zhiting Wang","Qiangong Zhou","Zongyang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.18977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14064v3","updated":"2024-12-02T00:42:44Z","published":"2023-11-23T15:42:42Z","title":"HGCLIP: Exploring Vision-Language Models with Graph Representations for\n  Hierarchical Understanding","summary":"  Object categories are typically organized into a multi-granularity taxonomic\nhierarchy. When classifying categories at different hierarchy levels,\ntraditional uni-modal approaches focus primarily on image features, revealing\nlimitations in complex scenarios. Recent studies integrating Vision-Language\nModels (VLMs) with class hierarchies have shown promise, yet they fall short of\nfully exploiting the hierarchical relationships. These efforts are constrained\nby their inability to perform effectively across varied granularity of\ncategories. To tackle this issue, we propose a novel framework (HGCLIP) that\neffectively combines CLIP with a deeper exploitation of the Hierarchical class\nstructure via Graph representation learning. We explore constructing the class\nhierarchy into a graph, with its nodes representing the textual or image\nfeatures of each category. After passing through a graph encoder, the textual\nfeatures incorporate hierarchical structure information, while the image\nfeatures emphasize class-aware features derived from prototypes through the\nattention mechanism. Our approach demonstrates significant improvements on 11\ndiverse visual recognition benchmarks. Our codes are fully available at\nhttps://github.com/richard-peng-xia/HGCLIP.\n","authors":["Peng Xia","Xingtong Yu","Ming Hu","Lie Ju","Zhiyong Wang","Peibo Duan","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2311.14064v3.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2401.05738v3","updated":"2024-12-02T00:04:23Z","published":"2024-01-11T08:40:35Z","title":"Interpreting and Improving Attention From the Perspective of Large\n  Kernel Convolution","summary":"  Attention mechanisms have significantly advanced visual models by capturing\nglobal context effectively. However, their reliance on large-scale datasets and\nsubstantial computational resources poses challenges in data-scarce and\nresource-constrained scenarios. Moreover, traditional self-attention mechanisms\nlack inherent spatial inductive biases, making them suboptimal for modeling\nlocal features critical to tasks involving smaller datasets. In this work, we\nintroduce Large Kernel Convolutional Attention (LKCA), a novel formulation that\nreinterprets attention operations as a single large-kernel convolution. This\ndesign unifies the strengths of convolutional architectures locality and\ntranslation invariance with the global context modeling capabilities of\nself-attention. By embedding these properties into a computationally efficient\nframework, LKCA addresses key limitations of traditional attention mechanisms.\nThe proposed LKCA achieves competitive performance across various visual tasks,\nparticularly in data-constrained settings. Experimental results on CIFAR-10,\nCIFAR-100, SVHN, and Tiny-ImageNet demonstrate its ability to excel in image\nclassification, outperforming conventional attention mechanisms and vision\ntransformers in compact model settings. These findings highlight the\neffectiveness of LKCA in bridging local and global feature modeling, offering a\npractical and robust solution for real-world applications with limited data and\nresources.\n","authors":["Chenghao Li","Chaoning Zhang","Boheng Zeng","Yi Lu","Pengbo Shi","Qingzi Chen","Jirui Liu","Lingyun Zhu","Yang Yang","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2401.05738v3.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.05677v2","updated":"2024-12-02T18:13:28Z","published":"2024-09-09T14:44:19Z","title":"RIRAG: Regulatory Information Retrieval and Answer Generation","summary":"  Regulatory documents, issued by governmental regulatory bodies, establish\nrules, guidelines, and standards that organizations must adhere to for legal\ncompliance. These documents, characterized by their length, complexity and\nfrequent updates, are challenging to interpret, requiring significant\nallocation of time and expertise on the part of organizations to ensure ongoing\ncompliance. Regulatory Natural Language Processing (RegNLP) is a\nmultidisciplinary field aimed at simplifying access to and interpretation of\nregulatory rules and obligations. We introduce a task of generating\nquestion-passages pairs, where questions are automatically created and paired\nwith relevant regulatory passages, facilitating the development of regulatory\nquestion-answering systems. We create the ObliQA dataset, containing 27,869\nquestions derived from the collection of Abu Dhabi Global Markets (ADGM)\nfinancial regulation documents, design a baseline Regulatory Information\nRetrieval and Answer Generation (RIRAG) system and evaluate it with RePASs, a\nnovel evaluation metric that tests whether generated answers accurately capture\nall relevant obligations while avoiding contradictions.\n","authors":["Tuba Gokhan","Kexin Wang","Iryna Gurevych","Ted Briscoe"],"pdf_url":"https://arxiv.org/pdf/2409.05677v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11251v2","updated":"2024-12-02T13:26:14Z","published":"2024-06-17T06:27:35Z","title":"Unifying Multimodal Retrieval via Document Screenshot Embedding","summary":"  In the real world, documents are organized in different formats and varied\nmodalities. Traditional retrieval pipelines require tailored document parsing\ntechniques and content extraction modules to prepare input for indexing. This\nprocess is tedious, prone to errors, and has information loss. To this end, we\npropose Document Screenshot Embedding (DSE), a novel retrieval paradigm that\nregards document screenshots as a unified input format, which does not require\nany content extraction preprocess and preserves all the information in a\ndocument (e.g., text, image and layout). DSE leverages a large vision-language\nmodel to directly encode document screenshots into dense representations for\nretrieval. To evaluate our method, we first craft the dataset of Wiki-SS, a\n1.3M Wikipedia web page screenshots as the corpus to answer the questions from\nthe Natural Questions dataset. In such a text-intensive document retrieval\nsetting, DSE shows competitive effectiveness compared to other text retrieval\nmethods relying on parsing. For example, DSE outperforms BM25 by 17 points in\ntop-1 retrieval accuracy. Additionally, in a mixed-modality task of slide\nretrieval, DSE significantly outperforms OCR text retrieval methods by over 15\npoints in nDCG@10. These experiments show that DSE is an effective document\nretrieval paradigm for diverse types of documents. Model checkpoints, code, and\nWiki-SS collection will be released.\n","authors":["Xueguang Ma","Sheng-Chieh Lin","Minghan Li","Wenhu Chen","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2406.11251v2.pdf","comment":"EMNLP2024 main"},{"id":"http://arxiv.org/abs/2409.10825v3","updated":"2024-12-02T07:00:57Z","published":"2024-09-17T01:37:57Z","title":"Unveiling and Mitigating Bias in Large Language Model Recommendations: A\n  Path to Fairness","summary":"  excel in delivering comprehensive suggestions by deeply analyzing content and\nuser behavior. However, they often inherit biases from skewed training data,\nfavoring mainstream content while underrepresenting diverse or non-traditional\noptions. This study explores the interplay between bias and LLM-based\nrecommendation systems, focusing on music, song, and book recommendations\nacross diverse demographic and cultural groups. This paper analyzes bias in\nLLM-based recommendation systems across multiple models (GPT, LLaMA, and\nGemini), revealing its deep and pervasive impact on outcomes. Intersecting\nidentities and contextual factors, like socioeconomic status, further amplify\nbiases, complicating fair recommendations across diverse groups. Our findings\nreveal that bias in these systems is deeply ingrained, yet even simple\ninterventions like prompt engineering can significantly reduce it. We further\npropose a retrieval-augmented generation strategy to mitigate bias more\neffectively. Numerical experiments validate these strategies, demonstrating\nboth the pervasive nature of bias and the impact of the proposed solutions.\n","authors":["Anindya Bijoy Das","Shahnewaz Karim Sakib"],"pdf_url":"https://arxiv.org/pdf/2409.10825v3.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.16208v3","updated":"2024-12-02T18:59:28Z","published":"2024-10-21T17:11:21Z","title":"Compute-Constrained Data Selection","summary":"  Data selection can reduce the amount of training data needed to finetune\nLLMs; however, the efficacy of data selection scales directly with its compute.\nMotivated by the practical challenge of compute-constrained finetuning, we\nconsider the setting in which both the cost of selecting data and training are\nbudgeted for. We first formalize the problem of data selection with a\ncost-aware utility function, and model the data selection problem as trading\noff initial-selection cost for training gain. We run a comprehensive sweep of\nexperiments across multiple tasks, varying compute budget by scaling finetuning\ntokens, model sizes, and data selection compute. Interestingly we find that\nmany powerful data selection methods are almost never compute-optimal, and that\ncheaper data selection alternatives dominate both from a theoretical and\nempirical perspective. For compute-optimal training, we find that perplexity\nand gradient data selection require training-to-selection model size ratios of\n5x and 10x, respectively.\n","authors":["Junjie Oscar Yin","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.16208v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07978v3","updated":"2024-12-02T18:58:18Z","published":"2024-11-12T17:58:34Z","title":"A Note on Doubly Robust Estimator in Regression Continuity Designs","summary":"  This note introduces a doubly robust (DR) estimator for regression\ndiscontinuity (RD) designs. RD designs provide a quasi-experimental framework\nfor estimating treatment effects, where treatment assignment depends on whether\na running variable surpasses a predefined cutoff. A common approach in RD\nestimation is the use of nonparametric regression methods, such as local linear\nregression. However, the validity of these methods still relies on the\nconsistency of the nonparametric estimators. In this study, we propose the\nDR-RD estimator, which combines two distinct estimators for the conditional\nexpected outcomes. The primary advantage of the DR-RD estimator lies in its\nability to ensure the consistency of the treatment effect estimation as long as\nat least one of the two estimators is consistent. Consequently, our DR-RD\nestimator enhances robustness of treatment effect estimators in RD designs.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2411.07978v3.pdf","comment":"There is a critical error in the previous submission. We have revised\n  the original claim and present a weakened result"},{"id":"http://arxiv.org/abs/2411.17501v2","updated":"2024-12-02T18:54:28Z","published":"2024-11-26T15:13:06Z","title":"Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect\n  Verifiers","summary":"  Recent research has generated hope that inference scaling could allow weaker\nlanguage models to match or exceed the accuracy of stronger models, such as by\nrepeatedly sampling solutions to a coding problem until it passes unit tests.\nThe central thesis of this paper is that there is no free lunch for inference\nscaling: indefinite accuracy improvement through resampling can only be\nrealized if the \"verifier\" (in this case, a set of unit tests) is perfect. When\nthe verifier is imperfect, as it almost always is in domains such as reasoning\nor coding (for example, unit tests have imperfect coverage), there is a nonzero\nprobability of false positives: incorrect solutions that pass the verifier.\nResampling cannot decrease this probability, so it imposes an upper bound to\nthe accuracy of resampling-based inference scaling even with an infinite\ncompute budget. We find that there is a very strong correlation between the\nmodel's single-sample accuracy (i.e. accuracy without unit tests) and its false\npositive rate on coding benchmarks HumanEval and MBPP, whose unit tests have\nlimited coverage. Therefore, no amount of inference scaling of weaker models\ncan enable them to match the single-sample accuracy of a sufficiently strong\nmodel (Fig. 1a). When we consider that false positives have a negative utility\ncompared to abstaining from producing a solution, it bends the inference\nscaling curve further downward. Empirically, we find that the optimal number of\nsamples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we\nshow that beyond accuracy, false positives may have other undesirable\nqualities, such as poor adherence to coding style conventions.\n","authors":["Benedikt Stroebl","Sayash Kapoor","Arvind Narayanan"],"pdf_url":"https://arxiv.org/pdf/2411.17501v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05248v3","updated":"2024-12-02T18:54:09Z","published":"2023-12-08T18:55:40Z","title":"Topology-Based Reconstruction Prevention for Decentralised Learning","summary":"  Decentralised learning has recently gained traction as an alternative to\nfederated learning in which both data and coordination are distributed. To\npreserve the confidentiality of users' data, decentralised learning relies on\ndifferential privacy, multi-party computation, or both. However, running\nmultiple privacy-preserving summations in sequence may allow adversaries to\nperform reconstruction attacks. Current reconstruction countermeasures either\ncannot trivially be adapted to the distributed setting, or add excessive\namounts of noise.\n  In this work, we first show that passive honest-but-curious adversaries can\ninfer other users' private data after several privacy-preserving summations.\nFor example, in subgraphs with 18 users, we show that only three passive\nhonest-but-curious adversaries succeed at reconstructing private data 11.0% of\nthe time, requiring an average of 8.8 summations per adversary. The success\nrate depends only on the adversaries' direct neighbourhood, and is independent\nof the size of the full network. We consider weak adversaries that do not\ncontrol the graph topology, cannot exploit the summation's inner workings, and\ndo not have auxiliary knowledge; and show that these adversaries can still\ninfer private data.\n  We analyse how reconstruction relates to topology and propose the first\ntopology-based decentralised defence against reconstruction attacks. We show\nthat reconstruction requires a number of adversaries linear in the length of\nthe network's shortest cycle. Consequently, exact attacks over\nprivacy-preserving summations are impossible in acyclic networks.\n  Our work is a stepping stone for a formal theory of topology-based\ndecentralised reconstruction defences. Such a theory would generalise our\ncountermeasure beyond summation, define confidentiality in terms of entropy,\nand describe the interactions with (topology-aware) differential privacy.\n","authors":["Florine W. Dekker","Zekeriya Erkin","Mauro Conti"],"pdf_url":"https://arxiv.org/pdf/2312.05248v3.pdf","comment":"14 pages, 19 figures, for associated experiment source code see\n  doi:10.4121/21572601.v2"},{"id":"http://arxiv.org/abs/2410.09943v2","updated":"2024-12-02T18:39:06Z","published":"2024-10-13T17:55:58Z","title":"Dynamic Estimation of Learning Rates Using a Non-Linear Autoregressive\n  Model","summary":"  We introduce a new class of adaptive non-linear autoregressive (Nlar) models\nincorporating the concept of momentum, which dynamically estimate both the\nlearning rates and momentum as the number of iterations increases. In our\nmethod, the growth of the gradients is controlled using a scaling (clipping)\nfunction, leading to stable convergence. Within this framework, we propose\nthree distinct estimators for learning rates and provide theoretical proof of\ntheir convergence. We further demonstrate how these estimators underpin the\ndevelopment of effective Nlar optimizers. The performance of the proposed\nestimators and optimizers is rigorously evaluated through extensive experiments\nacross several datasets and a reinforcement learning environment. The results\nhighlight two key features of the Nlar optimizers: robust convergence despite\nvariations in underlying parameters, including large initial learning rates,\nand strong adaptability with rapid convergence during the initial epochs.\n","authors":["Ramin Okhrati"],"pdf_url":"https://arxiv.org/pdf/2410.09943v2.pdf","comment":"Typos corrected"},{"id":"http://arxiv.org/abs/2408.00170v2","updated":"2024-12-02T18:37:01Z","published":"2024-07-31T21:43:55Z","title":"CREW: Facilitating Human-AI Teaming Research","summary":"  With the increasing deployment of artificial intelligence (AI) technologies,\nthe potential of humans working with AI agents has been growing at a great\nspeed. Human-AI teaming is an important paradigm for studying various aspects\nwhen humans and AI agents work together. The unique aspect of Human-AI teaming\nresearch is the need to jointly study humans and AI agents, demanding\nmultidisciplinary research efforts from machine learning to human-computer\ninteraction, robotics, cognitive science, neuroscience, psychology, social\nscience, and complex systems. However, existing platforms for Human-AI teaming\nresearch are limited, often supporting oversimplified scenarios and a single\ntask, or specifically focusing on either human-teaming research or multi-agent\nAI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming\nresearch in real-time decision-making scenarios and engage collaborations from\nmultiple scientific disciplines, with a strong emphasis on human involvement.\nIt includes pre-built tasks for cognitive studies and Human-AI teaming with\nexpandable potentials from our modular design. Following conventional cognitive\nneuroscience research, CREW also supports multimodal human physiological signal\nrecording for behavior analysis. Moreover, CREW benchmarks real-time\nhuman-guided reinforcement learning agents using state-of-the-art algorithms\nand well-tuned baselines. With CREW, we were able to conduct 50 human subject\nstudies within a week to verify the effectiveness of our benchmark.\n","authors":["Lingyu Zhang","Zhengran Ji","Boyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2408.00170v2.pdf","comment":"Our project website is at: http://generalroboticslab.com/CREW"},{"id":"http://arxiv.org/abs/2402.08573v3","updated":"2024-12-02T18:33:58Z","published":"2024-02-13T16:21:18Z","title":"Two Tales of Single-Phase Contrastive Hebbian Learning","summary":"  The search for ``biologically plausible'' learning algorithms has converged\non the idea of representing gradients as activity differences. However, most\napproaches require a high degree of synchronization (distinct phases during\nlearning) and introduce substantial computational overhead, which raises doubts\nregarding their biological plausibility as well as their potential utility for\nneuromorphic computing. Furthermore, they commonly rely on applying\ninfinitesimal perturbations (nudges) to output units, which is impractical in\nnoisy environments. Recently it has been shown that by modelling artificial\nneurons as dyads with two oppositely nudged compartments, it is possible for a\nfully local learning algorithm named ``dual propagation'' to bridge the\nperformance gap to backpropagation, without requiring separate learning phases\nor infinitesimal nudging. However, the algorithm has the drawback that its\nnumerical stability relies on symmetric nudging, which may be restrictive in\nbiological and analog implementations. In this work we first provide a solid\nfoundation for the objective underlying the dual propagation method, which also\nreveals a surprising connection with adversarial robustness. Second, we\ndemonstrate how dual propagation is related to a particular adjoint state\nmethod, which is stable regardless of asymmetric nudging.\n","authors":["Rasmus Kjær Høier","Christopher Zach"],"pdf_url":"https://arxiv.org/pdf/2402.08573v3.pdf","comment":"ICML 2024; 21 pages"},{"id":"http://arxiv.org/abs/2406.16738v2","updated":"2024-12-02T18:27:02Z","published":"2024-06-24T15:45:20Z","title":"Inducing Group Fairness in Prompt-Based Language Model Decisions","summary":"  Classifiers are used throughout industry to enforce policies, ranging from\nthe detection of toxic content to age-appropriate content filtering. While\nthese classifiers serve important functions, it is also essential that they are\nbuilt in ways that minimize unfair biases for users.\n  One such fairness consideration is called group fairness, which desires that\ndifferent sub-population of users receive equal treatment. This is a\nwell-studied problem in the context of 'classical' classifiers. However, the\nemergence of prompt-based language model (LM) decision making has created new\nopportunities to solve text-based classification tasks, and the fairness\nproperties of these new classifiers are not yet well understood. Further, the\n`remediation toolkit' is incomplete for LM-based decision makers and little is\nunderstood about how to improve decision maker group fairness while maintaining\nclassifier performance.\n  This work sets out to add more tools to that toolbox. We introduce\nadaptations of existing effective approaches from the classical classifier\nfairness to the prompt-based classifier space. We also devise simple methods\nthat take advantage of the new structure of prompt-based decision makers and\noperate at the prompt level. We compare these approaches empirically on real\ndata. Our results suggest that adaptations of approaches that are effective for\nclassical classifiers remain effective in the LM-based classifier environment.\nHowever, there is room for further exploration of prompt-based remediation\nmethods (and other remediation methods that take advantage of LM structure).\n","authors":["James Atwood","Nino Scherrer","Preethi Lahoti","Ananth Balashankar","Flavien Prost","Ahmad Beirami"],"pdf_url":"https://arxiv.org/pdf/2406.16738v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13846v2","updated":"2024-12-02T18:03:51Z","published":"2024-05-22T17:14:03Z","title":"Regression Trees Know Calculus","summary":"  Regression trees have emerged as a preeminent tool for solving real-world\nregression problems due to their ability to deal with nonlinearities,\ninteraction effects and sharp discontinuities. In this article, we rather study\nregression trees applied to well-behaved, differentiable functions, and\ndetermine the relationship between node parameters and the local gradient of\nthe function being approximated. We find a simple estimate of the gradient\nwhich can be efficiently computed using quantities exposed by popular tree\nlearning libraries. This allows the tools developed in the context of\ndifferentiable algorithms, like neural nets and Gaussian processes, to be\ndeployed to tree-based models. To demonstrate this, we study measures of model\nsensitivity defined in terms of integrals of gradients and demonstrate how to\ncompute them for regression trees using the proposed gradient estimates.\nQuantitative and qualitative numerical experiments reveal the capability of\ngradients estimated by regression trees to improve predictive analysis, solve\ntasks in uncertainty quantification, and provide interpretation of model\nbehavior.\n","authors":["Nathan Wycoff"],"pdf_url":"https://arxiv.org/pdf/2405.13846v2.pdf","comment":"Comments very welcome!"},{"id":"http://arxiv.org/abs/2311.04604v3","updated":"2024-12-02T18:02:53Z","published":"2023-11-08T11:12:27Z","title":"Asynchronous Message-Passing and Zeroth-Order Optimization Based\n  Distributed Learning with a Use-Case in Resource Allocation in Communication\n  Networks","summary":"  Distributed learning and adaptation have received significant interest and\nfound wide-ranging applications in machine learning and signal processing.\nWhile various approaches, such as shared-memory optimization, multi-task\nlearning, and consensus-based learning (e.g., federated learning and learning\nover graphs), focus on optimizing either local costs or a global cost, there\nremains a need for further exploration of their interconnections. This paper\nspecifically focuses on a scenario where agents collaborate towards a common\ntask (i.e., optimizing a global cost equal to aggregated local costs) while\neffectively having distinct individual tasks (i.e., optimizing individual local\nparameters in a local cost). Each agent's actions can potentially impact other\nagents' performance through interactions. Notably, each agent has access to\nonly its local zeroth-order oracle (i.e., cost function value) and shares\nscalar values, rather than gradient vectors, with other agents, leading to\ncommunication bandwidth efficiency and agent privacy. Agents employ\nzeroth-order optimization to update their parameters, and the asynchronous\nmessage-passing between them is subject to bounded but possibly random\ncommunication delays. This paper presents theoretical convergence analyses and\nestablishes a convergence rate for nonconvex problems. Furthermore, it\naddresses the relevant use-case of deep learning-based resource allocation in\ncommunication networks and conducts numerical experiments in which agents,\nacting as transmitters, collaboratively train their individual policies to\nmaximize a global reward, e.g., a sum of data rates.\n","authors":["Pourya Behmandpoor","Marc Moonen","Panagiotis Patrinos"],"pdf_url":"https://arxiv.org/pdf/2311.04604v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24060v5","updated":"2024-12-02T18:00:18Z","published":"2024-10-31T15:57:04Z","title":"Understanding Generalizability of Diffusion Models Requires Rethinking\n  the Hidden Gaussian Structure","summary":"  In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models.\n","authors":["Xiang Li","Yixiang Dai","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2410.24060v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15098v3","updated":"2024-12-02T17:59:40Z","published":"2024-11-22T17:55:15Z","title":"OminiControl: Minimal and Universal Control for Diffusion Transformer","summary":"  In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.\n","authors":["Zhenxiong Tan","Songhua Liu","Xingyi Yang","Qiaochu Xue","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15098v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17593v3","updated":"2024-12-02T17:43:20Z","published":"2024-11-26T17:01:27Z","title":"What Differentiates Educational Literature? A Multimodal Fusion Approach\n  of Transformers and Computational Linguistics","summary":"  The integration of new literature into the English curriculum remains a\nchallenge since educators often lack scalable tools to rapidly evaluate\nreadability and adapt texts for diverse classroom needs. This study proposes to\naddress this gap through a multimodal approach that combines transformer-based\ntext classification with linguistic feature analysis to align texts with UK Key\nStages. Eight state-of-the-art Transformers were fine-tuned on segmented text\ndata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,\n500 deep neural network topologies were searched for the classification of\nlinguistic characteristics, achieving an F1 score of 0.392. The fusion of these\nmodalities shows a significant improvement, with every multimodal approach\noutperforming all unimodal models. In particular, the ELECTRA Transformer fused\nwith the neural network achieved an F1 score of 0.996. Unimodal and multimodal\napproaches are shown to have statistically significant differences in all\nvalidation metrics (accuracy, precision, recall, F1 score) except for inference\ntime. The proposed approach is finally encapsulated in a stakeholder-facing web\napplication, providing non-technical stakeholder access to real-time insights\non text complexity, reading difficulty, curriculum alignment, and\nrecommendations for learning age range. The application empowers data-driven\ndecision making and reduces manual workload by integrating AI-based\nrecommendations into lesson planning for English literature.\n","authors":["Jordan J. Bird"],"pdf_url":"https://arxiv.org/pdf/2411.17593v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14973v2","updated":"2024-12-02T17:35:07Z","published":"2024-01-26T16:06:01Z","title":"Discovering group dynamics in coordinated time series via hierarchical\n  recurrent switching-state models","summary":"  We seek a computationally efficient model for a collection of time series\narising from multiple interacting entities (a.k.a. \"agents\"). Recent models of\nspatiotemporal patterns across individuals fail to incorporate explicit\nsystem-level collective behavior that can influence the trajectories of\nindividual entities. To address this gap in the literature, we present a new\nhierarchical switching-state model that can be trained in an unsupervised\nfashion to simultaneously learn both system-level and individual-level\ndynamics. We employ a latent system-level discrete state Markov chain that\nprovides top-down influence on latent entity-level chains which in turn govern\nthe emission of each observed time series. Recurrent feedback from the\nobservations to the latent chains at both entity and system levels allows\nrecent situational context to inform how dynamics unfold at all levels in\nbottom-up fashion. We hypothesize that including both top-down and bottom-up\ninfluences on group dynamics will improve interpretability of the learned\ndynamics and reduce error when forecasting. Our hierarchical switching\nrecurrent dynamical model can be learned via closed-form variational coordinate\nascent updates to all latent chains that scale linearly in the number of\nentities. This is asymptotically no more costly than fitting a separate model\nfor each entity. Analysis of both synthetic data and real basketball team\nmovements suggests our lean parametric model can achieve competitive forecasts\ncompared to larger neural network models that require far more computational\nresources. Further experiments on soldier data as well as a synthetic task with\n64 cooperating entities show how our approach can yield interpretable insights\nabout team dynamics over time.\n","authors":["Michael T. Wojnowicz","Kaitlin Gili","Preetish Rath","Eric Miller","Jeffrey Miller","Clifford Hancock","Meghan O'Donovan","Seth Elkin-Frankston","Tad T. Brunyé","Michael C. Hughes"],"pdf_url":"https://arxiv.org/pdf/2401.14973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17644v4","updated":"2024-12-02T17:12:54Z","published":"2024-04-26T18:08:15Z","title":"A Conditional Independence Test in the Presence of Discretization","summary":"  Testing conditional independence has many applications, such as in Bayesian\nnetwork learning and causal discovery. Different test methods have been\nproposed. However, existing methods generally can not work when only\ndiscretized observations are available. Specifically, consider $X_1$,\n$\\tilde{X}_2$ and $X_3$ are observed variables, where $\\tilde{X}_2$ is a\ndiscretization of latent variables $X_2$. Applying existing test methods to the\nobservations of $X_1$, $\\tilde{X}_2$ and $X_3$ can lead to a false conclusion\nabout the underlying conditional independence of variables $X_1$, $X_2$ and\n$X_3$. Motivated by this, we propose a conditional independence test\nspecifically designed to accommodate the presence of such discretization. To\nachieve this, we design the bridge equations to recover the parameter\nreflecting the statistical information of the underlying latent continuous\nvariables. An appropriate test statistic and its asymptotic distribution under\nthe null hypothesis of conditional independence have also been derived. Both\ntheoretical results and empirical validation have been provided, demonstrating\nthe effectiveness of our test methods.\n","authors":["Boyang Sun","Yu Yao","Huangyuan Hao","Yumou Qiu","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.17644v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07118v3","updated":"2024-12-02T17:11:07Z","published":"2024-11-11T16:45:18Z","title":"ConvMixFormer- A Resource-efficient Convolution Mixer for\n  Transformer-based Dynamic Hand Gesture Recognition","summary":"  Transformer models have demonstrated remarkable success in many domains such\nas natural language processing (NLP) and computer vision. With the growing\ninterest in transformer-based architectures, they are now utilized for gesture\nrecognition. So, we also explore and devise a novel ConvMixFormer architecture\nfor dynamic hand gestures. The transformers use quadratic scaling of the\nattention features with the sequential data, due to which these models are\ncomputationally complex and heavy. We have considered this drawback of the\ntransformer and designed a resource-efficient model that replaces the\nself-attention in the transformer with the simple convolutional layer-based\ntoken mixer. The computational cost and the parameters used for the\nconvolution-based mixer are comparatively less than the quadratic\nself-attention. Convolution-mixer helps the model capture the local spatial\nfeatures that self-attention struggles to capture due to their sequential\nprocessing nature. Further, an efficient gate mechanism is employed instead of\na conventional feed-forward network in the transformer to help the model\ncontrol the flow of features within different stages of the proposed model.\nThis design uses fewer learnable parameters which is nearly half the vanilla\ntransformer that helps in fast and efficient training. The proposed method is\nevaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has\nachieved state-of-the-art results on single and multimodal inputs. We have also\nshown the parameter efficiency of the proposed ConvMixFormer model compared to\nother methods. The source code is available at\nhttps://github.com/mallikagarg/ConvMixFormer.\n","authors":["Mallika Garg","Debashis Ghosh","Pyari Mohan Pradhan"],"pdf_url":"https://arxiv.org/pdf/2411.07118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17251v2","updated":"2024-12-02T16:37:41Z","published":"2024-11-26T09:29:27Z","title":"DGNN-YOLO: Dynamic Graph Neural Networks with YOLO11 for Small Object\n  Detection and Tracking in Traffic Surveillance","summary":"  Accurate detection and tracking of small objects such as pedestrians,\ncyclists, and motorbikes are critical for traffic surveillance systems, which\nare crucial in improving road safety and decision-making in intelligent\ntransportation systems. However, traditional methods struggle with challenges\nsuch as occlusion, low resolution, and dynamic traffic conditions,\nnecessitating innovative approaches to address these limitations. This paper\nintroduces DGNN-YOLO, a novel framework integrating dynamic graph neural\nnetworks (DGNN) with YOLO11 to enhance small object detection and tracking in\ntraffic surveillance systems. The framework leverages YOLO11's advanced spatial\nfeature extraction capabilities for precise object detection and incorporates\nDGNN to model spatial-temporal relationships for robust real-time tracking\ndynamically. By constructing and updating graph structures, DGNN-YOLO\neffectively represents objects as nodes and their interactions as edges,\nensuring adaptive and accurate tracking in complex and dynamic environments.\nExtensive experiments demonstrate that DGNN-YOLO consistently outperforms\nstate-of-the-art methods in detecting and tracking small objects under diverse\ntraffic conditions, achieving the highest precision (0.8382), recall (0.6875),\nand mAP@0.5:0.95 (0.6476), showcasing its robustness and scalability,\nparticularly in challenging scenarios involving small and occluded objects.\nThis work provides a scalable, real-time traffic surveillance and analysis\nsolution, significantly contributing to intelligent transportation systems.\n","authors":["Shahriar Soudeep","M. F. Mridha","Md Abrar Jahin","Nilanjan Dey"],"pdf_url":"https://arxiv.org/pdf/2411.17251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17311v3","updated":"2024-12-02T16:29:47Z","published":"2024-05-27T16:11:49Z","title":"Probabilistic Graph Rewiring via Virtual Nodes","summary":"  Message-passing graph neural networks (MPNNs) have emerged as a powerful\nparadigm for graph-based machine learning. Despite their effectiveness, MPNNs\nface challenges such as under-reaching and over-squashing, where limited\nreceptive fields and structural bottlenecks hinder information flow in the\ngraph. While graph transformers hold promise in addressing these issues, their\nscalability is limited due to quadratic complexity regarding the number of\nnodes, rendering them impractical for larger graphs. Here, we propose\nimplicitly rewired message-passing neural networks (IPR-MPNNs), a novel\napproach that integrates implicit probabilistic graph rewiring into MPNNs. By\nintroducing a small number of virtual nodes, i.e., adding additional nodes to a\ngiven graph and connecting them to existing nodes, in a differentiable,\nend-to-end manner, IPR-MPNNs enable long-distance message propagation,\ncircumventing quadratic complexity. Theoretically, we demonstrate that\nIPR-MPNNs surpass the expressiveness of traditional MPNNs. Empirically, we\nvalidate our approach by showcasing its ability to mitigate under-reaching and\nover-squashing effects, achieving state-of-the-art performance across multiple\ngraph datasets. Notably, IPR-MPNNs outperform graph transformers while\nmaintaining significantly faster computational efficiency.\n","authors":["Chendi Qian","Andrei Manolache","Christopher Morris","Mathias Niepert"],"pdf_url":"https://arxiv.org/pdf/2405.17311v3.pdf","comment":"Accepted at 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024), Vancouver, Canada"},{"id":"http://arxiv.org/abs/2409.19839v3","updated":"2024-12-02T16:27:16Z","published":"2024-09-30T00:41:51Z","title":"ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities","summary":"  Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM (p-value\n$<0.01$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.\n","authors":["Ezra Karger","Houtan Bastani","Chen Yueh-Han","Zachary Jacobs","Danny Halawi","Fred Zhang","Philip E. Tetlock"],"pdf_url":"https://arxiv.org/pdf/2409.19839v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17339v2","updated":"2024-12-02T16:08:41Z","published":"2024-05-27T16:42:51Z","title":"Physics-Informed Real NVP for Satellite Power System Fault Detection","summary":"  The unique challenges posed by the space environment, characterized by\nextreme conditions and limited accessibility, raise the need for robust and\nreliable techniques to identify and prevent satellite faults. Fault detection\nmethods in the space sector are required to ensure mission success and to\nprotect valuable assets. In this context, this paper proposes an Artificial\nIntelligence (AI) based fault detection methodology and evaluates its\nperformance on ADAPT (Advanced Diagnostics and Prognostics Testbed), an\nElectrical Power System (EPS) dataset, crafted in laboratory by NASA. Our study\nfocuses on the application of a physics-informed (PI) real-valued non-volume\npreserving (Real NVP) model for fault detection in space systems. The efficacy\nof this method is systematically compared against other AI approaches such as\nGated Recurrent Unit (GRU) and Autoencoder-based techniques. Results show that\nour physics-informed approach outperforms existing methods of fault detection,\ndemonstrating its suitability for addressing the unique challenges of satellite\nEPS sub-system faults. Furthermore, we unveil the competitive advantage of\nphysics-informed loss in AI models to address specific space needs, namely\nrobustness, reliability, and power constraints, crucial for space exploration\nand satellite missions.\n","authors":["Carlo Cena","Umberto Albertin","Mauro Martini","Silvia Bucci","Marcello Chiaberge"],"pdf_url":"https://arxiv.org/pdf/2405.17339v2.pdf","comment":"C. Cena, U. Albertin, M. Martini, S. Bucci and M. Chiaberge,\n  \"Physics-Informed Real NVP for Satellite Power System Fault Detection,\" 2024\n  IEEE International Conference on Advanced Intelligent Mechatronics (AIM),\n  Boston, MA, USA, 2024, pp. 679-684, doi: 10.1109/AIM55361.2024.10636990"},{"id":"http://arxiv.org/abs/2407.02861v2","updated":"2024-12-02T16:04:40Z","published":"2024-07-03T07:19:41Z","title":"A Self-Supervised Task for Fault Detection in Satellite Multivariate\n  Time Series","summary":"  In the space sector, due to environmental conditions and restricted\naccessibility, robust fault detection methods are imperative for ensuring\nmission success and safeguarding valuable assets. This work proposes a novel\napproach leveraging Physics-Informed Real NVP neural networks, renowned for\ntheir ability to model complex and high-dimensional distributions, augmented\nwith a self-supervised task based on sensors' data permutation. It focuses on\nenhancing fault detection within the satellite multivariate time series. The\nexperiments involve various configurations, including pre-training with\nself-supervision, multi-task learning, and standalone self-supervised training.\nResults indicate significant performance improvements across all settings. In\nparticular, employing only the self-supervised loss yields the best overall\nresults, suggesting its efficacy in guiding the network to extract relevant\nfeatures for fault detection. This study presents a promising direction for\nimproving fault detection in space systems and warrants further exploration in\nother datasets and applications.\n","authors":["Carlo Cena","Silvia Bucci","Alessandro Balossino","Marcello Chiaberge"],"pdf_url":"https://arxiv.org/pdf/2407.02861v2.pdf","comment":"SPAICE: AI in and for Space, 2024"},{"id":"http://arxiv.org/abs/2411.04630v2","updated":"2024-12-02T15:47:17Z","published":"2024-11-07T11:29:55Z","title":"Brain Tumour Removing and Missing Modality Generation using 3D WDM","summary":"  This paper presents the second-placed solution for task 8 and the\nparticipation solution for task 7 of BraTS 2024. The adoption of automated\nbrain analysis algorithms to support clinical practice is increasing. However,\nmany of these algorithms struggle with the presence of brain lesions or the\nabsence of certain MRI modalities. The alterations in the brain's morphology\nleads to high variability and thus poor performance of predictive models that\nwere trained only on healthy brains. The lack of information that is usually\nprovided by some of the missing MRI modalities also reduces the reliability of\nthe prediction models trained with all modalities. In order to improve the\nperformance of these models, we propose the use of conditional 3D wavelet\ndiffusion models. The wavelet transform enabled full-resolution image training\nand prediction on a GPU with 48 GB VRAM, without patching or downsampling,\npreserving all information for prediction. The code for these tasks is\navailable at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.\n","authors":["André Ferreira","Gijs Luijten","Behrus Puladi","Jens Kleesiek","Victor Alves","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2411.04630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12850v2","updated":"2024-12-02T15:46:35Z","published":"2024-07-08T09:50:49Z","title":"Limits to Predicting Online Speech Using Large Language Models","summary":"  We study the predictability of online speech on social media, and whether\npredictability improves with information outside a user's own posts. Recent\ntheoretical results suggest that posts from a user's social circle are as\npredictive of the user's future posts as that of the user's past posts.\nMotivated by the success of large language models, we empirically test this\nhypothesis. We define predictability as a measure of the model's uncertainty,\ni.e., its negative log-likelihood on future tokens given context. As the basis\nof our study, we collect 10M tweets for ``tweet-tuning'' base models and a\nfurther 6.25M posts from more than five thousand X (previously Twitter) users\nand their peers. Across four large language models ranging in size from 1.5\nbillion to 70 billion parameters, we find that predicting a user's posts from\ntheir peers' posts performs poorly. Moreover, the value of the user's own posts\nfor prediction is consistently higher than that of their peers'. We extend our\ninvestigation with a detailed analysis on what's learned in-context and the\nrobustness of our findings. From context, base models learn to correctly\npredict @-mentions and hashtags. Moreover, our results replicate if instead of\nprompting the model with additional context, we finetune on it. Across the\nboard, we find that predicting the posts of individual users remains hard.\n","authors":["Mina Remeli","Moritz Hardt","Robert C. Williamson"],"pdf_url":"https://arxiv.org/pdf/2407.12850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16867v2","updated":"2024-12-02T15:42:53Z","published":"2024-07-23T22:23:47Z","title":"From Text to Insight: Large Language Models for Materials Science Data\n  Extraction","summary":"  The vast majority of materials science knowledge exists in unstructured\nnatural language, yet structured data is crucial for innovative and systematic\nmaterials design. Traditionally, the field has relied on manual curation and\npartial automation for data extraction for specific use cases. The advent of\nlarge language models (LLMs) represents a significant shift, potentially\nenabling efficient extraction of structured, actionable data from unstructured\ntext by non-experts. While applying LLMs to materials science data extraction\npresents unique challenges, domain knowledge offers opportunities to guide and\nvalidate LLM outputs. This review provides a comprehensive overview of\nLLM-based structured data extraction in materials science, synthesizing current\nknowledge and outlining future directions. We address the lack of standardized\nguidelines and present frameworks for leveraging the synergy between LLMs and\nmaterials science expertise. This work serves as a foundational resource for\nresearchers aiming to harness LLMs for data-driven materials research. The\ninsights presented here could significantly enhance how researchers across\ndisciplines access and utilize scientific information, potentially accelerating\nthe development of novel materials for critical societal needs.\n","authors":["Mara Schilling-Wilhelmi","Martiño Ríos-García","Sherjeel Shabih","María Victoria Gil","Santiago Miret","Christoph T. Koch","José A. Márquez","Kevin Maik Jablonka"],"pdf_url":"https://arxiv.org/pdf/2407.16867v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06562v2","updated":"2024-12-02T15:32:41Z","published":"2023-12-11T17:46:44Z","title":"On Meta-Prompting","summary":"  Modern generative language models are capable of interpreting input strings\nas instructions, or prompts, and carry out tasks based on them. Many approaches\nto prompting and pre-training these models involve the automated generation of\nthese prompts: meta-prompting, or prompting to obtain prompts. We propose a\ntheoretical framework based on category theory to generalize and describe them.\nThis framework is flexible enough to account for stochasticity, and allows us\nto obtain formal results around task agnosticity and equivalence of various\nmeta-prompting approaches. Experimentally, we test our framework in two active\nareas of model research: creativity and ideation. We find that user preference\nstrongly favors (p < 0.01) the prompts generated under meta-prompting, as well\nas their corresponding outputs, over a series of hardcoded baseline prompts\nthat include the original task definition. Using our framework, we argue that\nmeta-prompting is more effective than basic prompting at generating desirable\noutputs.\n","authors":["Adrian de Wynter","Xun Wang","Qilong Gu","Si-Qing Chen"],"pdf_url":"https://arxiv.org/pdf/2312.06562v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.01036v2","updated":"2024-12-02T15:31:12Z","published":"2024-07-01T07:40:08Z","title":"Ranking by Lifts: A Cost-Benefit Approach to Large-Scale A/B Tests","summary":"  A/B testers that conduct large-scale tests often prioritize lifts as the main\noutcome metric and want to be able to control costs resulting from false\nrejections of the null. This work develops a decision-theoretic framework for\nmaximizing profits subject to false discovery rate (FDR) control. We build an\nempirical Bayes solution for the problem via a greedy knapsack approach. We\nderive an oracle rule based on ranking the ratio of expected lifts and the cost\nof wrong rejections using the local false discovery rate (lfdr) statistic. Our\noracle decision rule is valid and optimal for large-scale tests. Further, we\nestablish asymptotic validity for the data-driven procedure and demonstrate\nfinite-sample validity in experimental studies. We also demonstrate the merit\nof the proposed method over other FDR control methods. Finally, we discuss an\napplication to data collected by experiments on the Optimizely platform.\n","authors":["Pallavi Basu","Ron Berman"],"pdf_url":"https://arxiv.org/pdf/2407.01036v2.pdf","comment":"Updated"},{"id":"http://arxiv.org/abs/2411.08085v2","updated":"2024-12-02T15:20:08Z","published":"2024-11-12T16:52:51Z","title":"Deep Learning 2.0: Artificial Neurons That Matter -- Reject Correlation,\n  Embrace Orthogonality","summary":"  We introduce a yat-product-powered neural network, the Neural Matter Network\n(NMN), a breakthrough in deep learning that achieves non-linear pattern\nrecognition without activation functions. Our key innovation relies on the\nyat-product and yat-product, which naturally induces non-linearity by\nprojecting inputs into a pseudo-metric space, eliminating the need for\ntraditional activation functions while maintaining only a softmax layer for\nfinal class probability distribution. This approach simplifies network\narchitecture and provides unprecedented transparency into the network's\ndecision-making process. Our comprehensive empirical evaluation across\ndifferent datasets demonstrates that NMN consistently outperforms traditional\nMLPs. The results challenge the assumption that separate activation functions\nare necessary for effective deep-learning models. The implications of this work\nextend beyond immediate architectural benefits, by eliminating intermediate\nactivation functions while preserving non-linear capabilities, yat-MLP\nestablishes a new paradigm for neural network design that combines simplicity\nwith effectiveness. Most importantly, our approach provides unprecedented\ninsights into the traditionally opaque \"black-box\" nature of neural networks,\noffering a clearer understanding of how these models process and classify\ninformation.\n","authors":["Taha Bouhsine"],"pdf_url":"https://arxiv.org/pdf/2411.08085v2.pdf","comment":"fixed proof, added softermax"},{"id":"http://arxiv.org/abs/2407.08751v2","updated":"2024-12-02T15:16:03Z","published":"2024-06-27T13:47:06Z","title":"Latent Diffusion for Neural Spiking Data","summary":"  Modern datasets in neuroscience enable unprecedented inquiries into the\nrelationship between complex behaviors and the activity of many simultaneously\nrecorded neurons. While latent variable models can successfully extract\nlow-dimensional embeddings from such recordings, using them to generate\nrealistic spiking data, especially in a behavior-dependent manner, still poses\na challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS),\na diffusion-based generative model with a low-dimensional latent space: LDNS\nemploys an autoencoder with structured state-space (S4) layers to project\ndiscrete high-dimensional spiking data into continuous time-aligned latents. On\nthese inferred latents, we train expressive (conditional) diffusion models,\nenabling us to sample neural activity with realistic single-neuron and\npopulation spiking statistics. We validate LDNS on synthetic data, accurately\nrecovering latent structure, firing rates, and spiking statistics. Next, we\ndemonstrate its flexibility by generating variable-length data that mimics\nhuman cortical activity during attempted speech. We show how to equip LDNS with\nan expressive observation model that accounts for single-neuron dynamics not\nmediated by the latent state, further increasing the realism of generated\nsamples. Finally, conditional LDNS trained on motor cortical activity during\ndiverse reaching behaviors can generate realistic spiking data given reach\ndirection or unseen reach trajectories. In summary, LDNS simultaneously enables\ninference of low-dimensional latents and realistic conditional generation of\nneural spiking datasets, opening up further possibilities for simulating\nexperimentally testable hypotheses.\n","authors":["Jaivardhan Kapoor","Auguste Schulz","Julius Vetter","Felix Pei","Richard Gao","Jakob H. Macke"],"pdf_url":"https://arxiv.org/pdf/2407.08751v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2405.04101v2","updated":"2024-12-02T14:54:31Z","published":"2024-05-07T08:15:48Z","title":"Continual Learning in the Presence of Repetition","summary":"  Continual learning (CL) provides a framework for training models in\never-evolving environments. Although re-occurrence of previously seen objects\nor tasks is common in real-world problems, the concept of repetition in the\ndata stream is not often considered in standard benchmarks for CL. Unlike with\nthe rehearsal mechanism in buffer-based strategies, where sample repetition is\ncontrolled by the strategy, repetition in the data stream naturally stems from\nthe environment. This report provides a summary of the CLVision challenge at\nCVPR 2023, which focused on the topic of repetition in class-incremental\nlearning. The report initially outlines the challenge objective and then\ndescribes three solutions proposed by finalist teams that aim to effectively\nexploit the repetition in the stream to learn continually. The experimental\nresults from the challenge highlight the effectiveness of ensemble-based\nsolutions that employ multiple versions of similar modules, each trained on\ndifferent but overlapping subsets of classes. This report underscores the\ntransformative potential of taking a different perspective in CL by employing\nrepetition in the data stream to foster innovative strategy design.\n","authors":["Hamed Hemati","Lorenzo Pellegrini","Xiaotian Duan","Zixuan Zhao","Fangfang Xia","Marc Masana","Benedikt Tscheschner","Eduardo Veas","Yuxiang Zheng","Shiji Zhao","Shao-Yuan Li","Sheng-Jun Huang","Vincenzo Lomonaco","Gido M. van de Ven"],"pdf_url":"https://arxiv.org/pdf/2405.04101v2.pdf","comment":"Accepted version, to appear in Neural Networks; Challenge Report of\n  the 4th Workshop on Continual Learning in Computer Vision at CVPR"},{"id":"http://arxiv.org/abs/2405.09273v7","updated":"2024-12-02T14:49:26Z","published":"2024-05-15T11:42:41Z","title":"Fair Generalized Linear Mixed Models","summary":"  When using machine learning for automated prediction, it is important to\naccount for fairness in the prediction. Fairness in machine learning aims to\nensure that biases in the data and model inaccuracies do not lead to\ndiscriminatory decisions. E.g., predictions from fair machine learning models\nshould not discriminate against sensitive variables such as sexual orientation\nand ethnicity. The training data often in obtained from social surveys. In\nsocial surveys, oftentimes the data collection process is a strata sampling,\ne.g. due to cost restrictions. In strata samples, the assumption of\nindependence between the observation is not fulfilled. Hence, if the machine\nlearning models do not account for the strata correlations, the results may be\nbiased. Especially high is the bias in cases where the strata assignment is\ncorrelated to the variable of interest. We present in this paper an algorithm\nthat can handle both problems simultaneously, and we demonstrate the impact of\nstratified sampling on the quality of fair machine learning predictions in a\nreproducible simulation study.\n","authors":["Jan Pablo Burgard","João Vitor Pamplona"],"pdf_url":"https://arxiv.org/pdf/2405.09273v7.pdf","comment":"25 pages, 12 figures. arXiv admin note: text overlap with\n  arXiv:2405.06433"},{"id":"http://arxiv.org/abs/2405.06433v6","updated":"2024-12-02T14:47:51Z","published":"2024-05-10T12:25:06Z","title":"Fair Mixed Effects Support Vector Machine","summary":"  To ensure unbiased and ethical automated predictions, fairness must be a core\nprinciple in machine learning applications. Fairness in machine learning aims\nto mitigate biases present in the training data and model imperfections that\ncould lead to discriminatory outcomes. This is achieved by preventing the model\nfrom making decisions based on sensitive characteristics like ethnicity or\nsexual orientation. A fundamental assumption in machine learning is the\nindependence of observations. However, this assumption often does not hold true\nfor data describing social phenomena, where data points are often clustered\nbased. Hence, if the machine learning models do not account for the cluster\ncorrelations, the results may be biased. Especially high is the bias in cases\nwhere the cluster assignment is correlated to the variable of interest. We\npresent a fair mixed effects support vector machine algorithm that can handle\nboth problems simultaneously. With a reproducible simulation study we\ndemonstrate the impact of clustered data on the quality of fair machine\nlearning predictions.\n","authors":["Jan Pablo Burgard","João Vitor Pamplona"],"pdf_url":"https://arxiv.org/pdf/2405.06433v6.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2405.15158v2","updated":"2024-12-02T14:42:34Z","published":"2024-05-24T02:26:45Z","title":"ProtFAD: Introducing function-aware domains as implicit modality towards\n  protein function prediction","summary":"  Protein function prediction is currently achieved by encoding its sequence or\nstructure, where the sequence-to-function transcendence and high-quality\nstructural data scarcity lead to obvious performance bottlenecks. Protein\ndomains are \"building blocks\" of proteins that are functionally independent,\nand their combinations determine the diverse biological functions. However,\nmost existing studies have yet to thoroughly explore the intricate functional\ninformation contained in the protein domains. To fill this gap, we propose a\nsynergistic integration approach for a function-aware domain representation,\nand a domain-joint contrastive learning strategy to distinguish different\nprotein functions while aligning the modalities. Specifically, we align the\ndomain semantics with GO terms and text description to pre-train domain\nembeddings. Furthermore, we partition proteins into multiple sub-views based on\ncontinuous joint domains for contrastive training under the supervision of a\nnovel triplet InfoNCE loss. Our approach significantly and comprehensively\noutperforms the state-of-the-art methods on various benchmarks, and clearly\ndifferentiates proteins carrying distinct functions compared to the competitor.\nOur implementation is available at\nhttps://github.com/AI-HPC-Research-Team/ProtFAD.\n","authors":["Mingqing Wang","Zhiwei Nie","Yonghong He","Athanasios V. Vasilakos","Zhixiang Ren"],"pdf_url":"https://arxiv.org/pdf/2405.15158v2.pdf","comment":"17 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2108.11986v2","updated":"2024-12-02T14:25:58Z","published":"2021-08-25T11:45:40Z","title":"Anomaly Detection in Medical Imaging -- A Mini Review","summary":"  The increasing digitization of medical imaging enables machine learning based\nimprovements in detecting, visualizing and segmenting lesions, easing the\nworkload for medical experts. However, supervised machine learning requires\nreliable labelled data, which is is often difficult or impossible to collect or\nat least time consuming and thereby costly. Therefore methods requiring only\npartly labeled data (semi-supervised) or no labeling at all (unsupervised\nmethods) have been applied more regularly. Anomaly detection is one possible\nmethodology that is able to leverage semi-supervised and unsupervised methods\nto handle medical imaging tasks like classification and segmentation. This\npaper uses a semi-exhaustive literature review of relevant anomaly detection\npapers in medical imaging to cluster into applications, highlight important\nresults, establish lessons learned and give further advice on how to approach\nanomaly detection in medical imaging. The qualitative analysis is based on\ngoogle scholar and 4 different search terms, resulting in 120 different\nanalysed papers. The main results showed that the current research is mostly\nmotivated by reducing the need for labelled data. Also, the successful and\nsubstantial amount of research in the brain MRI domain shows the potential for\napplications in further domains like OCT and chest X-ray.\n","authors":["Maximilian E. Tschuchnig","Michael Gadermayr"],"pdf_url":"https://arxiv.org/pdf/2108.11986v2.pdf","comment":"Accepted and presented at iDSC2021 edit: During work on this\n  publication Maximilian Ernst Tschuchnig was affiliated with Salzburg\n  University of Applied Sciences and University of Salzburg"},{"id":"http://arxiv.org/abs/2410.01639v2","updated":"2024-12-02T14:25:30Z","published":"2024-10-02T15:09:36Z","title":"Moral Alignment for LLM Agents","summary":"  Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2410.01639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15166v2","updated":"2024-12-02T14:20:12Z","published":"2023-06-27T02:47:59Z","title":"Constraining Generative Models for Engineering Design with Negative Data","summary":"  Generative models have recently achieved remarkable success and widespread\nadoption in society, yet they often struggle to generate realistic and accurate\noutputs. This challenge extends beyond language and vision into fields like\nengineering design, where safety-critical engineering standards and\nnon-negotiable physical laws tightly constrain what outputs are considered\nacceptable. In this work, we introduce a novel training method to guide a\ngenerative model toward constraint-satisfying outputs using `negative data' --\nexamples of what to avoid. Our negative-data generative model (NDGM)\nformulation easily outperforms classic models, generating 1/6 as many\nconstraint-violating samples using 1/8 as much data in certain problems. It\nalso consistently outperforms other baselines, achieving a balance between\nconstraint satisfaction and distributional similarity that is unsurpassed by\nany other model in 12 of the 14 problems tested. This widespread superiority is\nrigorously demonstrated across numerous synthetic tests and real engineering\nproblems, such as ship hull synthesis with hydrodynamic constraints and vehicle\ndesign with impact safety constraints. Our benchmarks showcase both the\nbest-in-class performance of our new NDGM formulation and the overall dominance\nof NDGMs versus classic generative models. We publicly release the code and\nbenchmarks at https://github.com/Lyleregenwetter/NDGMs.\n","authors":["Lyle Regenwetter","Giorgio Giannone","Akash Srivastava","Dan Gutfreund","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2306.15166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13306v6","updated":"2024-12-02T14:13:35Z","published":"2023-01-30T21:59:30Z","title":"Autobidders with Budget and ROI Constraints: Efficiency, Regret, and\n  Pacing Dynamics","summary":"  We study a game between autobidding algorithms that compete in an online\nadvertising platform. Each autobidder is tasked with maximizing its\nadvertiser's total value over multiple rounds of a repeated auction, subject to\nbudget and return-on-investment constraints. We propose a gradient-based\nlearning algorithm that is guaranteed to satisfy all constraints and achieves\nvanishing individual regret. Our algorithm uses only bandit feedback and can be\nused with the first- or second-price auction, as well as with any\n\"intermediate\" auction format. Our main result is that when these autobidders\nplay against each other, the resulting expected liquid welfare over all rounds\nis at least half of the expected optimal liquid welfare achieved by any\nallocation. This holds whether or not the bidding dynamics converges to an\nequilibrium.\n","authors":["Brendan Lucier","Sarath Pattathil","Aleksandrs Slivkins","Mengxiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.13306v6.pdf","comment":"Appeared at COLT 2024. Numerical experiments added since Jun'24\n  version"},{"id":"http://arxiv.org/abs/2204.10942v2","updated":"2024-12-02T14:12:18Z","published":"2022-04-22T21:48:56Z","title":"Evaluation of Multi-Scale Multiple Instance Learning to Improve Thyroid\n  Cancer Classification","summary":"  Thyroid cancer is currently the fifth most common malignancy diagnosed in\nwomen. Since differentiation of cancer sub-types is important for treatment and\ncurrent, manual methods are time consuming and subjective, automatic\ncomputer-aided differentiation of cancer types is crucial. Manual\ndifferentiation of thyroid cancer is based on tissue sections, analysed by\npathologists using histological features. Due to the enormous size of gigapixel\nwhole slide images, holistic classification using deep learning methods is not\nfeasible. Patch based multiple instance learning approaches, combined with\naggregations such as bag-of-words, is a common approach. This work's\ncontribution is to extend a patch based state-of-the-art method by generating\nand combining feature vectors of three different patch resolutions and\nanalysing three distinct ways of combining them. The results showed\nimprovements in one of the three multi-scale approaches, while the others led\nto decreased scores. This provides motivation for analysis and discussion of\nthe individual approaches.\n","authors":["Maximilian E. Tschuchnig","Philipp Grubmüller","Lea M. Stangassinger","Christina Kreutzer","Sébastien Couillard-Després","Gertie J. Oostingh","Anton Hittmair","Michael Gadermayr"],"pdf_url":"https://arxiv.org/pdf/2204.10942v2.pdf","comment":"Accepted and presented at IPTA 2022 (Best Paper) edit: During work on\n  this publication Maximilian Ernst Tschuchnig was affiliated with Salzburg\n  University of Applied Sciences and University of Salzburg"},{"id":"http://arxiv.org/abs/2409.11141v2","updated":"2024-12-02T14:03:32Z","published":"2024-09-17T12:52:16Z","title":"Sample Complexity Bounds for Linear System Identification from a Finite\n  Set","summary":"  This paper considers a finite sample perspective on the problem of\nidentifying an LTI system from a finite set of possible systems using\ntrajectory data. To this end, we use the maximum likelihood estimator to\nidentify the true system and provide an upper bound for its sample complexity.\nCrucially, the derived bound does not rely on a potentially restrictive\nstability assumption. Additionally, we leverage tools from information theory\nto provide a lower bound to the sample complexity that holds independently of\nthe used estimator. The derived sample complexity bounds are analyzed\nanalytically and numerically.\n","authors":["Nicolas Chatzikiriakos","Andrea Iannelli"],"pdf_url":"https://arxiv.org/pdf/2409.11141v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07123v2","updated":"2024-12-02T13:04:18Z","published":"2024-09-11T09:21:20Z","title":"Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem","summary":"  Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German.\n","authors":["Qianli Wang","Tatiana Anikina","Nils Feldhus","Simon Ostermann","Sebastian Möller","Vera Schmitt"],"pdf_url":"https://arxiv.org/pdf/2409.07123v2.pdf","comment":"Accepted at COLING 2025; long paper"},{"id":"http://arxiv.org/abs/2305.15798v4","updated":"2024-12-02T12:58:23Z","published":"2023-05-25T07:28:28Z","title":"BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion","summary":"  Text-to-image (T2I) generation with Stable Diffusion models (SDMs) involves\nhigh computing demands due to billion-scale parameters. To enhance efficiency,\nrecent studies have reduced sampling steps and applied network quantization\nwhile retaining the original architectures. The lack of architectural reduction\nattempts may stem from worries over expensive retraining for such massive\nmodels. In this work, we uncover the surprising potential of block pruning and\nfeature distillation for low-cost general-purpose T2I. By removing several\nresidual and attention blocks from the U-Net of SDMs, we achieve 30%~50%\nreduction in model size, MACs, and latency. We show that distillation\nretraining is effective even under limited resources: using only 13 A100 days\nand a tiny dataset, our compact models can imitate the original SDMs (v1.4 and\nv2.1-base with over 6,000 A100 days). Benefiting from the transferred\nknowledge, our BK-SDMs deliver competitive results on zero-shot MS-COCO against\nlarger multi-billion parameter models. We further demonstrate the applicability\nof our lightweight backbones in personalized generation and image-to-image\ntranslation. Deployment of our models on edge devices attains 4-second\ninference. Code and models can be found at:\nhttps://github.com/Nota-NetsPresso/BK-SDM\n","authors":["Bo-Kyeong Kim","Hyoung-Kyu Song","Thibault Castells","Shinkook Choi"],"pdf_url":"https://arxiv.org/pdf/2305.15798v4.pdf","comment":"ECCV 2024 Camera-Ready Version"},{"id":"http://arxiv.org/abs/2312.02522v2","updated":"2024-12-02T12:49:50Z","published":"2023-12-05T06:05:04Z","title":"MASP: Scalable GNN-based Planning for Multi-Agent Navigation","summary":"  We investigate multi-agent navigation tasks, where multiple agents need to\nreach initially unassigned goals in a limited time. Classical planning-based\nmethods suffer from expensive computation overhead at each step and offer\nlimited expressiveness for complex cooperation strategies. In contrast,\nreinforcement learning (RL) has recently become a popular approach for\naddressing this issue. However, RL struggles with low data efficiency and\ncooperation when directly exploring (nearly) optimal policies in a large\nexploration space, especially with an increased number of agents(e.g., 10+\nagents) or in complex environments (e.g., 3-D simulators). In this paper, we\npropose the Multi-Agent Scalable Graph-based Planner (MASP), a goal-conditioned\nhierarchical planner for navigation tasks with a substantial number of agents\nin the decentralized setting. MASP employs a hierarchical framework to reduce\nspace complexity by decomposing a large exploration space into multiple\ngoal-conditioned subspaces, where a high-level policy assigns agents goals, and\na low-level policy navigates agents toward designated goals. For agent\ncooperation and the adaptation to varying team sizes, we model agents and goals\nas graphs to better capture their relationship. The high-level policy, the Goal\nMatcher, leverages a graph-based Self-Encoder and Cross-Encoder to optimize\ngoal assignment by updating the agent and the goal graphs. The low-level\npolicy, the Coordinated Action Executor, introduces the Group Information\nFusion to facilitate group division and extract agent relationships across\ngroups, enhancing training efficiency for agent cooperation. The results\ndemonstrate that MASP outperforms RL and planning-based baselines in task\nefficiency.\n","authors":["Xinyi Yang","Xinting Yang","Chao Yu","Jiayu Chen","Wenbo Ding","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2312.02522v2.pdf","comment":"Submitted to IEEE RA-L"},{"id":"http://arxiv.org/abs/2410.07836v4","updated":"2024-12-02T12:44:48Z","published":"2024-10-10T11:52:07Z","title":"Masked Generative Priors Improve World Models Sequence Modelling\n  Capabilities","summary":"  Deep Reinforcement Learning (RL) has become the leading approach for creating\nartificial agents in complex environments. Model-based approaches, which are RL\nmethods with world models that predict environment dynamics, are among the most\npromising directions for improving data efficiency, forming a critical step\ntoward bridging the gap between research and real-world deployment. In\nparticular, world models enhance sample efficiency by learning in imagination,\nwhich involves training a generative sequence model of the environment in a\nself-supervised manner. Recently, Masked Generative Modelling has emerged as a\nmore efficient and superior inductive bias for modelling and generating token\nsequences. Building on the Efficient Stochastic Transformer-based World Models\n(STORM) architecture, we replace the traditional MLP prior with a Masked\nGenerative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our\nmodel on two downstream tasks: reinforcement learning and video prediction.\nGIT-STORM demonstrates substantial performance gains in RL tasks on the Atari\n100k benchmark. Moreover, we apply Transformer-based World Models to continuous\naction environments for the first time, addressing a significant gap in prior\nresearch. To achieve this, we employ a state mixer function that integrates\nlatent state representations with actions, enabling our model to handle\ncontinuous control tasks. We validate this approach through qualitative and\nquantitative analyses on the DeepMind Control Suite, showcasing the\neffectiveness of Transformer-based World Models in this new domain. Our results\nhighlight the versatility and efficacy of the MaskGIT dynamics prior, paving\nthe way for more accurate world models and effective RL policies.\n","authors":["Cristian Meo","Mircea Lica","Zarif Ikram","Akihiro Nakano","Vedant Shah","Aniket Rajiv Didolkar","Dianbo Liu","Anirudh Goyal","Justin Dauwels"],"pdf_url":"https://arxiv.org/pdf/2410.07836v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03976v4","updated":"2024-12-02T12:44:25Z","published":"2023-11-07T13:24:01Z","title":"Topology Only Pre-Training: Towards Generalised Multi-Domain Graph\n  Models","summary":"  The principal benefit of unsupervised representation learning is that a\npre-trained model can be fine-tuned where data or labels are scarce. Existing\napproaches for graph representation learning are domain specific, maintaining\nconsistent node and edge features across the pre-training and target datasets.\nThis has precluded transfer to multiple domains. We present Topology Only\nPre-Training (ToP), a graph pre-training method based on node and edge feature\nexclusion. We show positive transfer on evaluation datasets from multiple\ndomains, including domains not present in pre-training data, running directly\ncontrary to assumptions made in contemporary works. On 75% of experiments, ToP\nmodels perform significantly $p \\leq 0.01$ better than a supervised baseline.\nPerformance is significantly positive on 85.7% of tasks when node and edge\nfeatures are used in fine-tuning. We further show that out-of-domain topologies\ncan produce more useful pre-training than in-domain. Under ToP we show better\ntransfer from non-molecule pre-training, compared to molecule pre-training, on\n79% of molecular benchmarks. Against the limited set of other generalist graph\nmodels ToP performs strongly, including against models with many orders of\nmagnitude larger. These findings show that ToP opens broad areas of research in\nboth transfer learning on scarcely populated graph domains and in graph\nfoundation models.\n","authors":["Alex O. Davies","Riku W. Green","Nirav S. Ajmeri","Telmo M. Silva Filho"],"pdf_url":"https://arxiv.org/pdf/2311.03976v4.pdf","comment":"28 pages, 5 figures, 5 tables. For in-development code see\n  https://github.com/neutralpronoun/general-gcl"},{"id":"http://arxiv.org/abs/2405.14655v2","updated":"2024-12-02T12:37:46Z","published":"2024-05-23T14:53:54Z","title":"Multi-turn Reinforcement Learning from Preference Human Feedback","summary":"  Reinforcement Learning from Human Feedback (RLHF) has become the standard\napproach for aligning Large Language Models (LLMs) with human preferences,\nallowing LLMs to demonstrate remarkable abilities in various tasks. Existing\nmethods work by emulating the preferences at the single decision (turn) level,\nlimiting their capabilities in settings that require planning or multi-turn\ninteractions to achieve a long-term goal. In this paper, we address this issue\nby developing novel methods for Reinforcement Learning (RL) from preference\nfeedback between two full multi-turn conversations. In the tabular setting, we\npresent a novel mirror-descent-based policy optimization algorithm for the\ngeneral multi-turn preference-based RL problem, and prove its convergence to\nNash equilibrium. To evaluate performance, we create a new environment,\nEducation Dialogue, where a teacher agent guides a student in learning a random\ntopic, and show that a deep RL variant of our algorithm outperforms RLHF\nbaselines. Finally, we show that in an environment with explicit rewards, our\nalgorithm recovers the same performance as a reward-based RL baseline, despite\nrelying solely on a weaker preference signal.\n","authors":["Lior Shani","Aviv Rosenberg","Asaf Cassel","Oran Lang","Daniele Calandriello","Avital Zipori","Hila Noga","Orgad Keller","Bilal Piot","Idan Szpektor","Avinatan Hassidim","Yossi Matias","Rémi Munos"],"pdf_url":"https://arxiv.org/pdf/2405.14655v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12380v3","updated":"2024-12-02T12:36:45Z","published":"2023-09-21T12:44:31Z","title":"Methods for generating and evaluating synthetic longitudinal patient\n  data: a systematic review","summary":"  The rapid growth in data availability has facilitated research and\ndevelopment, yet not all industries have benefited equally due to legal and\nprivacy constraints. The healthcare sector faces significant challenges in\nutilizing patient data because of concerns about data security and\nconfidentiality. To address this, various privacy-preserving methods, including\nsynthetic data generation, have been proposed. Synthetic data replicate\nexisting data as closely as possible, acting as a proxy for sensitive\ninformation. While patient data are often longitudinal, this aspect remains\nunderrepresented in existing reviews of synthetic data generation in\nhealthcare. This paper maps and describes methods for generating and evaluating\nsynthetic longitudinal patient data in real-life settings through a systematic\nliterature review, conducted following the PRISMA guidelines and incorporating\ndata from five databases up to May 2024. Thirty-nine methods were identified,\nwith four addressing all challenges of longitudinal data generation, though\nnone included privacy-preserving mechanisms. Resemblance was evaluated in most\nstudies, utility in the majority, and privacy in just over half. Only a small\nfraction of studies assessed all three aspects. Our findings highlight the need\nfor further research in this area.\n","authors":["Katariina Perkonoja","Kari Auranen","Joni Virta"],"pdf_url":"https://arxiv.org/pdf/2309.12380v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02272v4","updated":"2024-12-02T12:36:30Z","published":"2024-11-04T17:03:55Z","title":"Combining Induction and Transduction for Abstract Reasoning","summary":"  When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC by training neural models for induction (inferring latent\nfunctions) and transduction (directly predicting the test output for a given\ntest input). We train on synthetically generated variations of Python programs\nthat solve ARC training tasks. We find inductive and transductive models solve\ndifferent kinds of test problems, despite having the same training problems and\nsharing the same neural architecture: Inductive program synthesis excels at\nprecise computations, and at composing multiple concepts, while transduction\nsucceeds on fuzzier perceptual concepts. Ensembling them approaches human-level\nperformance on ARC.\n","authors":["Wen-Ding Li","Keya Hu","Carter Larsen","Yuqing Wu","Simon Alford","Caleb Woo","Spencer M. Dunn","Hao Tang","Michelangelo Naim","Dat Nguyen","Wei-Long Zheng","Zenna Tavares","Yewen Pu","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2411.02272v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07818v5","updated":"2024-12-02T12:29:47Z","published":"2024-02-12T17:24:15Z","title":"Differentially Private Zeroth-Order Methods for Scalable Large Language\n  Model Finetuning","summary":"  Fine-tuning on task-specific datasets is a widely-embraced paradigm of\nharnessing the powerful capability of pretrained LLMs for various downstream\ntasks. Due to the popularity of LLMs fine-tuning and its accompanying privacy\nconcerns, differentially private (DP) fine-tuning of pretrained LLMs has been\nwidely used to safeguarding the privacy of task-specific datasets. Lying at the\ndesign core of DP LLM fine-tuning methods is the satisfactory tradeoff among\nprivacy, utility, and scalability. Most existing methods build upon the seminal\nwork of DP-SGD. Despite pushing the scalability of DP-SGD to its limit,\nDP-SGD-based fine-tuning methods are unfortunately limited by the inherent\ninefficiency of SGD.\n  In this paper, we investigate the potential of DP zeroth-order methods for\nLLM pretraining, which avoids the scalability bottleneck of SGD by\napproximating the gradient with the more efficient zeroth-order gradient.\nRather than treating the zeroth-order method as a drop-in replacement for SGD,\nthis paper presents a comprehensive study both theoretically and empirically.\nFirst, we propose the stagewise DP zeroth-order method (DP-ZOSO) that\ndynamically schedules key hyperparameters. This design is grounded on the\nsynergy between DP random perturbation and the gradient approximation error of\nthe zeroth-order method, and its effect on fine-tuning trajectory.\n  We provide theoretical analysis for both proposed methods. We conduct\nextensive empirical analysis on both encoder-only masked language model and\ndecoder-only autoregressive language model, achieving impressive results in\nterms of scalability and utility regardless of the class of tasks (compared\nwith DPZero, DP-ZOPO improves $4.5\\%$ on SST-5, $5.5\\%$ on MNLI with\nRoBERTa-Large and 9.2\\% on CB, 3.9\\% on BoolQ with OPT-2.7b when $\\epsilon=4$,\ndemonstrates more significant enhancement in performance on more complicated\ntasks).\n","authors":["Z Liu","J Lou","W Bao","Y Hu","B Li","Z Qin","K Ren"],"pdf_url":"https://arxiv.org/pdf/2402.07818v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08559v3","updated":"2024-12-02T12:16:19Z","published":"2024-10-11T06:30:48Z","title":"Learning General Representation of 12-Lead Electrocardiogram with a\n  Joint-Embedding Predictive Architecture","summary":"  Electrocardiogram (ECG) captures the heart's electrical signals, offering\nvaluable information for diagnosing cardiac conditions. However, the scarcity\nof labeled data makes it challenging to fully leverage supervised learning in\nmedical domain. Self-supervised learning (SSL) offers a promising solution,\nenabling models to learn from unlabeled data and uncover meaningful patterns.\nIn this paper, we show that masked modeling in the latent space can be a\npowerful alternative to existing self-supervised methods in the ECG domain. We\nintroduce ECG-JEPA, a SSL model for 12-lead ECG analysis that learns semantic\nrepresentations of ECG data by predicting in the hidden latent space, bypassing\nthe need to reconstruct raw signals. This approach offers several advantages in\nthe ECG domain: (1) it avoids producing unnecessary details, such as noise,\nwhich is common in ECG; and (2) it addresses the limitations of na\\\"ive L2 loss\nbetween raw signals. Another key contribution is the introduction of\nCross-Pattern Attention (CroPA), a specialized masked attention mechanism\ntailored for 12-lead ECG data. ECG-JEPA is trained on the union of several open\nECG datasets, totaling approximately 180,000 samples, and achieves\nstate-of-the-art performance in various downstream tasks including ECG\nclassification and feature prediction. Our code is openly available at\nhttps://github.com/sehunfromdaegu/ECG_JEPA.\n","authors":["Sehun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.08559v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11883v3","updated":"2024-12-02T12:09:05Z","published":"2024-10-11T10:09:46Z","title":"Simulation-based inference with scattering representations: scattering\n  is all you need","summary":"  We demonstrate the successful use of scattering representations without\nfurther compression for simulation-based inference (SBI) with images (i.e.\nfield-level), illustrated with a cosmological case study. Scattering\nrepresentations provide a highly effective representational space for\nsubsequent learning tasks, although the higher dimensional compressed space\nintroduces challenges. We overcome these through spatial averaging, coupled\nwith more expressive density estimators. Compared to alternative methods, such\nan approach does not require additional simulations for either training or\ncomputing derivatives, is interpretable, and resilient to covariate shift. As\nexpected, we show that a scattering only approach extracts more information\nthan traditional second order summary statistics.\n","authors":["Kiyam Lin","Benjamin Joachimi","Jason D. McEwen"],"pdf_url":"https://arxiv.org/pdf/2410.11883v3.pdf","comment":"9 pages, 2 figures, accepted by NeurIPS workshop on Machine Learning\n  and the Physical Sciences"},{"id":"http://arxiv.org/abs/2410.23132v2","updated":"2024-12-02T12:05:29Z","published":"2024-10-30T15:42:59Z","title":"Revisiting MAE pre-training for 3D medical image segmentation","summary":"  Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the\npotential of vast, untapped clinical datasets, for various downstream\napplications that suffer from the scarcity of labeled data. While SSL has\nrevolutionized fields like natural language processing and computer vision, its\nadoption in 3D medical image computing has been limited by three key pitfalls:\nSmall pre-training dataset sizes, architectures inadequate for 3D medical image\nanalysis, and insufficient evaluation practices. In this paper, we address\nthese issues by i) leveraging a large-scale dataset of 39k 3D brain MRI volumes\nand ii) using a Residual Encoder U-Net architecture within the state-of-the-art\nnnU-Net framework. iii) A robust development framework, incorporating 5\ndevelopment and 8 testing brain MRI segmentation datasets, allowed\nperformance-driven design decisions to optimize the simple concept of Masked\nAuto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses\nprevious SSL methods but also outperforms the strong nnU-Net baseline by an\naverage of approximately 3 Dice points setting a new state-of-the-art. Our code\nand models are made available here.\n","authors":["Tassilo Wald","Constantin Ulrich","Stanislav Lukyanenko","Andrei Goncharov","Alberto Paderno","Leander Maerkisch","Paul F. Jäger","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2410.23132v2.pdf","comment":"Arxiv Preprint. Revised and under review"},{"id":"http://arxiv.org/abs/2303.16668v3","updated":"2024-12-02T12:01:58Z","published":"2023-03-29T13:22:20Z","title":"Protecting Federated Learning from Extreme Model Poisoning Attacks via\n  Multidimensional Time Series Anomaly Detection","summary":"  Current defense mechanisms against model poisoning attacks in federated\nlearning (FL) systems have proven effective up to a certain threshold of\nmalicious clients. In this work, we introduce FLANDERS, a novel pre-aggregation\nfilter for FL resilient to large-scale model poisoning attacks, i.e., when\nmalicious clients far exceed legitimate participants. FLANDERS treats the\nsequence of local models sent by clients in each FL round as a matrix-valued\ntime series. Then, it identifies malicious client updates as outliers in this\ntime series by comparing actual observations with estimates generated by a\nmatrix autoregressive forecasting model maintained by the server. Experiments\nconducted in several non-iid FL setups show that FLANDERS significantly\nimproves robustness across a wide spectrum of attacks when paired with standard\nand robust existing aggregation methods.\n","authors":["Edoardo Gabrielli","Dimitri Belli","Zoe Matrullo","Vittorio Miori","Gabriele Tolomei"],"pdf_url":"https://arxiv.org/pdf/2303.16668v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15290v6","updated":"2024-12-02T11:46:33Z","published":"2023-10-23T18:56:01Z","title":"Reliable Generation of Privacy-preserving Synthetic Electronic Health\n  Record Time Series via Diffusion Models","summary":"  Electronic Health Records (EHRs) are rich sources of patient-level data,\noffering valuable resources for medical data analysis. However, privacy\nconcerns often restrict access to EHRs, hindering downstream analysis. Current\nEHR de-identification methods are flawed and can lead to potential privacy\nleakage. Additionally, existing publicly available EHR databases are limited,\npreventing the advancement of medical research using EHR. This study aims to\novercome these challenges by generating realistic and privacy-preserving\nsynthetic electronic health records (EHRs) time series efficiently. We\nintroduce a new method for generating diverse and realistic synthetic EHR time\nseries data using Denoising Diffusion Probabilistic Models (DDPM). We conducted\nexperiments on six databases: Medical Information Mart for Intensive Care III\nand IV (MIMIC-III/IV), the eICU Collaborative Research Database (eICU), and\nnon-EHR datasets on Stocks and Energy. We compared our proposed method with\neight existing methods. Our results demonstrate that our approach significantly\noutperforms all existing methods in terms of data fidelity while requiring less\ntraining effort. Additionally, data generated by our method yields a lower\ndiscriminative accuracy compared to other baseline methods, indicating the\nproposed method can generate data with less privacy risk. The proposed\ndiffusion-model-based method can reliably and efficiently generate synthetic\nEHR time series, which facilitates the downstream medical data analysis. Our\nnumerical results show the superiority of the proposed method over all other\nexisting methods.\n","authors":["Muhang Tian","Bernie Chen","Allan Guo","Shiyi Jiang","Anru R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.15290v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11932v3","updated":"2024-12-02T10:57:58Z","published":"2024-05-20T10:16:26Z","title":"Nonequilbrium physics of generative diffusion models","summary":"  Generative diffusion models apply the concept of Langevin dynamics in physics\nto machine leaning, attracting a lot of interests from engineering, statistics\nand physics, but a complete picture about inherent mechanisms is still lacking.\nIn this paper, we provide a transparent physics analysis of diffusion models,\nformulating the fluctuation theorem, entropy production, equilibrium measure,\nand Franz-Parisi potential to understand the dynamic process and intrinsic\nphase transitions. Our analysis is rooted in a path integral representation of\nboth forward and backward dynamics, and in treating the reverse diffusion\ngenerative process as a statistical inference, where the time-dependent state\nvariables serve as quenched disorder akin to that in spin glass theory. Our\nstudy thus links stochastic thermodynamics, statistical inference and geometry\nbased analysis together to yield a coherent picture about how the generative\ndiffusion models work.\n","authors":["Zhendong Yu","Haiping Huang"],"pdf_url":"https://arxiv.org/pdf/2405.11932v3.pdf","comment":"26 pages, 11 figures, 31 refs"},{"id":"http://arxiv.org/abs/2411.14708v2","updated":"2024-12-02T10:52:21Z","published":"2024-11-22T03:33:51Z","title":"Understanding LLM Embeddings for Regression","summary":"  With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.\n","authors":["Eric Tang","Bangding Yang","Xingyou Song"],"pdf_url":"https://arxiv.org/pdf/2411.14708v2.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.23178v2","updated":"2024-12-02T10:51:10Z","published":"2024-10-30T16:36:55Z","title":"Uncertainty quantification for fast reconstruction methods using\n  augmented equivariant bootstrap: Application to radio interferometry","summary":"  The advent of next-generation radio interferometers like the Square Kilometer\nArray promises to revolutionise our radio astronomy observational capabilities.\nThe unprecedented volume of data these devices generate requires fast and\naccurate image reconstruction algorithms to solve the ill-posed radio\ninterferometric imaging problem. Most state-of-the-art reconstruction methods\nlack trustworthy and scalable uncertainty quantification, which is critical for\nthe rigorous scientific interpretation of radio observations. We propose an\nunsupervised technique based on a conformalized version of a radio-augmented\nequivariant bootstrapping method, which allows us to quantify uncertainties for\nfast reconstruction methods. Noticeably, we rely on reconstructions from\nultra-fast unrolled algorithms. The proposed method brings more reliable\nuncertainty estimations to our problem than existing alternatives.\n","authors":["Mostafa Cherif","Tobías I. Liaudat","Jonathan Kern","Christophe Kervazo","Jérôme Bobin"],"pdf_url":"https://arxiv.org/pdf/2410.23178v2.pdf","comment":"14 pages, 7 figures. Accepted at the Machine Learning and the\n  Physical Sciences Workshop, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.04632v2","updated":"2024-12-02T10:48:28Z","published":"2024-11-07T11:35:31Z","title":"Improved Multi-Task Brain Tumour Segmentation with Synthetic Data\n  Augmentation","summary":"  This paper presents the winning solution of task 1 and the third-placed\nsolution of task 3 of the BraTS challenge. The use of automated tools in\nclinical practice has increased due to the development of more and more\nsophisticated and reliable algorithms. However, achieving clinical standards\nand developing tools for real-life scenarios is a major challenge. To this end,\nBraTS has organised tasks to find the most advanced solutions for specific\npurposes. In this paper, we propose the use of synthetic data to train\nstate-of-the-art frameworks in order to improve the segmentation of adult\ngliomas in a post-treatment scenario, and the segmentation of meningioma for\nradiotherapy planning. Our results suggest that the use of synthetic data leads\nto more robust algorithms, although the synthetic data generation pipeline is\nnot directly suited to the meningioma task. In task 1, we achieved a DSC of\n0.7900, 0.8076, 0.7760, 0.8926, 0.7874, 0.8938 and a HD95 of 35.63, 30.35,\n44.58, 16.87, 38.19, 17.95 for ET, NETC, RC, SNFH, TC and WT, respectively and,\nin task 3, we achieved a DSC of 0.801 and HD95 of 38.26, in the testing phase.\nThe code for these tasks is available at\nhttps://github.com/ShadowTwin41/BraTS_2023_2024_solutions.\n","authors":["André Ferreira","Tiago Jesus","Behrus Puladi","Jens Kleesiek","Victor Alves","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2411.04632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19211v2","updated":"2024-12-02T10:44:08Z","published":"2024-03-28T08:19:33Z","title":"Dual-Personalizing Adapter for Federated Foundation Models","summary":"  Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\ndiverse instruction data. Notably, federated foundation models (FedFM) emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to FedFM for better user preferences alignment.\nHowever, a critical gap in existing research is the neglect of test-time\ndistribution shifts in real-world applications, and conventional methods for\ntest-time distribution shifts in personalized FL are less effective for FedFM\ndue to their failure to adapt to complex distribution shift scenarios and the\nrequirement to train all parameters. To bridge this gap, we refine the setting\nin FedFM, termed test-time personalization, which aims to learn personalized\nfederated foundation models on clients while effectively handling test-time\ndistribution shifts simultaneously. To address challenges in this setting, we\nexplore a simple yet effective solution, a Federated Dual-Personalizing Adapter\n(FedDPA) architecture. By co-working with a foundation model, a global adapter\nand a local adapter jointly tackle the test-time distribution shifts and\nclient-specific personalization. Additionally, we introduce an instance-wise\ndynamic weighting mechanism that dynamically integrates the global and local\nadapters for each test instance during inference, facilitating effective\ntest-time personalization. The effectiveness of the proposed method has been\nevaluated on benchmark datasets across different NLP tasks.\n","authors":["Yiyuan Yang","Guodong Long","Tao Shen","Jing Jiang","Michael Blumenstein"],"pdf_url":"https://arxiv.org/pdf/2403.19211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02595v3","updated":"2024-12-02T10:36:05Z","published":"2024-04-03T09:19:46Z","title":"QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection","summary":"  This study introduces the Quantum Federated Neural Network for Financial\nFraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine\nLearning (QML) and quantum computing with Federated Learning (FL) for financial\nfraud detection. Using quantum technologies' computational power and the robust\ndata privacy protections offered by FL, QFNN-FFD emerges as a secure and\nefficient method for identifying fraudulent transactions within the financial\nsector. Implementing a dual-phase training model across distributed clients\nenhances data integrity and enables superior performance metrics, achieving\nprecision rates consistently above 95%. Additionally, QFNN-FFD demonstrates\nexceptional resilience by maintaining an impressive 80% accuracy, highlighting\nits robustness and readiness for real-world applications. This combination of\nhigh performance, security, and robustness against noise positions QFNN-FFD as\na transformative advancement in financial technology solutions and establishes\nit as a new benchmark for privacy-focused fraud detection systems. This\nframework facilitates the broader adoption of secure, quantum-enhanced\nfinancial services and inspires future innovations that could use QML to tackle\ncomplex challenges in other areas requiring high confidentiality and accuracy.\n","authors":["Nouhaila Innan","Alberto Marchisio","Mohamed Bennai","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2404.02595v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09874v4","updated":"2024-12-02T10:33:19Z","published":"2023-03-17T10:38:27Z","title":"Image Statistics Predict the Sensitivity of Perceptual Quality Metrics","summary":"  Previously, Barlow and Attneave hypothesised a link between biological vision\nand information maximisation. Following Shannon, information was defined using\nthe probability of natural images. Several physiological and psychophysical\nphenomena have been derived from principles like info-max, efficient coding, or\noptimal denoising. However, it remains unclear how this link is expressed in\nmathematical terms from image probability. Classical derivations were subjected\nto strong assumptions on the probability models and on the behaviour of the\nsensors. Moreover, the direct evaluation of the hypothesis was limited by the\ninability of classical image models to deliver accurate estimates of the\nprobability. Here, we directly evaluate image probabilities using a generative\nmodel for natural images, and analyse how probability-related factors can be\ncombined to predict the sensitivity of state-of-the-art subjective image\nquality metrics, a proxy for human perception. We use information theory and\nregression analysis to find a simple model that when combining just two\nprobability-related factors achieves 0.77 correlation with subjective metrics.\nThis probability-based model is validated in two ways: through direct\ncomparison with the opinion of real observers in a subjective quality\nexperiment, and by reproducing basic trends of classical psychophysical facts\nsuch as the Contrast Sensitivity Function, the Weber-law, and contrast masking.\n","authors":["Alexander Hepburn","Valero Laparra","Raúl Santos-Rodriguez","Jesús Malo"],"pdf_url":"https://arxiv.org/pdf/2303.09874v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00115v2","updated":"2024-12-02T10:25:47Z","published":"2024-08-28T04:07:40Z","title":"Self-Adaptive Quantum Kernel Principal Components Analysis for Compact\n  Readout of Chemiresistive Sensor Arrays","summary":"  The rapid growth of Internet of Things (IoT) devices necessitates efficient\ndata compression techniques to handle the vast amounts of data generated by\nthese devices. Chemiresistive sensor arrays (CSAs), a simple-to-fabricate but\ncrucial component in IoT systems, generate large volumes of data due to their\nsimultaneous multi-sensor operations. Classical principal component analysis\n(cPCA) methods, a common solution to the data compression challenge, face\nlimitations in preserving critical information during dimensionality reduction.\nIn this study, we present self-adaptive quantum kernel (SAQK) PCA as a superior\nalternative to enhance information retention. Our findings demonstrate that\nSAQK PCA outperforms cPCA in various back-end machine-learning tasks,\nespecially in low-dimensional scenarios where access to quantum bits is\nlimited. These results highlight the potential of noisy intermediate-scale\nquantum (NISQ) computers to revolutionize data processing in real-world IoT\napplications by improving the efficiency and reliability of CSA data\ncompression and readout, despite the current constraints on qubit availability.\n","authors":["Zeheng Wang","Timothy van der Laan","Muhammad Usman"],"pdf_url":"https://arxiv.org/pdf/2409.00115v2.pdf","comment":"Version 2"},{"id":"http://arxiv.org/abs/2402.08711v3","updated":"2024-12-02T10:21:10Z","published":"2024-02-13T18:31:55Z","title":"Correction to \"Wasserstein distance estimates for the distributions of\n  numerical approximations to ergodic stochastic differential equations\"","summary":"  A method for analyzing non-asymptotic guarantees of numerical discretizations\nof ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and\nZygalakis in ``Wasserstein distance estimates for the distributions of\nnumerical approximations to ergodic stochastic differential equations\". They\nanalyze the UBU integrator which is strong order two and only requires one\ngradient evaluation per step, resulting in desirable non-asymptotic guarantees,\nin particular $\\mathcal{O}(d^{1/4}\\epsilon^{-1/2})$ steps to reach a distance\nof $\\epsilon > 0$ in Wasserstein-2 distance away from the target distribution.\nHowever, there is a mistake in the local error estimates in Sanz-Serna and\nZygalakis (2021), in particular, a stronger assumption is needed to achieve\nthese complexity estimates. This note reconciles the theory with the dimension\ndependence observed in practice in many applications of interest.\n","authors":["Daniel Paulin","Peter A. Whalley"],"pdf_url":"https://arxiv.org/pdf/2402.08711v3.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2409.06067v2","updated":"2024-12-02T10:18:38Z","published":"2024-09-09T21:04:16Z","title":"MLLM-LLaVA-FL: Multimodal Large Language Model Assisted Federated\n  Learning","summary":"  Previous studies on federated learning (FL) often encounter performance\ndegradation due to data heterogeneity among different clients. In light of the\nrecent advances in multimodal large language models (MLLMs), such as GPT-4v and\nLLaVA, which demonstrate their exceptional proficiency in multimodal tasks,\nsuch as image captioning and multimodal question answering. We introduce a\nnovel federated learning framework, named Multimodal Large Language Model\nAssisted Federated Learning (MLLM-LLaVA-FL), which employs powerful MLLMs at\nthe server end to address the heterogeneous and long-tailed challenges. Owing\nto the advanced cross-modality representation capabilities and the extensive\nopen-vocabulary prior knowledge of MLLMs, our framework is adept at harnessing\nthe extensive, yet previously underexploited, open-source data accessible from\nwebsites and powerful server-side computational resources. Hence, the\nMLLM-LLaVA-FL not only enhances the performance but also avoids increasing the\nrisk of privacy leakage and the computational burden on local devices,\ndistinguishing it from prior methodologies. Our framework has three key stages.\nInitially, we conduct global visual-text pretraining of the model. This\npretraining is facilitated by utilizing the extensive open-source data\navailable online, with the assistance of MLLMs. Subsequently, the pretrained\nmodel is distributed among various clients for local training. Finally, once\nthe locally trained models are transmitted back to the server, a global\nalignment is carried out under the supervision of MLLMs to further enhance the\nperformance. Experimental evaluations on established benchmarks, show that our\nframework delivers promising performance in the typical scenarios with data\nheterogeneity and long-tail distribution across different clients in FL.\n","authors":["Jianyi Zhang","Hao Frank Yang","Ang Li","Xin Guo","Pu Wang","Haiming Wang","Yiran Chen","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2409.06067v2.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2409.19437v3","updated":"2024-12-02T10:15:47Z","published":"2024-09-28T18:56:48Z","title":"Strongly-polynomial time and validation analysis of policy gradient\n  methods","summary":"  This paper proposes a novel termination criterion, termed the advantage gap\nfunction, for finite state and action Markov decision processes (MDP) and\nreinforcement learning (RL). By incorporating this advantage gap function into\nthe design of step size rules and deriving a new linear rate of convergence\nthat is independent of the stationary state distribution of the optimal policy,\nwe demonstrate that policy gradient methods can solve MDPs in\nstrongly-polynomial time. To the best of our knowledge, this is the first time\nthat such strong convergence properties have been established for policy\ngradient methods. Moreover, in the stochastic setting, where only stochastic\nestimates of policy gradients are available, we show that the advantage gap\nfunction provides close approximations of the optimality gap for each\nindividual state and exhibits a sublinear rate of convergence at every state.\nThe advantage gap function can be easily estimated in the stochastic case, and\nwhen coupled with easily computable upper bounds on policy values, they provide\na convenient way to validate the solutions generated by policy gradient\nmethods. Therefore, our developments offer a principled and computable measure\nof optimality for RL, whereas current practice tends to rely on\nalgorithm-to-algorithm or baselines comparisons with no certificate of\noptimality.\n","authors":["Caleb Ju","Guanghui Lan"],"pdf_url":"https://arxiv.org/pdf/2409.19437v3.pdf","comment":"Add numerical experiments"},{"id":"http://arxiv.org/abs/2406.17490v2","updated":"2024-12-02T09:49:23Z","published":"2024-06-25T12:17:44Z","title":"BricksRL: A Platform for Democratizing Robotics and Reinforcement\n  Learning Research and Education with LEGO","summary":"  We present BricksRL, a platform designed to democratize access to robotics\nfor reinforcement learning research and education. BricksRL facilitates the\ncreation, design, and training of custom LEGO robots in the real world by\ninterfacing them with the TorchRL library for reinforcement learning agents.\nThe integration of TorchRL with the LEGO hubs, via Bluetooth bidirectional\ncommunication, enables state-of-the-art reinforcement learning training on GPUs\nfor a wide variety of LEGO builds. This offers a flexible and cost-efficient\napproach for scaling and also provides a robust infrastructure for\nrobot-environment-algorithm communication. We present various experiments\nacross tasks and robot configurations, providing built plans and training\nresults. Furthermore, we demonstrate that inexpensive LEGO robots can be\ntrained end-to-end in the real world to achieve simple tasks, with training\ntimes typically under 120 minutes on a normal laptop. Moreover, we show how\nusers can extend the capabilities, exemplified by the successful integration of\nnon-LEGO sensors. By enhancing accessibility to both robotics and reinforcement\nlearning, BricksRL establishes a strong foundation for democratized robotic\nlearning in research and educational settings.\n","authors":["Sebastian Dittert","Vincent Moens","Gianni De Fabritiis"],"pdf_url":"https://arxiv.org/pdf/2406.17490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14377v2","updated":"2024-12-02T09:48:21Z","published":"2024-05-23T09:52:15Z","title":"CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive\n  Tensor Optimization","summary":"  Training large AI models such as LLMs and DLRMs costs massive GPUs and\ncomputing time. The high training cost has become only affordable to big tech\ncompanies, meanwhile also causing increasing concerns about the environmental\nimpact. This paper presents CoMERA, a Computing- and Memory-Efficient training\nmethod via Rank-Adaptive tensor optimization. CoMERA achieves rank-adaptive\ntensor-compressed (pre)-training via a multi-objective optimization formulation\nand improves the training to provide both a high compression ratio and\nexcellent accuracy in the training process. Our optimized numerical computation\n(e.g., optimized tensorized embedding and tensor-network contractions) and GPU\nimplementation eliminate part of the run-time overhead in the tensorized\ntraining on GPU. This leads to, for the first time, $2-3\\times$ speedup per\ntraining epoch compared with standard training. CoMERA also outperforms the\nrecent GaLore in terms of both memory and computing efficiency. Specifically,\nCoMERA is $2\\times$ faster per training epoch and $9\\times$ more\nmemory-efficient than GaLore on a tested six-encoder transformer with\nsingle-batch training. Our method also shows $\\sim 2\\times$ speedup than\nstandard pre-training on a BERT-like code-generation LLM while achieving\n$4.23\\times$ compression ratio in pre-training. With further HPC optimization,\nCoMERA may reduce the pre-training cost of many other LLMs. An implementation\nof CoMERA is available at https://github.com/ziyangjoy/CoMERA.\n","authors":["Zi Yang","Ziyue Liu","Samridhi Choudhary","Xinfeng Xie","Cao Gao","Siegfried Kunzmann","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.14377v2.pdf","comment":"Accepted by Neurips 2024"},{"id":"http://arxiv.org/abs/2409.15344v2","updated":"2024-12-02T09:45:07Z","published":"2024-09-10T07:04:48Z","title":"Video-Driven Graph Network-Based Simulators","summary":"  Lifelike visualizations in design, cinematography, and gaming rely on precise\nphysics simulations, typically requiring extensive computational resources and\ndetailed physical input. This paper presents a method that can infer a system's\nphysical properties from a short video, eliminating the need for explicit\nparameter input, provided it is close to the training condition. The learned\nrepresentation is then used within a Graph Network-based Simulator to emulate\nthe trajectories of physical systems. We demonstrate that the video-derived\nencodings effectively capture the physical properties of the system and\nshowcase a linear dependence between some of the encodings and the system's\nmotion.\n","authors":["Franciszek Szewczyk","Gilles Louppe","Matthia Sabatelli"],"pdf_url":"https://arxiv.org/pdf/2409.15344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04125v2","updated":"2024-12-02T09:42:24Z","published":"2024-07-04T18:54:30Z","title":"Query-Guided Self-Supervised Summarization of Nursing Notes","summary":"  Nursing notes, an important part of Electronic Health Records (EHRs), track a\npatient's health during a care episode. Summarizing key information in nursing\nnotes can help clinicians quickly understand patients' conditions. However,\nexisting summarization methods in the clinical setting, especially abstractive\nmethods, have overlooked nursing notes and require reference summaries for\ntraining. We introduce QGSumm, a novel query-guided self-supervised domain\nadaptation approach for abstractive nursing note summarization. The method uses\npatient-related clinical queries for guidance, and hence does not need\nreference summaries for training. Through automatic experiments and manual\nevaluation by an expert clinician, we study our approach and other\nstate-of-the-art Large Language Models (LLMs) for nursing note summarization.\nOur experiments show: 1) GPT-4 is competitive in maintaining information in the\noriginal nursing notes, 2) QGSumm can generate high-quality summaries with a\ngood balance between recall of the original content and hallucination rate\nlower than other top methods. Ultimately, our work offers a new perspective on\nconditional text summarization, tailored to clinical applications.\n","authors":["Ya Gao","Hans Moen","Saila Koivusalo","Miika Koskinen","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2407.04125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11918v2","updated":"2024-12-02T09:21:40Z","published":"2024-11-18T03:37:33Z","title":"Artificial Intelligence Mangrove Monitoring System Based on Deep\n  Learning and Sentinel-2 Satellite Data in the UAE (2017-2024)","summary":"  Mangroves play a crucial role in maintaining coastal ecosystem health and\nprotecting biodiversity. Therefore, continuous mapping of mangroves is\nessential for understanding their dynamics. Earth observation imagery typically\nprovides a cost-effective way to monitor mangrove dynamics. However, there is a\nlack of regional studies on mangrove areas in the UAE. This study utilizes the\nUNet++ deep learning model combined with Sentinel-2 multispectral data and\nmanually annotated labels to monitor the spatiotemporal dynamics of densely\ndistributed mangroves (coverage greater than 70%) in the UAE from 2017 to 2024,\nachieving an mIoU of 87.8% on the validation set. Results show that the total\nmangrove area in the UAE in 2024 was approximately 9,142.21 hectares, an\nincrease of 2,061.33 hectares compared to 2017, with carbon sequestration\nincreasing by approximately 194,383.42 tons, equivalent to fixing about\n713,367.36 tons of carbon dioxide. Abu Dhabi has the largest mangrove area and\nplays a dominant role in the UAE's mangrove growth, increasing by 1,855.6\nhectares between 2017-2024, while other emirates have also contributed to\nmangrove expansion through stable and sustainable growth in mangrove areas.\nThis comprehensive growth pattern reflects the collective efforts of all\nemirates in mangrove restoration.\n","authors":["Linlin Tan","Haishan Wu"],"pdf_url":"https://arxiv.org/pdf/2411.11918v2.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.06400v2","updated":"2024-12-02T09:17:21Z","published":"2024-04-09T15:46:00Z","title":"Dynamic Deep Learning Based Super-Resolution For The Shallow Water\n  Equations","summary":"  Using the nonlinear shallow water equations as benchmark, we demonstrate that\na simulation with the ICON-O ocean model with a 20km resolution that is\nfrequently corrected by a U-net-type neural network can achieve discretization\nerrors of a simulation with 10km resolution. The network, originally developed\nfor image-based super-resolution in post-processing, is trained to compute the\ndifference between solutions on both meshes and is used to correct the coarse\nmesh every 12h. Our setup is the Galewsky test case, modeling transition of a\nbarotropic instability into turbulent flow. We show that the ML-corrected\ncoarse resolution run correctly maintains a balance flow and captures the\ntransition to turbulence in line with the higher resolution simulation. After 8\nday of simulation, the $L_2$-error of the corrected run is similar to a\nsimulation run on the finer mesh. While mass is conserved in the corrected\nruns, we observe some spurious generation of kinetic energy.\n","authors":["Maximilian Witte","Fabricio Rodrigues Lapolli","Philip Freese","Sebastian Götschel","Daniel Ruprecht","Peter Korn","Christopher Kadow"],"pdf_url":"https://arxiv.org/pdf/2404.06400v2.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2411.18810v2","updated":"2024-12-02T09:10:34Z","published":"2024-11-27T23:32:54Z","title":"Enhancing Compositional Text-to-Image Generation with Reliable Random\n  Seeds","summary":"  Text-to-image diffusion models have demonstrated remarkable capability in\ngenerating realistic images from arbitrary text prompts. However, they often\nproduce inconsistent results for compositional prompts such as \"two dogs\" or \"a\npenguin on the right of a bowl\". Understanding these inconsistencies is crucial\nfor reliable image generation. In this paper, we highlight the significant role\nof initial noise in these inconsistencies, where certain noise patterns are\nmore reliable for compositional prompts than others. Our analyses reveal that\ndifferent initial random seeds tend to guide the model to place objects in\ndistinct image areas, potentially adhering to specific patterns of camera\nangles and image composition associated with the seed. To improve the model's\ncompositional ability, we propose a method for mining these reliable cases,\nresulting in a curated training set of generated images without requiring any\nmanual annotation. By fine-tuning text-to-image models on these generated\nimages, we significantly enhance their compositional capabilities. For\nnumerical composition, we observe relative increases of 29.3% and 19.5% for\nStable Diffusion and PixArt-{\\alpha}, respectively. Spatial composition sees\neven larger gains, with 60.7% for Stable Diffusion and 21.1% for\nPixArt-{\\alpha}.\n","authors":["Shuangqi Li","Hieu Le","Jingyi Xu","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2411.18810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17781v4","updated":"2024-12-02T08:27:06Z","published":"2024-07-25T05:22:08Z","title":"Ensemble data assimilation to diagnose AI-based weather prediction\n  model: A case with ClimaX version 0.3.1","summary":"  Artificial intelligence (AI)-based weather prediction research is growing\nrapidly and has shown to be competitive with the advanced dynamic numerical\nweather prediction models. However, research combining AI-based weather\nprediction models with data assimilation remains limited partially because\nlong-term sequential data assimilation cycles are required to evaluate data\nassimilation systems. This study proposes using ensemble data assimilation for\ndiagnosing AI-based weather prediction models, and marked the first successful\nimplementation of ensemble Kalman filter with AI-based weather prediction\nmodels. Our experiments with an AI-based model ClimaX demonstrated that the\nensemble data assimilation cycled stably for the AI-based weather prediction\nmodel using covariance inflation and localization techniques within the\nensemble Kalman filter. While ClimaX showed some limitations in capturing\nflow-dependent error covariance compared to dynamical models, the AI-based\nensemble forecasts provided reasonable and beneficial error covariance in\nsparsely observed regions. In addition, ensemble data assimilation revealed\nthat error growth based on ensemble ClimaX predictions was weaker than that of\ndynamical NWP models, leading to higher inflation factors. A series of\nexperiments demonstrated that ensemble data assimilation can be used to\ndiagnose properties of AI weather prediction models such as physical\nconsistency and accurate error growth representation.\n","authors":["Shunji Kotsuki","Kenta Shiraishi","Atsushi Okazaki"],"pdf_url":"https://arxiv.org/pdf/2407.17781v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09710v2","updated":"2024-12-02T08:05:45Z","published":"2024-02-15T05:06:53Z","title":"Preserving Data Privacy for ML-driven Applications in Open Radio Access\n  Networks","summary":"  Deep learning offers a promising solution to improve spectrum access\ntechniques by utilizing data-driven approaches to manage and share limited\nspectrum resources for emerging applications. For several of these\napplications, the sensitive wireless data (such as spectrograms) are stored in\na shared database or multistakeholder cloud environment and are therefore prone\nto privacy leaks. This paper aims to address such privacy concerns by examining\nthe representative case study of shared database scenarios in 5G Open Radio\nAccess Network (O-RAN) networks where we have a shared database within the\nnear-real-time (near-RT) RAN intelligent controller. We focus on securing the\ndata that can be used by machine learning (ML) models for spectrum sharing and\ninterference mitigation applications without compromising the model and network\nperformances. The underlying idea is to leverage a (i) Shuffling-based\nlearnable encryption technique to encrypt the data, following which, (ii)\nemploy a custom Vision transformer (ViT) as the trained ML model that is\ncapable of performing accurate inferences on such encrypted data. The paper\noffers a thorough analysis and comparisons with analogous convolutional neural\nnetworks (CNN) as well as deeper architectures (such as ResNet-50) as\nbaselines. Our experiments showcase that the proposed approach significantly\noutperforms the baseline CNN with an improvement of 24.5% and 23.9% for the\npercent accuracy and F1-Score respectively when operated on encrypted data.\nThough deeper ResNet-50 architecture is obtained as a slightly more accurate\nmodel, with an increase of 4.4%, the proposed approach boasts a reduction of\nparameters by 99.32%, and thus, offers a much-improved prediction time by\nnearly 60%.\n","authors":["Pranshav Gajjar","Azuka Chiejina","Vijay K. Shah"],"pdf_url":"https://arxiv.org/pdf/2402.09710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02408v2","updated":"2024-12-02T07:47:00Z","published":"2024-02-04T08:57:54Z","title":"GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large\n  Language Model","summary":"  Despite the rapid progress of large language models (LLMs), their task\nperformance remains sensitive to prompt design. Recent studies have explored\nleveraging the LLM itself as an optimizer to identify optimal prompts that\nmaximize task accuracy. However, when evaluating prompts, such approaches\nheavily rely on elusive manually annotated gold labels to calculate task\naccuracy for each candidate prompt, which hinders the widespread implementation\nand generality. To overcome the limitation, this work proposes a gold\nlabel-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold\nlabels. Motivated by the observed correlation between self-consistency and the\naccuracy of the answer, we adopt self-consistency as the initial evaluation\nscore. Subsequently, we refine the scores of prompts producing identical\nanswers to be mutually consistent. Experimental results show that GLaPE\nprovides reliable evaluations uniform with accuracy, even in the absence of\ngold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt\noptimization yields effective prompts comparable to accuracy-based ones. The\ncode is publicly available at https://github.com/thunderous77/GLaPE.\n","authors":["Xuanchang Zhang","Zhuosheng Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.02408v2.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.13381v2","updated":"2024-12-02T07:18:46Z","published":"2024-10-17T09:36:01Z","title":"Learning Counterfactual Distributions via Kernel Nearest Neighbors","summary":"  Consider a setting with multiple units (e.g., individuals, cohorts,\ngeographic locations) and outcomes (e.g., treatments, times, items), where the\ngoal is to learn a multivariate distribution for each unit-outcome entry, such\nas the distribution of a user's weekly spend and engagement under a specific\nmobile app version. A common challenge is the prevalence of missing not at\nrandom data, where observations are available only for certain unit-outcome\ncombinations and the observation availability can be correlated with the\nproperties of distributions themselves, i.e., there is unobserved confounding.\nAn additional challenge is that for any observed unit-outcome entry, we only\nhave a finite number of samples from the underlying distribution. We tackle\nthese two challenges by casting the problem into a novel distributional matrix\ncompletion framework and introduce a kernel based distributional generalization\nof nearest neighbors to estimate the underlying distributions. By leveraging\nmaximum mean discrepancies and a suitable factor model on the kernel mean\nembeddings of the underlying distributions, we establish consistent recovery of\nthe underlying distributions even when data is missing not at random and\npositivity constraints are violated. Furthermore, we demonstrate that our\nnearest neighbors approach is robust to heteroscedastic noise, provided we have\naccess to two or more measurements for the observed unit-outcome entries, a\nrobustness not present in prior works on nearest neighbors with single\nmeasurements.\n","authors":["Kyuseong Choi","Jacob Feitelberg","Caleb Chin","Anish Agarwal","Raaz Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2410.13381v2.pdf","comment":"39 pages, 8 figures"},{"id":"http://arxiv.org/abs/2301.13516v3","updated":"2024-12-02T07:07:42Z","published":"2023-01-31T10:07:23Z","title":"Recurrences reveal shared causal drivers of complex time series","summary":"  Unmeasured causal forces influence diverse experimental time series, such as\nthe transcription factors that regulate genes, or the descending neurons that\nsteer motor circuits. Combining the theory of skew-product dynamical systems\nwith topological data analysis, we show that simultaneous recurrence events\nacross multiple time series reveal the structure of their shared unobserved\ndriving signal. We introduce a physics-based unsupervised learning algorithm\nthat reconstructs causal drivers by iteratively building a recurrence graph\nwith glass-like structure. As the amount of data increases, a percolation\ntransition on this graph leads to weak ergodicity breaking for random walks --\nrevealing the shared driver's dynamics, even from strongly-corrupted\nmeasurements. We relate reconstruction accuracy to the rate of information\ntransfer from a chaotic driver to the response systems, and we find that\neffective reconstruction proceeds through gradual approximation of the driver's\ndynamical attractor. Through extensive benchmarks against classical signal\nprocessing and machine learning techniques, we demonstrate our method's ability\nto extract causal drivers from diverse experimental datasets spanning ecology,\ngenomics, fluid dynamics, and physiology.\n","authors":["William Gilpin"],"pdf_url":"https://arxiv.org/pdf/2301.13516v3.pdf","comment":"Physical Review X (to appear). Code available online at\n  https://github.com/williamgilpin/shrec"},{"id":"http://arxiv.org/abs/2406.06594v2","updated":"2024-12-02T07:04:17Z","published":"2024-06-06T03:13:34Z","title":"Stock Movement Prediction with Multimodal Stable Fusion via Gated\n  Cross-Attention Mechanism","summary":"  The accurate prediction of stock movements is crucial for investment\nstrategies. Stock prices are subject to the influence of various forms of\ninformation, including financial indicators, sentiment analysis, news\ndocuments, and relational structures. Predominant analytical approaches,\nhowever, tend to address only unimodal or bimodal sources, neglecting the\ncomplexity of multimodal data. Further complicating the landscape are the\nissues of data sparsity and semantic conflicts between these modalities, which\nare frequently overlooked by current models, leading to unstable performance\nand limiting practical applicability. To address these shortcomings, this study\nintroduces a novel architecture, named Multimodal Stable Fusion with Gated\nCross-Attention (MSGCA), designed to robustly integrate multimodal input for\nstock movement prediction. The MSGCA framework consists of three integral\ncomponents: (1) a trimodal encoding module, responsible for processing\nindicator sequences, dynamic documents, and a relational graph, and\nstandardizing their feature representations; (2) a cross-feature fusion module,\nwhere primary and consistent features guide the multimodal fusion of the three\nmodalities via a pair of gated cross-attention networks; and (3) a prediction\nmodule, which refines the fused features through temporal and dimensional\nreduction to execute precise movement forecasting. Empirical evaluations\ndemonstrate that the MSGCA framework exceeds current leading methods, achieving\nperformance gains of 8.1%, 6.1%, 21.7% and 31.6% on four multimodal datasets,\nrespectively, attributed to its enhanced multimodal fusion stability.\n","authors":["Chang Zong","Hang Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.06594v2.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.10825v3","updated":"2024-12-02T07:00:57Z","published":"2024-09-17T01:37:57Z","title":"Unveiling and Mitigating Bias in Large Language Model Recommendations: A\n  Path to Fairness","summary":"  excel in delivering comprehensive suggestions by deeply analyzing content and\nuser behavior. However, they often inherit biases from skewed training data,\nfavoring mainstream content while underrepresenting diverse or non-traditional\noptions. This study explores the interplay between bias and LLM-based\nrecommendation systems, focusing on music, song, and book recommendations\nacross diverse demographic and cultural groups. This paper analyzes bias in\nLLM-based recommendation systems across multiple models (GPT, LLaMA, and\nGemini), revealing its deep and pervasive impact on outcomes. Intersecting\nidentities and contextual factors, like socioeconomic status, further amplify\nbiases, complicating fair recommendations across diverse groups. Our findings\nreveal that bias in these systems is deeply ingrained, yet even simple\ninterventions like prompt engineering can significantly reduce it. We further\npropose a retrieval-augmented generation strategy to mitigate bias more\neffectively. Numerical experiments validate these strategies, demonstrating\nboth the pervasive nature of bias and the impact of the proposed solutions.\n","authors":["Anindya Bijoy Das","Shahnewaz Karim Sakib"],"pdf_url":"https://arxiv.org/pdf/2409.10825v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10137v3","updated":"2024-12-02T06:55:10Z","published":"2024-10-14T04:07:45Z","title":"Variational autoencoders with latent high-dimensional steady geometric\n  flows for dynamics","summary":"  We develop Riemannian approaches to variational autoencoders (VAEs) for\nPDE-type ambient data with regularizing geometric latent dynamics, which we\nrefer to as VAE-DLM, or VAEs with dynamical latent manifolds. We redevelop the\nVAE framework such that manifold geometries, subject to our geometric flow,\nembedded in Euclidean space are learned in the intermediary latent space\ndeveloped by encoders and decoders. By tailoring the geometric flow in which\nthe latent space evolves, we induce latent geometric properties of our\nchoosing, which are reflected in empirical performance. We reformulate the\ntraditional evidence lower bound (ELBO) loss with a considerate choice of\nprior. We develop a linear geometric flow with a steady-state regularizing\nterm. This flow requires only automatic differentiation of one time derivative,\nand can be solved in moderately high dimensions in a physics-informed approach,\nallowing more expressive latent representations. We discuss how this flow can\nbe formulated as a gradient flow, and maintains entropy away from metric\nsingularity. This, along with an eigenvalue penalization condition, helps\nensure the manifold is sufficiently large in measure, nondegenerate, and a\ncanonical geometry, which contribute to a robust representation. Our methods\nfocus on the modified multi-layer perceptron architecture with tanh activations\nfor the manifold encoder-decoder. We demonstrate, on our datasets of interest,\nour methods perform at least as well as the traditional VAE, and oftentimes\nbetter. Our methods can outperform this and a VAE endowed with our proposed\narchitecture by up to 25% reduction in out-of-distribution (OOD) error and\npotentially greater. We highlight our method on ambient PDEs whose solutions\nmaintain minimal variation in late times. We provide empirical justification\ntowards how we can improve robust learning for external dynamics with VAEs.\n","authors":["Andrew Gracyk"],"pdf_url":"https://arxiv.org/pdf/2410.10137v3.pdf","comment":"Minor fixes; added details to proofs in the appendix"},{"id":"http://arxiv.org/abs/2411.19951v2","updated":"2024-12-02T06:54:47Z","published":"2024-11-29T18:59:54Z","title":"T2Vid: Translating Long Text into Multi-Image is the Catalyst for\n  Video-LLMs","summary":"  The success of Multimodal Large Language Models (MLLMs) in the image domain\nhas garnered wide attention from the research community. Drawing on previous\nsuccessful experiences, researchers have recently explored extending the\nsuccess to the video understanding realms. Apart from training from scratch, an\nefficient way is to utilize the pre-trained image-LLMs, leading to two\nmainstream approaches, i.e. zero-shot inference and further fine-tuning with\nvideo data. In this work, our study of these approaches harvests an effective\ndata augmentation method. We first make a deeper inspection of the zero-shot\ninference way and identify two limitations, i.e. limited generalization and\nlack of temporal understanding capabilities. Thus, we further investigate the\nfine-tuning approach and find a low learning efficiency when simply using all\nthe video data samples, which can be attributed to a lack of instruction\ndiversity. Aiming at this issue, we develop a method called T2Vid to synthesize\nvideo-like samples to enrich the instruction diversity in the training corpus.\nIntegrating these data enables a simple and efficient training scheme, which\nachieves performance comparable to or even superior to using full video\ndatasets by training with just 15% the sample size. Meanwhile, we find that the\nproposed scheme can boost the performance of long video understanding without\ntraining with long video samples. We hope our study will spark more thinking\nabout using MLLMs for video understanding and curation of high-quality data.\nThe code is released at https://github.com/xjtupanda/T2Vid.\n","authors":["Shukang Yin","Chaoyou Fu","Sirui Zhao","Yunhang Shen","Chunjiang Ge","Yan Yang","Zuwei Long","Yuhan Dai","Tong Xu","Xing Sun","Ran He","Caifeng Shan","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.19951v2.pdf","comment":"Project page: https://github.com/xjtupanda/T2Vid"},{"id":"http://arxiv.org/abs/2410.13025v2","updated":"2024-12-02T06:40:50Z","published":"2024-10-16T20:33:06Z","title":"LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks","summary":"  Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient\nfine-tuning of Large Language Models (LLMs). We study how different LoRA\nmodules can be merged to achieve skill composition -- testing the performance\nof the merged model on a target task that involves combining multiple skills,\neach skill coming from a single LoRA. This setup is favorable when it is\ndifficult to obtain training data for the target task and when it can be\ndecomposed into multiple skills. First, we identify practically occurring\nuse-cases that can be studied under the realm of skill composition, e.g.\nsolving hard math-word problems with code, creating a bot to answer questions\non proprietary manuals or about domain-specialized corpora. Our main\ncontribution is to show that concatenation of LoRAs (CAT), which optimally\nweights LoRAs that were individually trained on different skills, outperforms\nexisting model- and data- merging techniques; for instance on math-word\nproblems, CAT beats these methods by an average of 43% and 12% respectively.\nThus, this paper advocates model merging as an efficient way to solve\ncompositional tasks and underscores CAT as a simple, compute-friendly and\neffective procedure. To our knowledge, this is the first work demonstrating the\nsuperiority of model merging over data mixing for binary skill composition\ntasks. Code and data are available at https://github.com/aksh555/LoRA-Soups\n","authors":["Akshara Prabhakar","Yuanzhi Li","Karthik Narasimhan","Sham Kakade","Eran Malach","Samy Jelassi"],"pdf_url":"https://arxiv.org/pdf/2410.13025v2.pdf","comment":"COLING 2025 Industry track; 9 pages plus references and appendices"},{"id":"http://arxiv.org/abs/2411.16698v2","updated":"2024-12-02T06:31:31Z","published":"2024-11-10T18:28:30Z","title":"Universal on-chip polarization handling with deep photonic networks","summary":"  We propose a novel design paradigm for arbitrarily capable deep photonic\nnetworks of cascaded Mach-Zehnder Interferometers (MZIs) for on-chip universal\npolarization handling. Using a device architecture made of cascaded\nMach-Zehnder interferometers, we modify and train the phase difference between\ninterferometer arms for both polarizations through wide operation bandwidths.\nThree proof-of-concept polarization handling devices are illustrated using a\nsoftware-defined, physics-informed neural framework, to achieve user-specified\ntarget device responses as functions of polarization and wavelength. These\ndevices include a polarization splitter, a polarization-independent power\nsplitter, and an arbitrary polarization-dependent splitter to illustrate the\ncapabilities of the design framework. The performance for all three devices is\noptimized using transfer matrix calculations; and their final responses are\nverified through 3D-FDTD simulations. All devices demonstrate state-of-the-art\nperformance metrics with over 20 dB extinction, and flat-top transmission bands\nthrough bandwidths of 120 nm. In addition to the functional diversity enabled,\nthe optimization for each device is completed in under a minute, highlighting\nthe computational efficiency of the design paradigm presented. These results\ndemonstrate the versatility of the deep photonic network design ecosystem in\npolarization management, unveiling promising prospects for advanced on-chip\napplications in optical communications, sensing, and computing.\n","authors":["Aycan Deniz Vit","Ujal Rzayev","Bahrem Serhat Danis","Ali Najjar Amiri","Kazim Gorgulu","Emir Salih Magden"],"pdf_url":"https://arxiv.org/pdf/2411.16698v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19943v2","updated":"2024-12-02T06:26:38Z","published":"2024-11-29T18:58:22Z","title":"Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability","summary":"  Large Language Models (LLMs) have exhibited remarkable performance on\nreasoning tasks. They utilize autoregressive token generation to construct\nreasoning trajectories, enabling the development of a coherent chain of\nthought. In this work, we explore the impact of individual tokens on the final\noutcomes of reasoning tasks. We identify the existence of ``critical tokens''\nthat lead to incorrect reasoning trajectories in LLMs. Specifically, we find\nthat LLMs tend to produce positive outcomes when forced to decode other tokens\ninstead of critical tokens. Motivated by this observation, we propose a novel\napproach - cDPO - designed to automatically recognize and conduct token-level\nrewards for the critical tokens during the alignment process. Specifically, we\ndevelop a contrastive estimation approach to automatically identify critical\ntokens. It is achieved by comparing the generation likelihood of positive and\nnegative models. To achieve this, we separately fine-tune the positive and\nnegative models on various reasoning trajectories, consequently, they are\ncapable of identifying identify critical tokens within incorrect trajectories\nthat contribute to erroneous outcomes. Moreover, to further align the model\nwith the critical token information during the alignment process, we extend the\nconventional DPO algorithms to token-level DPO and utilize the differential\nlikelihood from the aforementioned positive and negative model as important\nweight for token-level DPO learning.Experimental results on GSM8K and MATH500\nbenchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math\n(7B) demonstrate the effectiveness of the propsoed approach cDPO.\n","authors":["Zicheng Lin","Tian Liang","Jiahao Xu","Xing Wang","Ruilin Luo","Chufan Shi","Siheng Li","Yujiu Yang","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2411.19943v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.18122v3","updated":"2024-12-02T06:15:54Z","published":"2024-11-27T08:02:31Z","title":"Fighting Bias with Bias: A Machine Learning Approach to Assess Human\n  Bias","summary":"  Biased human decisions have consequential impacts across various domains,\nyielding unfair treatment of individuals and resulting in suboptimal outcomes\nfor organizations and society. In recognition of this fact, organizations\nregularly design and deploy interventions aimed at mitigating these biases.\nHowever, measuring human decision biases remains an important but elusive task.\nOrganizations are frequently concerned with mistaken decisions\ndisproportionately affecting one group. In practice, however, this is typically\nnot possible to assess due to the scarcity of a gold standard: a label that\nindicates what the correct decision would have been. In this work, we propose a\nmachine learning-based framework to assess bias in human-generated decisions\nwhen gold standard labels are scarce. We provide theoretical guarantees and\nempirical evidence demonstrating the superiority of our method over existing\nalternatives. This proposed methodology establishes a foundation for\ntransparency in human decision-making, carrying substantial implications for\nmanagerial duties, and offering potential for alleviating algorithmic biases\nwhen human decisions are used as labels to train algorithms.\n","authors":["Wanxue Dong","Maria De-arteaga","Maytal Saar-Tsechansky"],"pdf_url":"https://arxiv.org/pdf/2411.18122v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14778v5","updated":"2024-12-02T05:02:11Z","published":"2024-08-27T04:56:45Z","title":"GPU-Accelerated Counterfactual Regret Minimization","summary":"  Counterfactual regret minimization is a family of algorithms of no-regret\nlearning dynamics capable of solving large-scale imperfect information games.\nWe propose implementing this algorithm as a series of dense and sparse matrix\nand vector operations, thereby making it highly parallelizable for a graphical\nprocessing unit, at a cost of higher memory usage. Our experiments show that\nour implementation performs up to about 401.2 times faster than OpenSpiel's\nPython implementation and, on an expanded set of games, up to about 203.6 times\nfaster than OpenSpiel's C++ implementation and the speedup becomes more\npronounced as the size of the game being solved grows.\n","authors":["Juho Kim"],"pdf_url":"https://arxiv.org/pdf/2408.14778v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02038v2","updated":"2024-12-02T04:20:10Z","published":"2024-10-02T21:08:11Z","title":"Realizable Continuous-Space Shields for Safe Reinforcement Learning","summary":"  While Deep Reinforcement Learning (DRL) has achieved remarkable success\nacross various domains, it remains vulnerable to occasional catastrophic\nfailures without additional safeguards. An effective solution to prevent these\nfailures is to use a shield that validates and adjusts the agent's actions to\nensure compliance with a provided set of safety specifications. For real-world\nrobotic domains, it is essential to define safety specifications over\ncontinuous state and action spaces to accurately account for system dynamics\nand compute new actions that minimally deviate from the agent's original\ndecision. In this paper, we present the first shielding approach specifically\ndesigned to ensure the satisfaction of safety requirements in continuous state\nand action spaces, making it suitable for practical robotic applications. Our\nmethod builds upon realizability, an essential property that confirms the\nshield will always be able to generate a safe action for any state in the\nenvironment. We formally prove that realizability can be verified for stateful\nshields, enabling the incorporation of non-Markovian safety requirements, such\nas loop avoidance. Finally, we demonstrate the effectiveness of our approach in\nensuring safety without compromising the policy's success rate by applying it\nto a navigation problem and a multi-agent particle environment.\n","authors":["Kyungmin Kim","Davide Corsi","Andoni Rodriguez","JB Lanier","Benjami Parellada","Pierre Baldi","Cesar Sanchez","Roy Fox"],"pdf_url":"https://arxiv.org/pdf/2410.02038v2.pdf","comment":"Kim, Corsi, and Rodriguez contributed equally"},{"id":"http://arxiv.org/abs/2406.12336v2","updated":"2024-12-02T04:08:49Z","published":"2024-06-18T07:03:34Z","title":"Towards Understanding Domain Adapted Sentence Embeddings for Document\n  Retrieval","summary":"  A plethora of sentence embedding models makes it challenging to choose one,\nespecially for technical domains rich with specialized vocabulary. In this\nwork, we domain adapt embeddings using telecom, health and science datasets for\nquestion answering. We evaluate embeddings obtained from publicly available\nmodels and their domain-adapted variants, on both point retrieval accuracies,\nas well as their (95\\%) confidence intervals. We establish a systematic method\nto obtain thresholds for similarity scores for different embeddings. As\nexpected, we observe that fine-tuning improves mean bootstrapped accuracies. We\nalso observe that it results in tighter confidence intervals, which further\nimprove when pre-training is preceded by fine-tuning. We introduce metrics\nwhich measure the distributional overlaps of top-$K$, correct and random\ndocument similarities with the question. Further, we show that these metrics\nare correlated with retrieval accuracy and similarity thresholds. Recent\nliterature shows conflicting effects of isotropy on retrieval accuracies. Our\nexperiments establish that the isotropy of embeddings (as measured by two\nindependent state-of-the-art isotropy metric definitions) is poorly correlated\nwith retrieval performance. We show that embeddings for domain-specific\nsentences have little overlap with those for domain-agnostic ones, and\nfine-tuning moves them further apart. Based on our results, we provide\nrecommendations for use of our methodology and metrics by researchers and\npractitioners.\n","authors":["Sujoy Roychowdhury","Sumit Soman","H. G. Ranjani","Vansh Chhabra","Neeraj Gunda","Shashank Gautam","Subhadip Bandyopadhyay","Sai Krishna Bala"],"pdf_url":"https://arxiv.org/pdf/2406.12336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12707v3","updated":"2024-12-02T03:45:42Z","published":"2024-07-17T16:30:27Z","title":"TTSDS -- Text-to-Speech Distribution Score","summary":"  Many recently published Text-to-Speech (TTS) systems produce audio close to\nreal speech. However, TTS evaluation needs to be revisited to make sense of the\nresults obtained with the new architectures, approaches and datasets. We\npropose evaluating the quality of synthetic speech as a combination of multiple\nfactors such as prosody, speaker identity, and intelligibility. Our approach\nassesses how well synthetic speech mirrors real speech by obtaining correlates\nof each factor and measuring their distance from both real speech datasets and\nnoise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and\nshow that our score computed as an unweighted average of factors strongly\ncorrelates with the human evaluations from each time period.\n","authors":["Christoph Minixhofer","Ondřej Klejch","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2407.12707v3.pdf","comment":"SLT 2024"},{"id":"http://arxiv.org/abs/2410.09640v3","updated":"2024-12-02T03:41:51Z","published":"2024-10-12T20:33:37Z","title":"Provable Acceleration of Nesterov's Accelerated Gradient for Rectangular\n  Matrix Factorization and Linear Neural Networks","summary":"  We study the convergence rate of first-order methods for rectangular matrix\nfactorization, which is a canonical nonconvex optimization problem.\nSpecifically, given a rank-$r$ matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$, we\nprove that gradient descent (GD) can find a pair of $\\epsilon$-optimal\nsolutions $\\mathbf{X}_T\\in\\mathbb{R}^{m\\times d}$ and\n$\\mathbf{Y}_T\\in\\mathbb{R}^{n\\times d}$, where $d\\geq r$, satisfying\n$\\lVert\\mathbf{X}_T\\mathbf{Y}_T^\\top-\\mathbf{A}\\rVert_\\mathrm{F}\\leq\\epsilon\\lVert\\mathbf{A}\\rVert_\\mathrm{F}$\nin $T=O(\\kappa^2\\log\\frac{1}{\\epsilon})$ iterations with high probability,\nwhere $\\kappa$ denotes the condition number of $\\mathbf{A}$. Furthermore, we\nprove that Nesterov's accelerated gradient (NAG) attains an iteration\ncomplexity of $O(\\kappa\\log\\frac{1}{\\epsilon})$, which is the best-known bound\nof first-order methods for rectangular matrix factorization. Different from\nsmall balanced random initialization in the existing literature, we adopt an\nunbalanced initialization, where $\\mathbf{X}_0$ is large and $\\mathbf{Y}_0$ is\n$0$. Moreover, our initialization and analysis can be further extended to\nlinear neural networks, where we prove that NAG can also attain an accelerated\nlinear convergence rate. In particular, we only require the width of the\nnetwork to be greater than or equal to the rank of the output label matrix. In\ncontrast, previous results achieving the same rate require excessive widths\nthat additionally depend on the condition number and the rank of the input data\nmatrix.\n","authors":["Zhenghao Xu","Yuqing Wang","Tuo Zhao","Rachel Ward","Molei Tao"],"pdf_url":"https://arxiv.org/pdf/2410.09640v3.pdf","comment":"30 pages (checklist included)"},{"id":"http://arxiv.org/abs/2411.19527v2","updated":"2024-12-02T03:34:45Z","published":"2024-11-29T07:54:56Z","title":"DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow\n  Decoding","summary":"  Human motion, inherently continuous and dynamic, presents significant\nchallenges for generative models. Despite their dominance, discrete\nquantization methods, such as VQ-VAEs, suffer from inherent limitations,\nincluding restricted expressiveness and frame-wise noise artifacts. Continuous\napproaches, while producing smoother and more natural motions, often falter due\nto high-dimensional complexity and limited training data. To resolve this\n\"discord\" between discrete and continuous representations, we introduce\nDisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a\nnovel method that decodes discrete motion tokens into continuous motion through\nrectified flow. By employing an iterative refinement process in the continuous\nspace, DisCoRD captures fine-grained dynamics and ensures smoother and more\nnatural motions. Compatible with any discrete-based framework, our method\nenhances naturalness without compromising faithfulness to the conditioning\nsignals. Extensive evaluations demonstrate that DisCoRD achieves\nstate-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on\nKIT-ML. These results solidify DisCoRD as a robust solution for bridging the\ndivide between discrete efficiency and continuous realism. Our project page is\navailable at: https://whwjdqls.github.io/discord.github.io/.\n","authors":["Jungbin Cho","Junwan Kim","Jisoo Kim","Minseo Kim","Mingu Kang","Sungeun Hong","Tae-Hyun Oh","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2411.19527v2.pdf","comment":"20 pages 18 figures"},{"id":"http://arxiv.org/abs/2404.01245v3","updated":"2024-12-02T03:27:10Z","published":"2024-04-01T17:03:41Z","title":"A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules","summary":"  Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.\n","authors":["Xiang Li","Feng Ruan","Huiyuan Wang","Qi Long","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2404.01245v3.pdf","comment":"To appear in the Annals of Statistics"},{"id":"http://arxiv.org/abs/2405.10515v2","updated":"2024-12-02T02:16:35Z","published":"2024-05-17T03:47:30Z","title":"Improved AdaBoost for Virtual Reality Experience Prediction Based on\n  Long Short-Term Memory Network","summary":"  A classification prediction algorithm based on Long Short-Term Memory Network\n(LSTM) improved AdaBoost is used to predict virtual reality (VR) user\nexperience. The dataset is randomly divided into training and test sets in the\nratio of 7:3.During the training process, the model's loss value decreases from\n0.65 to 0.31, which shows that the model gradually reduces the discrepancy\nbetween the prediction results and the actual labels, and improves the accuracy\nand generalisation ability.The final loss value of 0.31 indicates that the\nmodel fits the training data well, and is able to make predictions and\nclassifications more accurately. The confusion matrix for the training set\nshows a total of 177 correct predictions and 52 incorrect predictions, with an\naccuracy of 77%, precision of 88%, recall of 77% and f1 score of 82%. The\nconfusion matrix for the test set shows a total of 167 correct and 53 incorrect\npredictions with 75% accuracy, 87% precision, 57% recall and 69% f1 score. In\nsummary, the classification prediction algorithm based on LSTM with improved\nAdaBoost shows good prediction ability for virtual reality user experience.\nThis study is of great significance to enhance the application of virtual\nreality technology in user experience. By combining LSTM and AdaBoost\nalgorithms, significant progress has been made in user experience prediction,\nwhich not only improves the accuracy and generalisation ability of the model,\nbut also provides useful insights for related research in the field of virtual\nreality. This approach can help developers better understand user requirements,\noptimise virtual reality product design, and enhance user satisfaction,\npromoting the wide application of virtual reality technology in various fields.\n","authors":["Wenhan Fan","Zhicheng Ding","Ruixin Huang","Chang Zhou","Xuyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.10515v2.pdf","comment":"This work has been peer-reviewed in The 2nd International Conference\n  on Software Engineering and Machine Learning and published in Applied and\n  Computational Engineering, DOI:\n  https://doi.org/10.54254/2755-2721/77/20240678"},{"id":"http://arxiv.org/abs/2405.02326v2","updated":"2024-12-02T01:59:30Z","published":"2024-04-23T18:55:49Z","title":"Evaluating LLMs for Hardware Design and Test","summary":"  Large Language Models (LLMs) have demonstrated capabilities for producing\ncode in Hardware Description Languages (HDLs). However, most of the focus\nremains on their abilities to write functional code, not test code. The\nhardware design process consists of both design and test, and so eschewing\nvalidation and verification leaves considerable potential benefit unexplored,\ngiven that a design and test framework may allow for progress towards full\nautomation of the digital design pipeline. In this work, we perform one of the\nfirst studies exploring how a LLM can both design and test hardware modules\nfrom provided specifications. Using a suite of 8 representative benchmarks, we\nexamined the capabilities and limitations of the state-of-the-art\nconversational LLMs when producing Verilog for functional and verification\npurposes. We taped out the benchmarks on a Skywater 130nm shuttle and received\nthe functional chip.\n","authors":["Jason Blocklove","Siddharth Garg","Ramesh Karri","Hammond Pearce"],"pdf_url":"https://arxiv.org/pdf/2405.02326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04374v2","updated":"2024-12-02T01:38:05Z","published":"2023-12-07T15:44:56Z","title":"Deep Dynamics: Vehicle Dynamics Modeling with a Physics-Constrained\n  Neural Network for Autonomous Racing","summary":"  Autonomous racing is a critical research area for autonomous driving,\npresenting significant challenges in vehicle dynamics modeling, such as\nbalancing model precision and computational efficiency at high speeds\n(>280km/h), where minor errors in modeling have severe consequences. Existing\nphysics-based models for vehicle dynamics require elaborate testing setups and\ntuning, which are hard to implement, time-intensive, and cost-prohibitive.\nConversely, purely data-driven approaches do not generalize well and cannot\nadequately ensure physical constraints on predictions. This paper introduces\nDeep Dynamics, a physics-constrained neural network (PCNN) for vehicle dynamics\nmodeling of an autonomous racecar. It combines physics coefficient estimation\nand dynamical equations to accurately predict vehicle states at high speeds and\nincludes a unique Physics Guard layer to ensure internal coefficient estimates\nremain within their nominal physical ranges. Open-loop and closed-loop\nperformance assessments, using a physics-based simulator and full-scale\nautonomous Indy racecar data, highlight Deep Dynamics as a promising approach\nfor modeling racecar vehicle dynamics.\n","authors":["John Chrosniak","Jingyun Ning","Madhur Behl"],"pdf_url":"https://arxiv.org/pdf/2312.04374v2.pdf","comment":"Published in the IEEE Robotics and Automation Letters and presented\n  at the IEEE International Conference on Intelligent Robots and Systems"},{"id":"http://arxiv.org/abs/2208.08287v2","updated":"2024-12-02T01:00:34Z","published":"2022-08-17T13:29:14Z","title":"Noisy Nonnegative Tucker Decomposition with Sparse Factors and Missing\n  Data","summary":"  Tensor decomposition is a powerful tool for extracting physically meaningful\nlatent factors from multi-dimensional nonnegative data, and has been an\nincreasing interest in a variety of fields such as image processing, machine\nlearning, and computer vision. In this paper, we propose a sparse nonnegative\nTucker decomposition and completion method for the recovery of underlying\nnonnegative data under noisy observations. Here the underlying nonnegative data\ntensor is decomposed into a core tensor and several factor matrices with all\nentries being nonnegative and the factor matrices being sparse. The loss\nfunction is derived by the maximum likelihood estimation of the noisy\nobservations, and the $\\ell_0$ norm is employed to enhance the sparsity of the\nfactor matrices. We establish the error bound of the estimator of the proposed\nmodel under generic noise scenarios, which is then specified to the\nobservations with additive Gaussian noise, additive Laplace noise, and Poisson\nobservations, respectively. Our theoretical results are better than those by\nexisting tensor-based or matrix-based methods. Moreover, the minimax lower\nbounds are shown to be matched with the derived upper bounds up to logarithmic\nfactors. Numerical examples on both synthetic and real-world data sets\ndemonstrate the superiority of the proposed method for nonnegative tensor data\ncompletion.\n","authors":["Xiongjun Zhang","Michael K. Ng"],"pdf_url":"https://arxiv.org/pdf/2208.08287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03641v3","updated":"2024-12-02T00:54:47Z","published":"2023-04-07T13:44:59Z","title":"A Block Coordinate Descent Method for Nonsmooth Composite Optimization\n  under Orthogonality Constraints","summary":"  Nonsmooth composite optimization with orthogonality constraints has a wide\nrange of applications in statistical learning and data science. However, this\nproblem is challenging due to its nonsmooth objective and computationally\nexpensive, non-convex constraints. In this paper, we propose a new approach\ncalled \\textbf{OBCD}, which leverages Block Coordinate Descent to address these\nchallenges. \\textbf{OBCD} is a feasible method with a small computational\nfootprint. In each iteration, it updates $k$ rows of the solution matrix, where\n$k \\geq 2$, by globally solving a small nonsmooth optimization problem under\northogonality constraints. We prove that the limiting points of \\textbf{OBCD},\nreferred to as (global) block-$k$ stationary points, offer stronger optimality\nthan standard critical points. Furthermore, we show that \\textbf{OBCD}\nconverges to $\\epsilon$-block-$k$ stationary points with an ergodic convergence\nrate of $\\mathcal{O}(1/\\epsilon)$. Additionally, under the Kurdyka-Lojasiewicz\n(KL) inequality, we establish the non-ergodic convergence rate of\n\\textbf{OBCD}. We also extend \\textbf{OBCD} by incorporating breakpoint\nsearching methods for subproblem solving and greedy strategies for working set\nselection. Comprehensive experiments demonstrate the superior performance of\nour approach across various tasks.\n","authors":["Ganzhao Yuan"],"pdf_url":"https://arxiv.org/pdf/2304.03641v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16791v2","updated":"2024-12-02T00:12:35Z","published":"2024-06-24T16:55:03Z","title":"Enabling more efficient and cost-effective AI/ML systems with Collective\n  Mind, virtualized MLOps, MLPerf, Collective Knowledge Playground and\n  reproducible optimization tournaments","summary":"  This white paper introduces my educational community initiative to learn how\nto run AI, ML and other emerging workloads in the most efficient and\ncost-effective way across diverse models, data sets, software and hardware.\nThis project leverages Collective Mind (CM), virtualized MLOps and DevOps\n(CM4MLOps), MLPerf benchmarks, and the Collective Knowledge playground (CK),\nwhich I have developed in collaboration with the community and MLCommons.\n  I created Collective Mind as a small and portable Python package with minimal\ndependencies, a unified CLI and Python API to help researchers and engineers\nautomate repetitive, tedious, and time-consuming tasks. I also designed CM as a\ndistributed framework, continuously enhanced by the community through the CM4*\nrepositories, which function as the unified interface for organizing and\nmanaging various collections of automations and artifacts. For example,\nCM4MLOps repository includes many automations, also known as CM scripts, to\nstreamline the process of building, running, benchmarking, and optimizing AI,\nML, and other workflows across ever-evolving models, data, and systems.\n  I donated CK, CM and CM4MLOps to MLCommons to foster collaboration between\nacademia and industry to learn how to co-design more efficient and\ncost-effective AI systems while capturing and encoding knowledge within\nCollective Mind, protecting intellectual property, enabling portable skills,\nand accelerating the transition of the state-of-the-art research into\nproduction. My ultimate goal is to collaborate with the community to complete\nmy two-decade journey toward creating self-optimizing software and hardware\nthat can automatically learn how to run any workload in the most efficient and\ncost-effective manner based on user requirements and constraints such as cost,\nlatency, throughput, accuracy, power consumption, size, and other critical\nfactors.\n","authors":["Grigori Fursin"],"pdf_url":"https://arxiv.org/pdf/2406.16791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01401v4","updated":"2024-12-02T00:03:53Z","published":"2024-02-02T13:33:30Z","title":"An Information Theoretic Approach to Machine Unlearning","summary":"  To comply with AI and data regulations, the need to forget private or\ncopyrighted information from trained machine learning models is increasingly\nimportant. The key challenge in unlearning is forgetting the necessary data in\na timely manner, while preserving model performance. In this work, we address\nthe zero-shot unlearning scenario, whereby an unlearning algorithm must be able\nto remove data given only a trained model and the data to be forgotten. We\nexplore unlearning from an information theoretic perspective, connecting the\ninfluence of a sample to the information gain a model receives by observing it.\nFrom this, we derive a simple but principled zero-shot unlearning method based\non the geometry of the model. Our approach takes the form of minimising the\ngradient of a learned function with respect to a small neighbourhood around a\ntarget forget point. This induces a smoothing effect, causing forgetting by\nmoving the boundary of the classifier. We explore the intuition behind why this\napproach can jointly unlearn forget samples while preserving general model\nperformance through a series of low-dimensional experiments. We perform\nextensive empirical evaluation of our method over a range of contemporary\nbenchmarks, verifying that our method is competitive with state-of-the-art\nperformance under the strict constraints of zero-shot unlearning. Code for the\nproject can be found at\nhttps://github.com/jwf40/Information-Theoretic-Unlearning\n","authors":["Jack Foster","Kyle Fogarty","Stefan Schoepf","Zack Dugue","Cengiz Öztireli","Alexandra Brintrup"],"pdf_url":"https://arxiv.org/pdf/2402.01401v4.pdf","comment":"Updated, new low-dimensional experiments and updated perspective on\n  unlearning from an information theoretic view"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.17550v6","updated":"2024-12-02T10:06:28Z","published":"2023-03-30T17:18:31Z","title":"DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with\n  Diffusion Autoencoder","summary":"  While recent research has made significant progress in speech-driven talking\nface generation, the quality of the generated video still lags behind that of\nreal recordings. One reason for this is the use of handcrafted intermediate\nrepresentations like facial landmarks and 3DMM coefficients, which are designed\nbased on human knowledge and are insufficient to precisely describe facial\nmovements. Additionally, these methods require an external pretrained model for\nextracting these representations, whose performance sets an upper bound on\ntalking face generation. To address these limitations, we propose a novel\nmethod called DAE-Talker that leverages data-driven latent representations\nobtained from a diffusion autoencoder (DAE). DAE contains an image encoder that\nencodes an image into a latent vector and a DDIM image decoder that\nreconstructs the image from it. We train our DAE on talking face video frames\nand then extract their latent representations as the training target for a\nConformer-based speech2latent model. This allows DAE-Talker to synthesize full\nvideo frames and produce natural head movements that align with the content of\nspeech, rather than relying on a predetermined head pose from a template video.\nWe also introduce pose modelling in speech2latent for pose controllability.\nAdditionally, we propose a novel method for generating continuous video frames\nwith the DDIM image decoder trained on individual frames, eliminating the need\nfor modelling the joint distribution of consecutive frames directly. Our\nexperiments show that DAE-Talker outperforms existing popular methods in\nlip-sync, video fidelity, and pose naturalness. We also conduct ablation\nstudies to analyze the effectiveness of the proposed techniques and demonstrate\nthe pose controllability of DAE-Talker.\n","authors":["Chenpeng Du","Qi Chen","Tianyu He","Xu Tan","Xie Chen","Kai Yu","Sheng Zhao","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2303.17550v6.pdf","comment":"Accepted to ACM Multimedia 2023"}]},"2024-12-01T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2308.10792v8","updated":"2024-12-01T22:01:51Z","published":"2023-08-21T15:35:16Z","title":"Instruction Tuning for Large Language Models: A Survey","summary":"  This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\nsupervised fine-tuning (SFT) and instruction tuning (IT) are used\ninterchangeably.}, a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of SFT, the\nconstruction of SFT datasets, the training of SFT models, and applications to\ndifferent modalities, domains and application, along with analysis on aspects\nthat influence the outcome of SFT (e.g., generation of instruction outputs,\nsize of the instruction dataset, etc). We also review the potential pitfalls of\nSFT along with criticism against it, along with efforts pointing out current\ndeficiencies of existing strategies and suggest some avenues for fruitful\nresearch. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey\n","authors":["Shengyu Zhang","Linfeng Dong","Xiaoya Li","Sen Zhang","Xiaofei Sun","Shuhe Wang","Jiwei Li","Runyi Hu","Tianwei Zhang","Fei Wu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10792v8.pdf","comment":"V5; Last update: Dec. 1, 2024"},{"id":"http://arxiv.org/abs/2411.06548v2","updated":"2024-12-01T21:35:43Z","published":"2024-11-10T18:04:41Z","title":"CineXDrama: Relevance Detection and Sentiment Analysis of Bangla YouTube\n  Comments on Movie-Drama using Transformers: Insights from Interpretability\n  Tool","summary":"  In recent years, YouTube has become the leading platform for Bangla movies\nand dramas, where viewers express their opinions in comments that convey their\nsentiments about the content. However, not all comments are relevant for\nsentiment analysis, necessitating a filtering mechanism. We propose a system\nthat first assesses the relevance of comments and then analyzes the sentiment\nof those deemed relevant. We introduce a dataset of 14,000 manually collected\nand preprocessed comments, annotated for relevance (relevant or irrelevant) and\nsentiment (positive or negative). Eight transformer models, including\nBanglaBERT, were used for classification tasks, with BanglaBERT achieving the\nhighest accuracy (83.99% for relevance detection and 93.3% for sentiment\nanalysis). The study also integrates LIME to interpret model decisions,\nenhancing transparency.\n","authors":["Usafa Akther Rifa","Pronay Debnath","Busra Kamal Rafa","Shamaun Safa Hridi","Md. Aminur Rahman"],"pdf_url":"https://arxiv.org/pdf/2411.06548v2.pdf","comment":"Accepted for publication in Fifth International Conference on\n  Advances in Electrical, Computing, Communications and Sustainable\n  Technologies (ICAECT 2025)"},{"id":"http://arxiv.org/abs/2404.04393v2","updated":"2024-12-01T20:48:11Z","published":"2024-04-05T20:36:30Z","title":"Counting Like Transformers: Compiling Temporal Counting Logic Into\n  Softmax Transformers","summary":"  Deriving formal bounds on the expressivity of transformers, as well as\nstudying transformers that are constructed to implement known algorithms, are\nboth effective methods for better understanding the computational power of\ntransformers. Towards both ends, we introduce the temporal counting logic\n$\\textsf{K}_\\text{t}$[#] alongside the RASP variant $\\textsf{C-RASP}$. We show\nthey are equivalent to each other, and that together they are the best-known\nlower bound on the formal expressivity of future-masked soft attention\ntransformers with unbounded input size. We prove this by showing all\n$\\textsf{K}_\\text{t}$[#] formulas can be compiled into these transformers.\n","authors":["Andy Yang","David Chiang"],"pdf_url":"https://arxiv.org/pdf/2404.04393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.03730v3","updated":"2024-12-01T18:45:39Z","published":"2022-11-07T17:59:05Z","title":"DPCSpell: A Transformer-based Detector-Purificator-Corrector Framework\n  for Spelling Error Correction of Bangla and Resource Scarce Indic Languages","summary":"  Spelling error correction is the task of identifying and rectifying\nmisspelled words in texts. It is a potential and active research topic in\nNatural Language Processing because of numerous applications in human language\nunderstanding. The phonetically or visually similar yet semantically distinct\ncharacters make it an arduous task in any language. Earlier efforts on spelling\nerror correction in Bangla and resource-scarce Indic languages focused on\nrule-based, statistical, and machine learning-based methods which we found\nrather inefficient. In particular, machine learning-based approaches, which\nexhibit superior performance to rule-based and statistical methods, are\nineffective as they correct each character regardless of its appropriateness.\nIn this paper, we propose a novel detector-purificator-corrector framework,\nDPCSpell based on denoising transformers by addressing previous issues. In\naddition to that, we present a method for large-scale corpus creation from\nscratch which in turn resolves the resource limitation problem of any\nleft-to-right scripted language. The empirical outcomes demonstrate the\neffectiveness of our approach, which outperforms previous state-of-the-art\nmethods by attaining an exact match (EM) score of 94.78%, a precision score of\n0.9487, a recall score of 0.9478, an f1 score of 0.948, an f0.5 score of\n0.9483, and a modified accuracy (MA) score of 95.16% for Bangla spelling error\ncorrection. The models and corpus are publicly available at\nhttps://tinyurl.com/DPCSpell.\n","authors":["Mehedi Hasan Bijoy","Nahid Hossain","Salekul Islam","Swakkhar Shatabda"],"pdf_url":"https://arxiv.org/pdf/2211.03730v3.pdf","comment":"29 pages, 4 figures, and 9 tables"},{"id":"http://arxiv.org/abs/2406.12644v3","updated":"2024-12-01T17:45:28Z","published":"2024-06-18T14:12:27Z","title":"Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for\n  Large Language Models","summary":"  Assessing the effectiveness of large language models (LLMs) in performing\ndifferent tasks is crucial for understanding their strengths and weaknesses.\nThis paper presents the Hierarchical Prompting Taxonomy (HPT), grounded on\nhuman cognitive principles and designed to assess LLMs by examining the\ncognitive demands of various tasks. The HPT uses the Hierarchical Prompting\nFramework (HPF), a prompt selection framework that organizes five distinct\nprompting strategies by their cognitive load on LLMs. This study introduces the\nHierarchical Prompting Index (HPI) to measure task complexity, which\ndemonstrates LLMs' abilities across different datasets and serves as a\nuniversal metric for task complexity. The HPT offers a reliable method for\nevaluating LLMs' problem-solving skills in diverse scenarios, leading to\nclearer conclusions. Extensive experiments with multiple datasets and LLMs show\nthat the HPF enhances LLM performance by 2\\% to 63\\% compared to standard\nbenchmark datasets, confirming the effectiveness of the HPT. To support future\nresearch in this domain, the implementations of HPT and HPF are publicly\navailable\n","authors":["Devichand Budagam","Ashutosh Kumar","Mahsa Khoshnoodi","Sankalp KJ","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2406.12644v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17661v2","updated":"2024-12-01T17:10:16Z","published":"2024-11-26T18:25:57Z","title":"BERT or FastText? A Comparative Analysis of Contextual as well as\n  Non-Contextual Embeddings","summary":"  Natural Language Processing (NLP) for low-resource languages presents\nsignificant challenges, particularly due to the scarcity of high-quality\nannotated data and linguistic resources. The choice of embeddings plays a\ncritical role in enhancing the performance of NLP tasks, such as news\nclassification, sentiment analysis, and hate speech detection, especially for\nlow-resource languages like Marathi. In this study, we investigate the impact\nof various embedding techniques- Contextual BERT-based, Non-Contextual\nBERT-based, and FastText-based on NLP classification tasks specific to the\nMarathi language. Our research includes a thorough evaluation of both\ncompressed and uncompressed embeddings, providing a comprehensive overview of\nhow these embeddings perform across different scenarios. Specifically, we\ncompare two BERT model embeddings, Muril and MahaBERT, as well as two FastText\nmodel embeddings, IndicFT and MahaFT. Our evaluation includes applying\nembeddings to a Multiple Logistic Regression (MLR) classifier for task\nperformance assessment, as well as TSNE visualizations to observe the spatial\ndistribution of these embeddings. The results demonstrate that contextual\nembeddings outperform non-contextual embeddings. Furthermore, BERT-based\nnon-contextual embeddings extracted from the first BERT embedding layer yield\nbetter results than FastText-based embeddings, suggesting a potential\nalternative to FastText embeddings.\n","authors":["Abhay Shanbhag","Suramya Jadhav","Amogh Thakurdesai","Ridhima Sinare","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2411.17661v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19128v3","updated":"2024-12-01T15:07:29Z","published":"2024-10-24T19:56:28Z","title":"Retrieving Implicit and Explicit Emotional Events Using Large Language\n  Models","summary":"  Large language models (LLMs) have garnered significant attention in recent\nyears due to their impressive performance. While considerable research has\nevaluated these models from various perspectives, the extent to which LLMs can\nperform implicit and explicit emotion retrieval remains largely unexplored. To\naddress this gap, this study investigates LLMs' emotion retrieval capabilities\nin commonsense. Through extensive experiments involving multiple models, we\nsystematically evaluate the ability of LLMs on emotion retrieval. Specifically,\nwe propose a supervised contrastive probing method to verify LLMs' performance\nfor implicit and explicit emotion retrieval, as well as the diversity of the\nemotional events they retrieve. The results offer valuable insights into the\nstrengths and limitations of LLMs in handling emotion retrieval.\n","authors":["Guimin Hu","Hasti Seifi"],"pdf_url":"https://arxiv.org/pdf/2410.19128v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01100v2","updated":"2024-12-01T14:32:47Z","published":"2024-10-01T22:03:34Z","title":"Unlocking Korean Verbs: A User-Friendly Exploration into the Verb\n  Lexicon","summary":"  The Sejong dictionary dataset offers a valuable resource, providing extensive\ncoverage of morphology, syntax, and semantic representation. This dataset can\nbe utilized to explore linguistic information in greater depth. The labeled\nlinguistic structures within this dataset form the basis for uncovering\nrelationships between words and phrases and their associations with target\nverbs. This paper introduces a user-friendly web interface designed for the\ncollection and consolidation of verb-related information, with a particular\nfocus on subcategorization frames. Additionally, it outlines our efforts in\nmapping this information by aligning subcategorization frames with\ncorresponding illustrative sentence examples. Furthermore, we provide a Python\nlibrary that would simplify syntactic parsing and semantic role labeling. These\ntools are intended to assist individuals interested in harnessing the Sejong\ndictionary dataset to develop applications for Korean language processing.\n","authors":["Seohyun Song","Eunkyul Leah Jo","Yige Chen","Jeen-Pyo Hong","Kyuwon Kim","Jin Wee","Miyoung Kang","KyungTae Lim","Jungyeul Park","Chulwoo Park"],"pdf_url":"https://arxiv.org/pdf/2410.01100v2.pdf","comment":"NAACL 2025 System Demonstrations (Submitted)"},{"id":"http://arxiv.org/abs/2305.14225v3","updated":"2024-12-01T13:55:56Z","published":"2023-05-23T16:40:07Z","title":"ManiTweet: A New Benchmark for Identifying Manipulation of News on\n  Social Media","summary":"  Considerable advancements have been made to tackle the misrepresentation of\ninformation derived from reference articles in the domains of fact-checking and\nfaithful summarization. However, an unaddressed aspect remains - the\nidentification of social media posts that manipulate information within\nassociated news articles. This task presents a significant challenge, primarily\ndue to the prevalence of personal opinions in such posts. We present a novel\ntask, identifying manipulation of news on social media, which aims to detect\nmanipulation in social media posts and identify manipulated or inserted\ninformation. To study this task, we have proposed a data collection schema and\ncurated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and\ncorresponding articles. Our analysis demonstrates that this task is highly\nchallenging, with large language models (LLMs) yielding unsatisfactory\nperformance. Additionally, we have developed a simple yet effective basic model\nthat outperforms LLMs significantly on the ManiTweet dataset. Finally, we have\nconducted an exploratory analysis of human-written tweets, unveiling intriguing\nconnections between manipulation and the domain and factuality of news\narticles, as well as revealing that manipulated sentences are more likely to\nencapsulate the main story or consequences of a news outlet.\n","authors":["Kung-Hsiang Huang","Hou Pong Chan","Kathleen McKeown","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2305.14225v3.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2406.17305v2","updated":"2024-12-01T09:02:35Z","published":"2024-06-25T06:24:50Z","title":"Retrieval Augmented Instruction Tuning for Open NER with Large Language\n  Models","summary":"  The strong capability of large language models (LLMs) has been applied to\ninformation extraction (IE) through either retrieval augmented prompting or\ninstruction tuning (IT). However, the best way to incorporate information with\nLLMs for IE remains an open question. In this paper, we explore Retrieval\nAugmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named\nentity recognition (NER). Specifically, for each training sample, we retrieve\nsemantically similar examples from the training dataset as the context and\nprepend them to the input of the original instruction. To evaluate our RA-IT\napproach more thoroughly, we construct a Chinese IT dataset for open NER and\nevaluate RA-IT in both English and Chinese scenarios. Experimental results\nverify the effectiveness of RA-IT across various data sizes and in both English\nand Chinese scenarios. We also conduct thorough studies to explore the impacts\nof various retrieval strategies in the proposed RA-IT framework. Code and data\nare available at: https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER\n","authors":["Tingyu Xie","Jian Zhang","Yan Zhang","Yuanyuan Liang","Qi Li","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2406.17305v2.pdf","comment":"To be appeared at COLING 2025"},{"id":"http://arxiv.org/abs/2411.14491v3","updated":"2024-12-01T08:37:51Z","published":"2024-11-20T12:34:44Z","title":"A Survey on Human-Centric LLMs","summary":"  The rapid evolution of large language models (LLMs) and their capacity to\nsimulate human cognition and behavior has given rise to LLM-based frameworks\nand tools that are evaluated and applied based on their ability to perform\ntasks traditionally performed by humans, namely those involving cognition,\ndecision-making, and social interaction. This survey provides a comprehensive\nexamination of such human-centric LLM capabilities, focusing on their\nperformance in both individual tasks (where an LLM acts as a stand-in for a\nsingle human) and collective tasks (where multiple LLMs coordinate to mimic\ngroup dynamics). We first evaluate LLM competencies across key areas including\nreasoning, perception, and social cognition, comparing their abilities to\nhuman-like skills. Then, we explore real-world applications of LLMs in\nhuman-centric domains such as behavioral science, political science, and\nsociology, assessing their effectiveness in replicating human behaviors and\ninteractions. Finally, we identify challenges and future research directions,\nsuch as improving LLM adaptability, emotional intelligence, and cultural\nsensitivity, while addressing inherent biases and enhancing frameworks for\nhuman-AI collaboration. This survey aims to provide a foundational\nunderstanding of LLMs from a human-centric perspective, offering insights into\ntheir current capabilities and potential for future development.\n","authors":["Jing Yi Wang","Nicholas Sukiennik","Tong Li","Weikang Su","Qianyue Hao","Jingbo Xu","Zihan Huang","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2411.14491v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07602v2","updated":"2024-12-01T06:39:41Z","published":"2024-11-12T07:24:41Z","title":"Circuit Complexity Bounds for RoPE-based Transformer Architecture","summary":"  Characterizing the express power of the Transformer architecture is critical\nto understanding its capacity limits and scaling law. Recent works provide the\ncircuit complexity bounds to Transformer-like architecture. On the other hand,\nRotary Position Embedding ($\\mathsf{RoPE}$) has emerged as a crucial technique\nin modern large language models, offering superior performance in capturing\npositional information compared to traditional position embeddings, which shows\ngreat potential in application prospects, particularly for the long context\nscenario. Empirical evidence also suggests that $\\mathsf{RoPE}$-based\nTransformer architectures demonstrate greater generalization capabilities\ncompared to conventional Transformer models. In this work, we establish a\ncircuit complexity bound for Transformers with $\\mathsf{RoPE}$ attention. Our\nkey contribution is that we show that unless $\\mathsf{TC}^0 = \\mathsf{NC}^1$, a\n$\\mathsf{RoPE}$-based Transformer with $\\mathrm{poly}(n)$-precision, $O(1)$\nlayers, hidden dimension $d \\leq O(n)$ cannot solve the Arithmetic formula\nevaluation problem or the Boolean formula value problem. This result\nsignificantly demonstrates the fundamental limitation of the expressivity of\nthe $\\mathsf{RoPE}$-based Transformer architecture, although it achieves giant\nempirical success. Our theoretical result not only establishes the complexity\nbound but also may instruct further work on the $\\mathsf{RoPE}$-based\nTransformer.\n","authors":["Bo Chen","Xiaoyu Li","Yingyu Liang","Jiangxuan Long","Zhenmei Shi","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2411.07602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00765v2","updated":"2024-12-01T06:08:00Z","published":"2024-08-01T17:59:54Z","title":"MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models\n  for Integrated Capabilities","summary":"  MM-Vet, with open-ended vision-language questions targeting at evaluating\nintegrated capabilities, has become one of the most popular benchmarks for\nlarge multimodal model evaluation. MM-Vet assesses six core vision-language\n(VL) capabilities: recognition, knowledge, spatial awareness, language\ngeneration, OCR, and math. However, its question format is restricted to single\nimage-text pairs, lacking the interleaved image and text sequences prevalent in\nreal-world scenarios. To address this limitation, we introduce MM-Vet v2, which\nincludes a new VL capability called \"image-text sequence understanding\",\nevaluating models' ability to process VL sequences. Furthermore, we maintain\nthe high quality of evaluation samples while further expanding the evaluation\nset size. Using MM-Vet v2 to benchmark large multimodal models, we found that\nClaude 3.5 Sonnet is the best model with a score of 71.8, slightly\noutperforming GPT-4o which scored 71.0. Among open-weight models,\nInternVL2-Llama3-76B leads with a score of 68.4. The code, data, and\nleaderboard are accessible at https://github.com/yuweihao/MM-Vet.\n","authors":["Weihao Yu","Zhengyuan Yang","Lingfeng Ren","Linjie Li","Jianfeng Wang","Kevin Lin","Chung-Ching Lin","Zicheng Liu","Lijuan Wang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00765v2.pdf","comment":"Code, data and leaderboard: https://github.com/yuweihao/MM-Vet"},{"id":"http://arxiv.org/abs/2308.02490v4","updated":"2024-12-01T05:46:03Z","published":"2023-08-04T17:59:47Z","title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities","summary":"  We propose MM-Vet, an evaluation benchmark that examines large multimodal\nmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown various\nintriguing abilities, such as solving math problems written on the blackboard,\nreasoning about events and celebrities in news images, and explaining visual\njokes. Rapid model advancements pose challenges to evaluation benchmark\ndevelopment. Problems include: (1) How to systematically structure and evaluate\nthe complicated multimodal tasks; (2) How to design evaluation metrics that\nwork well across question and answer types; and (3) How to give model insights\nbeyond a simple performance ranking. To this end, we present MM-Vet, designed\nbased on the insight that the intriguing ability to solve complicated tasks is\noften achieved by a generalist model being able to integrate different core\nvision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and\nexamines the 16 integrations of interest derived from the capability\ncombination. For evaluation metrics, we propose an LLM-based evaluator for\nopen-ended outputs. The evaluator enables the evaluation across different\nquestion types and answer styles, resulting in a unified scoring metric. We\nevaluate representative LMMs on MM-Vet, providing insights into the\ncapabilities of different LMM system paradigms and models.\n","authors":["Weihao Yu","Zhengyuan Yang","Linjie Li","Jianfeng Wang","Kevin Lin","Zicheng Liu","Xinchao Wang","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2308.02490v4.pdf","comment":"ICML 2024. Code, data and leaderboard:\n  https://github.com/yuweihao/MM-Vet"},{"id":"http://arxiv.org/abs/2309.17249v3","updated":"2024-12-01T01:36:50Z","published":"2023-09-29T13:55:45Z","title":"Batch Calibration: Rethinking Calibration for In-Context Learning and\n  Prompt Engineering","summary":"  Prompting and in-context learning (ICL) have become efficient learning\nparadigms for large language models (LLMs). However, LLMs suffer from prompt\nbrittleness and various bias factors in the prompt, including but not limited\nto the formatting, the choice verbalizers, and the ICL examples. To address\nthis problem that results in unexpected performance degradation, calibration\nmethods have been developed to mitigate the effects of these biases while\nrecovering LLM performance. In this work, we first conduct a systematic\nanalysis of the existing calibration methods, where we both provide a unified\nview and reveal the failure cases. Inspired by these analyses, we propose Batch\nCalibration (BC), a simple yet intuitive method that controls the contextual\nbias from the batched input, unifies various prior approaches, and effectively\naddresses the aforementioned issues. BC is zero-shot, inference-only, and\nincurs negligible additional costs. In the few-shot setup, we further extend BC\nto allow it to learn the contextual bias from labeled data. We validate the\neffectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate\nstate-of-the-art performance over previous calibration baselines across more\nthan 10 natural language understanding and image classification tasks.\n","authors":["Han Zhou","Xingchen Wan","Lev Proleev","Diana Mincu","Jilin Chen","Katherine Heller","Subhrajit Roy"],"pdf_url":"https://arxiv.org/pdf/2309.17249v3.pdf","comment":"ICLR 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.17994v2","updated":"2024-12-01T22:35:05Z","published":"2024-11-27T02:22:14Z","title":"Differentiable Inverse Rendering with Interpretable Basis BRDFs","summary":"  Inverse rendering seeks to reconstruct both geometry and spatially varying\nBRDFs (SVBRDFs) from captured images. To address the inherent ill-posedness of\ninverse rendering, basis BRDF representations are commonly used, modeling\nSVBRDFs as spatially varying blends of a set of basis BRDFs. However, existing\nmethods often yield basis BRDFs that lack intuitive separation and have limited\nscalability to scenes of varying complexity. In this paper, we introduce a\ndifferentiable inverse rendering method that produces interpretable basis\nBRDFs. Our approach models a scene using 2D Gaussians, where the reflectance of\neach Gaussian is defined by a weighted blend of basis BRDFs. We efficiently\nrender an image from the 2D Gaussians and basis BRDFs using differentiable\nrasterization and impose a rendering loss with the input images. During this\nanalysis-by-synthesis optimization process of differentiable inverse rendering,\nwe dynamically adjust the number of basis BRDFs to fit the target scene while\nencouraging sparsity in the basis weights. This ensures that the reflectance of\neach Gaussian is represented by only a few basis BRDFs. This approach enables\nthe reconstruction of accurate geometry and interpretable basis BRDFs that are\nspatially separated. Consequently, the resulting scene representation,\ncomprising basis BRDFs and 2D Gaussians, supports physically-based novel-view\nrelighting and intuitive scene editing.\n","authors":["Hoon-Gyu Chung","Seokjun Choi","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2411.17994v2.pdf","comment":"This is a different paper from my previous paper \"Differentiable\n  Point-based Inverse Rendering\". It must not be removed automatically"},{"id":"http://arxiv.org/abs/2311.09614v3","updated":"2024-12-01T22:01:58Z","published":"2023-11-16T06:58:46Z","title":"Comprehensive framework for evaluation of deep neural networks in\n  detection and quantification of lymphoma from PET/CT images: clinical\n  insights, pitfalls, and observer agreement analyses","summary":"  This study addresses critical gaps in automated lymphoma segmentation from\nPET/CT images, focusing on issues often overlooked in existing literature.\nWhile deep learning has been applied for lymphoma lesion segmentation, few\nstudies incorporate out-of-distribution testing, raising concerns about model\ngeneralizability across diverse imaging conditions and patient populations. We\nhighlight the need to compare model performance with expert human annotators,\nincluding intra- and inter-observer variability, to understand task difficulty\nbetter. Most approaches focus on overall segmentation accuracy but overlook\nlesion-specific metrics important for precise lesion detection and disease\nquantification.To address these gaps, we propose a clinically-relevant\nframework for evaluating deep neural networks. Using this lesion-specific\nevaluation, we assess the performance of four deep segmentation networks\n(ResUNet, SegResNet, DynUNet, and SwinUNETR) across 611 cases from\nmulti-institutional datasets, covering various lymphoma subtypes and lesion\ncharacteristics. Beyond standard metrics like the Dice similarity coefficient\n(DSC), we evaluate clinical lesion measures and their prediction errors. We\nalso introduce detection criteria for lesion localization and propose a new\ndetection Criterion 3 based on metabolic characteristics. We show that networks\nperform better on large, intense lesions with higher metabolic\nactivity.Finally, we compare network performance to expert human observers via\nintra- and inter-observer variability analyses, demonstrating that network\nerrors closely resemble those made by experts. Some small, faint lesions remain\nchallenging for both humans and networks. This study aims to improve automated\nlesion segmentation's clinical relevance, supporting better treatment decisions\nfor lymphoma patients. The code is available at:\nhttps://github.com/microsoft/lymphoma-segmentation-dnn\n","authors":["Shadab Ahamed","Yixi Xu","Sara Kurkowska","Claire Gowdy","Joo H. O","Ingrid Bloise","Don Wilson","Patrick Martineau","François Bénard","Fereshteh Yousefirizi","Rahul Dodhia","Juan M. Lavista","William B. Weeks","Carlos F. Uribe","Arman Rahmim"],"pdf_url":"https://arxiv.org/pdf/2311.09614v3.pdf","comment":"32 pages, 15 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.09905v2","updated":"2024-12-01T21:42:37Z","published":"2024-03-14T22:33:22Z","title":"Right Place, Right Time! Generalizing ObjectNav to Dynamic Environments\n  with Portable Targets","summary":"  ObjectNav is a popular task in Embodied AI, where an agent navigates to a\ntarget object in an unseen environment. Prior literature makes the assumption\nof a static environment with stationary objects, which lacks realism. To\naddress this, we present a novel formulation to generalize ObjectNav to dynamic\nenvironments with non-stationary objects, and refer to it as Portable ObjectNav\nor P-ObjectNav. In our formulation, we first address several challenging issues\nwith dynamizing existing topological scene graphs by developing a novel method\nthat introduces multiple transition behaviors to portable objects in the scene.\nWe use this technique to dynamize Matterport3D, a popular simulator for\nevaluating embodied tasks. We then present a benchmark for P-ObjectNav using a\ncombination of heuristic, reinforcement learning, and Large Language Model\n(LLM)-based navigation approaches on the dynamized environment, while\nintroducing novel evaluation metrics tailored for our task. Our work\nfundamentally challenges the \"static-environment\" notion of prior ObjectNav\nwork; the code and dataset for P-ObjectNav will be made publicly available to\nfoster research on embodied navigation in dynamic scenes. We provide an\nanonymized repository for our code and dataset:\nhttps://anonymous.4open.science/r/PObjectNav-1C6D.\n","authors":["Vishnu Sashank Dorbala","Bhrij Patel","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2403.09905v2.pdf","comment":"19"},{"id":"http://arxiv.org/abs/2302.10883v2","updated":"2024-12-01T20:17:28Z","published":"2023-02-21T18:58:32Z","title":"Combining Blockchain and Biometrics: A Survey on Technical Aspects and a\n  First Legal Analysis","summary":"  Biometric recognition as a unique, hard-to-forge, and efficient way of\nidentification and verification has become an indispensable part of the current\ndigital world. The fast evolution of this technology has been a strong\nincentive for integrating it into many applications. Meanwhile, blockchain, the\nvery attractive decentralized ledger technology, has been widely received both\nby the research and industry in the past years and it is being increasingly\ndeployed nowadays in many different applications, such as money transfer, IoT,\nhealthcare, or logistics. Recently, researchers have started to speculate what\nwould be the pros and cons and what would be the best applications when these\ntwo technologies cross paths. This paper provides a survey of technical\nliterature research on the combination of blockchain and biometrics and\nincludes a first legal analysis of this integration to shed light on challenges\nand potentials. While this combination is still in its infancy and a growing\nbody of literature discusses specific blockchain applications and solutions in\nan advanced technological set-up, this paper presents a holistic understanding\nof blockchains applicability in the biometric sector. This study demonstrates\nthat combining blockchain and biometrics would be beneficial for novel\napplications in biometrics such as the PKI mechanism, distributed trusted\nservice, and identity management. However, blockchain networks at their current\nstage are not efficient and economical for real-time applications. From a legal\npoint of view, the allocation of accountability remains a main issue, while\nother difficulties remain, such as conducting a proper Data Protection Impact\nAssessment. Finally, it supplies technical and legal recommendations to reap\nthe benefits and mitigate the risks of the combination.\n","authors":["Mahdi Ghafourian","Bilgesu Sumer","Ruben Vera-Rodriguez","Julian Fierrez","Ruben Tolosana","Aythami Moralez","Els Kindt"],"pdf_url":"https://arxiv.org/pdf/2302.10883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04346v2","updated":"2024-12-01T20:10:57Z","published":"2023-11-07T21:06:06Z","title":"SaFL: Sybil-aware Federated Learning with Application to Face\n  Recognition","summary":"  Federated Learning (FL) is a machine learning paradigm to conduct\ncollaborative learning among clients on a joint model. The primary goal is to\nshare clients' local training parameters with an integrating server while\npreserving their privacy. This method permits to exploit the potential of\nmassive mobile users' data for the benefit of machine learning models'\nperformance while keeping sensitive data on local devices. On the downside, FL\nraises security and privacy concerns that have just started to be studied. To\naddress some of the key threats in FL, researchers have proposed to use secure\naggregation methods (e.g. homomorphic encryption, secure multiparty\ncomputation, etc.). These solutions improve some security and privacy metrics,\nbut at the same time bring about other serious threats such as poisoning\nattacks, backdoor attacks, and free running attacks. This paper proposes a new\ndefense method against poisoning attacks in FL called SaFL (Sybil-aware\nFederated Learning) that minimizes the effect of sybils with a novel\ntime-variant aggregation scheme.\n","authors":["Mahdi Ghafourian","Julian Fierrez","Ruben Vera-Rodriguez","Ruben Tolosana","Aythami Morales"],"pdf_url":"https://arxiv.org/pdf/2311.04346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16740v2","updated":"2024-12-01T19:13:25Z","published":"2024-11-23T18:14:42Z","title":"Document Haystacks: Vision-Language Reasoning Over Piles of 1000+\n  Documents","summary":"  Large multimodal models (LMMs) have achieved impressive progress in\nvision-language understanding, yet they face limitations in real-world\napplications requiring complex reasoning over a large number of images.\nExisting benchmarks for multi-image question-answering are limited in scope,\neach question is paired with only up to 30 images, which does not fully capture\nthe demands of large-scale retrieval tasks encountered in the real-world\nusages. To reduce these gaps, we introduce two document haystack benchmarks,\ndubbed DocHaystack and InfoHaystack, designed to evaluate LMM performance on\nlarge-scale visual document retrieval and understanding. Additionally, we\npropose V-RAG, a novel, vision-centric retrieval-augmented generation (RAG)\nframework that leverages a suite of multimodal vision encoders, each optimized\nfor specific strengths, and a dedicated question-document relevance module.\nV-RAG sets a new standard, with a 9% and 11% improvement in Recall@1 on the\nchallenging DocHaystack-1000 and InfoHaystack-1000 benchmarks, respectively,\ncompared to the previous best baseline models. Additionally, integrating V-RAG\nwith LMMs enables them to efficiently operate across thousands of images,\nyielding significant improvements on our DocHaystack and InfoHaystack\nbenchmarks. Our code and datasets are available at\nhttps://github.com/Vision-CAIR/dochaystacks\n","authors":["Jun Chen","Dannong Xu","Junjie Fei","Chun-Mei Feng","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2411.16740v2.pdf","comment":"the correct arxiv version"},{"id":"http://arxiv.org/abs/2404.07410v2","updated":"2024-12-01T18:48:37Z","published":"2024-04-11T00:49:38Z","title":"Improving Shift Invariance in Convolutional Neural Networks with\n  Translation Invariant Polyphase Sampling","summary":"  Downsampling operators break the shift invariance of convolutional neural\nnetworks (CNNs) and this affects the robustness of features learned by CNNs\nwhen dealing with even small pixel-level shift. Through a large-scale\ncorrelation analysis framework, we study shift invariance of CNNs by inspecting\nexisting downsampling operators in terms of their maximum-sampling bias (MSB),\nand find that MSB is negatively correlated with shift invariance. Based on this\ncrucial insight, we propose a learnable pooling operator called Translation\nInvariant Polyphase Sampling (TIPS) and two regularizations on the intermediate\nfeature maps of TIPS to reduce MSB and learn translation-invariant\nrepresentations. TIPS can be integrated into any CNN and can be trained\nend-to-end with marginal computational overhead. Our experiments demonstrate\nthat TIPS results in consistent performance gains in terms of accuracy, shift\nconsistency, and shift fidelity on multiple benchmarks for image classification\nand semantic segmentation compared to previous methods and also leads to\nimprovements in adversarial and distributional robustness. TIPS results in the\nlowest MSB compared to all previous methods, thus explaining our strong\nempirical results.\n","authors":["Sourajit Saha","Tejas Gokhale"],"pdf_url":"https://arxiv.org/pdf/2404.07410v2.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2409.09566v2","updated":"2024-12-01T17:48:24Z","published":"2024-09-15T00:53:44Z","title":"Learning Transferable Features for Implicit Neural Representations","summary":"  Implicit neural representations (INRs) have demonstrated success in a variety\nof applications, including inverse problems and neural rendering. An INR is\ntypically trained to capture one signal of interest, resulting in learned\nneural features that are highly attuned to that signal. Assumed to be less\ngeneralizable, we explore the aspect of transferability of such learned neural\nfeatures for fitting similar signals. We introduce a new INR training\nframework, STRAINER that learns transferrable features for fitting INRs to new\nsignals from a given distribution, faster and with better reconstruction\nquality. Owing to the sequential layer-wise affine operations in an INR, we\npropose to learn transferable representations by sharing initial encoder layers\nacross multiple INRs with independent decoder layers. At test time, the learned\nencoder representations are transferred as initialization for an otherwise\nrandomly initialized INR. We find STRAINER to yield extremely powerful\ninitialization for fitting images from the same domain and allow for $\\approx\n+10dB$ gain in signal quality early on compared to an untrained INR itself.\nSTRAINER also provides a simple way to encode data-driven priors in INRs. We\nevaluate STRAINER on multiple in-domain and out-of-domain signal fitting tasks\nand inverse problems and further provide detailed analysis and discussion on\nthe transferability of STRAINER's features. Our demo can be accessed at\nhttps://colab.research.google.com/drive/1fBZAwqE8C_lrRPAe-hQZJTWrMJuAKtG2?usp=sharing .\n","authors":["Kushal Vyas","Ahmed Imtiaz Humayun","Aniket Dashpute","Richard G. Baraniuk","Ashok Veeraraghavan","Guha Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2409.09566v2.pdf","comment":"Project Website: https://kushalvyas.github.io/strainer.html"},{"id":"http://arxiv.org/abs/2209.05227v5","updated":"2024-12-01T16:50:02Z","published":"2022-09-12T13:26:26Z","title":"DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation\n  Framework for Efficient Device Model Generalization","summary":"  Device Model Generalization (DMG) is a practical yet under-investigated\nresearch topic for on-device machine learning applications. It aims to improve\nthe generalization ability of pre-trained models when deployed on\nresource-constrained devices, such as improving the performance of pre-trained\ncloud models on smart mobiles. While quite a lot of works have investigated the\ndata distribution shift across clouds and devices, most of them focus on model\nfine-tuning on personalized data for individual devices to facilitate DMG.\nDespite their promising, these approaches require on-device re-training, which\nis practically infeasible due to the overfitting problem and high time delay\nwhen performing gradient calculation on real-time data. In this paper, we argue\nthat the computational cost brought by fine-tuning can be rather unnecessary.\nWe consequently present a novel perspective to improving DMG without increasing\ncomputational cost, i.e., device-specific parameter generation which directly\nmaps data distribution to parameters. Specifically, we propose an efficient\nDevice-cloUd collaborative parametErs generaTion framework DUET. DUET is\ndeployed on a powerful cloud server that only requires the low cost of\nforwarding propagation and low time delay of data transmission between the\ndevice and the cloud. By doing so, DUET can rehearse the device-specific model\nweight realizations conditioned on the personalized real-time data for an\nindividual device. Importantly, our DUET elegantly connects the cloud and\ndevice as a 'duet' collaboration, frees the DMG from fine-tuning, and enables a\nfaster and more accurate DMG paradigm. We conduct an extensive experimental\nstudy of DUET on three public datasets, and the experimental results confirm\nour framework's effectiveness and generalisability for different DMG tasks.\n","authors":["Zheqi Lv","Wenqiao Zhang","Shengyu Zhang","Kun Kuang","Feng Wang","Yongwei Wang","Zhengyu Chen","Tao Shen","Hongxia Yang","Beng Chin Ooi","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2209.05227v5.pdf","comment":"Published on WWW'23: Proceedings of the ACM on Web Conference 2023\n  (pp. 3077 - 3085)"},{"id":"http://arxiv.org/abs/2411.18499v2","updated":"2024-12-01T16:07:41Z","published":"2024-11-27T16:39:04Z","title":"GATE OpenING: A Comprehensive Benchmark for Judging Open-ended\n  Interleaved Image-Text Generation","summary":"  Multimodal Large Language Models (MLLMs) have made significant strides in\nvisual understanding and generation tasks. However, generating interleaved\nimage-text content remains a challenge, which requires integrated multimodal\nunderstanding and generation abilities. While the progress in unified models\noffers new solutions, existing benchmarks are insufficient for evaluating these\nmethods due to data size and diversity limitations. To bridge this gap, we\nintroduce GATE OpenING (OpenING), a comprehensive benchmark comprising 5,400\nhigh-quality human-annotated instances across 56 real-world tasks. OpenING\ncovers diverse daily scenarios such as travel guide, design, and brainstorming,\noffering a robust platform for challenging interleaved generation methods. In\naddition, we present IntJudge, a judge model for evaluating open-ended\nmultimodal generation methods. Trained with a novel data pipeline, our IntJudge\nachieves an agreement rate of 82. 42% with human judgments, outperforming\nGPT-based evaluators by 11.34%. Extensive experiments on OpenING reveal that\ncurrent interleaved generation methods still have substantial room for\nimprovement. Key findings on interleaved image-text generation are further\npresented to guide the development of next-generation models. The OpenING is\nopen-sourced at https://opening-benchmark.github.io.\n","authors":["Pengfei Zhou","Xiaopeng Peng","Jiajun Song","Chuanhao Li","Zhaopan Xu","Yue Yang","Ziyao Guo","Hao Zhang","Yuqi Lin","Yefei He","Lirui Zhao","Shuo Liu","Tianhua Li","Yuxuan Xie","Xiaojun Chang","Yu Qiao","Wenqi Shao","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.18499v2.pdf","comment":"53 pages, 19 figures"},{"id":"http://arxiv.org/abs/2407.04041v2","updated":"2024-12-01T15:58:23Z","published":"2024-07-04T16:29:05Z","title":"Towards Cross-View-Consistent Self-Supervised Surround Depth Estimation","summary":"  Depth estimation is a cornerstone for autonomous driving, yet acquiring\nper-pixel depth ground truth for supervised learning is challenging.\nSelf-Supervised Surround Depth Estimation (SSSDE) from consecutive images\noffers an economical alternative. While previous SSSDE methods have proposed\ndifferent mechanisms to fuse information across images, few of them explicitly\nconsider the cross-view constraints, leading to inferior performance,\nparticularly in overlapping regions. This paper proposes an efficient and\nconsistent pose estimation design and two loss functions to enhance cross-view\nconsistency for SSSDE. For pose estimation, we propose to use only front-view\nimages to reduce training memory and sustain pose estimation consistency. The\nfirst loss function is the dense depth consistency loss, which penalizes the\ndifference between predicted depths in overlapping regions. The second one is\nthe multi-view reconstruction consistency loss, which aims to maintain\nconsistency between reconstruction from spatial and spatial-temporal contexts.\nAdditionally, we introduce a novel flipping augmentation to improve the\nperformance further. Our techniques enable a simple neural model to achieve\nstate-of-the-art performance on the DDAD and nuScenes datasets. Last but not\nleast, our proposed techniques can be easily applied to other methods. The code\nwill be made public.\n","authors":["Laiyan Ding","Hualie Jiang","Jie Li","Yongquan Chen","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2407.04041v2.pdf","comment":"Accepted by IROS2024"},{"id":"http://arxiv.org/abs/2403.03077v4","updated":"2024-12-01T14:57:40Z","published":"2024-03-05T16:01:55Z","title":"MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual\n  Grounding","summary":"  3D visual grounding involves matching natural language descriptions with\ntheir corresponding objects in 3D spaces. Existing methods often face\nchallenges with accuracy in object recognition and struggle in interpreting\ncomplex linguistic queries, particularly with descriptions that involve\nmultiple anchors or are view-dependent. In response, we present the MiKASA\n(Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model\nintegrates a self-attention-based scene-aware object encoder and an original\nmulti-key-anchor technique, enhancing object recognition accuracy and the\nunderstanding of spatial relationships. Furthermore, MiKASA improves the\nexplainability of decision-making, facilitating error diagnosis. Our model\nachieves the highest overall accuracy in the Referit3D challenge for both the\nSr3D and Nr3D datasets, particularly excelling by a large margin in categories\nthat require viewpoint-dependent descriptions.\n","authors":["Chun-Peng Chang","Shaoxiang Wang","Alain Pagani","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2403.03077v4.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2408.08855v2","updated":"2024-12-01T14:53:07Z","published":"2024-08-16T17:30:27Z","title":"DPA: Dual Prototypes Alignment for Unsupervised Adaptation of\n  Vision-Language Models","summary":"  Vision-language models (VLMs), e.g., CLIP, have shown remarkable potential in\nzero-shot image classification. However, adapting these models to new domains\nremains challenging, especially in unsupervised settings where labeled data is\nunavailable. Recent research has proposed pseudo-labeling approaches to adapt\nCLIP in an unsupervised manner using unlabeled target data. Nonetheless, these\nmethods struggle due to noisy pseudo-labels resulting from the misalignment\nbetween CLIP's visual and textual representations. This study introduces DPA,\nan unsupervised domain adaptation method for VLMs. DPA introduces the concept\nof dual prototypes, acting as distinct classifiers, along with the convex\ncombination of their outputs, thereby leading to accurate pseudo-label\nconstruction. Next, it ranks pseudo-labels to facilitate robust self-training,\nparticularly during early training. Finally, it addresses visual-textual\nmisalignment by aligning textual prototypes with image prototypes to further\nimprove the adaptation performance. Experiments on 13 downstream vision tasks\ndemonstrate that DPA significantly outperforms zero-shot CLIP and the\nstate-of-the-art unsupervised adaptation baselines.\n","authors":["Eman Ali","Sathira Silva","Muhammad Haris Khan"],"pdf_url":"https://arxiv.org/pdf/2408.08855v2.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2411.17017v2","updated":"2024-12-01T14:37:22Z","published":"2024-11-26T01:00:09Z","title":"TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On","summary":"  Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task. Project page:\n\\url{https://zhenchenwan.github.io/TED-VITON/}\n","authors":["Zhenchen Wan","Yanwu Xu","Zhaoqing Wang","Feng Liu","Tongliang Liu","Mingming Gong"],"pdf_url":"https://arxiv.org/pdf/2411.17017v2.pdf","comment":"Project page: \\href{https://github.com/ZhenchenWan/TED-VITON}{this\n  URL}"},{"id":"http://arxiv.org/abs/2408.02555v3","updated":"2024-12-01T14:34:01Z","published":"2024-08-05T15:33:45Z","title":"MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh\n  Tokenization","summary":"  Meshes are the de facto 3D representation in the industry but are\nlabor-intensive to produce. Recently, a line of research has focused on\nautoregressively generating meshes. This approach processes meshes into a\nsequence composed of vertices and then generates them vertex by vertex, similar\nto how a language model generates text. These methods have achieved some\nsuccess but still struggle to generate complex meshes. One primary reason for\nthis limitation is their inefficient tokenization methods. To address this\nissue, we introduce MeshAnything V2, an advanced mesh generation model designed\nto create Artist-Created Meshes that align precisely with specified shapes. A\nkey innovation behind MeshAnything V2 is our novel Adjacent Mesh Tokenization\n(AMT) method. Unlike traditional approaches that represent each face using\nthree vertices, AMT optimizes this by employing a single vertex wherever\nfeasible, effectively reducing the token sequence length by about half on\naverage. This not only streamlines the tokenization process but also results in\nmore compact and well-structured sequences, enhancing the efficiency of mesh\ngeneration. With these improvements, MeshAnything V2 effectively doubles the\nface limit compared to previous models, delivering superior performance without\nincreasing computational costs. We will make our code and models publicly\navailable. Project Page: https://buaacyw.github.io/meshanything-v2/\n","authors":["Yiwen Chen","Yikai Wang","Yihao Luo","Zhengyi Wang","Zilong Chen","Jun Zhu","Chi Zhang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2408.02555v3.pdf","comment":"Project Page: https://buaacyw.github.io/meshanything-v2/ Github:\n  https://github.com/buaacyw/MeshAnythingV2"},{"id":"http://arxiv.org/abs/2310.13026v2","updated":"2024-12-01T14:27:28Z","published":"2023-10-19T07:16:54Z","title":"Weakly-Supervised Semantic Segmentation with Image-Level Labels: from\n  Traditional Models to Foundation Models","summary":"  The rapid development of deep learning has driven significant progress in\nimage semantic segmentation - a fundamental task in computer vision. Semantic\nsegmentation algorithms often depend on the availability of pixel-level labels\n(i.e., masks of objects), which are expensive, time-consuming, and\nlabor-intensive. Weakly-supervised semantic segmentation (WSSS) is an effective\nsolution to avoid such labeling. It utilizes only partial or incomplete\nannotations and provides a cost-effective alternative to fully-supervised\nsemantic segmentation. In this journal, our focus is on the WSSS with\nimage-level labels, which is the most challenging form of WSSS. Our work has\ntwo parts. First, we conduct a comprehensive survey on traditional methods,\nprimarily focusing on those presented at premier research conferences. We\ncategorize them into four groups based on where their methods operate:\npixel-wise, image-wise, cross-image, and external data. Second, we investigate\nthe applicability of visual foundation models, such as the Segment Anything\nModel (SAM), in the context of WSSS. We scrutinize SAM in two intriguing\nscenarios: text prompting and zero-shot learning. We provide insights into the\npotential and challenges of deploying visual foundational models for WSSS,\nfacilitating future developments in this exciting research area.\n","authors":["Zhaozheng Chen","Qianru Sun"],"pdf_url":"https://arxiv.org/pdf/2310.13026v2.pdf","comment":"Accepted to ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2303.13495v2","updated":"2024-12-01T14:04:22Z","published":"2023-03-23T17:56:10Z","title":"ReVersion: Diffusion-Based Relation Inversion from Images","summary":"  Diffusion models gain increasing popularity for their generative\ncapabilities. Recently, there have been surging needs to generate customized\nimages by inverting diffusion models from exemplar images, and existing\ninversion methods mainly focus on capturing object appearances (i.e., the\n\"look\"). However, how to invert object relations, another important pillar in\nthe visual world, remains unexplored. In this work, we propose the Relation\nInversion task, which aims to learn a specific relation (represented as\n\"relation prompt\") from exemplar images. Specifically, we learn a relation\nprompt with a frozen pre-trained text-to-image diffusion model. The learned\nrelation prompt can then be applied to generate relation-specific images with\nnew objects, backgrounds, and styles.\n  To tackle the Relation Inversion task, we propose the ReVersion Framework.\nSpecifically, we propose a novel \"relation-steering contrastive learning\"\nscheme to steer the relation prompt towards relation-dense regions, and\ndisentangle it away from object appearances. We further devise \"relation-focal\nimportance sampling\" to emphasize high-level interactions over low-level\nappearances (e.g., texture, color). To comprehensively evaluate this new task,\nwe contribute the ReVersion Benchmark, which provides various exemplar images\nwith diverse relations. Extensive experiments validate the superiority of our\napproach over existing methods across a wide range of visual relations. Our\nproposed task and method could be good inspirations for future research in\nvarious domains like generative inversion, few-shot learning, and visual\nrelation detection.\n","authors":["Ziqi Huang","Tianxing Wu","Yuming Jiang","Kelvin C. K. Chan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.13495v2.pdf","comment":"SIGGRAPH Asia (Conference Track) 2024, Project page:\n  https://ziqihuangg.github.io/projects/reversion.html Code:\n  https://github.com/ziqihuangg/ReVersion"},{"id":"http://arxiv.org/abs/2403.06090v4","updated":"2024-12-01T12:13:57Z","published":"2024-03-10T04:23:24Z","title":"What Matters When Repurposing Diffusion Models for General Dense\n  Perception Tasks?","summary":"  Extensive pre-training with large data is indispensable for downstream\ngeometry and semantic visual perception tasks. Thanks to large-scale\ntext-to-image (T2I) pretraining, recent works show promising results by simply\nfine-tuning T2I diffusion models for dense perception tasks. However, several\ncrucial design decisions in this process still lack comprehensive\njustification, encompassing the necessity of the multi-step stochastic\ndiffusion mechanism, training strategy, inference ensemble strategy, and\nfine-tuning data quality. In this work, we conduct a thorough investigation\ninto critical factors that affect transfer efficiency and performance when\nusing diffusion priors. Our key findings are: 1) High-quality fine-tuning data\nis paramount for both semantic and geometry perception tasks. 2) The stochastic\nnature of diffusion models has a slightly negative impact on deterministic\nvisual perception tasks. 3) Apart from fine-tuning the diffusion model with\nonly latent space supervision, task-specific image-level supervision is\nbeneficial to enhance fine-grained details. These observations culminate in the\ndevelopment of GenPercept, an effective deterministic one-step fine-tuning\nparadigm tailed for dense visual perception tasks. Different from the previous\nmulti-step methods, our paradigm has a much faster inference speed, and can be\nseamlessly integrated with customized perception decoders and loss functions\nfor image-level supervision, which is critical to improving the fine-grained\ndetails of predictions. Comprehensive experiments on diverse dense visual\nperceptual tasks, including monocular depth estimation, surface normal\nestimation, image segmentation, and matting, are performed to demonstrate the\nremarkable adaptability and effectiveness of our proposed method.\n","authors":["Guangkai Xu","Yongtao Ge","Mingyu Liu","Chengxiang Fan","Kangyang Xie","Zhiyue Zhao","Hao Chen","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2403.06090v4.pdf","comment":"Code is at: https://github.com/aim-uofa/GenPercept"},{"id":"http://arxiv.org/abs/2305.03614v5","updated":"2024-12-01T12:06:46Z","published":"2023-05-05T15:20:27Z","title":"Denoising-Contrastive Alignment for Continuous Sign Language Recognition","summary":"  Continuous sign language recognition (CSLR) aims to recognize signs in\nuntrimmed sign language videos to textual glosses. A key challenge of CSLR is\nachieving effective cross-modality alignment between video and gloss sequences\nto enhance video representation. However, current cross-modality alignment\nparadigms often neglect the role of textual grammar to guide the video\nrepresentation in learning global temporal context, which adversely affects\nrecognition performance. To tackle this limitation, we propose a\nDenoising-Contrastive Alignment (DCA) paradigm. DCA creatively leverages\ntextual grammar to enhance video representations through two complementary\napproaches: modeling the instance correspondence between signs and glosses from\na discrimination perspective and aligning their global context from a\ngenerative perspective. Specifically, DCA accomplishes flexible instance-level\ncorrespondence between signs and glosses using a contrastive loss. Building on\nthis, DCA models global context alignment between the video and gloss sequences\nby denoising the gloss representation from noise, guided by video\nrepresentation. Additionally, DCA introduces gradient modulation to optimize\nthe alignment and recognition gradients, ensuring a more effective learning\nprocess. By integrating gloss-wise and global context knowledge, DCA\nsignificantly enhances video representations for CSLR tasks. Experimental\nresults across public benchmarks validate the effectiveness of DCA and confirm\nits video representation enhancement feasibility.\n","authors":["Leming Guo","Wanli Xue","Shengyong Chen"],"pdf_url":"https://arxiv.org/pdf/2305.03614v5.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2410.18879v2","updated":"2024-12-01T11:58:37Z","published":"2024-10-24T16:13:06Z","title":"Multi-Class Abnormality Classification in Video Capsule Endoscopy Using\n  Deep Learning","summary":"  This report outlines Team Seq2Cure's deep learning approach for the Capsule\nVision 2024 Challenge, leveraging an ensemble of convolutional neural networks\n(CNNs) and transformer-based architectures for multi-class abnormality\nclassification in video capsule endoscopy frames. The dataset comprised over\n50,000 frames from three public sources and one private dataset, labeled across\n10 abnormality classes. To overcome the limitations of traditional CNNs in\ncapturing global context, we integrated CNN and transformer models within a\nmulti-model ensemble. Our approach achieved a balanced accuracy of 86.34\npercent and a mean AUC-ROC score of 0.9908 on the validation set, earning our\nsubmission 5th place in the challenge. Code is available at\nhttp://github.com/arnavs04/capsule-vision-2024 .\n","authors":["Arnav Samal","Ranya Batsyas"],"pdf_url":"https://arxiv.org/pdf/2410.18879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07671v2","updated":"2024-12-01T11:49:00Z","published":"2024-04-11T12:06:50Z","title":"Deep learning-driven pulmonary artery and vein segmentation reveals\n  demography-associated vasculature anatomical differences","summary":"  Pulmonary artery-vein segmentation is crucial for disease diagnosis and\nsurgical planning and is traditionally achieved by Computed Tomography\nPulmonary Angiography (CTPA). However, concerns regarding adverse health\neffects from contrast agents used in CTPA have constrained its clinical\nutility. In contrast, identifying arteries and veins using non-contrast CT, a\nconventional and low-cost clinical examination routine, has long been\nconsidered impossible. Here we propose a High-abundant Pulmonary Artery-vein\nSegmentation (HiPaS) framework achieving accurate artery-vein segmentation on\nboth non-contrast CT and CTPA across various spatial resolutions. HiPaS first\nperforms spatial normalization on raw CT volumes via a super-resolution module,\nand then iteratively achieves segmentation results at different branch levels\nby utilizing the lower-level vessel segmentation as a prior for higher-level\nvessel segmentation. We trained and validated HiPaS on our established\nmulti-centric dataset comprising 1,073 CT volumes with meticulous manual\nannotations. Both quantitative experiments and clinical evaluation demonstrated\nthe superior performance of HiPaS, achieving an average dice score of 91.8% and\na sensitivity of 98.0%. Further experiments showed the non-inferiority of HiPaS\nsegmentation on non-contrast CT compared to segmentation on CTPA. Employing\nHiPaS, we have conducted an anatomical study of pulmonary vasculature on 11,784\nparticipants in China (six sites), discovering a new association of pulmonary\nvessel anatomy with sex, age, and disease states: vessel abundance suggests a\nsignificantly higher association with females than males with slightly\ndecreasing with age, and is also influenced by certain diseases, under the\ncontrolling of lung volumes.\n","authors":["Yuetan Chu","Gongning Luo","Longxi Zhou","Shaodong Cao","Guolin Ma","Xianglin Meng","Juexiao Zhou","Changchun Yang","Dexuan Xie","Dan Mu","Ricardo Henao","Gianluca Setti","Xigang Xiao","Lianming Wu","Zhaowen Qiu","Xin Gao"],"pdf_url":"https://arxiv.org/pdf/2404.07671v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10462v2","updated":"2024-12-01T11:39:46Z","published":"2024-06-15T01:27:58Z","title":"CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal\n  Understanding and Generation","summary":"  Interleaved image-text generation has emerged as a crucial multimodal task,\naiming at creating sequences of interleaved visual and textual content given a\nquery. Despite notable advancements in recent multimodal large language models\n(MLLMs), generating integrated image-text sequences that exhibit narrative\ncoherence and entity and style consistency remains challenging due to poor\ntraining data quality. To address this gap, we introduce CoMM, a high-quality\nCoherent interleaved image-text MultiModal dataset designed to enhance the\ncoherence, consistency, and alignment of generated multimodal content.\nInitially, CoMM harnesses raw data from diverse sources, focusing on\ninstructional content and visual storytelling, establishing a foundation for\ncoherent and consistent content. To further refine the data quality, we devise\na multi-perspective filter strategy that leverages advanced pre-trained models\nto ensure the development of sentences, consistency of inserted images, and\nsemantic alignment between them. Various quality evaluation metrics are\ndesigned to prove the high quality of the filtered dataset. Meanwhile,\nextensive few-shot experiments on various downstream tasks demonstrate CoMM's\neffectiveness in significantly enhancing the in-context learning capabilities\nof MLLMs. Moreover, we propose four new tasks to evaluate MLLMs' interleaved\ngeneration abilities, supported by a comprehensive evaluation framework. We\nbelieve CoMM opens a new avenue for advanced MLLMs with superior multimodal\nin-context learning and understanding ability.\n","authors":["Wei Chen","Lin Li","Yongqi Yang","Bin Wen","Fan Yang","Tingting Gao","Yu Wu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2406.10462v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2410.22059v2","updated":"2024-12-01T11:38:07Z","published":"2024-10-29T14:10:28Z","title":"PACA: Perspective-Aware Cross-Attention Representation for Zero-Shot\n  Scene Rearrangement","summary":"  Scene rearrangement, like table tidying, is a challenging task in robotic\nmanipulation due to the complexity of predicting diverse object arrangements.\nWeb-scale trained generative models such as Stable Diffusion can aid by\ngenerating natural scenes as goals. To facilitate robot execution, object-level\nrepresentations must be extracted to match the real scenes with the generated\ngoals and to calculate object pose transformations. Current methods typically\nuse a multi-step design that involves separate models for generation,\nsegmentation, and feature encoding, which can lead to a low success rate due to\nerror accumulation. Furthermore, they lack control over the viewing\nperspectives of the generated goals, restricting the tasks to 3-DoF settings.\nIn this paper, we propose PACA, a zero-shot pipeline for scene rearrangement\nthat leverages perspective-aware cross-attention representation derived from\nStable Diffusion. Specifically, we develop a representation that integrates\ngeneration, segmentation, and feature encoding into a single step to produce\nobject-level representations. Additionally, we introduce perspective control,\nthus enabling the matching of 6-DoF camera views and extending past approaches\nthat were limited to 3-DoF top-down views. The efficacy of our method is\ndemonstrated through its zero-shot performance in real robot experiments across\nvarious scenes, achieving an average matching accuracy and execution success\nrate of 87% and 67%, respectively.\n","authors":["Shutong Jin","Ruiyu Wang","Kuangyi Chen","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2410.22059v2.pdf","comment":"Accepted by WACV2025"},{"id":"http://arxiv.org/abs/2408.15829v3","updated":"2024-12-01T10:30:10Z","published":"2024-08-28T14:44:42Z","title":"SITransformer: Shared Information-Guided Transformer for Extreme\n  Multimodal Summarization","summary":"  Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes an\nattractive summarization approach by integrating various types of information\nto create extremely concise yet informative summaries for individual\nmodalities. Existing methods overlook the issue that multimodal data often\ncontains more topic irrelevant information, which can mislead the model into\nproducing inaccurate summaries especially for extremely short ones. In this\npaper, we propose SITransformer, a Shared Information-guided Transformer for\nextreme multimodal summarization. It has a shared information guided pipeline\nwhich involves a cross-modal shared information extractor and a cross-modal\ninteraction module. The extractor formulates semantically shared salient\ninformation from different modalities by devising a novel filtering process\nconsisting of a differentiable top-k selector and a shared-information guided\ngating unit. As a result, the common, salient, and relevant contents across\nmodalities are identified. Next, a transformer with cross-modal attentions is\ndeveloped for intra- and inter-modality learning with the shared information\nguidance to produce the extreme summary. Comprehensive experiments demonstrate\nthat SITransformer significantly enhances the summarization quality for both\nvideo and text summaries for XMSMO. Our code will be publicly available at\nhttps://github.com/SichengLeoLiu/MMAsia24-XMSMO.\n","authors":["Sicheng Liu","Lintao Wang","Xiaogang Zhu","Xuequan Lu","Zhiyong Wang","Kun Hu"],"pdf_url":"https://arxiv.org/pdf/2408.15829v3.pdf","comment":"8 pages, 5 figures, submitted to ACM Multimedia Asia 2024"},{"id":"http://arxiv.org/abs/2411.18207v2","updated":"2024-12-01T10:23:18Z","published":"2024-11-27T10:33:51Z","title":"From Open Vocabulary to Open World: Teaching Vision Language Models to\n  Detect Novel Objects","summary":"  Traditional object detection methods operate under the closed-set assumption,\nwhere models can only detect a fixed number of objects predefined in the\ntraining set. Recent works on open vocabulary object detection (OVD) enable the\ndetection of objects defined by an unbounded vocabulary, which reduces the cost\nof training models for specific tasks. However, OVD heavily relies on accurate\nprompts provided by an ''oracle'', which limits their use in critical\napplications such as driving scene perception. OVD models tend to misclassify\nnear-out-of-distribution (NOOD) objects that have similar semantics to known\nclasses, and ignore far-out-of-distribution (FOOD) objects. To address theses\nlimitations, we propose a framework that enables OVD models to operate in open\nworld settings, by identifying and incrementally learning novel objects. To\ndetect FOOD objects, we propose Open World Embedding Learning (OWEL) and\nintroduce the concept of Pseudo Unknown Embedding which infers the location of\nunknown classes in a continuous semantic space based on the information of\nknown classes. We also propose Multi-Scale Contrastive Anchor Learning (MSCAL),\nwhich enables the identification of misclassified unknown objects by promoting\nthe intra-class consistency of object embeddings at different scales. The\nproposed method achieves state-of-the-art performance in common open world\nobject detection and autonomous driving benchmarks.\n","authors":["Zizhao Li","Zhengkang Xiang","Joseph West","Kourosh Khoshelham"],"pdf_url":"https://arxiv.org/pdf/2411.18207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04788v5","updated":"2024-12-01T08:24:11Z","published":"2024-05-08T03:43:58Z","title":"SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-supervised\n  Change Detector","summary":"  Change Detection (CD) aims to identify pixels with semantic changes between\nimages. However, annotating massive numbers of pixel-level images is\nlabor-intensive and costly, especially for multi-temporal images, which require\npixel-wise comparisons by human experts. Considering the excellent performance\nof visual language models (VLMs) for zero-shot, open-vocabulary, etc. with\nprompt-based reasoning, it is promising to utilize VLMs to make better CD under\nlimited labeled data. In this paper, we propose a VLM guidance-based\nsemi-supervised CD method, namely SemiCD-VL. The insight of SemiCD-VL is to\nsynthesize free change labels using VLMs to provide additional supervision\nsignals for unlabeled data. However, almost all current VLMs are designed for\nsingle-temporal images and cannot be directly applied to bi- or multi-temporal\nimages. Motivated by this, we first propose a VLM-based mixed change event\ngeneration (CEG) strategy to yield pseudo labels for unlabeled CD data. Since\nthe additional supervised signals provided by these VLM-driven pseudo labels\nmay conflict with the pseudo labels from the consistency regularization\nparadigm (e.g. FixMatch), we propose the dual projection head for de-entangling\ndifferent signal sources. Further, we explicitly decouple the bi-temporal\nimages semantic representation through two auxiliary segmentation decoders,\nwhich are also guided by VLM. Finally, to make the model more adequately\ncapture change representations, we introduce metric-aware supervision by\nfeature-level contrastive loss in auxiliary branches. Extensive experiments\nshow the advantage of SemiCD-VL. For instance, SemiCD-VL improves the FixMatch\nbaseline by +5.3 IoU on WHU-CD and by +2.4 IoU on LEVIR-CD with 5% labels. In\naddition, our CEG strategy, in an un-supervised manner, can achieve performance\nfar superior to state-of-the-art un-supervised CD methods.\n","authors":["Kaiyu Li","Xiangyong Cao","Yupeng Deng","Jiayi Song","Junmin Liu","Deyu Meng","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2405.04788v5.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.17788v2","updated":"2024-12-01T08:00:56Z","published":"2024-11-26T15:29:38Z","title":"Geometric Point Attention Transformer for 3D Shape Reassembly","summary":"  Shape assembly, which aims to reassemble separate parts into a complete\nobject, has gained significant interest in recent years. Existing methods\nprimarily rely on networks to predict the poses of individual parts, but often\nfail to effectively capture the geometric interactions between the parts and\ntheir poses. In this paper, we present the Geometric Point Attention\nTransformer (GPAT), a network specifically designed to address the challenges\nof reasoning about geometric relationships. In the geometric point attention\nmodule, we integrate both global shape information and local pairwise geometric\nfeatures, along with poses represented as rotation and translation vectors for\neach part. To enable iterative updates and dynamic reasoning, we introduce a\ngeometric recycling scheme, where each prediction is fed into the next\niteration for refinement. We evaluate our model on both the semantic and\ngeometric assembly tasks, showing that it outperforms previous methods in\nabsolute pose estimation, achieving accurate pose predictions and high\nalignment accuracy.\n","authors":["Jiahan Li","Chaoran Cheng","Jianzhu Ma","Ge Liu"],"pdf_url":"https://arxiv.org/pdf/2411.17788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18003v2","updated":"2024-12-01T07:20:36Z","published":"2024-11-27T02:47:17Z","title":"HAAT: Hybrid Attention Aggregation Transformer for Image\n  Super-Resolution","summary":"  In the research area of image super-resolution, Swin-transformer-based models\nare favored for their global spatial modeling and shifting window attention\nmechanism. However, existing methods often limit self-attention to non\noverlapping windows to cut costs and ignore the useful information that exists\nacross channels. To address this issue, this paper introduces a novel model,\nthe Hybrid Attention Aggregation Transformer (HAAT), designed to better\nleverage feature information. HAAT is constructed by integrating\nSwin-Dense-Residual-Connected Blocks (SDRCB) with Hybrid Grid Attention Blocks\n(HGAB). SDRCB expands the receptive field while maintaining a streamlined\narchitecture, resulting in enhanced performance. HGAB incorporates channel\nattention, sparse attention, and window attention to improve nonlocal feature\nfusion and achieve more visually compelling results. Experimental evaluations\ndemonstrate that HAAT surpasses state-of-the-art methods on benchmark datasets.\n  Keywords: Image super-resolution, Computer vision, Attention mechanism,\nTransformer\n","authors":["Song-Jiang Lai","Tsun-Hin Cheung","Ka-Chun Fung","Kai-wen Xue","Kin-Man Lam"],"pdf_url":"https://arxiv.org/pdf/2411.18003v2.pdf","comment":"6 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2406.19048v2","updated":"2024-12-01T07:07:02Z","published":"2024-06-27T09:56:38Z","title":"BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for\n  Semantic- and Spatial-Aware 3D Object Detection","summary":"  3D object detection is an important task that has been widely applied in\nautonomous driving. To perform this task, a new trend is to fuse multi-modal\ninputs, i.e., LiDAR and camera. Under such a trend, recent methods fuse these\ntwo modalities by unifying them in the same 3D space. However, during direct\nfusion in a unified space, the drawbacks of both modalities (LiDAR features\nstruggle with detailed semantic information and the camera lacks accurate 3D\nspatial information) are also preserved, diluting semantic and spatial\nawareness of the final unified representation. To address the issue, this\nletter proposes a novel bidirectional complementary LiDAR-camera fusion\nframework, called BiCo-Fusion that can achieve robust semantic- and\nspatial-aware 3D object detection. The key insight is to fuse LiDAR and camera\nfeatures in a bidirectional complementary way to enhance the semantic awareness\nof the LiDAR and the 3D spatial awareness of the camera. The enhanced features\nfrom both modalities are then adaptively fused to build a semantic- and\nspatial-aware unified representation. Specifically, we introduce Pre-Fusion\nconsisting of a Voxel Enhancement Module (VEM) to enhance the semantic\nawareness of voxel features from 2D camera features and Image Enhancement\nModule (IEM) to enhance the 3D spatial awareness of camera features from 3D\nvoxel features. We then introduce Unified Fusion (U-Fusion) to adaptively fuse\nthe enhanced features from the last stage to build a unified representation.\nExtensive experiments demonstrate the superiority of our BiCo-Fusion against\nthe prior arts. Project page: https://t-ys.github.io/BiCo-Fusion/.\n","authors":["Yang Song","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19048v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.00765v2","updated":"2024-12-01T06:08:00Z","published":"2024-08-01T17:59:54Z","title":"MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models\n  for Integrated Capabilities","summary":"  MM-Vet, with open-ended vision-language questions targeting at evaluating\nintegrated capabilities, has become one of the most popular benchmarks for\nlarge multimodal model evaluation. MM-Vet assesses six core vision-language\n(VL) capabilities: recognition, knowledge, spatial awareness, language\ngeneration, OCR, and math. However, its question format is restricted to single\nimage-text pairs, lacking the interleaved image and text sequences prevalent in\nreal-world scenarios. To address this limitation, we introduce MM-Vet v2, which\nincludes a new VL capability called \"image-text sequence understanding\",\nevaluating models' ability to process VL sequences. Furthermore, we maintain\nthe high quality of evaluation samples while further expanding the evaluation\nset size. Using MM-Vet v2 to benchmark large multimodal models, we found that\nClaude 3.5 Sonnet is the best model with a score of 71.8, slightly\noutperforming GPT-4o which scored 71.0. Among open-weight models,\nInternVL2-Llama3-76B leads with a score of 68.4. The code, data, and\nleaderboard are accessible at https://github.com/yuweihao/MM-Vet.\n","authors":["Weihao Yu","Zhengyuan Yang","Lingfeng Ren","Linjie Li","Jianfeng Wang","Kevin Lin","Chung-Ching Lin","Zicheng Liu","Lijuan Wang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00765v2.pdf","comment":"Code, data and leaderboard: https://github.com/yuweihao/MM-Vet"},{"id":"http://arxiv.org/abs/2308.02490v4","updated":"2024-12-01T05:46:03Z","published":"2023-08-04T17:59:47Z","title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities","summary":"  We propose MM-Vet, an evaluation benchmark that examines large multimodal\nmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown various\nintriguing abilities, such as solving math problems written on the blackboard,\nreasoning about events and celebrities in news images, and explaining visual\njokes. Rapid model advancements pose challenges to evaluation benchmark\ndevelopment. Problems include: (1) How to systematically structure and evaluate\nthe complicated multimodal tasks; (2) How to design evaluation metrics that\nwork well across question and answer types; and (3) How to give model insights\nbeyond a simple performance ranking. To this end, we present MM-Vet, designed\nbased on the insight that the intriguing ability to solve complicated tasks is\noften achieved by a generalist model being able to integrate different core\nvision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and\nexamines the 16 integrations of interest derived from the capability\ncombination. For evaluation metrics, we propose an LLM-based evaluator for\nopen-ended outputs. The evaluator enables the evaluation across different\nquestion types and answer styles, resulting in a unified scoring metric. We\nevaluate representative LMMs on MM-Vet, providing insights into the\ncapabilities of different LMM system paradigms and models.\n","authors":["Weihao Yu","Zhengyuan Yang","Linjie Li","Jianfeng Wang","Kevin Lin","Zicheng Liu","Xinchao Wang","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2308.02490v4.pdf","comment":"ICML 2024. Code, data and leaderboard:\n  https://github.com/yuweihao/MM-Vet"},{"id":"http://arxiv.org/abs/2405.18560v2","updated":"2024-12-01T05:22:22Z","published":"2024-05-28T20:10:06Z","title":"Potential Field Based Deep Metric Learning","summary":"  Deep metric learning (DML) involves training a network to learn a\nsemantically meaningful representation space. Many current approaches mine\nn-tuples of examples and model interactions within each tuplets. We present a\nnovel, compositional DML model, inspired by electrostatic fields in physics\nthat, instead of in tuples, represents the influence of each example\n(embedding) by a continuous potential field, and superposes the fields to\nobtain their combined global potential field. We use attractive/repulsive\npotential fields to represent interactions among embeddings from images of the\nsame/different classes. Contrary to typical learning methods, where mutual\ninfluence of samples is proportional to their distance, we enforce reduction in\nsuch influence with distance, leading to a decaying field. We show that such\ndecay helps improve performance on real world datasets with large intra-class\nvariations and label noise. Like other proxy-based methods, we also use proxies\nto succinctly represent sub-populations of examples. We evaluate our method on\nthree standard DML benchmarks- Cars-196, CUB-200-2011, and SOP datasets where\nit outperforms state-of-the-art baselines.\n","authors":["Shubhang Bhatnagar","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2405.18560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05846v2","updated":"2024-12-01T05:03:30Z","published":"2024-05-09T15:32:00Z","title":"An Inversion-based Measure of Memorization for Diffusion Models","summary":"  The past few years have witnessed substantial advances in image generation\npowered by diffusion models. However, it was shown that diffusion models are\nvulnerable to training data memorization, raising concerns regarding copyright\ninfringement and privacy invasion. This study delves into a rigorous analysis\nof memorization in diffusion models. We introduce an inversion-based measure of\nmemorization, InvMM, which searches for a sensitive latent noise distribution\naccounting for the replication of an image. For accurate estimation of the\nmemorization score, we propose an adaptive algorithm that balances the\nnormality and sensitivity of the inverted distribution. Comprehensive\nexperiments, conducted on both unconditional and text-guided diffusion models,\ndemonstrate that InvMM is capable of detecting heavily memorized images and\nelucidating the effect of various factors on memorization. Additionally, we\ndiscuss how memorization differs from membership. In practice, InvMM serves as\na useful tool for model developers to reliably assess the risk of memorization,\nthereby contributing to the enhancement of trustworthiness and\nprivacy-preserving capabilities of diffusion models.\n","authors":["Zhe Ma","Qingming Li","Xuhong Zhang","Tianyu Du","Ruixiao Lin","Zonghui Wang","Shouling Ji","Wenzhi Chen"],"pdf_url":"https://arxiv.org/pdf/2405.05846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08149v2","updated":"2024-12-01T03:29:46Z","published":"2024-08-15T13:35:59Z","title":"Unsupervised Variational Translator for Bridging Image Restoration and\n  High-Level Vision Tasks","summary":"  Recent research tries to extend image restoration capabilities from human\nperception to machine perception, thereby enhancing the performance of\nhigh-level vision tasks in degraded environments. These methods, primarily\nbased on supervised learning, typically involve the retraining of restoration\nnetworks or high-level vision networks. However, collecting paired data in\nreal-world scenarios and retraining large-scale models are challenge. To this\nend, we propose an unsupervised learning method called \\textbf{Va}riational\n\\textbf{T}ranslator (VaT), which does not require retraining existing\nrestoration and high-level vision networks. Instead, it establishes a\nlightweight network that serves as an intermediate bridge between them. By\nvariational inference, VaT approximates the joint distribution of restoration\noutput and high-level vision input, dividing the optimization objective into\npreserving content and maximizing marginal likelihood associated with\nhigh-level vision tasks. By cleverly leveraging self-training paradigms, VaT\nachieves the above optimization objective without requiring labels. As a\nresult, the translated images maintain a close resemblance to their original\ncontent while also demonstrating exceptional performance on high-level vision\ntasks. Extensive experiments in dehazing and low-light enhancement for\ndetection and classification show the superiority of our method over other\nstate-of-the-art unsupervised counterparts, even significantly surpassing\nsupervised methods in some complex real-world scenarios.\n","authors":["Jiawei Wu","Zhi Jin"],"pdf_url":"https://arxiv.org/pdf/2408.08149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12279v3","updated":"2024-12-01T02:12:08Z","published":"2024-11-19T06:57:45Z","title":"HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation","summary":"  This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use.\n","authors":["Ziyang Zong","Zhaohuan Zhan","Guang Tan"],"pdf_url":"https://arxiv.org/pdf/2411.12279v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23904v2","updated":"2024-12-01T01:35:19Z","published":"2024-10-31T13:06:29Z","title":"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection","summary":"  Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI.\n","authors":["Qinqian Lei","Bo Wang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2410.23904v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.17146v2","updated":"2024-12-01T00:19:01Z","published":"2024-06-24T21:36:01Z","title":"Vastextures: Vast repository of textures and PBR materials extracted\n  from real-world images using unsupervised methods","summary":"  Vastextures is a vast repository of 500,000 textures and PBR materials\nextracted from real-world images using an unsupervised process. The extracted\nmaterials and textures are extremely diverse and cover a vast range of\nreal-world patterns, but at the same time less refined compared to existing\nrepositories. The repository is composed of 2D textures cropped from natural\nimages and SVBRDF/PBR materials generated from these textures. Textures and PBR\nmaterials are essential for CGI. Existing materials repositories focus on\ngames, animation, and arts, that demand a limited amount of high-quality\nassets. However, virtual worlds and synthetic data are becoming increasingly\nimportant for training A.I systems for computer vision. This application\ndemands a huge amount of diverse assets but at the same time less affected by\nnoisy and unrefined assets. Vastexture aims to address this need by creating a\nfree, huge, and diverse assets repository that covers as many real-world\nmaterials as possible. The materials are automatically extracted from natural\nimages in two steps: 1) Automatically scanning a giant amount of images to\nidentify and crop regions with uniform textures. This is done by splitting the\nimage into a grid of cells and identifying regions in which all of the cells\nshare a similar statistical distribution. 2) Extracting the properties of the\nPBR material from the cropped texture. This is done by randomly guessing every\ncorrelation between the properties of the texture image and the properties of\nthe PBR material. The resulting PBR materials exhibit a vast amount of\nreal-world patterns as well as unexpected emergent properties. Neutral nets\ntrained on this repository outperformed nets trained using handcrafted assets.\n","authors":["Sagi Eppel"],"pdf_url":"https://arxiv.org/pdf/2406.17146v2.pdf","comment":"Vastexture was published as part of Learning Zero-Shot Material\n  States Segmentation, by Implanting Natural Image Patterns in Synthetic Data,\n  refer to this work in citations. This document gives a more detailed and\n  technical discussion of this repository"},{"id":"http://arxiv.org/abs/2309.03468v2","updated":"2024-12-01T00:09:44Z","published":"2023-09-07T03:33:49Z","title":"Support-Set Context Matters for Bongard Problems","summary":"  Current machine learning methods struggle to solve Bongard problems, which\nare a type of IQ test that requires deriving an abstract \"concept\" from a set\nof positive and negative \"support\" images, and then classifying whether or not\na new query image depicts the key concept. On Bongard-HOI, a benchmark for\nnatural-image Bongard problems, most existing methods have reached at best 69%\naccuracy (where chance is 50%). Low accuracy is often attributed to neural\nnets' lack of ability to find human-like symbolic rules. In this work, we point\nout that many existing methods are forfeiting accuracy due to a much simpler\nproblem: they do not adapt image features given information contained in the\nsupport set as a whole, and rely instead on information extracted from\nindividual supports. This is a critical issue, because the \"key concept\" in a\ntypical Bongard problem can often only be distinguished using multiple\npositives and multiple negatives. We explore simple methods to incorporate this\ncontext and show substantial gains over prior works, leading to new\nstate-of-the-art accuracy on Bongard-LOGO (75.3%) and Bongard-HOI (76.4%)\ncompared to methods with equivalent vision backbone architectures and strong\nperformance on the original Bongard problem set (60.8%).\n","authors":["Nikhil Raghuraman","Adam W. Harley","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2309.03468v2.pdf","comment":"TMLR October 2024. Code:\n  https://github.com/nraghuraman/bongard-context"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2209.05227v5","updated":"2024-12-01T16:50:02Z","published":"2022-09-12T13:26:26Z","title":"DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation\n  Framework for Efficient Device Model Generalization","summary":"  Device Model Generalization (DMG) is a practical yet under-investigated\nresearch topic for on-device machine learning applications. It aims to improve\nthe generalization ability of pre-trained models when deployed on\nresource-constrained devices, such as improving the performance of pre-trained\ncloud models on smart mobiles. While quite a lot of works have investigated the\ndata distribution shift across clouds and devices, most of them focus on model\nfine-tuning on personalized data for individual devices to facilitate DMG.\nDespite their promising, these approaches require on-device re-training, which\nis practically infeasible due to the overfitting problem and high time delay\nwhen performing gradient calculation on real-time data. In this paper, we argue\nthat the computational cost brought by fine-tuning can be rather unnecessary.\nWe consequently present a novel perspective to improving DMG without increasing\ncomputational cost, i.e., device-specific parameter generation which directly\nmaps data distribution to parameters. Specifically, we propose an efficient\nDevice-cloUd collaborative parametErs generaTion framework DUET. DUET is\ndeployed on a powerful cloud server that only requires the low cost of\nforwarding propagation and low time delay of data transmission between the\ndevice and the cloud. By doing so, DUET can rehearse the device-specific model\nweight realizations conditioned on the personalized real-time data for an\nindividual device. Importantly, our DUET elegantly connects the cloud and\ndevice as a 'duet' collaboration, frees the DMG from fine-tuning, and enables a\nfaster and more accurate DMG paradigm. We conduct an extensive experimental\nstudy of DUET on three public datasets, and the experimental results confirm\nour framework's effectiveness and generalisability for different DMG tasks.\n","authors":["Zheqi Lv","Wenqiao Zhang","Shengyu Zhang","Kun Kuang","Feng Wang","Yongwei Wang","Zhengyu Chen","Tao Shen","Hongxia Yang","Beng Chin Ooi","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2209.05227v5.pdf","comment":"Published on WWW'23: Proceedings of the ACM on Web Conference 2023\n  (pp. 3077 - 3085)"},{"id":"http://arxiv.org/abs/2302.07335v3","updated":"2024-12-01T16:41:49Z","published":"2023-02-14T20:44:12Z","title":"Intelligent Model Update Strategy for Sequential Recommendation","summary":"  Modern online platforms are increasingly employing recommendation systems to\naddress information overload and improve user engagement. There is an evolving\nparadigm in this research field that recommendation network learning occurs\nboth on the cloud and on edges with knowledge transfer in between (i.e.,\nedge-cloud collaboration). Recent works push this field further by enabling\nedge-specific context-aware adaptivity, where model parameters are updated in\nreal-time based on incoming on-edge data. However, we argue that frequent data\nexchanges between the cloud and edges often lead to inefficiency and waste of\ncommunication/computation resources, as considerable parameter updates might be\nredundant. To investigate this problem, we introduce Intelligent Edge-Cloud\nParameter Request Model, abbreviated as IntellectReq.\n  IntellectReq is designed to operate on edge, evaluating the cost-benefit\nlandscape of parameter requests with minimal computation and communication\noverhead. We formulate this as a novel learning task, aimed at the detection of\nout-of-distribution data, thereby fine-tuning adaptive communication\nstrategies. Further, we employ statistical mapping techniques to convert\nreal-time user behavior into a normal distribution, thereby employing\nmulti-sample outputs to quantify the model's uncertainty and thus its\ngeneralization capabilities. Rigorous empirical validation on four\nwidely-adopted benchmarks evaluates our approach, evidencing a marked\nimprovement in the efficiency and generalizability of edge-cloud collaborative\nand dynamic recommendation systems.\n","authors":["Zheqi Lv","Wenqiao Zhang","Zhengyu Chen","Shengyu Zhang","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2302.07335v3.pdf","comment":"Published on WWW'24(Oral): Proceedings of the ACM on Web Conference\n  2024 (pp. 3117-3128)"},{"id":"http://arxiv.org/abs/2411.06237v2","updated":"2024-12-01T13:31:14Z","published":"2024-11-09T17:38:01Z","title":"Leveraging Retrieval-Augmented Generation for Persian University\n  Knowledge Retrieval","summary":"  This paper introduces an innovative approach using Retrieval-Augmented\nGeneration (RAG) pipelines with Large Language Models (LLMs) to enhance\ninformation retrieval and query response systems for university-related\nquestion answering. By systematically extracting data from the university\nofficial webpage and employing advanced prompt engineering techniques, we\ngenerate accurate, contextually relevant responses to user queries.\n  We developed a comprehensive university benchmark, UniversityQuestionBench\n(UQB), to rigorously evaluate our system performance, based on common key\nmetrics in the filed of RAG pipelines, assessing accuracy and reliability\nthrough various metrics and real-world scenarios. Our experimental results\ndemonstrate significant improvements in the precision and relevance of\ngenerated responses, enhancing user experience and reducing the time required\nto obtain relevant answers. In summary, this paper presents a novel application\nof RAG pipelines and LLMs, supported by a meticulously prepared university\nbenchmark, offering valuable insights into advanced AI techniques for academic\ndata retrieval and setting the stage for future research in this domain.\n","authors":["Arshia Hemmat","Kianoosh Vadaei","Mohammad Hassan Heydari","Afsaneh Fatemi"],"pdf_url":"https://arxiv.org/pdf/2411.06237v2.pdf","comment":"6 pages, 2 figures, 1 table, Submitted to 15th IKT conference"},{"id":"http://arxiv.org/abs/2411.17229v2","updated":"2024-12-01T13:20:02Z","published":"2024-11-26T08:51:46Z","title":"Efficient Data-aware Distance Comparison Operations for High-Dimensional\n  Approximate Nearest Neighbor Search","summary":"  High-dimensional approximate $K$ nearest neighbor search (AKNN) is a\nfundamental task for various applications, including information retrieval.\nMost existing algorithms for AKNN can be decomposed into two main components,\ni.e., candidate generation and distance comparison operations (DCOs). While\ndifferent methods have unique ways of generating candidates, they all share the\nsame DCO process. In this study, we focus on accelerating the process of DCOs\nthat dominates the time cost in most existing AKNN algorithms. To achieve this,\nwe propose an Data-Aware Distance Estimation approach, called DADE, which\napproximates the exact distance in a lower-dimensional space. We theoretically\nprove that the distance estimation in DADE is unbiased in terms of data\ndistribution. Furthermore, we propose an optimized estimation based on the\nunbiased distance estimation formulation. In addition, we propose a hypothesis\ntesting approach to adaptively determine the number of dimensions needed to\nestimate the exact distance with sufficient confidence. We integrate DADE into\nwidely-used AKNN search algorithms, e.g., IVF and HNSW, and conduct extensive\nexperiments to demonstrate the superiority.\n","authors":["Liwei Deng","Penghao Chen","Ximu Zeng","Tianfu Wang","Yan Zhao","Kai Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.17229v2.pdf","comment":"Accepted by VLDB 2025"},{"id":"http://arxiv.org/abs/2405.18560v2","updated":"2024-12-01T05:22:22Z","published":"2024-05-28T20:10:06Z","title":"Potential Field Based Deep Metric Learning","summary":"  Deep metric learning (DML) involves training a network to learn a\nsemantically meaningful representation space. Many current approaches mine\nn-tuples of examples and model interactions within each tuplets. We present a\nnovel, compositional DML model, inspired by electrostatic fields in physics\nthat, instead of in tuples, represents the influence of each example\n(embedding) by a continuous potential field, and superposes the fields to\nobtain their combined global potential field. We use attractive/repulsive\npotential fields to represent interactions among embeddings from images of the\nsame/different classes. Contrary to typical learning methods, where mutual\ninfluence of samples is proportional to their distance, we enforce reduction in\nsuch influence with distance, leading to a decaying field. We show that such\ndecay helps improve performance on real world datasets with large intra-class\nvariations and label noise. Like other proxy-based methods, we also use proxies\nto succinctly represent sub-populations of examples. We evaluate our method on\nthree standard DML benchmarks- Cars-196, CUB-200-2011, and SOP datasets where\nit outperforms state-of-the-art baselines.\n","authors":["Shubhang Bhatnagar","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2405.18560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14592v2","updated":"2024-12-01T01:21:24Z","published":"2024-11-21T21:22:58Z","title":"G-RAG: Knowledge Expansion in Material Science","summary":"  In the field of Material Science, effective information retrieval systems are\nessential for facilitating research. Traditional Retrieval-Augmented Generation\n(RAG) approaches in Large Language Models (LLMs) often encounter challenges\nsuch as outdated information, hallucinations, limited interpretability due to\ncontext constraints, and inaccurate retrieval. To address these issues, Graph\nRAG integrates graph databases to enhance the retrieval process. Our proposed\nmethod processes Material Science documents by extracting key entities\n(referred to as MatIDs) from sentences, which are then utilized to query\nexternal Wikipedia knowledge bases (KBs) for additional relevant information.\nWe implement an agent-based parsing technique to achieve a more detailed\nrepresentation of the documents. Our improved version of Graph RAG called G-RAG\nfurther leverages a graph database to capture relationships between these\nentities, improving both retrieval accuracy and contextual understanding. This\nenhanced approach demonstrates significant improvements in performance for\ndomains that require precise information retrieval, such as Material Science.\n","authors":["Radeen Mostafa","Mirza Nihal Baig","Mashaekh Tausif Ehsan","Jakir Hasan"],"pdf_url":"https://arxiv.org/pdf/2411.14592v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.12269v2","updated":"2024-12-01T23:07:27Z","published":"2024-07-17T02:35:24Z","title":"UTG: Towards a Unified View of Snapshot and Event Based Models for\n  Temporal Graphs","summary":"  Many real world graphs are inherently dynamic, constantly evolving with node\nand edge additions. These graphs can be represented by temporal graphs, either\nthrough a stream of edge events or a sequence of graph snapshots. Until now,\nthe development of machine learning methods for both types has occurred largely\nin isolation, resulting in limited experimental comparison and theoretical\ncrosspollination between the two. In this paper, we introduce Unified Temporal\nGraph (UTG), a framework that unifies snapshot-based and event-based machine\nlearning models under a single umbrella, enabling models developed for one\nrepresentation to be applied effectively to datasets of the other. We also\npropose a novel UTG training procedure to boost the performance of\nsnapshot-based models in the streaming setting. We comprehensively evaluate\nboth snapshot and event-based models across both types of temporal graphs on\nthe temporal link prediction task. Our main findings are threefold: first, when\ncombined with UTG training, snapshot-based models can perform competitively\nwith event-based models such as TGN and GraphMixer even on event datasets.\nSecond, snapshot-based models are at least an order of magnitude faster than\nmost event-based models during inference. Third, while event-based methods such\nas NAT and DyGFormer outperforms snapshot-based methods on both types of\ntemporal graphs, this is because they leverage joint neighborhood structural\nfeatures thus emphasizing the potential to incorporate these features into\nsnapshotbased models as well. These findings highlight the importance of\ncomparing model architectures independent of the data format and suggest the\npotential of combining the efficiency of snapshot-based models with the\nperformance of event-based models in the future.\n","authors":["Shenyang Huang","Farimah Poursafaei","Reihaneh Rabbany","Guillaume Rabusseau","Emanuele Rossi"],"pdf_url":"https://arxiv.org/pdf/2407.12269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17367v2","updated":"2024-12-01T22:48:24Z","published":"2024-11-26T12:20:18Z","title":"Efficient Deployment of Transformer Models in Analog In-Memory Computing\n  Hardware","summary":"  Analog in-memory computing (AIMC) has emerged as a promising solution to\novercome the von Neumann bottleneck, accelerating neural network computations\nand improving computational efficiency. While AIMC has demonstrated success\nwith architectures such as CNNs, MLPs, and RNNs, deploying transformer-based\nmodels using AIMC presents unique challenges. Transformers are expected to\nhandle diverse downstream tasks and adapt to new user data or instructions\nafter deployment, which requires more flexible approaches to suit AIMC\nconstraints.\n  In this paper, we propose a novel method for deploying pre-trained\ntransformer models onto AIMC hardware. Unlike traditional approaches requiring\nhardware-aware training, our technique allows direct deployment without the\nneed for retraining the original model. Instead, we utilize lightweight,\nlow-rank adapters -- compact modules stored in digital cores -- to adapt the\nmodel to hardware constraints. We validate our approach on MobileBERT,\ndemonstrating accuracy on par with, or even exceeding, a traditional\nhardware-aware training approach. Our method is particularly appealing in\nmulti-task scenarios, as it enables a single analog model to be reused across\nmultiple tasks. Moreover, it supports on-chip adaptation to new hardware\nconstraints and tasks without updating analog weights, providing a flexible and\nversatile solution for real-world AI applications. Code is available.\n","authors":["Chen Li","Corey Lammie","Manuel Le Gallo","Bipin Rajendran"],"pdf_url":"https://arxiv.org/pdf/2411.17367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13166v2","updated":"2024-12-01T22:34:05Z","published":"2024-06-19T02:45:32Z","title":"Enhancing supply chain security with automated machine learning","summary":"  The increasing scale and complexity of global supply chains have led to new\nchallenges spanning various fields, such as supply chain disruptions due to\nlong waiting lines at the ports, material shortages, and inflation. Coupled\nwith the size of supply chains and the availability of vast amounts of data,\nefforts towards tackling such challenges have led to an increasing interest in\napplying machine learning methods in many aspects of supply chains. Unlike\nother solutions, ML techniques, including Random Forest, XGBoost, LightGBM, and\nNeural Networks, make predictions and approximate optimal solutions faster.\nThis paper presents an automated ML framework to enhance supply chain security\nby detecting fraudulent activities, predicting maintenance needs, and\nforecasting material backorders. Using datasets of varying sizes, results show\nthat fraud detection achieves an 88% accuracy rate using sampling methods,\nmachine failure prediction reaches 93.4% accuracy, and material backorder\nprediction achieves 89.3% accuracy. Hyperparameter tuning significantly\nimproved the performance of these models, with certain supervised techniques\nlike XGBoost and LightGBM reaching up to 100% precision. This research\ncontributes to supply chain security by streamlining data preprocessing,\nfeature selection, model optimization, and inference deployment, addressing\ncritical challenges and boosting operational efficiency.\n","authors":["Haibo Wang","Lutfu S. Sua","Bahram Alidaee"],"pdf_url":"https://arxiv.org/pdf/2406.13166v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2311.09614v3","updated":"2024-12-01T22:01:58Z","published":"2023-11-16T06:58:46Z","title":"Comprehensive framework for evaluation of deep neural networks in\n  detection and quantification of lymphoma from PET/CT images: clinical\n  insights, pitfalls, and observer agreement analyses","summary":"  This study addresses critical gaps in automated lymphoma segmentation from\nPET/CT images, focusing on issues often overlooked in existing literature.\nWhile deep learning has been applied for lymphoma lesion segmentation, few\nstudies incorporate out-of-distribution testing, raising concerns about model\ngeneralizability across diverse imaging conditions and patient populations. We\nhighlight the need to compare model performance with expert human annotators,\nincluding intra- and inter-observer variability, to understand task difficulty\nbetter. Most approaches focus on overall segmentation accuracy but overlook\nlesion-specific metrics important for precise lesion detection and disease\nquantification.To address these gaps, we propose a clinically-relevant\nframework for evaluating deep neural networks. Using this lesion-specific\nevaluation, we assess the performance of four deep segmentation networks\n(ResUNet, SegResNet, DynUNet, and SwinUNETR) across 611 cases from\nmulti-institutional datasets, covering various lymphoma subtypes and lesion\ncharacteristics. Beyond standard metrics like the Dice similarity coefficient\n(DSC), we evaluate clinical lesion measures and their prediction errors. We\nalso introduce detection criteria for lesion localization and propose a new\ndetection Criterion 3 based on metabolic characteristics. We show that networks\nperform better on large, intense lesions with higher metabolic\nactivity.Finally, we compare network performance to expert human observers via\nintra- and inter-observer variability analyses, demonstrating that network\nerrors closely resemble those made by experts. Some small, faint lesions remain\nchallenging for both humans and networks. This study aims to improve automated\nlesion segmentation's clinical relevance, supporting better treatment decisions\nfor lymphoma patients. The code is available at:\nhttps://github.com/microsoft/lymphoma-segmentation-dnn\n","authors":["Shadab Ahamed","Yixi Xu","Sara Kurkowska","Claire Gowdy","Joo H. O","Ingrid Bloise","Don Wilson","Patrick Martineau","François Bénard","Fereshteh Yousefirizi","Rahul Dodhia","Juan M. Lavista","William B. Weeks","Carlos F. Uribe","Arman Rahmim"],"pdf_url":"https://arxiv.org/pdf/2311.09614v3.pdf","comment":"32 pages, 15 figures, 5 tables"},{"id":"http://arxiv.org/abs/2308.10792v8","updated":"2024-12-01T22:01:51Z","published":"2023-08-21T15:35:16Z","title":"Instruction Tuning for Large Language Models: A Survey","summary":"  This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\nsupervised fine-tuning (SFT) and instruction tuning (IT) are used\ninterchangeably.}, a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of SFT, the\nconstruction of SFT datasets, the training of SFT models, and applications to\ndifferent modalities, domains and application, along with analysis on aspects\nthat influence the outcome of SFT (e.g., generation of instruction outputs,\nsize of the instruction dataset, etc). We also review the potential pitfalls of\nSFT along with criticism against it, along with efforts pointing out current\ndeficiencies of existing strategies and suggest some avenues for fruitful\nresearch. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey\n","authors":["Shengyu Zhang","Linfeng Dong","Xiaoya Li","Sen Zhang","Xiaofei Sun","Shuhe Wang","Jiwei Li","Runyi Hu","Tianwei Zhang","Fei Wu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10792v8.pdf","comment":"V5; Last update: Dec. 1, 2024"},{"id":"http://arxiv.org/abs/2405.15722v3","updated":"2024-12-01T21:18:32Z","published":"2024-05-24T17:10:08Z","title":"Models That Prove Their Own Correctness","summary":"  How can we trust the correctness of a learned model on a particular input of\ninterest? Model accuracy is typically measured *on average* over a distribution\nof inputs, giving no guarantee for any fixed input. This paper proposes a\ntheoretically-founded solution to this problem: to train *Self-Proving models*\nthat prove the correctness of their output to a verification algorithm $V$ via\nan Interactive Proof. Self-Proving models satisfy that, with high probability\nover a random input, the model generates a correct output *and* successfully\nproves its correctness to $V\\!$. The *soundness* property of $V$ guarantees\nthat, for *every* input, no model can convince $V$ of the correctness of an\nincorrect output. Thus, a Self-Proving model proves correctness of most of its\noutputs, while *all* incorrect outputs (of any model) are detected by $V$. We\ndevise a generic method for learning Self-Proving models, and we prove\nconvergence bounds under certain assumptions. The theoretical framework and\nresults are complemented by experiments on an arithmetic capability: computing\nthe greatest common divisor (GCD) of two integers. Our learning method is used\nto train a Self-Proving transformer that computes the GCD *and* proves the\ncorrectness of its answer.\n","authors":["Noga Amit","Shafi Goldwasser","Orr Paradise","Guy Rothblum"],"pdf_url":"https://arxiv.org/pdf/2405.15722v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08562v3","updated":"2024-12-01T20:58:49Z","published":"2024-10-11T06:35:48Z","title":"Adaptive Constraint Integration for Simultaneously Optimizing Crystal\n  Structures with Multiple Targeted Properties","summary":"  In materials science, finding crystal structures that have targeted\nproperties is crucial. While recent methodologies such as Bayesian optimization\nand deep generative models have made some advances on this issue, these methods\noften face difficulties in adaptively incorporating various constraints, such\nas electrical neutrality and targeted properties optimization, while keeping\nthe desired specific crystal structure. To address these challenges, we have\ndeveloped the Simultaneous Multi-property Optimization using Adaptive Crystal\nSynthesizer (SMOACS), which utilizes state-of-the-art property prediction\nmodels and their gradients to directly optimize input crystal structures for\ntargeted properties simultaneously. SMOACS enables the integration of adaptive\nconstraints into the optimization process without necessitating model\nretraining. Thanks to this feature, SMOACS has succeeded in simultaneously\noptimizing targeted properties while maintaining perovskite structures, even\nwith models trained on diverse crystal types. We have demonstrated the band gap\noptimization while meeting a challenging constraint, that is, maintaining\nelectrical neutrality in large atomic configurations up to 135 atom sites,\nwhere the verification of the electrical neutrality is challenging. The\nproperties of the most promising materials have been confirmed by density\nfunctional theory calculations.\n","authors":["Akihiro Fujii","Yoshitaka Ushiku","Koji Shimizu","Anh Khoa Augustin Lu","Satoshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2410.08562v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04393v2","updated":"2024-12-01T20:48:11Z","published":"2024-04-05T20:36:30Z","title":"Counting Like Transformers: Compiling Temporal Counting Logic Into\n  Softmax Transformers","summary":"  Deriving formal bounds on the expressivity of transformers, as well as\nstudying transformers that are constructed to implement known algorithms, are\nboth effective methods for better understanding the computational power of\ntransformers. Towards both ends, we introduce the temporal counting logic\n$\\textsf{K}_\\text{t}$[#] alongside the RASP variant $\\textsf{C-RASP}$. We show\nthey are equivalent to each other, and that together they are the best-known\nlower bound on the formal expressivity of future-masked soft attention\ntransformers with unbounded input size. We prove this by showing all\n$\\textsf{K}_\\text{t}$[#] formulas can be compiled into these transformers.\n","authors":["Andy Yang","David Chiang"],"pdf_url":"https://arxiv.org/pdf/2404.04393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10883v2","updated":"2024-12-01T20:17:28Z","published":"2023-02-21T18:58:32Z","title":"Combining Blockchain and Biometrics: A Survey on Technical Aspects and a\n  First Legal Analysis","summary":"  Biometric recognition as a unique, hard-to-forge, and efficient way of\nidentification and verification has become an indispensable part of the current\ndigital world. The fast evolution of this technology has been a strong\nincentive for integrating it into many applications. Meanwhile, blockchain, the\nvery attractive decentralized ledger technology, has been widely received both\nby the research and industry in the past years and it is being increasingly\ndeployed nowadays in many different applications, such as money transfer, IoT,\nhealthcare, or logistics. Recently, researchers have started to speculate what\nwould be the pros and cons and what would be the best applications when these\ntwo technologies cross paths. This paper provides a survey of technical\nliterature research on the combination of blockchain and biometrics and\nincludes a first legal analysis of this integration to shed light on challenges\nand potentials. While this combination is still in its infancy and a growing\nbody of literature discusses specific blockchain applications and solutions in\nan advanced technological set-up, this paper presents a holistic understanding\nof blockchains applicability in the biometric sector. This study demonstrates\nthat combining blockchain and biometrics would be beneficial for novel\napplications in biometrics such as the PKI mechanism, distributed trusted\nservice, and identity management. However, blockchain networks at their current\nstage are not efficient and economical for real-time applications. From a legal\npoint of view, the allocation of accountability remains a main issue, while\nother difficulties remain, such as conducting a proper Data Protection Impact\nAssessment. Finally, it supplies technical and legal recommendations to reap\nthe benefits and mitigate the risks of the combination.\n","authors":["Mahdi Ghafourian","Bilgesu Sumer","Ruben Vera-Rodriguez","Julian Fierrez","Ruben Tolosana","Aythami Moralez","Els Kindt"],"pdf_url":"https://arxiv.org/pdf/2302.10883v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04346v2","updated":"2024-12-01T20:10:57Z","published":"2023-11-07T21:06:06Z","title":"SaFL: Sybil-aware Federated Learning with Application to Face\n  Recognition","summary":"  Federated Learning (FL) is a machine learning paradigm to conduct\ncollaborative learning among clients on a joint model. The primary goal is to\nshare clients' local training parameters with an integrating server while\npreserving their privacy. This method permits to exploit the potential of\nmassive mobile users' data for the benefit of machine learning models'\nperformance while keeping sensitive data on local devices. On the downside, FL\nraises security and privacy concerns that have just started to be studied. To\naddress some of the key threats in FL, researchers have proposed to use secure\naggregation methods (e.g. homomorphic encryption, secure multiparty\ncomputation, etc.). These solutions improve some security and privacy metrics,\nbut at the same time bring about other serious threats such as poisoning\nattacks, backdoor attacks, and free running attacks. This paper proposes a new\ndefense method against poisoning attacks in FL called SaFL (Sybil-aware\nFederated Learning) that minimizes the effect of sybils with a novel\ntime-variant aggregation scheme.\n","authors":["Mahdi Ghafourian","Julian Fierrez","Ruben Vera-Rodriguez","Ruben Tolosana","Aythami Morales"],"pdf_url":"https://arxiv.org/pdf/2311.04346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03106v3","updated":"2024-12-01T19:15:57Z","published":"2023-03-03T10:53:30Z","title":"Rotation Invariant Quantization for Model Compression","summary":"  Post-training Neural Network (NN) model compression is an attractive approach\nfor deploying large, memory-consuming models on devices with limited memory\nresources. In this study, we investigate the rate-distortion tradeoff for NN\nmodel compression. First, we suggest a Rotation-Invariant Quantization (RIQ)\ntechnique that utilizes a single parameter to quantize the entire NN model,\nyielding a different rate at each layer, i.e., mixed-precision quantization.\nThen, we prove that our rotation-invariant approach is optimal in terms of\ncompression. We rigorously evaluate RIQ and demonstrate its capabilities on\nvarious models and tasks. For example, RIQ facilitates $\\times 19.4$ and\n$\\times 52.9$ compression ratios on pre-trained VGG dense and pruned models,\nrespectively, with $<0.4\\%$ accuracy degradation. Code is available in\n\\href{https://github.com/ehaleva/RIQ}{github.com/ehaleva/RIQ}.\n","authors":["Joseph Kampeas","Yury Nahshan","Hanoch Kremer","Gil Lederman","Shira Zaloshinski","Zheng Li","Emir Haleva"],"pdf_url":"https://arxiv.org/pdf/2303.03106v3.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.07410v2","updated":"2024-12-01T18:48:37Z","published":"2024-04-11T00:49:38Z","title":"Improving Shift Invariance in Convolutional Neural Networks with\n  Translation Invariant Polyphase Sampling","summary":"  Downsampling operators break the shift invariance of convolutional neural\nnetworks (CNNs) and this affects the robustness of features learned by CNNs\nwhen dealing with even small pixel-level shift. Through a large-scale\ncorrelation analysis framework, we study shift invariance of CNNs by inspecting\nexisting downsampling operators in terms of their maximum-sampling bias (MSB),\nand find that MSB is negatively correlated with shift invariance. Based on this\ncrucial insight, we propose a learnable pooling operator called Translation\nInvariant Polyphase Sampling (TIPS) and two regularizations on the intermediate\nfeature maps of TIPS to reduce MSB and learn translation-invariant\nrepresentations. TIPS can be integrated into any CNN and can be trained\nend-to-end with marginal computational overhead. Our experiments demonstrate\nthat TIPS results in consistent performance gains in terms of accuracy, shift\nconsistency, and shift fidelity on multiple benchmarks for image classification\nand semantic segmentation compared to previous methods and also leads to\nimprovements in adversarial and distributional robustness. TIPS results in the\nlowest MSB compared to all previous methods, thus explaining our strong\nempirical results.\n","authors":["Sourajit Saha","Tejas Gokhale"],"pdf_url":"https://arxiv.org/pdf/2404.07410v2.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2403.03871v2","updated":"2024-12-01T18:13:30Z","published":"2024-03-06T17:23:28Z","title":"Decoupled Vertical Federated Learning for Practical Training on\n  Vertically Partitioned Data","summary":"  Vertical Federated Learning (VFL) is an emergent distributed machine learning\nparadigm for collaborative learning between clients who have disjoint features\nof common entities. However, standard VFL lacks fault tolerance, with each\nparticipant and connection being a single point of failure. Prior attempts to\ninduce fault tolerance in VFL focus on the scenario of \"straggling clients\",\nusually entailing that all messages eventually arrive or that there is an upper\nbound on the number of late messages. To handle the more general problem of\narbitrary crashes, we propose Decoupled VFL (DVFL). To handle training with\nfaults, DVFL decouples training between communication rounds using local\nunsupervised objectives. By further decoupling label supervision from\naggregation, DVFL also enables redundant aggregators. As secondary benefits,\nDVFL can enhance data efficiency and provides immunity against gradient-based\nattacks. In this work, we implement DVFL for split neural networks with a\nself-supervised autoencoder loss. When there are faults, DVFL outperforms the\nbest VFL-based alternative (97.58% vs 96.95% on an MNIST task). Even under\nperfect conditions, performance is comparable.\n","authors":["Avi Amalanshu","Yash Sirvi","David I. Inouye"],"pdf_url":"https://arxiv.org/pdf/2403.03871v2.pdf","comment":"Revised manuscript. Nothing removed, additional baseline results\n  added"},{"id":"http://arxiv.org/abs/2411.17661v2","updated":"2024-12-01T17:10:16Z","published":"2024-11-26T18:25:57Z","title":"BERT or FastText? A Comparative Analysis of Contextual as well as\n  Non-Contextual Embeddings","summary":"  Natural Language Processing (NLP) for low-resource languages presents\nsignificant challenges, particularly due to the scarcity of high-quality\nannotated data and linguistic resources. The choice of embeddings plays a\ncritical role in enhancing the performance of NLP tasks, such as news\nclassification, sentiment analysis, and hate speech detection, especially for\nlow-resource languages like Marathi. In this study, we investigate the impact\nof various embedding techniques- Contextual BERT-based, Non-Contextual\nBERT-based, and FastText-based on NLP classification tasks specific to the\nMarathi language. Our research includes a thorough evaluation of both\ncompressed and uncompressed embeddings, providing a comprehensive overview of\nhow these embeddings perform across different scenarios. Specifically, we\ncompare two BERT model embeddings, Muril and MahaBERT, as well as two FastText\nmodel embeddings, IndicFT and MahaFT. Our evaluation includes applying\nembeddings to a Multiple Logistic Regression (MLR) classifier for task\nperformance assessment, as well as TSNE visualizations to observe the spatial\ndistribution of these embeddings. The results demonstrate that contextual\nembeddings outperform non-contextual embeddings. Furthermore, BERT-based\nnon-contextual embeddings extracted from the first BERT embedding layer yield\nbetter results than FastText-based embeddings, suggesting a potential\nalternative to FastText embeddings.\n","authors":["Abhay Shanbhag","Suramya Jadhav","Amogh Thakurdesai","Ridhima Sinare","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2411.17661v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13682v3","updated":"2024-12-01T16:49:16Z","published":"2024-05-22T14:25:02Z","title":"Unified Universality Theorem for Deep and Shallow\n  Joint-Group-Equivariant Machines","summary":"  We present a constructive universal approximation theorem for learning\nmachines equipped with joint-group-equivariant feature maps, called the\njoint-equivariant machines, based on the group representation theory.\n\"Constructive\" here indicates that the distribution of parameters is given in a\nclosed-form expression known as the ridgelet transform.\nJoint-group-equivariance encompasses a broad class of feature maps that\ngeneralize classical group-equivariance. Particularly, fully-connected networks\nare not group-equivariant but are joint-group-equivariant. Our main theorem\nalso unifies the universal approximation theorems for both shallow and deep\nnetworks. Until this study, the universality of deep networks has been shown in\na different manner from the universality of shallow networks, but our results\ndiscuss them on common ground. Now we can understand the approximation schemes\nof various learning machines in a unified manner. As applications, we show the\nconstructive universal approximation properties of four examples: depth-$n$\njoint-equivariant machine, depth-$n$ fully-connected network, depth-$n$\ngroup-convolutional network, and a new depth-$2$ network with quadratic forms\nwhose universality has not been known.\n","authors":["Sho Sonoda","Yuka Hashimoto","Isao Ishikawa","Masahiro Ikeda"],"pdf_url":"https://arxiv.org/pdf/2405.13682v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01357v2","updated":"2024-12-01T16:18:23Z","published":"2024-11-02T20:27:51Z","title":"WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy\n  Principles","summary":"  In this paper, we introduce WaKA (Wasserstein K-nearest-neighbors\nAttribution), a novel attribution method that leverages principles from the\nLiRA (Likelihood Ratio Attack) framework and k-nearest neighbors classifiers\n(k-NN). WaKA efficiently measures the contribution of individual data points to\nthe model's loss distribution, analyzing every possible k-NN that can be\nconstructed using the training set, without requiring to sample subsets of the\ntraining set. WaKA is versatile and can be used a posteriori as a membership\ninference attack (MIA) to assess privacy risks or a priori for privacy\ninfluence measurement and data valuation. Thus, WaKA can be seen as bridging\nthe gap between data attribution and membership inference attack (MIA) by\nproviding a unified framework to distinguish between a data point's value and\nits privacy risk. For instance, we have shown that self-attribution values are\nmore strongly correlated with the attack success rate than the contribution of\na point to the model generalization. WaKA's different usage were also evaluated\nacross diverse real-world datasets, demonstrating performance very close to\nLiRA when used as an MIA on k-NN classifiers, but with greater computational\nefficiency. Additionally, WaKA shows greater robustness than Shapley Values for\ndata minimization tasks (removal or addition) on imbalanced datasets.\n","authors":["Patrick Mesana","Clément Bénesse","Hadrien Lautraite","Gilles Caporossi","Sébastien Gambs"],"pdf_url":"https://arxiv.org/pdf/2411.01357v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02511v3","updated":"2024-12-01T15:55:50Z","published":"2024-02-04T14:51:49Z","title":"PoCo: Policy Composition from and for Heterogeneous Robot Learning","summary":"  Training general robotic policies from heterogeneous data for different tasks\nis a significant challenge. Existing robotic datasets vary in different\nmodalities such as color, depth, tactile, and proprioceptive information, and\ncollected in different domains such as simulation, real robots, and human\nvideos. Current methods usually collect and pool all data from one domain to\ntrain a single policy to handle such heterogeneity in tasks and domains, which\nis prohibitively expensive and difficult. In this work, we present a flexible\napproach, dubbed Policy Composition, to combine information across such diverse\nmodalities and domains for learning scene-level and task-level generalized\nmanipulation skills, by composing different data distributions represented with\ndiffusion models. Our method can use task-level composition for multi-task\nmanipulation and be composed with analytic cost functions to adapt policy\nbehaviors at inference time. We train our method on simulation, human, and real\nrobot data and evaluate in tool-use tasks. The composed policy achieves robust\nand dexterous performance under varying scenes and tasks and outperforms\nbaselines from a single data source in both simulation and real-world\nexperiments. See https://liruiw.github.io/policycomp for more details .\n","authors":["Lirui Wang","Jialiang Zhao","Yilun Du","Edward H. Adelson","Russ Tedrake"],"pdf_url":"https://arxiv.org/pdf/2402.02511v3.pdf","comment":"R:SS 2024"},{"id":"http://arxiv.org/abs/2410.10578v4","updated":"2024-12-01T15:49:16Z","published":"2024-10-14T14:52:23Z","title":"Burning RED: Unlocking Subtask-Driven Reinforcement Learning and\n  Risk-Awareness in Average-Reward Markov Decision Processes","summary":"  Average-reward Markov decision processes (MDPs) provide a foundational\nframework for sequential decision-making under uncertainty. However,\naverage-reward MDPs have remained largely unexplored in reinforcement learning\n(RL) settings, with the majority of RL-based efforts having been allocated to\nepisodic and discounted MDPs. In this work, we study a unique structural\nproperty of average-reward MDPs and utilize it to introduce Reward-Extended\nDifferential (or RED) reinforcement learning: a novel RL framework that can be\nused to effectively and efficiently solve various subtasks simultaneously in\nthe average-reward setting. We introduce a family of RED learning algorithms\nfor prediction and control, including proven-convergent algorithms for the\ntabular case. We then showcase the power of these algorithms by demonstrating\nhow they can be used to learn a policy that optimizes, for the first time, the\nwell-known conditional value-at-risk (CVaR) risk measure in a fully-online\nmanner, without the use of an explicit bi-level optimization scheme or an\naugmented state-space.\n","authors":["Juan Sebastian Rojas","Chi-Guhn Lee"],"pdf_url":"https://arxiv.org/pdf/2410.10578v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07154v2","updated":"2024-12-01T15:07:43Z","published":"2024-09-11T09:59:56Z","title":"Recurrent Aggregators in Neural Algorithmic Reasoning","summary":"  Neural algorithmic reasoning (NAR) is an emerging field that seeks to design\nneural networks that mimic classical algorithmic computations. Today, graph\nneural networks (GNNs) are widely used in neural algorithmic reasoners due to\ntheir message passing framework and permutation equivariance. In this extended\nabstract, we challenge this design choice, and replace the equivariant\naggregation function with a recurrent neural network. While seemingly\ncounter-intuitive, this approach has appropriate grounding when nodes have a\nnatural ordering -- and this is the case frequently in established reasoning\nbenchmarks like CLRS-30. Indeed, our recurrent NAR (RNAR) model performs very\nstrongly on such tasks, while handling many others gracefully. A notable\nachievement of RNAR is its decisive state-of-the-art result on the Heapsort and\nQuickselect tasks, both deemed as a significant challenge for contemporary\nneural algorithmic reasoners -- especially the latter, where RNAR achieves a\nmean micro-F1 score of 87%.\n","authors":["Kaijia Xu","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2409.07154v2.pdf","comment":"Presented at the Third Learning on Graphs Conference (LoG 2024). 10\n  pages, 1 figure"},{"id":"http://arxiv.org/abs/2312.00640v2","updated":"2024-12-01T15:02:17Z","published":"2023-12-01T15:00:59Z","title":"One to beat them all: \"RYU\" -- a unifying framework for the construction\n  of safe balls","summary":"  In this paper, we present a new framework, called \"RYU\" for constructing\n\"safe\" regions -- specifically, bounded sets that are guaranteed to contain the\ndual solution of a target optimization problem. Our framework applies to the\nstandard case where the objective function is composed of two components: a\nclosed, proper, convex function with Lipschitz-smooth gradient and another\nclosed, proper, convex function. We show that the RYU framework not only\nencompasses but also improves upon the state-of-the-art methods proposed over\nthe past decade for this class of optimization problems.\n","authors":["Thu-Le Tran","Clément Elvira","Hong-Phuong Dang","Cédric Herzet"],"pdf_url":"https://arxiv.org/pdf/2312.00640v2.pdf","comment":"19 pages, 1 table"},{"id":"http://arxiv.org/abs/2411.06237v2","updated":"2024-12-01T13:31:14Z","published":"2024-11-09T17:38:01Z","title":"Leveraging Retrieval-Augmented Generation for Persian University\n  Knowledge Retrieval","summary":"  This paper introduces an innovative approach using Retrieval-Augmented\nGeneration (RAG) pipelines with Large Language Models (LLMs) to enhance\ninformation retrieval and query response systems for university-related\nquestion answering. By systematically extracting data from the university\nofficial webpage and employing advanced prompt engineering techniques, we\ngenerate accurate, contextually relevant responses to user queries.\n  We developed a comprehensive university benchmark, UniversityQuestionBench\n(UQB), to rigorously evaluate our system performance, based on common key\nmetrics in the filed of RAG pipelines, assessing accuracy and reliability\nthrough various metrics and real-world scenarios. Our experimental results\ndemonstrate significant improvements in the precision and relevance of\ngenerated responses, enhancing user experience and reducing the time required\nto obtain relevant answers. In summary, this paper presents a novel application\nof RAG pipelines and LLMs, supported by a meticulously prepared university\nbenchmark, offering valuable insights into advanced AI techniques for academic\ndata retrieval and setting the stage for future research in this domain.\n","authors":["Arshia Hemmat","Kianoosh Vadaei","Mohammad Hassan Heydari","Afsaneh Fatemi"],"pdf_url":"https://arxiv.org/pdf/2411.06237v2.pdf","comment":"6 pages, 2 figures, 1 table, Submitted to 15th IKT conference"},{"id":"http://arxiv.org/abs/2411.09111v3","updated":"2024-12-01T13:08:57Z","published":"2024-11-14T00:59:13Z","title":"Reducing Reasoning Costs -- The Path of Optimization for Chain of\n  Thought via Sparse Attention Mechanism","summary":"  In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git.\n","authors":["Libo Wang"],"pdf_url":"https://arxiv.org/pdf/2411.09111v3.pdf","comment":"The main text is 5 pages, totaling 9 pages; 4 figures, 1 table. It\n  have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview"},{"id":"http://arxiv.org/abs/2409.15100v3","updated":"2024-12-01T13:04:28Z","published":"2024-09-23T15:11:40Z","title":"Robust Federated Learning Over the Air: Combating Heavy-Tailed Noise\n  with Median Anchored Clipping","summary":"  Leveraging over-the-air computations for model aggregation is an effective\napproach to cope with the communication bottleneck in federated edge learning.\nBy exploiting the superposition properties of multi-access channels, this\napproach facilitates an integrated design of communication and computation,\nthereby enhancing system privacy while reducing implementation costs. However,\nthe inherent electromagnetic interference in radio channels often exhibits\nheavy-tailed distributions, giving rise to exceptionally strong noise in\nglobally aggregated gradients that can significantly deteriorate the training\nperformance. To address this issue, we propose a novel gradient clipping\nmethod, termed Median Anchored Clipping (MAC), to combat the detrimental\neffects of heavy-tailed noise. We also derive analytical expressions for the\nconvergence rate of model training with analog over-the-air federated learning\nunder MAC, which quantitatively demonstrates the effect of MAC on training\nperformance. Extensive experimental results show that the proposed MAC\nalgorithm effectively mitigates the impact of heavy-tailed noise, hence\nsubstantially enhancing system robustness.\n","authors":["Jiaxing Li","Zihan Chen","Kai Fong Ernest Chong","Bikramjit Das","Tony Q. S. Quek","Howard H. Yang"],"pdf_url":"https://arxiv.org/pdf/2409.15100v3.pdf","comment":"This is the full version of the paper, and the appendix contains a\n  complete convergence analysis under non-convex conditions"},{"id":"http://arxiv.org/abs/2312.05878v2","updated":"2024-12-01T11:36:53Z","published":"2023-12-10T13:12:55Z","title":"Skew-Probabilistic Neural Networks for Learning from Imbalanced Data","summary":"  Real-world datasets often exhibit imbalanced data distribution, where certain\nclass levels are severely underrepresented. In such cases, traditional pattern\nclassifiers have shown a bias towards the majority class, impeding accurate\npredictions for the minority class. This paper introduces an imbalanced\ndata-oriented classifier using probabilistic neural networks (PNN) with a\nskew-normal kernel function to address this major challenge. PNN is known for\nproviding probabilistic outputs, enabling quantification of prediction\nconfidence, interpretability, and the ability to handle limited data. By\nleveraging the skew-normal distribution, which offers increased flexibility,\nparticularly for imbalanced and non-symmetric data, our proposed\nSkew-Probabilistic Neural Networks (SkewPNN) can better represent underlying\nclass densities. Hyperparameter fine-tuning is imperative to optimize the\nperformance of the proposed approach on imbalanced datasets. To this end, we\nemploy a population-based heuristic algorithm, the Bat optimization algorithm,\nto explore the hyperparameter space effectively. We also prove the statistical\nconsistency of the density estimates, suggesting that the true distribution\nwill be approached smoothly as the sample size increases. Theoretical analysis\nof the computational complexity of the proposed SkewPNN and BA-SkewPNN is also\nprovided. Numerical simulations have been conducted on different synthetic\ndatasets, comparing various benchmark-imbalanced learners. Real-data analysis\non several datasets shows that SkewPNN and BA-SkewPNN substantially outperform\nmost state-of-the-art machine-learning methods for both balanced and imbalanced\ndatasets (binary and multi-class categories) in most experimental settings.\n","authors":["Shraddha M. Naik","Tanujit Chakraborty","Madhurima Panja","Abdenour Hadid","Bibhas Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2312.05878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19456v2","updated":"2024-12-01T11:27:09Z","published":"2024-10-25T10:30:21Z","title":"Computational Bottlenecks of Training Small-scale Large Language Models","summary":"  While large language models (LLMs) dominate the AI landscape, Small-scale\nlarge Language Models (SLMs) are gaining attention due to cost and efficiency\ndemands from consumers. However, there is limited research on the training\nbehavior and computational requirements of SLMs. In this study, we explore the\ncomputational bottlenecks of training SLMs (up to 2B parameters) by examining\nthe effects of various hyperparameters and configurations, including GPU type,\nbatch size, model size, communication protocol, attention type, and the number\nof GPUs. We assess these factors on popular cloud services using metrics such\nas loss per dollar and tokens per second. Our findings aim to support the\nbroader adoption and optimization of language model training for low-resource\nAI research institutes.\n","authors":["Saleh Ashkboos","Iman Mirzadeh","Keivan Alizadeh","Mohammad Hossein Sekhavat","Moin Nabi","Mehrdad Farajtabar","Fartash Faghri"],"pdf_url":"https://arxiv.org/pdf/2410.19456v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.17180v2","updated":"2024-12-01T11:10:31Z","published":"2024-11-26T07:41:15Z","title":"Training a neural netwok for data reduction and better generalization","summary":"  The motivation for sparse learners is to compress the inputs (features) by\nselecting only the ones needed for good generalization. Linear models with\nLASSO-type regularization achieve this by setting the weights of irrelevant\nfeatures to zero, effectively identifying and ignoring them. In artificial\nneural networks, this selective focus can be achieved by pruning the input\nlayer. Given a cost function enhanced with a sparsity-promoting penalty, our\nproposal selects a regularization term $\\lambda$ (without the use of\ncross-validation or a validation set) that creates a local minimum in the cost\nfunction at the origin where no features are selected. This local minimum acts\nas a baseline, meaning that if there is no strong enough signal to justify a\nfeature inclusion, the local minimum remains at zero with a high prescribed\nprobability. The method is flexible, applying to complex models ranging from\nshallow to deep artificial neural networks and supporting various cost\nfunctions and sparsity-promoting penalties. We empirically show a remarkable\nphase transition in the probability of retrieving the relevant features, as\nwell as good generalization thanks to the choice of $\\lambda$, the non-convex\npenalty and the optimization scheme developed. This approach can be seen as a\nform of compressed sensing for complex models, allowing us to distill\nhigh-dimensional data into a compact, interpretable subset of meaningful\nfeatures.\n","authors":["Sylvain Sardy","Maxime van Cutsem","Xiaoyu Ma"],"pdf_url":"https://arxiv.org/pdf/2411.17180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12644v2","updated":"2024-12-01T09:09:53Z","published":"2024-01-23T10:54:13Z","title":"Binary Feature Mask Optimization for Feature Selection","summary":"  We investigate feature selection problem for generic machine learning models.\nWe introduce a novel framework that selects features considering the outcomes\nof the model. Our framework introduces a novel feature masking approach to\neliminate the features during the selection process, instead of completely\nremoving them from the dataset. This allows us to use the same machine learning\nmodel during feature selection, unlike other feature selection methods where we\nneed to train the machine learning model again as the dataset has different\ndimensions on each iteration. We obtain the mask operator using the predictions\nof the machine learning model, which offers a comprehensive view on the subsets\nof the features essential for the predictive performance of the model. A\nvariety of approaches exist in the feature selection literature. However, to\nour knowledge, no study has introduced a training-free framework for a generic\nmachine learning model to select features while considering the importance of\nthe feature subsets as a whole, instead of focusing on the individual features.\nWe demonstrate significant performance improvements on the real-life datasets\nunder different settings using LightGBM and Multi-Layer Perceptron as our\nmachine learning models. The high performance of our General Binary Mask\nOptimization algorithm stems from its feature masking approach to select\nfeatures and its flexibility in the number of selected features. The algorithm\nselects features based on the validation performance of the machine learning\nmodel. Hence, the number of selected features is not predetermined and adjusts\ndynamically to the dataset. Additionally, we openly share the implementation or\nour code to encourage further research in this area.\n","authors":["Mehmet E. Lorasdagi","Mehmet Y. Turali","Suleyman S. Kozat"],"pdf_url":"https://arxiv.org/pdf/2401.12644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03768v4","updated":"2024-12-01T08:43:03Z","published":"2024-01-08T09:47:19Z","title":"Corn Yield Prediction Model with Deep Neural Networks for Smallholder\n  Farmer Decision Support System","summary":"  Crop yield prediction has been modeled on the assumption that there is no\ninteraction between weather and soil variables. However, this paper argues that\nan interaction exists, and it can be finely modelled using the Kendall\nCorrelation coefficient. Given the nonlinearity of the interaction between\nweather and soil variables, a deep neural network regressor (DNNR) is carefully\ndesigned with consideration to the depth, number of neurons of the hidden\nlayers, and the hyperparameters with their optimizations. Additionally, a new\nmetric, the average of absolute root squared error (ARSE) is proposed to\ncombine the strengths of root mean square error (RMSE) and mean absolute error\n(MAE). With the ARSE metric, the proposed DNNR(s), optimised random forest\nregressor (RFR) and the extreme gradient boosting regressor (XGBR) achieved\nimpressively small yield errors, 0.0172 t/ha, and 0.0243 t/ha, 0.0001 t/ha, and\n0.001 t/ha, respectively. However, the DNNR(s), with changes to the explanatory\nvariables to ensure generalizability to unforeseen data, DNNR(s) performed\nbest. Further analysis reveals that a strong interaction does exist between\nweather and soil variables. Precisely, yield is observed to increase when\nprecipitation is reduced and silt increased, and vice-versa. However, the\ndegree of decrease or increase is not quantified in this paper. Contrary to\nexisting yield models targeted towards agricultural policies and global food\nsecurity, the goal of the proposed corn yield model is to empower the\nsmallholder farmer to farm smartly and intelligently, thus the prediction model\nis integrated into a mobile application that includes education, and a\nfarmer-to-market access module.\n","authors":["Chollette C. Olisah","Lyndon Smith","Melvyn Smith","Morolake O. Lawrence","Osita Ojukwu"],"pdf_url":"https://arxiv.org/pdf/2401.03768v4.pdf","comment":"30 Pages, 11 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2402.09427v2","updated":"2024-12-01T08:33:42Z","published":"2024-01-24T05:28:29Z","title":"DoorINet: Door Heading Prediction through Inertial Deep Learning","summary":"  Inertial sensors are widely used in a variety of applications. A common task\nis orientation estimation. To tackle such a task, attitude and heading\nreference system algorithms are applied. Relying on the gyroscope readings, the\naccelerometer measurements are used to update the attitude angles, and\nmagnetometer measurements are utilized to update the heading angle. In indoor\nenvironments, magnetometers suffer from interference that degrades their\nperformance resulting in poor heading angle estimation. Therefore, applications\nthat estimate the heading angle of moving objects, such as walking pedestrians,\nclosets, and refrigerators, are prone to error. To circumvent such situations,\nwe propose DoorINet, an end-to-end deep-learning framework to calculate the\nheading angle from door-mounted, low-cost inertial sensors without using\nmagnetometers. To evaluate our approach, we record a unique dataset containing\n391 minutes of accelerometer and gyroscope measurements and corresponding\nground-truth heading angle. We show that our proposed approach outperforms\ncommonly used, model based approaches and data-driven methods.\n","authors":["Aleksei Zakharchenko","Sharon Farber","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2402.09427v2.pdf","comment":"10 pages, 14 figures, 4 tables"},{"id":"http://arxiv.org/abs/2411.17788v2","updated":"2024-12-01T08:00:56Z","published":"2024-11-26T15:29:38Z","title":"Geometric Point Attention Transformer for 3D Shape Reassembly","summary":"  Shape assembly, which aims to reassemble separate parts into a complete\nobject, has gained significant interest in recent years. Existing methods\nprimarily rely on networks to predict the poses of individual parts, but often\nfail to effectively capture the geometric interactions between the parts and\ntheir poses. In this paper, we present the Geometric Point Attention\nTransformer (GPAT), a network specifically designed to address the challenges\nof reasoning about geometric relationships. In the geometric point attention\nmodule, we integrate both global shape information and local pairwise geometric\nfeatures, along with poses represented as rotation and translation vectors for\neach part. To enable iterative updates and dynamic reasoning, we introduce a\ngeometric recycling scheme, where each prediction is fed into the next\niteration for refinement. We evaluate our model on both the semantic and\ngeometric assembly tasks, showing that it outperforms previous methods in\nabsolute pose estimation, achieving accurate pose predictions and high\nalignment accuracy.\n","authors":["Jiahan Li","Chaoran Cheng","Jianzhu Ma","Ge Liu"],"pdf_url":"https://arxiv.org/pdf/2411.17788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08958v2","updated":"2024-12-01T06:47:45Z","published":"2024-09-13T16:23:17Z","title":"PINNfluence: Influence Functions for Physics-Informed Neural Networks","summary":"  Recently, physics-informed neural networks (PINNs) have emerged as a flexible\nand promising application of deep learning to partial differential equations in\nthe physical sciences. While offering strong performance and competitive\ninference speeds on forward and inverse problems, their black-box nature limits\ninterpretability, particularly regarding alignment with expected physical\nbehavior. In the present work, we explore the application of influence\nfunctions (IFs) to validate and debug PINNs post-hoc. Specifically, we apply\nvariations of IF-based indicators to gauge the influence of different types of\ncollocation points on the prediction of PINNs applied to a 2D Navier-Stokes\nfluid flow problem. Our results demonstrate how IFs can be adapted to PINNs to\nreveal the potential for further studies. The code is publicly available at\nhttps://github.com/aleks-krasowski/PINNfluence.\n","authors":["Jonas R. Naujoks","Aleksander Krasowski","Moritz Weckbecker","Thomas Wiegand","Sebastian Lapuschkin","Wojciech Samek","René P. Klausen"],"pdf_url":"https://arxiv.org/pdf/2409.08958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07602v2","updated":"2024-12-01T06:39:41Z","published":"2024-11-12T07:24:41Z","title":"Circuit Complexity Bounds for RoPE-based Transformer Architecture","summary":"  Characterizing the express power of the Transformer architecture is critical\nto understanding its capacity limits and scaling law. Recent works provide the\ncircuit complexity bounds to Transformer-like architecture. On the other hand,\nRotary Position Embedding ($\\mathsf{RoPE}$) has emerged as a crucial technique\nin modern large language models, offering superior performance in capturing\npositional information compared to traditional position embeddings, which shows\ngreat potential in application prospects, particularly for the long context\nscenario. Empirical evidence also suggests that $\\mathsf{RoPE}$-based\nTransformer architectures demonstrate greater generalization capabilities\ncompared to conventional Transformer models. In this work, we establish a\ncircuit complexity bound for Transformers with $\\mathsf{RoPE}$ attention. Our\nkey contribution is that we show that unless $\\mathsf{TC}^0 = \\mathsf{NC}^1$, a\n$\\mathsf{RoPE}$-based Transformer with $\\mathrm{poly}(n)$-precision, $O(1)$\nlayers, hidden dimension $d \\leq O(n)$ cannot solve the Arithmetic formula\nevaluation problem or the Boolean formula value problem. This result\nsignificantly demonstrates the fundamental limitation of the expressivity of\nthe $\\mathsf{RoPE}$-based Transformer architecture, although it achieves giant\nempirical success. Our theoretical result not only establishes the complexity\nbound but also may instruct further work on the $\\mathsf{RoPE}$-based\nTransformer.\n","authors":["Bo Chen","Xiaoyu Li","Yingyu Liang","Jiangxuan Long","Zhenmei Shi","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2411.07602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02490v4","updated":"2024-12-01T05:46:03Z","published":"2023-08-04T17:59:47Z","title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities","summary":"  We propose MM-Vet, an evaluation benchmark that examines large multimodal\nmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown various\nintriguing abilities, such as solving math problems written on the blackboard,\nreasoning about events and celebrities in news images, and explaining visual\njokes. Rapid model advancements pose challenges to evaluation benchmark\ndevelopment. Problems include: (1) How to systematically structure and evaluate\nthe complicated multimodal tasks; (2) How to design evaluation metrics that\nwork well across question and answer types; and (3) How to give model insights\nbeyond a simple performance ranking. To this end, we present MM-Vet, designed\nbased on the insight that the intriguing ability to solve complicated tasks is\noften achieved by a generalist model being able to integrate different core\nvision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and\nexamines the 16 integrations of interest derived from the capability\ncombination. For evaluation metrics, we propose an LLM-based evaluator for\nopen-ended outputs. The evaluator enables the evaluation across different\nquestion types and answer styles, resulting in a unified scoring metric. We\nevaluate representative LMMs on MM-Vet, providing insights into the\ncapabilities of different LMM system paradigms and models.\n","authors":["Weihao Yu","Zhengyuan Yang","Linjie Li","Jianfeng Wang","Kevin Lin","Zicheng Liu","Xinchao Wang","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2308.02490v4.pdf","comment":"ICML 2024. Code, data and leaderboard:\n  https://github.com/yuweihao/MM-Vet"},{"id":"http://arxiv.org/abs/2405.18560v2","updated":"2024-12-01T05:22:22Z","published":"2024-05-28T20:10:06Z","title":"Potential Field Based Deep Metric Learning","summary":"  Deep metric learning (DML) involves training a network to learn a\nsemantically meaningful representation space. Many current approaches mine\nn-tuples of examples and model interactions within each tuplets. We present a\nnovel, compositional DML model, inspired by electrostatic fields in physics\nthat, instead of in tuples, represents the influence of each example\n(embedding) by a continuous potential field, and superposes the fields to\nobtain their combined global potential field. We use attractive/repulsive\npotential fields to represent interactions among embeddings from images of the\nsame/different classes. Contrary to typical learning methods, where mutual\ninfluence of samples is proportional to their distance, we enforce reduction in\nsuch influence with distance, leading to a decaying field. We show that such\ndecay helps improve performance on real world datasets with large intra-class\nvariations and label noise. Like other proxy-based methods, we also use proxies\nto succinctly represent sub-populations of examples. We evaluate our method on\nthree standard DML benchmarks- Cars-196, CUB-200-2011, and SOP datasets where\nit outperforms state-of-the-art baselines.\n","authors":["Shubhang Bhatnagar","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2405.18560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09856v2","updated":"2024-12-01T05:18:12Z","published":"2024-11-15T00:31:45Z","title":"InvestESG: A multi-agent reinforcement learning benchmark for studying\n  climate investment as a social dilemma","summary":"  InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark\ndesigned to study the impact of Environmental, Social, and Governance (ESG)\ndisclosure mandates on corporate climate investments. Supported by both PyTorch\nand JAX implementation, the benchmark models an intertemporal social dilemma\nwhere companies balance short-term profit losses from climate mitigation\nefforts and long-term benefits from reducing climate risk, while ESG-conscious\ninvestors attempt to influence corporate behavior through their investment\ndecisions, in a scalable and hardware-accelerated manner. Companies allocate\ncapital across mitigation, greenwashing, and resilience, with varying\nstrategies influencing climate outcomes and investor preferences. Our\nexperiments show that without ESG-conscious investors with sufficient capital,\ncorporate mitigation efforts remain limited under the disclosure mandate.\nHowever, when a critical mass of investors prioritizes ESG, corporate\ncooperation increases, which in turn reduces climate risks and enhances\nlong-term financial stability. Additionally, providing more information about\nglobal climate risks encourages companies to invest more in mitigation, even\nwithout investor involvement. Our findings align with empirical research using\nreal-world data, highlighting MARL's potential to inform policy by providing\ninsights into large-scale socio-economic challenges through efficient testing\nof alternative policy and market designs.\n","authors":["Xiaoxuan Hou","Jiayi Yuan","Joel Z. Leibo","Natasha Jaques"],"pdf_url":"https://arxiv.org/pdf/2411.09856v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11858v4","updated":"2024-12-01T05:15:11Z","published":"2024-02-19T06:00:35Z","title":"Stochastic Hessian Fittings with Lie Groups","summary":"  This report studies the fitting of Hessian or its inverse for stochastic\noptimizations using a Hessian fitting criterion from the preconditioned\nstochastic gradient descent (PSGD) method, which is intimately related to many\ncommonly used second-order and adaptive gradient optimizers, e.g., BFGS,\nGaussian-Newton algorithm, natural gradient descent, AdaGrad, etc. Our analyses\nreveal the efficiency and reliability differences among a wide range of\npreconditioner fitting methods, from closed-form to iterative solutions, using\nHessian-vector products or stochastic gradients only, with Hessian fittings in\nthe Euclidean space, the manifold of symmetric positive definite (SPL)\nmatrices, to a variety of Lie groups. The most intriguing discovery is that the\nHessian fitting itself as an optimization problem is strongly convex under mild\nconditions in certain general Lie groups. This discovery turns Hessian fitting\ninto a well-behaved Lie group optimization problem and facilitates the designs\nof highly efficient and elegant Lie group sparse preconditioner fitting methods\nfor large-scale stochastic optimizations.\n","authors":["Xi-Lin Li"],"pdf_url":"https://arxiv.org/pdf/2402.11858v4.pdf","comment":"14 pages; 6 figures; 3 tables; code\n  https://github.com/lixilinx/psgd_torch"},{"id":"http://arxiv.org/abs/2407.05593v4","updated":"2024-12-01T04:36:57Z","published":"2024-07-08T04:15:43Z","title":"Unmasking Trees for Tabular Data","summary":"  Despite much work on advanced deep learning and generative modeling\ntechniques for tabular data generation and imputation, traditional methods have\ncontinued to win on imputation benchmarks. We herein present UnmaskingTrees, a\nsimple method for tabular imputation (and generation) employing\ngradient-boosted decision trees which are used to incrementally unmask\nindividual features. This approach offers state-of-the-art performance on\nimputation, and on generation given training data with missingness; and it has\ncompetitive performance on vanilla generation. To solve the conditional\ngeneration subproblem, we propose a tabular probabilistic prediction method,\nBaltoBot, which fits a balanced tree of boosted tree classifiers. Unlike older\nmethods, it requires no parametric assumption on the conditional distribution,\naccommodating features with multimodal distributions; unlike newer diffusion\nmethods, it offers fast sampling, closed-form density estimation, and flexible\nhandling of discrete variables. We finally consider our two approaches as\nmeta-algorithms, demonstrating in-context learning-based generative modeling\nwith TabPFN.\n","authors":["Calvin McCarter"],"pdf_url":"https://arxiv.org/pdf/2407.05593v4.pdf","comment":"v0.3.0 of UnmaskingTrees software"},{"id":"http://arxiv.org/abs/2408.07712v2","updated":"2024-12-01T04:34:53Z","published":"2024-08-13T23:08:06Z","title":"Introduction to Reinforcement Learning","summary":"  Reinforcement Learning (RL), a subfield of Artificial Intelligence (AI),\nfocuses on training agents to make decisions by interacting with their\nenvironment to maximize cumulative rewards. This paper provides an overview of\nRL, covering its core concepts, methodologies, and resources for further\nlearning. It offers a thorough explanation of fundamental components such as\nstates, actions, policies, and reward signals, ensuring readers develop a solid\nfoundational understanding. Additionally, the paper presents a variety of RL\nalgorithms, categorized based on the key factors such as model-free,\nmodel-based, value-based, policy-based, and other key factors. Resources for\nlearning and implementing RL, such as books, courses, and online communities\nare also provided. By offering a clear, structured introduction, this paper\naims to simplify the complexities of RL for beginners, providing a\nstraightforward pathway to understanding and applying real-time techniques.\n","authors":["Majid Ghasemi","Dariush Ebrahimi"],"pdf_url":"https://arxiv.org/pdf/2408.07712v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2401.17133v2","updated":"2024-12-01T04:06:27Z","published":"2024-01-30T16:07:44Z","title":"SongBsAb: A Dual Prevention Approach against Singing Voice Conversion\n  based Illegal Song Covers","summary":"  Singing voice conversion (SVC) automates song covers by converting a source\nsinging voice from a source singer into a new singing voice with the same\nlyrics and melody as the source, but sounds like being covered by the target\nsinger of some given target singing voices. However, it raises serious concerns\nabout copyright and civil right infringements. We propose SongBsAb, the first\nproactive approach to tackle SVC-based illegal song covers. SongBsAb adds\nperturbations to singing voices before releasing them, so that when they are\nused, the process of SVC will be interfered, leading to unexpected singing\nvoices. Perturbations are carefully crafted to (1) provide a dual prevention,\ni.e., preventing the singing voice from being used as the source and target\nsinging voice in SVC, by proposing a gender-transformation loss and a high/low\nhierarchy multi-target loss, respectively; and (2) be harmless, i.e., no\nside-effect on the enjoyment of protected songs, by refining a psychoacoustic\nmodel-based loss with the backing track as an additional masker, a unique\naccompanying element for singing voices compared to ordinary speech voices. We\nalso adopt a frame-level interaction reduction-based loss and encoder ensemble\nto enhance the transferability of SongBsAb to unknown SVC models. We\ndemonstrate the prevention effectiveness, harmlessness, and robustness of\nSongBsAb on five diverse and promising SVC models, using both English and\nChinese datasets, and both objective and human study-based subjective metrics.\nOur work fosters an emerging research direction for mitigating illegal\nautomated song covers.\n","authors":["Guangke Chen","Yedi Zhang","Fu Song","Ting Wang","Xiaoning Du","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2401.17133v2.pdf","comment":"In Proceedings of the 32nd Network and Distributed System Security\n  (NDSS) Symposium 2025"},{"id":"http://arxiv.org/abs/2212.03853v6","updated":"2024-12-01T04:02:40Z","published":"2022-12-05T12:33:26Z","title":"Clustering with Neural Network and Index","summary":"  A new model called Clustering with Neural Network and Index (CNNI) is\nintroduced. CNNI uses a Neural Network to cluster data points. Training of the\nNeural Network mimics supervised learning, with an internal clustering\nevaluation index acting as the loss function. An experiment is conducted to\ntest the feasibility of the new model, and compared with results of other\nclustering models like K-means and Gaussian Mixture Model (GMM). The result\nshows CNNI can work properly for clustering data; CNNI equipped with MMJ-SC,\nachieves the first parametric (inductive) clustering model that can deal with\nnon-convex shaped (non-flat geometry) data.\n","authors":["Gangli Liu"],"pdf_url":"https://arxiv.org/pdf/2212.03853v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21712v2","updated":"2024-12-01T03:54:57Z","published":"2024-10-29T03:54:48Z","title":"Sliced-Wasserstein-based Anomaly Detection and Open Dataset for\n  Localized Critical Peak Rebates","summary":"  In this work, we present a new unsupervised anomaly (outlier) detection (AD)\nmethod using the sliced-Wasserstein metric. This filtering technique is\nconceptually interesting for MLOps pipelines deploying machine learning models\nin critical sectors, e.g., energy, as it offers a conservative data selection.\nAdditionally, we open the first dataset showcasing localized critical peak\nrebate demand response in a northern climate. We demonstrate the capabilities\nof our method on synthetic datasets as well as standard AD datasets and use it\nin the making of a first benchmark for our open-source localized critical peak\nrebate dataset.\n","authors":["Julien Pallage","Bertrand Scherrer","Salma Naccache","Christophe Bélanger","Antoine Lesage-Landry"],"pdf_url":"https://arxiv.org/pdf/2410.21712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11920v3","updated":"2024-12-01T03:49:57Z","published":"2024-06-17T07:22:51Z","title":"Job-SDF: A Multi-Granularity Dataset for Job Skill Demand Forecasting\n  and Benchmarking","summary":"  In a rapidly evolving job market, skill demand forecasting is crucial as it\nenables policymakers and businesses to anticipate and adapt to changes,\nensuring that workforce skills align with market needs, thereby enhancing\nproductivity and competitiveness. Additionally, by identifying emerging skill\nrequirements, it directs individuals towards relevant training and education\nopportunities, promoting continuous self-learning and development. However, the\nabsence of comprehensive datasets presents a significant challenge, impeding\nresearch and the advancement of this field. To bridge this gap, we present\nJob-SDF, a dataset designed to train and benchmark job-skill demand forecasting\nmodels. Based on 10.35 million public job advertisements collected from major\nonline recruitment platforms in China between 2021 and 2023, this dataset\nencompasses monthly recruitment demand for 2,324 types of skills across 521\ncompanies. Our dataset uniquely enables evaluating skill demand forecasting\nmodels at various granularities, including occupation, company, and regional\nlevels. We benchmark a range of models on this dataset, evaluating their\nperformance in standard scenarios, in predictions focused on lower value\nranges, and in the presence of structural breaks, providing new insights for\nfurther research. Our code and dataset are publicly accessible via the\nhttps://github.com/Job-SDF/benchmark.\n","authors":["Xi Chen","Chuan Qin","Chuyu Fang","Chao Wang","Chen Zhu","Fuzhen Zhuang","Hengshu Zhu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.11920v3.pdf","comment":"NeurIPS 2024 Accepted"},{"id":"http://arxiv.org/abs/2410.23686v2","updated":"2024-12-01T03:08:21Z","published":"2024-10-31T07:20:40Z","title":"Towards Dynamic Message Passing on Graphs","summary":"  Message passing plays a vital role in graph neural networks (GNNs) for\neffective feature learning. However, the over-reliance on input topology\ndiminishes the efficacy of message passing and restricts the ability of GNNs.\nDespite efforts to mitigate the reliance, existing study encounters\nmessage-passing bottlenecks or high computational expense problems, which\ninvokes the demands for flexible message passing with low complexity. In this\npaper, we propose a novel dynamic message-passing mechanism for GNNs. It\nprojects graph nodes and learnable pseudo nodes into a common space with\nmeasurable spatial relations between them. With nodes moving in the space,\ntheir evolving relations facilitate flexible pathway construction for a dynamic\nmessage-passing process. Associating pseudo nodes to input graphs with their\nmeasured relations, graph nodes can communicate with each other intermediately\nthrough pseudo nodes under linear complexity. We further develop a GNN model\nnamed $\\mathtt{\\mathbf{N^2}}$ based on our dynamic message-passing mechanism.\n$\\mathtt{\\mathbf{N^2}}$ employs a single recurrent layer to recursively\ngenerate the displacements of nodes and construct optimal dynamic pathways.\nEvaluation on eighteen benchmarks demonstrates the superior performance of\n$\\mathtt{\\mathbf{N^2}}$ over popular GNNs. $\\mathtt{\\mathbf{N^2}}$ successfully\nscales to large-scale benchmarks and requires significantly fewer parameters\nfor graph classification with the shared recurrent layer.\n","authors":["Junshu Sun","Chenxue Yang","Xiangyang Ji","Qingming Huang","Shuhui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23686v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15876v2","updated":"2024-12-01T02:38:17Z","published":"2024-10-21T10:57:45Z","title":"FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL","summary":"  Multi-agent reinforcement learning has demonstrated significant potential in\naddressing complex cooperative tasks across various real-world applications.\nHowever, existing MARL approaches often rely on the restrictive assumption that\nthe number of entities (e.g., agents, obstacles) remains constant between\ntraining and inference. This overlooks scenarios where entities are dynamically\nremoved or added during the inference trajectory -- a common occurrence in\nreal-world environments like search and rescue missions and dynamic combat\nsituations. In this paper, we tackle the challenge of intra-trajectory dynamic\nentity composition under zero-shot out-of-domain (OOD) generalization, where\nsuch dynamic changes cannot be anticipated beforehand. Our empirical studies\nreveal that existing MARL methods suffer significant performance degradation\nand increased uncertainty in these scenarios. In response, we propose\nFlickerFusion, a novel OOD generalization method that acts as a universally\napplicable augmentation technique for MARL backbone methods. FlickerFusion\nstochastically drops out parts of the observation space, emulating being\nin-domain when inferenced OOD. The results show that FlickerFusion not only\nachieves superior inference rewards but also uniquely reduces uncertainty\nvis-\\`a-vis the backbone, compared to existing methods. Benchmarks,\nimplementations, and model weights are organized and open-sourced at\nflickerfusion305.github.io, accompanied by ample demo video renderings.\n","authors":["Woosung Koh","Wonbeen Oh","Siyeol Kim","Suhin Shin","Hyeongjin Kim","Jaein Jang","Junghyun Lee","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2410.15876v2.pdf","comment":"NeurIPS '24 Open-World Agents Workshop (v2: minor revision)"},{"id":"http://arxiv.org/abs/2410.21107v2","updated":"2024-12-01T02:36:26Z","published":"2024-10-28T15:11:23Z","title":"Tree-Wasserstein Distance for High Dimensional Data with a Latent\n  Feature Hierarchy","summary":"  Finding meaningful distances between high-dimensional data samples is an\nimportant scientific task. To this end, we propose a new tree-Wasserstein\ndistance (TWD) for high-dimensional data with two key aspects. First, our TWD\nis specifically designed for data with a latent feature hierarchy, i.e., the\nfeatures lie in a hierarchical space, in contrast to the usual focus on\nembedding samples in hyperbolic space. Second, while the conventional use of\nTWD is to speed up the computation of the Wasserstein distance, we use its\ninherent tree as a means to learn the latent feature hierarchy. The key idea of\nour method is to embed the features into a multi-scale hyperbolic space using\ndiffusion geometry and then present a new tree decoding method by establishing\nanalogies between the hyperbolic embedding and trees. We show that our TWD\ncomputed based on data observations provably recovers the TWD defined with the\nlatent feature hierarchy and that its computation is efficient and scalable. We\nshowcase the usefulness of the proposed TWD in applications to word-document\nand single-cell RNA-sequencing datasets, demonstrating its advantages over\nexisting TWDs and methods based on pre-trained models.\n","authors":["Ya-Wei Eileen Lin","Ronald R. Coifman","Gal Mishne","Ronen Talmon"],"pdf_url":"https://arxiv.org/pdf/2410.21107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14803v4","updated":"2024-12-01T02:09:21Z","published":"2024-10-18T18:19:56Z","title":"DistRL: An Asynchronous Distributed Reinforcement Learning Framework for\n  On-Device Control Agents","summary":"  On-device control agents, especially on mobile devices, are responsible for\noperating mobile devices to fulfill users' requests, enabling seamless and\nintuitive interactions. Integrating Multimodal Large Language Models (MLLMs)\ninto these agents enhances their ability to understand and execute complex\ncommands, thereby improving user experience. However, fine-tuning MLLMs for\non-device control presents significant challenges due to limited data\navailability and inefficient online training processes. This paper introduces\nDistRL, a novel framework designed to enhance the efficiency of online RL\nfine-tuning for mobile device control agents. DistRL employs centralized\ntraining and decentralized data acquisition to ensure efficient fine-tuning in\nthe context of dynamic online interactions. Additionally, the framework is\nbacked by our tailor-made RL algorithm, which effectively balances exploration\nwith the prioritized utilization of collected data to ensure stable and robust\ntraining. Our experiments show that, on average, DistRL delivers a 3X\nimprovement in training efficiency and enables training data collection 2.4X\nfaster than the leading synchronous multi-machine methods. Notably, after\ntraining, DistRL achieves a 20% relative improvement in success rate compared\nto state-of-the-art methods on general Android tasks from an open benchmark,\nsignificantly outperforming existing approaches while maintaining the same\ntraining time. These results validate DistRL as a scalable and efficient\nsolution, offering substantial improvements in both training efficiency and\nagent performance for real-world, in-the-wild device control tasks.\n","authors":["Taiyi Wang","Zhihao Wu","Jianheng Liu","Jianye Hao","Jun Wang","Kun Shao"],"pdf_url":"https://arxiv.org/pdf/2410.14803v4.pdf","comment":"Paper and Appendix, 26 pages"},{"id":"http://arxiv.org/abs/2309.17249v3","updated":"2024-12-01T01:36:50Z","published":"2023-09-29T13:55:45Z","title":"Batch Calibration: Rethinking Calibration for In-Context Learning and\n  Prompt Engineering","summary":"  Prompting and in-context learning (ICL) have become efficient learning\nparadigms for large language models (LLMs). However, LLMs suffer from prompt\nbrittleness and various bias factors in the prompt, including but not limited\nto the formatting, the choice verbalizers, and the ICL examples. To address\nthis problem that results in unexpected performance degradation, calibration\nmethods have been developed to mitigate the effects of these biases while\nrecovering LLM performance. In this work, we first conduct a systematic\nanalysis of the existing calibration methods, where we both provide a unified\nview and reveal the failure cases. Inspired by these analyses, we propose Batch\nCalibration (BC), a simple yet intuitive method that controls the contextual\nbias from the batched input, unifies various prior approaches, and effectively\naddresses the aforementioned issues. BC is zero-shot, inference-only, and\nincurs negligible additional costs. In the few-shot setup, we further extend BC\nto allow it to learn the contextual bias from labeled data. We validate the\neffectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate\nstate-of-the-art performance over previous calibration baselines across more\nthan 10 natural language understanding and image classification tasks.\n","authors":["Han Zhou","Xingchen Wan","Lev Proleev","Diana Mincu","Jilin Chen","Katherine Heller","Subhrajit Roy"],"pdf_url":"https://arxiv.org/pdf/2309.17249v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2405.11848v2","updated":"2024-12-01T00:49:32Z","published":"2024-05-20T07:47:06Z","title":"Alternators For Sequence Modeling","summary":"  This paper introduces alternators, a novel family of non-Markovian dynamical\nmodels for sequences. An alternator features two neural networks: the\nobservation trajectory network (OTN) and the feature trajectory network (FTN).\nThe OTN and the FTN work in conjunction, alternating between outputting samples\nin the observation space and some feature space, respectively, over a cycle.\nThe parameters of the OTN and the FTN are not time-dependent and are learned\nvia a minimum cross-entropy criterion over the trajectories. Alternators are\nversatile. They can be used as dynamical latent-variable generative models or\nas sequence-to-sequence predictors. Alternators can uncover the latent dynamics\nunderlying complex sequential data, accurately forecast and impute missing\ndata, and sample new trajectories. We showcase the capabilities of alternators\nin three applications. We first used alternators to model the Lorenz equations,\noften used to describe chaotic behavior. We then applied alternators to\nNeuroscience, to map brain activity to physical activity. Finally, we applied\nalternators to Climate Science, focusing on sea-surface temperature\nforecasting. In all our experiments, we found alternators are stable to train,\nfast to sample from, yield high-quality generated samples and latent variables,\nand often outperform strong baselines such as Mambas, neural ODEs, and\ndiffusion models in the domains we studied.\n","authors":["Mohammad Reza Rezaei","Adji Bousso Dieng"],"pdf_url":"https://arxiv.org/pdf/2405.11848v2.pdf","comment":"A new versatile family of sequence models that can be used for both\n  generative modeling and supervised learning. The codebase will be made\n  available upon publication. This paper is dedicated to Thomas Sankara"},{"id":"http://arxiv.org/abs/2401.10989v3","updated":"2024-12-01T00:44:02Z","published":"2024-01-19T19:04:23Z","title":"Provably Scalable Black-Box Variational Inference with Structured\n  Variational Families","summary":"  Variational families with full-rank covariance approximations are known not\nto work well in black-box variational inference (BBVI), both empirically and\ntheoretically. In fact, recent computational complexity results for BBVI have\nestablished that full-rank variational families scale poorly with the\ndimensionality of the problem compared to e.g. mean-field families. This is\nparticularly critical to hierarchical Bayesian models with local variables;\ntheir dimensionality increases with the size of the datasets. Consequently, one\ngets an iteration complexity with an explicit $\\mathcal{O}(N^2)$ dependence on\nthe dataset size $N$. In this paper, we explore a theoretical middle ground\nbetween mean-field variational families and full-rank families: structured\nvariational families. We rigorously prove that certain scale matrix structures\ncan achieve a better iteration complexity of $\\mathcal{O}\\left(N\\right)$,\nimplying better scaling with respect to $N$. We empirically verify our\ntheoretical results on large-scale hierarchical models.\n","authors":["Joohwan Ko","Kyurae Kim","Woo Chang Kim","Jacob R. Gardner"],"pdf_url":"https://arxiv.org/pdf/2401.10989v3.pdf","comment":"Accepted to ICML'24; v3: fixed typos"},{"id":"http://arxiv.org/abs/2309.03468v2","updated":"2024-12-01T00:09:44Z","published":"2023-09-07T03:33:49Z","title":"Support-Set Context Matters for Bongard Problems","summary":"  Current machine learning methods struggle to solve Bongard problems, which\nare a type of IQ test that requires deriving an abstract \"concept\" from a set\nof positive and negative \"support\" images, and then classifying whether or not\na new query image depicts the key concept. On Bongard-HOI, a benchmark for\nnatural-image Bongard problems, most existing methods have reached at best 69%\naccuracy (where chance is 50%). Low accuracy is often attributed to neural\nnets' lack of ability to find human-like symbolic rules. In this work, we point\nout that many existing methods are forfeiting accuracy due to a much simpler\nproblem: they do not adapt image features given information contained in the\nsupport set as a whole, and rely instead on information extracted from\nindividual supports. This is a critical issue, because the \"key concept\" in a\ntypical Bongard problem can often only be distinguished using multiple\npositives and multiple negatives. We explore simple methods to incorporate this\ncontext and show substantial gains over prior works, leading to new\nstate-of-the-art accuracy on Bongard-LOGO (75.3%) and Bongard-HOI (76.4%)\ncompared to methods with equivalent vision backbone architectures and strong\nperformance on the original Bongard problem set (60.8%).\n","authors":["Nikhil Raghuraman","Adam W. Harley","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2309.03468v2.pdf","comment":"TMLR October 2024. Code:\n  https://github.com/nraghuraman/bongard-context"}],"Multimedia":[{"id":"http://arxiv.org/abs/2308.05037v3","updated":"2024-12-01T15:17:03Z","published":"2023-08-09T16:09:44Z","title":"Separate Anything You Describe","summary":"  Language-queried audio source separation (LASS) is a new paradigm for\ncomputational auditory scene analysis (CASA). LASS aims to separate a target\nsound from an audio mixture given a natural language query, which provides a\nnatural and scalable interface for digital audio applications. Recent works on\nLASS, despite attaining promising separation performance on specific sources\n(e.g., musical instruments, limited classes of audio events), are unable to\nseparate audio concepts in the open domain. In this work, we introduce\nAudioSep, a foundation model for open-domain audio source separation with\nnatural language queries. We train AudioSep on large-scale multimodal datasets\nand extensively evaluate its capabilities on numerous tasks including audio\nevent separation, musical instrument separation, and speech enhancement.\nAudioSep demonstrates strong separation performance and impressive zero-shot\ngeneralization ability using audio captions or text labels as queries,\nsubstantially outperforming previous audio-queried and language-queried sound\nseparation models. For reproducibility of this work, we will release the source\ncode, evaluation benchmark and pre-trained model at:\nhttps://github.com/Audio-AGI/AudioSep.\n","authors":["Xubo Liu","Qiuqiang Kong","Yan Zhao","Haohe Liu","Yi Yuan","Yuzhuo Liu","Rui Xia","Yuxuan Wang","Mark D. Plumbley","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2308.05037v3.pdf","comment":"Code, benchmark and pre-trained models:\n  https://github.com/Audio-AGI/AudioSep"},{"id":"http://arxiv.org/abs/2401.17133v2","updated":"2024-12-01T04:06:27Z","published":"2024-01-30T16:07:44Z","title":"SongBsAb: A Dual Prevention Approach against Singing Voice Conversion\n  based Illegal Song Covers","summary":"  Singing voice conversion (SVC) automates song covers by converting a source\nsinging voice from a source singer into a new singing voice with the same\nlyrics and melody as the source, but sounds like being covered by the target\nsinger of some given target singing voices. However, it raises serious concerns\nabout copyright and civil right infringements. We propose SongBsAb, the first\nproactive approach to tackle SVC-based illegal song covers. SongBsAb adds\nperturbations to singing voices before releasing them, so that when they are\nused, the process of SVC will be interfered, leading to unexpected singing\nvoices. Perturbations are carefully crafted to (1) provide a dual prevention,\ni.e., preventing the singing voice from being used as the source and target\nsinging voice in SVC, by proposing a gender-transformation loss and a high/low\nhierarchy multi-target loss, respectively; and (2) be harmless, i.e., no\nside-effect on the enjoyment of protected songs, by refining a psychoacoustic\nmodel-based loss with the backing track as an additional masker, a unique\naccompanying element for singing voices compared to ordinary speech voices. We\nalso adopt a frame-level interaction reduction-based loss and encoder ensemble\nto enhance the transferability of SongBsAb to unknown SVC models. We\ndemonstrate the prevention effectiveness, harmlessness, and robustness of\nSongBsAb on five diverse and promising SVC models, using both English and\nChinese datasets, and both objective and human study-based subjective metrics.\nOur work fosters an emerging research direction for mitigating illegal\nautomated song covers.\n","authors":["Guangke Chen","Yedi Zhang","Fu Song","Ting Wang","Xiaoning Du","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2401.17133v2.pdf","comment":"In Proceedings of the 32nd Network and Distributed System Security\n  (NDSS) Symposium 2025"}]},"2024-11-30T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.05804v6","updated":"2024-11-30T22:38:57Z","published":"2024-06-09T14:42:55Z","title":"A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning","summary":"  Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey.\n","authors":["Xinzhe Li"],"pdf_url":"https://arxiv.org/pdf/2406.05804v6.pdf","comment":"CoLing 2025 Camera Ready (extended to 9 pages)"},{"id":"http://arxiv.org/abs/2409.14165v3","updated":"2024-11-30T22:21:30Z","published":"2024-09-21T15:07:37Z","title":"A Survey on Large Language Model-empowered Autonomous Driving","summary":"  Artificial intelligence (AI) plays a crucial role in autonomous driving (AD)\nresearch, propelling its development towards intelligence and efficiency.\nCurrently, the development of AD technology follows two main technical paths:\nmodularization and end-to-end. Modularization decompose the driving task into\nmodules such as perception, prediction, planning, and control, and train them\nseparately. Due to the inconsistency of training objectives between modules,\nthe integrated effect suffers from bias. End-to-end attempts to address this\nissue by utilizing a single model that directly maps from sensor data to\ncontrol signals. This path has limited learning capabilities in a comprehensive\nset of features and struggles to handle unpredictable long-tail events and\ncomplex urban traffic scenarios. In the face of challenges encountered in both\npaths, many researchers believe that large language models (LLMs) with powerful\nreasoning capabilities and extensive knowledge understanding may be the\nsolution, expecting LLMs to provide AD systems with deeper levels of\nunderstanding and decision-making capabilities. In light of the challenges\nfaced by both paths, many researchers believe that LLMs, with their powerful\nreasoning abilities and extensive knowledge, could offer a solution. To\nunderstand if LLMs could enhance AD, this paper conducts a thorough analysis of\nthe potential applications of LLMs in AD systems, including exploring their\noptimization strategies in both modular and end-to-end approaches, with a\nparticular focus on how LLMs can tackle the problems and challenges present in\ncurrent solutions. Furthermore, we discuss an important question: Can LLM-based\nartificial general intelligence (AGI) be a key to achieve high-level AD? We\nfurther analyze the potential limitations and challenges that LLMs may\nencounter in promoting the development of AD technology.\n","authors":["Yuxuan Zhu","Shiyi Wang","Wenqing Zhong","Nianchen Shen","Yunqi Li","Siqi Wang","Zhiheng Li","Cathy Wu","Zhengbing He","Li Li"],"pdf_url":"https://arxiv.org/pdf/2409.14165v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11796v3","updated":"2024-11-30T22:01:07Z","published":"2024-08-21T17:38:48Z","title":"LLM Pruning and Distillation in Practice: The Minitron Approach","summary":"  We present a comprehensive report on compressing the Llama 3.1 8B and Mistral\nNeMo 12B models to 4B and 8B parameters, respectively, using pruning and\ndistillation. We explore two distinct pruning strategies: (1) depth pruning and\n(2) joint hidden/attention/MLP (width) pruning, and evaluate the results on\ncommon benchmarks from the LM Evaluation Harness. The models are then aligned\nwith NeMo Aligner and tested in instruct-tuned versions. This approach produces\na compelling 4B model from Llama 3.1 8B and a state-of-the-art\nMistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo\n12B. We found that with no access to the original data, it is beneficial to\nslightly fine-tune teacher models on the distillation dataset. We open-source\nour base model weights on Hugging Face with a permissive license.\n","authors":["Sharath Turuvekere Sreenivas","Saurav Muralidharan","Raviraj Joshi","Marcin Chochowski","Mostofa Patwary","Pavlo Molchanov","Mohammad Shoeybi","Jan Kautz","Ameya Sunil Mahabaleshwarkar","Gerald Shen","Jiaqi Zeng","Oleksii Kuchaiev","Zijia Chen","Yoshi Suhara","Shizhe Diao","Chenhan Yu","Wei-Chun Chen","Hayley Ross","Daniel Korzekwa","Oluwatobi Olabiyi","Ashwath Aithal","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2408.11796v3.pdf","comment":"v3: Update author list, other changes"},{"id":"http://arxiv.org/abs/2406.02532v3","updated":"2024-11-30T21:33:59Z","published":"2024-06-04T17:53:36Z","title":"SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices","summary":"  As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.\n","authors":["Ruslan Svirschevski","Avner May","Zhuoming Chen","Beidi Chen","Zhihao Jia","Max Ryabinin"],"pdf_url":"https://arxiv.org/pdf/2406.02532v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10492v2","updated":"2024-11-30T21:27:19Z","published":"2024-06-15T04:09:31Z","title":"Large Language Models as Interpolated and Extrapolated Event Predictors","summary":"  Salient facts of sociopolitical events are distilled into quadruples\nfollowing a format of subject, relation, object, and timestamp. Machine\nlearning methods, such as graph neural networks (GNNs) and recurrent neural\nnetworks (RNNs), have been built to make predictions and infer relations on the\nquadruple-based knowledge graphs (KGs). In many applications, quadruples are\nextended to quintuples with auxiliary attributes such as text summaries that\ndescribe the quadruple events. In this paper, we comprehensively investigate\nhow large language models (LLMs) streamline the design of event prediction\nframeworks using quadruple-based or quintuple-based data while maintaining\ncompetitive accuracy. We propose LEAP, a unified framework that leverages large\nlanguage models as event predictors. Specifically, we develop multiple prompt\ntemplates to frame the object prediction (OP) task as a standard\nquestion-answering (QA) task, suitable for instruction fine-tuning with an\nencoder-decoder LLM. For multi-event forecasting (MEF) task, we design a simple\nyet effective prompt template for each event quintuple. This novel approach\nremoves the need for GNNs and RNNs, instead utilizing an encoder-only LLM to\ngenerate fixed intermediate embeddings, which are processed by a customized\ndownstream head with a self-attention mechanism to predict potential relation\noccurrences in the future. Extensive experiments on multiple real-world\ndatasets using various evaluation metrics validate the effectiveness of our\napproach.\n","authors":["Libo Zhang","Yue Ning"],"pdf_url":"https://arxiv.org/pdf/2406.10492v2.pdf","comment":"11 pages, 3 figures, 10 tables"},{"id":"http://arxiv.org/abs/2407.16607v4","updated":"2024-11-30T19:31:38Z","published":"2024-07-23T16:13:22Z","title":"Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?","summary":"  The pretraining data of today's strongest language models is opaque; in\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation: byte-pair encoding (BPE) tokenizers, used by the vast majority of\nmodern language models. Our key insight is that the ordered list of merge rules\nlearned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data. Given a tokenizer's merge list along with\nexample data for each category of interest, we formulate a linear program that\nsolves for the proportion of each category in the tokenizer's training set. In\ncontrolled experiments, we show that our attack recovers mixture ratios with\nhigh precision for tokenizers trained on known mixtures of natural languages,\nprogramming languages, and data sources. We then apply our approach to\noff-the-shelf tokenizers released with recent LMs. We confirm much publicly\ndisclosed information about these models, and also make several new inferences:\nGPT-4o and Mistral NeMo's tokenizers are much more multilingual than their\npredecessors, training on 39% and 47% non-English language data, respectively;\nLlama 3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use;\nGPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). We\nhope our work sheds light on current design practices for pretraining data, and\ninspires continued research into data mixture inference for LMs.\n","authors":["Jonathan Hayase","Alisa Liu","Yejin Choi","Sewoong Oh","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2407.16607v4.pdf","comment":"NeurIPS camera-ready, code at\n  https://github.com/alisawuffles/tokenizer-attack"},{"id":"http://arxiv.org/abs/2309.00378v5","updated":"2024-11-30T19:08:48Z","published":"2023-09-01T10:27:04Z","title":"Long-Term Ad Memorability: Understanding & Generating Memorable Ads","summary":"  Despite the importance of long-term memory in marketing and brand building,\nuntil now, there has been no large-scale study on the memorability of ads. All\nprevious memorability studies have been conducted on short-term recall on\nspecific content types like action videos. On the other hand, long-term\nmemorability is crucial for the advertising industry, and ads are almost always\nhighly multimodal. Therefore, we release the first memorability dataset,\nLAMBDA, consisting of 1749 participants and 2205 ads covering 276 brands.\nRunning statistical tests over different participant subpopulations and ad\ntypes, we find many interesting insights into what makes an ad memorable, e.g.,\nfast-moving ads are more memorable than those with slower scenes; people who\nuse ad-blockers remember a lower number of ads than those who don't. Next, we\npresent a model, Henry, to predict the memorability of a content. Henry\nachieves state-of-the-art performance across all prominent literature\nmemorability datasets. It shows strong generalization performance with better\nresults in 0-shot on unseen datasets. Finally, with the intent of memorable ad\ngeneration, we present a scalable method to build a high-quality memorable ad\ngeneration model by leveraging automatically annotated data. Our approach, SEED\n(Self rEwarding mEmorability Modeling), starts with a language model trained on\nLAMBDA as seed data and progressively trains an LLM to generate more memorable\nads. We show that the generated advertisements have 44% higher memorability\nscores than the original ads. We release this large-scale ad dataset,\nUltraLAMBDA, consisting of 5 million ads. Our code and the datasets, LAMBDA and\nUltraLAMBDA, are open-sourced at\nhttps://behavior-in-the-wild.github.io/memorability.\n","authors":["Harini SI","Somesh Singh","Yaman K Singla","Aanisha Bhattacharyya","Veeky Baths","Changyou Chen","Rajiv Ratn Shah","Balaji Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2309.00378v5.pdf","comment":"Published in WACV-2025"},{"id":"http://arxiv.org/abs/2410.05168v3","updated":"2024-11-30T16:10:21Z","published":"2024-10-07T16:25:39Z","title":"ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation","summary":"  Reranking documents based on their relevance to a given query is a critical\ntask in information retrieval. Traditional reranking methods often lack\ntransparency and rely on proprietary models, hindering reproducibility and\ninterpretability. We propose Reason-to-Rank (R2R), a novel open-source\nreranking approach that enhances transparency by generating two types of\nreasoning: direct relevance reasoning, which explains how a document addresses\nthe query, and comparison reasoning, which justifies the relevance of one\ndocument over another. We leverage large language models (LLMs) as teacher\nmodels to generate these explanations and distill this knowledge into smaller,\nopenly available student models. Our student models are trained to generate\nmeaningful reasoning and rerank documents, achieving competitive performance\nacross multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate\nthat R2R not only improves reranking accuracy but also provides valuable\ninsights into the decision-making process. By offering a structured and\ninterpretable solution with openly accessible resources, R2R aims to bridge the\ngap between effectiveness and transparency in information retrieval, fostering\nreproducibility and further research in the field.\n","authors":["Yuelyu Ji","Zhuochun Li","Rui Meng","Daqing He"],"pdf_url":"https://arxiv.org/pdf/2410.05168v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02224v3","updated":"2024-11-30T14:27:59Z","published":"2024-06-04T11:36:09Z","title":"FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language\n  Models","summary":"  Recent research in federated large language models (LLMs) has primarily\nfocused on enabling clients to fine-tune their locally deployed homogeneous\nLLMs collaboratively or on transferring knowledge from server-based LLMs to\nsmall language models (SLMs) at downstream clients. However, a significant gap\nremains in the simultaneous mutual enhancement of both the server's LLM and\nclients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient\nfederated mutual knowledge transfer framework for large and small language\nmodels. This framework is designed to adaptively transfer knowledge from the\nserver's LLM to clients' SLMs while concurrently enriching the LLM with\nclients' unique domain insights. We facilitate token alignment using minimum\nedit distance (MinED) and then selective mutual knowledge transfer between\nclient-side SLMs and a server-side LLM, aiming to collectively enhance their\nperformance. Through extensive experiments across three distinct scenarios, we\nevaluate the effectiveness of FedMKT using various public LLMs and SLMs on a\nrange of NLP text generation tasks. Empirical results demonstrate that FedMKT\nsimultaneously boosts the performance of both LLMs and SLMs.\n","authors":["Tao Fan","Guoqiang Ma","Yan Kang","Hanlin Gu","Yuanfeng Song","Lixin Fan","Kai Chen","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.02224v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00177v3","updated":"2024-11-30T14:01:56Z","published":"2024-10-31T19:48:12Z","title":"LLM4Mat-Bench: Benchmarking Large Language Models for Materials Property\n  Prediction","summary":"  Large language models (LLMs) are increasingly being used in materials\nscience. However, little attention has been given to benchmarking and\nstandardized evaluation for LLM-based materials property prediction, which\nhinders progress. We present LLM4Mat-Bench, the largest benchmark to date for\nevaluating the performance of LLMs in predicting the properties of crystalline\nmaterials. LLM4Mat-Bench contains about 1.9M crystal structures in total,\ncollected from 10 publicly available materials data sources, and 45 distinct\nproperties. LLM4Mat-Bench features different input modalities: crystal\ncomposition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B\ntokens in total for each modality, respectively. We use LLM4Mat-Bench to\nfine-tune models with different sizes, including LLM-Prop and MatBERT, and\nprovide zero-shot and few-shot prompts to evaluate the property prediction\ncapabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. The\nresults highlight the challenges of general-purpose LLMs in materials science\nand the need for task-specific predictive models and task-specific\ninstruction-tuned LLMs in materials property prediction.\n","authors":["Andre Niyongabo Rubungo","Kangming Li","Jason Hattrick-Simpers","Adji Bousso Dieng"],"pdf_url":"https://arxiv.org/pdf/2411.00177v3.pdf","comment":"Accepted at NeurIPS 2024-AI4Mat Workshop. The Benchmark and code can\n  be found at https://github.com/vertaix/LLM4Mat-Bench"},{"id":"http://arxiv.org/abs/2410.02953v3","updated":"2024-11-30T12:16:51Z","published":"2024-10-03T19:53:47Z","title":"Unlocking Structured Thinking in Language Models with Cognitive\n  Prompting","summary":"  We propose cognitive prompting as a novel approach to guide problem-solving\nin large language models (LLMs) through structured, human-like cognitive\noperations, such as goal clarification, decomposition, filtering, abstraction,\nand pattern recognition. By employing systematic, step-by-step reasoning,\ncognitive prompting enables LLMs to tackle complex, multi-step tasks more\nefficiently. We introduce three variants: a deterministic sequence of cognitive\noperations, a self-adaptive variant in which the LLM dynamically selects the\nsequence of cognitive operations, and a hybrid variant that uses generated\ncorrect solutions as few-shot chain-of-thought prompts. Experiments with LLaMA,\nGemma~2, and Qwen models in each two sizes on the arithmetic reasoning\nbenchmark GSM8K demonstrate that cognitive prompting significantly improves\nperformance compared to standard question answering.\n","authors":["Oliver Kramer","Jill Baumann"],"pdf_url":"https://arxiv.org/pdf/2410.02953v3.pdf","comment":"6 pages, submitted to ESANN 2025"},{"id":"http://arxiv.org/abs/2410.03845v2","updated":"2024-11-30T11:19:39Z","published":"2024-10-04T18:22:58Z","title":"ORAssistant: A Custom RAG-based Conversational Assistant for OpenROAD","summary":"  Open-source Electronic Design Automation (EDA) tools are rapidly transforming\nchip design by addressing key barriers of commercial EDA tools such as\ncomplexity, costs, and access. Recent advancements in Large Language Models\n(LLMs) have further enhanced efficiency in chip design by providing user\nassistance across a range of tasks like setup, decision-making, and flow\nautomation. This paper introduces ORAssistant, a conversational assistant for\nOpenROAD, based on Retrieval-Augmented Generation (RAG). ORAssistant aims to\nimprove the user experience for the OpenROAD flow, from RTL-GDSII by providing\ncontext-specific responses to common user queries, including installation,\ncommand usage, flow setup, and execution, in prose format. Currently,\nORAssistant integrates OpenROAD, OpenROAD-flow-scripts, Yosys, OpenSTA, and\nKLayout. The data model is built from publicly available documentation and\nGitHub resources. The proposed architecture is scalable, supporting extensions\nto other open-source tools, operating modes, and LLM models. We use Google\nGemini as the base LLM model to build and test ORAssistant. Early evaluation\nresults of the RAG-based model show notable improvements in performance and\naccuracy compared to non-fine-tuned LLMs.\n","authors":["Aviral Kaintura","Palaniappan R","Shui Song Luar","Indira Iyer Almeida"],"pdf_url":"https://arxiv.org/pdf/2410.03845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08074v3","updated":"2024-11-30T10:48:21Z","published":"2024-06-12T10:48:53Z","title":"A Concept-Based Explainability Framework for Large Multimodal Models","summary":"  Large multimodal models (LMMs) combine unimodal encoders and large language\nmodels (LLMs) to perform multimodal tasks. Despite recent advancements towards\nthe interpretability of these models, understanding internal representations of\nLMMs remains largely a mystery. In this paper, we present a novel framework for\nthe interpretation of LMMs. We propose a dictionary learning based approach,\napplied to the representation of tokens. The elements of the learned dictionary\ncorrespond to our proposed concepts. We show that these concepts are well\nsemantically grounded in both vision and text. Thus we refer to these as\n``multi-modal concepts''. We qualitatively and quantitatively evaluate the\nresults of the learnt concepts. We show that the extracted multimodal concepts\nare useful to interpret representations of test samples. Finally, we evaluate\nthe disentanglement between different concepts and the quality of grounding\nconcepts visually and textually. Our code is publicly available at\nhttps://github.com/mshukor/xl-vlms\n","authors":["Jayneel Parekh","Pegah Khayatan","Mustafa Shukor","Alasdair Newson","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2406.08074v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.15380v2","updated":"2024-11-30T09:57:09Z","published":"2024-09-20T15:01:21Z","title":"Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for\n  Filipino","summary":"  Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs.\n","authors":["Jann Railey Montalan","Jian Gang Ngui","Wei Qi Leong","Yosephine Susanto","Hamsawardhini Rengarajan","Alham Fikri Aji","William Chandra Tjhi"],"pdf_url":"https://arxiv.org/pdf/2409.15380v2.pdf","comment":"Accepted for presentation at Paclic 38, 2024"},{"id":"http://arxiv.org/abs/2404.12038v5","updated":"2024-11-30T08:52:29Z","published":"2024-04-18T09:46:25Z","title":"Uncovering Safety Risks of Large Language Models through Concept\n  Activation Vector","summary":"  Despite careful safety alignment, current large language models (LLMs) remain\nvulnerable to various attacks. To further unveil the safety risks of LLMs, we\nintroduce a Safety Concept Activation Vector (SCAV) framework, which\neffectively guides the attacks by accurately interpreting LLMs' safety\nmechanisms. We then develop an SCAV-guided attack method that can generate both\nattack prompts and embedding-level attacks with automatically selected\nperturbation hyperparameters. Both automatic and human evaluations demonstrate\nthat our attack method significantly improves the attack success rate and\nresponse quality while requiring less training data. Additionally, we find that\nour generated attack prompts may be transferable to GPT-4, and the\nembedding-level attacks may also be transferred to other white-box LLMs whose\nparameters are known. Our experiments further uncover the safety risks present\nin current LLMs. For example, in our evaluation of seven open-source LLMs, we\nobserve an average attack success rate of 99.14%, based on the classic\nkeyword-matching criterion. Finally, we provide insights into the safety\nmechanism of LLMs. The code is available at\nhttps://github.com/SproutNan/AI-Safety_SCAV.\n","authors":["Zhihao Xu","Ruixuan Huang","Changyu Chen","Xiting Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12038v5.pdf","comment":"10 pages, accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.14811v2","updated":"2024-11-30T08:47:23Z","published":"2024-11-22T09:12:02Z","title":"Fine-Grained Alignment in Vision-and-Language Navigation through\n  Bayesian Optimization","summary":"  This paper addresses the challenge of fine-grained alignment in\nVision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D\nenvironments based on natural language instructions. Current approaches use\ncontrastive learning to align language with visual trajectory sequences.\nNevertheless, they encounter difficulties with fine-grained vision negatives.\nTo enhance cross-modal embeddings, we introduce a novel Bayesian\nOptimization-based adversarial optimization framework for creating fine-grained\ncontrastive vision samples. To validate the proposed methodology, we conduct a\nseries of experiments to assess the effectiveness of the enriched embeddings on\nfine-grained vision negatives. We conduct experiments on two common VLN\nbenchmarks R2R and REVERIE, experiments on the them demonstrate that these\nembeddings benefit navigation, and can lead to a promising performance\nenhancement. Our source code and trained models are available at:\nhttps://anonymous.4open.science/r/FGVLN.\n","authors":["Yuhang Song","Mario Gianni","Chenguang Yang","Kunyang Lin","Te-Chuan Chiu","Anh Nguyen","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2411.14811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06832v3","updated":"2024-11-30T04:53:04Z","published":"2024-03-11T15:48:43Z","title":"Noise-powered Multi-modal Knowledge Graph Representation Framework","summary":"  The rise of Multi-modal Pre-training highlights the necessity for a unified\nMulti-Modal Knowledge Graph (MMKG) representation learning framework. Such a\nframework is essential for embedding structured knowledge into multi-modal\nLarge Language Models effectively, alleviating issues like knowledge\nmisconceptions and multi-modal hallucinations. In this work, we explore the\nefficacy of models in accurately embedding entities within MMKGs through two\npivotal tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal\nEntity Alignment (MMEA). Building on this foundation, we propose a novel SNAG\nmethod that utilizes a Transformer-based architecture equipped with\nmodality-level noise masking to robustly integrate multi-modal entity features\nin KGs. By incorporating specific training objectives for both MKGC and MMEA,\nour approach achieves SOTA performance across a total of ten datasets,\ndemonstrating its versatility. Moreover, SNAG can not only function as a\nstandalone model but also enhance other existing methods, providing stable\nperformance improvements. Code and data are available at\nhttps://github.com/zjukg/SNAG.\n","authors":["Zhuo Chen","Yin Fang","Yichi Zhang","Lingbing Guo","Jiaoyan Che","Jeff Z. Pan","Huajun Chen","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06832v3.pdf","comment":"COLING 2025 Accpeted, Repo is available at\n  https://github.com/zjukg/SNAG"},{"id":"http://arxiv.org/abs/2402.14776v3","updated":"2024-11-30T04:29:53Z","published":"2024-02-22T18:35:05Z","title":"2D Matryoshka Sentence Embeddings","summary":"  Common approaches rely on fixed-length embedding vectors from language models\nas sentence embeddings for downstream tasks such as semantic textual similarity\n(STS). Such methods are limited in their flexibility due to unknown\ncomputational constraints and budgets across various applications. Matryoshka\nRepresentation Learning (MRL) \\cite{aditya2022matryoshka} encodes information\nat finer granularities, i.e., with lower embedding dimensions, to adaptively\naccommodate \\emph{ad hoc} tasks. Similar accuracy can be achieved with a\nsmaller embedding size, leading to speedups in downstream tasks. Despite its\nimproved efficiency, MRL still requires traversing all Transformer layers\nbefore obtaining the embedding, which remains the dominant factor in time and\nmemory consumption. This prompts consideration of whether the fixed number of\nTransformer layers affects representation quality and whether using\nintermediate layers for sentence representation is feasible. In this paper, we\nintroduce a novel sentence embedding model called \\textit{Two-dimensional\nMatryoshka Sentence Embedding} (2DMSE)\\footnote{Our code is available at\n\\url{https://github.com/SeanLee97/AnglE/blob/main/README_2DMSE.md}.}. It\nsupports elastic settings for both embedding sizes and Transformer layers,\noffering greater flexibility and efficiency than MRL. We conduct extensive\nexperiments on STS tasks and downstream applications. The experimental results\ndemonstrate the effectiveness of our proposed model in dynamically supporting\ndifferent embedding sizes and Transformer layers, allowing it to be highly\nadaptable to various scenarios.\n","authors":["Xianming Li","Zongxi Li","Jing Li","Haoran Xie","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2402.14776v3.pdf","comment":"Decoupled with ESE"},{"id":"http://arxiv.org/abs/2310.18964v3","updated":"2024-11-30T02:56:48Z","published":"2023-10-29T10:07:32Z","title":"LLMs and Finetuning: Benchmarking cross-domain performance for hate\n  speech detection","summary":"  In the evolving landscape of online communication, hate speech detection\nremains a formidable challenge, further compounded by the diversity of digital\nplatforms. This study investigates the effectiveness and adaptability of\npre-trained and fine-tuned Large Language Models (LLMs) in identifying hate\nspeech, to address two central questions: (1) To what extent does the model\nperformance depend on the fine-tuning and training parameters?, (2) To what\nextent do models generalize to cross-domain hate speech detection? and (3) What\nare the specific features of the datasets or models that influence the\ngeneralization potential? The experiment shows that LLMs offer a huge advantage\nover the state-of-the-art even without pretraining. Ordinary least squares\nanalyses suggest that the advantage of training with fine-grained hate speech\nlabels is washed away with the increase in dataset size. We conclude with a\nvision for the future of hate speech detection, emphasizing cross-domain\ngeneralizability and appropriate benchmarking practices.\n","authors":["Ahmad Nasir","Aadish Sharma","Kokil Jaidka"],"pdf_url":"https://arxiv.org/pdf/2310.18964v3.pdf","comment":"10 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2411.04330v2","updated":"2024-11-30T02:42:31Z","published":"2024-11-07T00:10:10Z","title":"Scaling Laws for Precision","summary":"  Low precision training and inference affect both the quality and cost of\nlanguage models, but current scaling laws do not account for this. In this\nwork, we devise \"precision-aware\" scaling laws for both training and inference.\nWe propose that training in lower precision reduces the model's \"effective\nparameter count,\" allowing us to predict the additional loss incurred from\ntraining in low precision and post-train quantization. For inference, we find\nthat the degradation introduced by post-training quantization increases as\nmodels are trained on more data, eventually making additional pretraining data\nactively harmful. For training, our scaling laws allow us to predict the loss\nof a model with different parts in different precisions, and suggest that\ntraining larger models in lower precision may be compute optimal. We unify the\nscaling laws for post and pretraining quantization to arrive at a single\nfunctional form that predicts degradation from training and inference in varied\nprecisions. We fit on over 465 pretraining runs and validate our predictions on\nmodel sizes up to 1.7B parameters trained on up to 26B tokens.\n","authors":["Tanishq Kumar","Zachary Ankner","Benjamin F. Spector","Blake Bordelon","Niklas Muennighoff","Mansheej Paul","Cengiz Pehlevan","Christopher Ré","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2411.04330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11579v3","updated":"2024-11-30T02:29:59Z","published":"2024-09-17T22:06:46Z","title":"HEARTS: A Holistic Framework for Explainable, Sustainable and Robust\n  Text Stereotype Detection","summary":"  Stereotypes are generalised assumptions about societal groups, and even\nstate-of-the-art LLMs using in-context learning struggle to identify them\naccurately. Due to the subjective nature of stereotypes, where what constitutes\na stereotype can vary widely depending on cultural, social, and individual\nperspectives, robust explainability is crucial. Explainable models ensure that\nthese nuanced judgments can be understood and validated by human users,\npromoting trust and accountability. We address these challenges by introducing\nHEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text\nStereotype Detection), a framework that enhances model performance, minimises\ncarbon footprint, and provides transparent, interpretable explanations. We\nestablish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising\n57,201 labelled texts across six groups, including under-represented\ndemographics like LGBTQ+ and regional stereotypes. Ablation studies confirm\nthat BERT models fine-tuned on EMGSD outperform those trained on individual\ncomponents. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model\nusing SHAP to generate token-level importance values, ensuring alignment with\nhuman understanding, and calculate explainability confidence scores by\ncomparing SHAP and LIME outputs...\n","authors":["Theo King","Zekun Wu","Adriano Koshiyama","Emre Kazim","Philip Treleaven"],"pdf_url":"https://arxiv.org/pdf/2409.11579v3.pdf","comment":"NeurIPS 2024 SoLaR Workshop and NeurIPS 2024 Safety Gen AI Workshop"},{"id":"http://arxiv.org/abs/2409.11353v3","updated":"2024-11-30T02:27:09Z","published":"2024-09-17T16:55:25Z","title":"THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation\n  in Large Language Models","summary":"  Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks.\n","authors":["Mengfei Liang","Archish Arun","Zekun Wu","Cristian Munoz","Jonathan Lutch","Emre Kazim","Adriano Koshiyama","Philip Treleaven"],"pdf_url":"https://arxiv.org/pdf/2409.11353v3.pdf","comment":"NeurIPS 2024 SoLaR (Socially Responsible Language Modelling Research\n  ) Workshop"},{"id":"http://arxiv.org/abs/2409.11149v4","updated":"2024-11-30T02:21:25Z","published":"2024-09-17T13:03:12Z","title":"SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with\n  Customisable Fairness Calibration","summary":"  The development of unbiased large language models is widely recognized as\ncrucial, yet existing benchmarks fall short in detecting biases due to limited\nscope, contamination, and lack of a fairness baseline. SAGED(-Bias) is the\nfirst holistic benchmarking pipeline to address these problems. The pipeline\nencompasses five core stages: scraping materials, assembling benchmarks,\ngenerating responses, extracting numeric features, and diagnosing with\ndisparity metrics. SAGED includes metrics for max disparity, such as impact\nratio, and bias concentration, such as Max Z-scores. Noticing that assessment\ntool bias and contextual bias in prompts can distort evaluation, SAGED\nimplements counterfactual branching and baseline calibration for mitigation.\nFor demonstration, we use SAGED on G20 Countries with popular 8b-level models\nincluding Gemma2, Llama3.1, Mistral, and Qwen2. With sentiment analysis, we\nfind that while Mistral and Qwen2 show lower max disparity and higher bias\nconcentration than Gemma2 and Llama3.1, all models are notably biased against\ncountries like Russia and (except for Qwen2) China. With further experiments to\nhave models role-playing U.S. (vice-/former-) presidents, we see bias amplifies\nand shifts in heterogeneous directions. Moreover, we see Qwen2 and Mistral not\nengage in role-playing, while Llama3.1 and Gemma2 role-play Trump notably more\nintensively than Biden and Harris, indicating role-playing performance bias in\nthese models.\n","authors":["Xin Guan","Nathaniel Demchak","Saloni Gupta","Ze Wang","Ediz Ertekin Jr.","Adriano Koshiyama","Emre Kazim","Zekun Wu"],"pdf_url":"https://arxiv.org/pdf/2409.11149v4.pdf","comment":"COLING 2025 Main Conference"},{"id":"http://arxiv.org/abs/2410.01169v2","updated":"2024-11-30T01:04:31Z","published":"2024-10-02T01:54:46Z","title":"GADFA: Generator-Assisted Decision-Focused Approach for Opinion\n  Expressing Timing Identification","summary":"  The advancement of text generation models has granted us the capability to\nproduce coherent and convincing text on demand. Yet, in real-life\ncircumstances, individuals do not continuously generate text or voice their\nopinions. For instance, consumers pen product reviews after weighing the merits\nand demerits of a product, and professional analysts issue reports following\nsignificant news releases. In essence, opinion expression is typically prompted\nby particular reasons or signals. Despite long-standing developments in opinion\nmining, the appropriate timing for expressing an opinion remains largely\nunexplored. To address this deficit, our study introduces an innovative task -\nthe identification of news-triggered opinion expressing timing. We ground this\ntask in the actions of professional stock analysts and develop a novel dataset\nfor investigation. Our approach is decision-focused, leveraging text generation\nmodels to steer the classification model, thus enhancing overall performance.\nOur experimental findings demonstrate that the text generated by our model\ncontributes fresh insights from various angles, effectively aiding in\nidentifying the optimal timing for opinion expression.\n","authors":["Chung-Chi Chen","Hiroya Takamura","Ichiro Kobayashi","Yusuke Miyao","Hsin-Hsi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01169v2.pdf","comment":"Accepted: COLING-2025"},{"id":"http://arxiv.org/abs/2411.00024v3","updated":"2024-11-30T00:17:01Z","published":"2024-10-28T22:30:06Z","title":"A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges","summary":"  The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements.\n","authors":["Zifeng Wang","Hanyin Wang","Benjamin Danek","Ying Li","Christina Mack","Hoifung Poon","Yajuan Wang","Pranav Rajpurkar","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2411.00024v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.17922v2","updated":"2024-11-30T23:10:40Z","published":"2024-11-26T22:31:09Z","title":"Exploring Superpixel Segmentation Methods in the Context of Citizen\n  Science and Deforestation Detection","summary":"  Tropical forests play an essential role in the planet's ecosystem, making the\nconservation of these biomes a worldwide priority. However, ongoing\ndeforestation and degradation pose a significant threat to their existence,\nnecessitating effective monitoring and the proposal of actions to mitigate the\ndamage caused by these processes. In this regard, initiatives range from\ngovernment and private sector monitoring programs to solutions based on citizen\nscience campaigns, for example. Particularly in the context of citizen science\ncampaigns, the segmentation of remote sensing images to identify deforested\nareas and subsequently submit them to analysis by non-specialized volunteers is\nnecessary. Thus, segmentation using superpixel-based techniques proves to be a\nviable solution for this important task. Therefore, this paper presents an\nanalysis of 22 superpixel-based segmentation methods applied to remote sensing\nimages, aiming to identify which of them are more suitable for generating\nsegments for citizen science campaigns. The results reveal that seven of the\nsegmentation methods outperformed the baseline method (SLIC) currently employed\nin the ForestEyes citizen science project, indicating an opportunity for\nimprovement in this important stage of campaign development.\n","authors":["Hugo Resende","Isabela Borlido","Victor Sundermann","Eduardo B. Neto","Silvio Jamil F. Guimarães","Fabio Faria","Alvaro Luiz Fazenda"],"pdf_url":"https://arxiv.org/pdf/2411.17922v2.pdf","comment":"Paper was accepted for presentation at SAC 2025"},{"id":"http://arxiv.org/abs/2411.11922v2","updated":"2024-11-30T22:32:34Z","published":"2024-11-18T05:59:03Z","title":"SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking\n  with Motion-Aware Memory","summary":"  The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in\nobject segmentation tasks but faces challenges in visual object tracking,\nparticularly when managing crowded scenes with fast-moving or self-occluding\nobjects. Furthermore, the fixed-window memory approach in the original model\ndoes not consider the quality of memories selected to condition the image\nfeatures for the next frame, leading to error propagation in videos. This paper\nintroduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for\nvisual object tracking. By incorporating temporal motion cues with the proposed\nmotion-aware memory selection mechanism, SAMURAI effectively predicts object\nmotion and refines mask selection, achieving robust, accurate tracking without\nthe need for retraining or fine-tuning. SAMURAI operates in real-time and\ndemonstrates strong zero-shot performance across diverse benchmark datasets,\nshowcasing its ability to generalize without fine-tuning. In evaluations,\nSAMURAI achieves significant improvements in success rate and precision over\nexisting trackers, with a 7.1% AUC gain on LaSOT$_{\\text{ext}}$ and a 3.5% AO\ngain on GOT-10k. Moreover, it achieves competitive results compared to fully\nsupervised methods on LaSOT, underscoring its robustness in complex tracking\nscenarios and its potential for real-world applications in dynamic\nenvironments.\n","authors":["Cheng-Yen Yang","Hsiang-Wei Huang","Wenhao Chai","Zhongyu Jiang","Jenq-Neng Hwang"],"pdf_url":"https://arxiv.org/pdf/2411.11922v2.pdf","comment":"Project page is available at https://yangchris11.github.io/samurai/"},{"id":"http://arxiv.org/abs/2309.00378v5","updated":"2024-11-30T19:08:48Z","published":"2023-09-01T10:27:04Z","title":"Long-Term Ad Memorability: Understanding & Generating Memorable Ads","summary":"  Despite the importance of long-term memory in marketing and brand building,\nuntil now, there has been no large-scale study on the memorability of ads. All\nprevious memorability studies have been conducted on short-term recall on\nspecific content types like action videos. On the other hand, long-term\nmemorability is crucial for the advertising industry, and ads are almost always\nhighly multimodal. Therefore, we release the first memorability dataset,\nLAMBDA, consisting of 1749 participants and 2205 ads covering 276 brands.\nRunning statistical tests over different participant subpopulations and ad\ntypes, we find many interesting insights into what makes an ad memorable, e.g.,\nfast-moving ads are more memorable than those with slower scenes; people who\nuse ad-blockers remember a lower number of ads than those who don't. Next, we\npresent a model, Henry, to predict the memorability of a content. Henry\nachieves state-of-the-art performance across all prominent literature\nmemorability datasets. It shows strong generalization performance with better\nresults in 0-shot on unseen datasets. Finally, with the intent of memorable ad\ngeneration, we present a scalable method to build a high-quality memorable ad\ngeneration model by leveraging automatically annotated data. Our approach, SEED\n(Self rEwarding mEmorability Modeling), starts with a language model trained on\nLAMBDA as seed data and progressively trains an LLM to generate more memorable\nads. We show that the generated advertisements have 44% higher memorability\nscores than the original ads. We release this large-scale ad dataset,\nUltraLAMBDA, consisting of 5 million ads. Our code and the datasets, LAMBDA and\nUltraLAMBDA, are open-sourced at\nhttps://behavior-in-the-wild.github.io/memorability.\n","authors":["Harini SI","Somesh Singh","Yaman K Singla","Aanisha Bhattacharyya","Veeky Baths","Changyou Chen","Rajiv Ratn Shah","Balaji Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2309.00378v5.pdf","comment":"Published in WACV-2025"},{"id":"http://arxiv.org/abs/2408.00672v2","updated":"2024-11-30T18:42:30Z","published":"2024-08-01T16:13:07Z","title":"ExpertAF: Expert Actionable Feedback from Video","summary":"  Feedback is essential for learning a new skill or improving one's current\nskill-level. However, current methods for skill-assessment from video only\nprovide scores or compare demonstrations, leaving the burden of knowing what to\ndo differently on the user. We introduce a novel method to generate actionable\nfeedback from video of a person doing a physical activity, such as basketball\nor soccer. Our method takes a video demonstration and its accompanying 3D body\npose and generates (1) free-form expert commentary describing what the person\nis doing well and what they could improve, and (2) a visual expert\ndemonstration that incorporates the required corrections. We show how to\nleverage Ego-Exo4D's videos of skilled activity and expert commentary together\nwith a strong language model to create a weakly-supervised training dataset for\nthis task, and we devise a multimodal video-language model to infer coaching\nfeedback. Our method is able to reason across multi-modal input combinations to\noutput full-spectrum, actionable coaching -- expert commentary, expert video\nretrieval, and expert pose generation -- outperforming strong vision-language\nmodels on both established metrics and human preference studies. Code and data\nwill be publicly released.\n","authors":["Kumar Ashutosh","Tushar Nagarajan","Georgios Pavlakos","Kris Kitani","Kristen Grauman"],"pdf_url":"https://arxiv.org/pdf/2408.00672v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2408.01372v3","updated":"2024-11-30T13:24:19Z","published":"2024-08-02T16:28:51Z","title":"Spatial and Spatial-Spectral Morphological Mamba for Hyperspectral Image\n  Classification","summary":"  Recent advancements in transformers, specifically self-attention mechanisms,\nhave significantly improved hyperspectral image (HSI) classification. However,\nthese models often suffer from inefficiencies, as their computational\ncomplexity scales quadratically with sequence length. To address these\nchallenges, we propose the morphological spatial mamba (SMM) and morphological\nspatial-spectral Mamba (SSMM) model (MorpMamba), which combines the strengths\nof morphological operations and the state space model framework, offering a\nmore computationally efficient alternative to transformers. In MorpMamba, a\nnovel token generation module first converts HSI patches into spatial-spectral\ntokens. These tokens are then processed through morphological operations such\nas erosion and dilation, utilizing depthwise separable convolutions to capture\nstructural and shape information. A token enhancement module refines these\nfeatures by dynamically adjusting the spatial and spectral tokens based on\ncentral HSI regions, ensuring effective feature fusion within each block.\nSubsequently, multi-head self-attention is applied to further enrich the\nfeature representations, allowing the model to capture complex relationships\nand dependencies within the data. Finally, the enhanced tokens are fed into a\nstate space module, which efficiently models the temporal evolution of the\nfeatures for classification. Experimental results on widely used HSI datasets\ndemonstrate that MorpMamba achieves superior parametric efficiency compared to\ntraditional CNN and transformer models while maintaining high accuracy. The\ncode will be made publicly available at\n\\url{https://github.com/mahmad000/MorpMamba}.\n","authors":["Muhammad Ahmad","Muhammad Hassaan Farooq Butt","Adil Mehmood Khan","Manuel Mazzara","Salvatore Distefano","Muhammad Usama","Swalpa Kumar Roy","Jocelyn Chanussot","Danfeng Hong"],"pdf_url":"https://arxiv.org/pdf/2408.01372v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.06769v5","updated":"2024-11-30T13:19:28Z","published":"2021-06-12T13:04:46Z","title":"Cross-Subject Domain Adaptation for Classifying Working Memory Load with\n  Multi-Frame EEG Images","summary":"  Working memory (WM), denoting the information temporally stored in the mind,\nis a fundamental research topic in the field of human cognition.\nElectroencephalograph (EEG), which can monitor the electrical activity of the\nbrain, has been widely used in measuring the level of WM. However, one of the\ncritical challenges is that individual differences may cause ineffective\nresults, especially when the established model meets an unfamiliar subject. In\nthis work, we propose a cross-subject deep adaptation model with spatial\nattention (CS-DASA) to generalize the workload classifications across subjects.\nFirst, we transform EEG time series into multi-frame EEG images incorporating\nspatial, spectral, and temporal information. First, the Subject-Shared module\nin CS-DASA receives multi-frame EEG image data from both source and target\nsubjects and learns the common feature representations. Then, in the\nsubject-specific module, the maximum mean discrepancy is implemented to measure\nthe domain distribution divergence in a reproducing kernel Hilbert space, which\ncan add an effective penalty loss for domain adaptation. Additionally, the\nsubject-to-subject spatial attention mechanism is employed to focus on the\ndiscriminative spatial features from the target image data. Experiments\nconducted on a public WM EEG dataset containing 13 subjects show that the\nproposed model is capable of achieving better performance than existing\nstate-of-the-art methods.\n","authors":["Junfu Chen","Sirui Li","Dechang Pi"],"pdf_url":"https://arxiv.org/pdf/2106.06769v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11512v2","updated":"2024-11-30T12:11:05Z","published":"2024-09-17T19:26:21Z","title":"Good Grasps Only: A data engine for self-supervised fine-tuning of pose\n  estimation using grasp poses for verification","summary":"  In this paper, we present a novel method for self-supervised fine-tuning of\npose estimation. Leveraging zero-shot pose estimation, our approach enables the\nrobot to automatically obtain training data without manual labeling. After pose\nestimation the object is grasped, and in-hand pose estimation is used for data\nvalidation. Our pipeline allows the system to fine-tune while the process is\nrunning, removing the need for a learning phase. The motivation behind our work\nlies in the need for rapid setup of pose estimation solutions. Specifically, we\naddress the challenging task of bin picking, which plays a pivotal role in\nflexible robotic setups. Our method is implemented on a robotics work-cell, and\ntested with four different objects. For all objects, our method increases the\nperformance and outperforms a state-of-the-art method trained on the CAD model\nof the objects.\n","authors":["Frederik Hagelskjær"],"pdf_url":"https://arxiv.org/pdf/2409.11512v2.pdf","comment":"8 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2411.17106v2","updated":"2024-11-30T11:59:54Z","published":"2024-11-26T04:49:42Z","title":"PassionSR: Post-Training Quantization with Adaptive Scale in One-Step\n  Diffusion based Image Super-Resolution","summary":"  Diffusion-based image super-resolution (SR) models have shown superior\nperformance at the cost of multiple denoising steps. However, even though the\ndenoising step has been reduced to one, they require high computational costs\nand storage requirements, making it difficult for deployment on hardware\ndevices. To address these issues, we propose a novel post-training quantization\napproach with adaptive scale in one-step diffusion (OSD) image SR, PassionSR.\nFirst, we simplify OSD model to two core components, UNet and Variational\nAutoencoder (VAE) by removing the CLIPEncoder. Secondly, we propose Learnable\nBoundary Quantizer (LBQ) and Learnable Equivalent Transformation (LET) to\noptimize the quantization process and manipulate activation distributions for\nbetter quantization. Finally, we design a Distributed Quantization Calibration\n(DQC) strategy that stabilizes the training of quantized parameters for rapid\nconvergence. Comprehensive experiments demonstrate that PassionSR with 8-bit\nand 6-bit obtains comparable visual results with full-precision model.\nMoreover, our PassionSR achieves significant advantages over recent leading\nlow-bit quantization methods for image SR. Our code will be at\nhttps://github.com/libozhu03/PassionSR.\n","authors":["Libo Zhu","Jianze Li","Haotong Qin","Wenbo Li","Yulun Zhang","Yong Guo","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2411.17106v2.pdf","comment":"https://github.com/libozhu03/PassionSR"},{"id":"http://arxiv.org/abs/2404.05268v3","updated":"2024-11-30T11:55:19Z","published":"2024-04-08T07:59:04Z","title":"MC$^2$: Multi-concept Guidance for Customized Multi-concept Generation","summary":"  Customized text-to-image generation, which synthesizes images based on\nuser-specified concepts, has made significant progress in handling individual\nconcepts. However, when extended to multiple concepts, existing methods often\nstruggle with properly integrating different models and avoiding the unintended\nblending of characteristics from distinct concepts. In this paper, we propose\nMC$^2$, a novel approach for multi-concept customization that enhances\nflexibility and fidelity through inference-time optimization. MC$^2$ enables\nthe integration of multiple single-concept models with heterogeneous\narchitectures. By adaptively refining attention weights between visual and\ntextual tokens, our method ensures that image regions accurately correspond to\ntheir associated concepts while minimizing interference between concepts.\nExtensive experiments demonstrate that MC$^2$ outperforms training-based\nmethods in terms of prompt-reference alignment. Furthermore, MC$^2$ can be\nseamlessly applied to text-to-image generation, providing robust compositional\ncapabilities. To facilitate the evaluation of multi-concept customization, we\nalso introduce a new benchmark, MC++. The code will be publicly available at\nhttps://github.com/JIANGJiaXiu/MC-2.\n","authors":["Jiaxiu Jiang","Yabo Zhang","Kailai Feng","Xiaohe Wu","Wenbo Li","Renjing Pei","Fan Li","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2404.05268v3.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2408.17065v2","updated":"2024-11-30T10:48:35Z","published":"2024-08-30T07:49:57Z","title":"Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level\n  Blending and Spatiotemporal Adapter Tuning","summary":"  Three key challenges hinder the development of current deepfake video\ndetection: (1) Temporal features can be complex and diverse: how can we\nidentify general temporal artifacts to enhance model generalization? (2)\nSpatiotemporal models often lean heavily on one type of artifact and ignore the\nother: how can we ensure balanced learning from both? (3) Videos are naturally\nresource-intensive: how can we tackle efficiency without compromising accuracy?\nThis paper attempts to tackle the three challenges jointly. First, inspired by\nthe notable generality of using image-level blending data for image forgery\ndetection, we investigate whether and how video-level blending can be effective\nin video. We then perform a thorough analysis and identify a previously\nunderexplored temporal forgery artifact: Facial Feature Drift (FFD), which\ncommonly exists across different forgeries. To reproduce FFD, we then propose a\nnovel Video-level Blending data (VB), where VB is implemented by blending the\noriginal image and its warped version frame-by-frame, serving as a hard\nnegative sample to mine more general artifacts. Second, we carefully design a\nlightweight Spatiotemporal Adapter (StA) to equip a pretrained image model\n(both ViTs and CNNs) with the ability to capture both spatial and temporal\nfeatures jointly and efficiently. StA is designed with two-stream 3D-Conv with\nvarying kernel sizes, allowing it to process spatial and temporal features\nseparately. Extensive experiments validate the effectiveness of the proposed\nmethods; and show our approach can generalize well to previously unseen forgery\nvideos, even the latest generation methods.\n","authors":["Zhiyuan Yan","Yandan Zhao","Shen Chen","Mingyi Guo","Xinghe Fu","Taiping Yao","Shouhong Ding","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.17065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08074v3","updated":"2024-11-30T10:48:21Z","published":"2024-06-12T10:48:53Z","title":"A Concept-Based Explainability Framework for Large Multimodal Models","summary":"  Large multimodal models (LMMs) combine unimodal encoders and large language\nmodels (LLMs) to perform multimodal tasks. Despite recent advancements towards\nthe interpretability of these models, understanding internal representations of\nLMMs remains largely a mystery. In this paper, we present a novel framework for\nthe interpretation of LMMs. We propose a dictionary learning based approach,\napplied to the representation of tokens. The elements of the learned dictionary\ncorrespond to our proposed concepts. We show that these concepts are well\nsemantically grounded in both vision and text. Thus we refer to these as\n``multi-modal concepts''. We qualitatively and quantitatively evaluate the\nresults of the learnt concepts. We show that the extracted multimodal concepts\nare useful to interpret representations of test samples. Finally, we evaluate\nthe disentanglement between different concepts and the quality of grounding\nconcepts visually and textually. Our code is publicly available at\nhttps://github.com/mshukor/xl-vlms\n","authors":["Jayneel Parekh","Pegah Khayatan","Mustafa Shukor","Alasdair Newson","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2406.08074v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.16767v2","updated":"2024-11-30T09:10:08Z","published":"2024-08-29T17:59:40Z","title":"ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion\n  Model","summary":"  Advancements in 3D scene reconstruction have transformed 2D images from the\nreal world into 3D models, producing realistic 3D results from hundreds of\ninput photos. Despite great success in dense-view reconstruction scenarios,\nrendering a detailed scene from insufficient captured views is still an\nill-posed optimization problem, often resulting in artifacts and distortions in\nunseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction\nparadigm that reframes the ambiguous reconstruction challenge as a temporal\ngeneration task. The key insight is to unleash the strong generative prior of\nlarge pre-trained video diffusion models for sparse-view reconstruction.\nHowever, 3D view consistency struggles to be accurately preserved in directly\ngenerated video frames from pre-trained models. To address this, given limited\ninput views, the proposed ReconX first constructs a global point cloud and\nencodes it into a contextual space as the 3D structure condition. Guided by the\ncondition, the video diffusion model then synthesizes video frames that are\nboth detail-preserved and exhibit a high degree of 3D consistency, ensuring the\ncoherence of the scene from various perspectives. Finally, we recover the 3D\nscene from the generated video through a confidence-aware 3D Gaussian Splatting\noptimization scheme. Extensive experiments on various real-world datasets show\nthe superiority of our ReconX over state-of-the-art methods in terms of quality\nand generalizability.\n","authors":["Fangfu Liu","Wenqiang Sun","Hanyang Wang","Yikai Wang","Haowen Sun","Junliang Ye","Jun Zhang","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2408.16767v2.pdf","comment":"Project page: https://liuff19.github.io/ReconX"},{"id":"http://arxiv.org/abs/2411.16727v2","updated":"2024-11-30T09:06:45Z","published":"2024-11-23T05:19:27Z","title":"An Information-Theoretic Regularizer for Lossy Neural Image Compression","summary":"  Lossy image compression networks aim to minimize the latent entropy of images\nwhile adhering to specific distortion constraints. However, optimizing the\nneural network can be challenging due to its nature of learning quantized\nlatent representations. In this paper, our key finding is that minimizing the\nlatent entropy is, to some extent, equivalent to maximizing the conditional\nsource entropy, an insight that is deeply rooted in information-theoretic\nequalities. Building on this insight, we propose a novel structural\nregularization method for the neural image compression task by incorporating\nthe negative conditional source entropy into the training objective, such that\nboth the optimization efficacy and the model's generalization ability can be\npromoted. The proposed information-theoretic regularizer is interpretable,\nplug-and-play, and imposes no inference overheads. Extensive experiments\ndemonstrate its superiority in regularizing the models and further squeezing\nbits from the latent representation across various compression structures and\nunseen domains.\n","authors":["Yingwen Zhang","Meng Wang","Xihua Sheng","Peilin Chen","Junru Li","Li Zhang","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16727v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.14811v2","updated":"2024-11-30T08:47:23Z","published":"2024-11-22T09:12:02Z","title":"Fine-Grained Alignment in Vision-and-Language Navigation through\n  Bayesian Optimization","summary":"  This paper addresses the challenge of fine-grained alignment in\nVision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D\nenvironments based on natural language instructions. Current approaches use\ncontrastive learning to align language with visual trajectory sequences.\nNevertheless, they encounter difficulties with fine-grained vision negatives.\nTo enhance cross-modal embeddings, we introduce a novel Bayesian\nOptimization-based adversarial optimization framework for creating fine-grained\ncontrastive vision samples. To validate the proposed methodology, we conduct a\nseries of experiments to assess the effectiveness of the enriched embeddings on\nfine-grained vision negatives. We conduct experiments on two common VLN\nbenchmarks R2R and REVERIE, experiments on the them demonstrate that these\nembeddings benefit navigation, and can lead to a promising performance\nenhancement. Our source code and trained models are available at:\nhttps://anonymous.4open.science/r/FGVLN.\n","authors":["Yuhang Song","Mario Gianni","Chenguang Yang","Kunyang Lin","Te-Chuan Chiu","Anh Nguyen","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2411.14811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18281v2","updated":"2024-11-30T07:34:24Z","published":"2024-11-27T12:15:52Z","title":"MotionCharacter: Identity-Preserving and Motion Controllable Human Video\n  Generation","summary":"  Recent advancements in personalized Text-to-Video (T2V) generation highlight\nthe importance of integrating character-specific identities and actions.\nHowever, previous T2V models struggle with identity consistency and\ncontrollable motion dynamics, mainly due to limited fine-grained facial and\naction-based textual prompts, and datasets that overlook key human attributes\nand actions. To address these challenges, we propose MotionCharacter, an\nefficient and high-fidelity human video generation framework designed for\nidentity preservation and fine-grained motion control. We introduce an\nID-preserving module to maintain identity fidelity while allowing flexible\nattribute modifications, and further integrate ID-consistency and region-aware\nloss mechanisms, significantly enhancing identity consistency and detail\nfidelity. Additionally, our approach incorporates a motion control module that\nprioritizes action-related text while maintaining subject consistency, along\nwith a dataset, Human-Motion, which utilizes large language models to generate\ndetailed motion descriptions. For simplify user control during inference, we\nparameterize motion intensity through a single coefficient, allowing for easy\nadjustments. Extensive experiments highlight the effectiveness of\nMotionCharacter, demonstrating significant improvements in ID-preserving,\nhigh-quality video generation.\n","authors":["Haopeng Fang","Di Qiu","Binjie Mao","Pengfei Yan","He Tang"],"pdf_url":"https://arxiv.org/pdf/2411.18281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08857v2","updated":"2024-11-30T07:22:57Z","published":"2024-09-13T14:19:27Z","title":"InstantDrag: Improving Interactivity in Drag-based Image Editing","summary":"  Drag-based image editing has recently gained popularity for its interactivity\nand precision. However, despite the ability of text-to-image models to generate\nsamples within a second, drag editing still lags behind due to the challenge of\naccurately reflecting user interaction while maintaining image content. Some\nexisting approaches rely on computationally intensive per-image optimization or\nintricate guidance-based methods, requiring additional inputs such as masks for\nmovable regions and text prompts, thereby compromising the interactivity of the\nediting process. We introduce InstantDrag, an optimization-free pipeline that\nenhances interactivity and speed, requiring only an image and a drag\ninstruction as input. InstantDrag consists of two carefully designed networks:\na drag-conditioned optical flow generator (FlowGen) and an optical\nflow-conditioned diffusion model (FlowDiffusion). InstantDrag learns motion\ndynamics for drag-based image editing in real-world video datasets by\ndecomposing the task into motion generation and motion-conditioned image\ngeneration. We demonstrate InstantDrag's capability to perform fast,\nphoto-realistic edits without masks or text prompts through experiments on\nfacial video datasets and general scenes. These results highlight the\nefficiency of our approach in handling drag-based image editing, making it a\npromising solution for interactive, real-time applications.\n","authors":["Joonghyuk Shin","Daehyeon Choi","Jaesik Park"],"pdf_url":"https://arxiv.org/pdf/2409.08857v2.pdf","comment":"SIGGRAPH Asia 2024. Project webpage:\n  https://joonghyuk.com/instantdrag-web/"},{"id":"http://arxiv.org/abs/2405.16116v2","updated":"2024-11-30T07:19:56Z","published":"2024-05-25T08:06:12Z","title":"REACT: Real-time Efficiency and Accuracy Compromise for Tradeoffs in\n  Scene Graph Generation","summary":"  Scene Graph Generation (SGG) is a task that encodes visual relationships\nbetween objects in images as graph structures. SGG shows significant promise as\na foundational component for downstream tasks, such as reasoning for embodied\nagents. To enable real-time applications, SGG must address the trade-off\nbetween performance and inference speed. However, current methods tend to focus\non one of the following: (1) improving relation prediction accuracy, (2)\nenhancing object detection accuracy, or (3) reducing latency, without aiming to\nbalance all three objectives simultaneously. To address this limitation, we\npropose a novel architecture, inference method, and relation prediction model.\nOur proposed solution, the REACT model, achieves the highest inference speed\namong existing SGG models, improving object detection accuracy without\nsacrificing relation prediction performance. Compared to state-of-the-art\napproaches, REACT is 2.7 times faster (with a latency of 23 ms) and improves\nobject detection accuracy by 58.51%. Furthermore, our proposal significantly\nreduces model size, with an average of 5.5x fewer parameters. Code is available\nat https://github.com/Maelic/SGG-Benchmark\n","authors":["Maëlic Neau","Paulo E. Santos","Anne-Gwenn Bosser","Cédric Buche"],"pdf_url":"https://arxiv.org/pdf/2405.16116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14332v2","updated":"2024-11-30T05:56:52Z","published":"2024-10-18T09:44:25Z","title":"Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension","summary":"  Recent advances in Large Language Models (LLMs) have catalyzed the\ndevelopment of Large Multimodal Models (LMMs). However, existing research\nprimarily focuses on tuning language and image instructions, ignoring the\ncritical pretraining phase where models learn to process textual and visual\nmodalities jointly. In this paper, we propose a new pretraining paradigm for\nLMMs to enhance the visual comprehension capabilities of LLMs by introducing a\nnovel cross-modal comprehension stage. Specifically, we design a dynamically\nlearnable prompt token pool and employ the Hungarian algorithm to replace part\nof the original visual tokens with the most relevant prompt tokens. Then, we\nconceptualize visual tokens as analogous to a \"foreign language\" for the LLMs\nand propose a mixed attention mechanism with bidirectional visual attention and\nunidirectional textual attention to comprehensively enhance the understanding\nof visual tokens. Meanwhile, we integrate a detailed caption generation task,\nleveraging rich descriptions to further facilitate LLMs in understanding visual\nsemantic information. After pretraining on 1.5 million publicly accessible\ndata, we present a new foundation model called Croc. Experimental results\ndemonstrate that Croc achieves new state-of-the-art performance on massive\nvision-language benchmarks. To support reproducibility and facilitate further\nresearch, we release the training code and pre-trained model weights at\nhttps://github.com/deepglint/Croc.\n","authors":["Yin Xie","Kaicheng Yang","Ninghua Yang","Weimo Deng","Xiangzi Dai","Tiancheng Gu","Yumeng Wang","Xiang An","Yongle Zhao","Ziyong Feng","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2410.14332v2.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.06169v3","updated":"2024-11-30T05:32:51Z","published":"2024-10-08T16:13:24Z","title":"Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to\n  See","summary":"  By treating visual tokens from visual encoders as text tokens, Multimodal\nLarge Language Models (MLLMs) have achieved remarkable progress across diverse\nvisual understanding tasks, leveraging the robust architectures of Large\nLanguage Models (LLMs). However, as token counts grow, the quadratic scaling of\ncomputation in LLMs introduces a significant efficiency bottleneck, impeding\nfurther scalability. Although recent approaches have explored pruning visual\ntokens or employing lighter LLM architectures, the computational overhead from\nan increasing number of visual tokens remains a substantial challenge.\n  In this study, we investigate the redundancy in visual computation at both\nthe parameter and computational pattern levels within LLaVA, a representative\nMLLM, and introduce a suite of streamlined strategies to enhance efficiency.\nThese include neighbor-aware visual token attention, pruning of inactive visual\nattention heads, and selective layer dropping for visual computations. By\nimplementing these strategies in LLaVA, we achieve a reduction in computational\ndemands of 88% while maintaining model performance across key benchmarks.\nAdditionally, we validate the existence of visual computational redundancy in\nother MLLMs, such as Qwen2-VL-7B and InternVL-2.0-4B/8B/26B. These results\npresent a novel pathway for MLLMs to handle dense visual tokens with minimal\ncomputational costs. Code and model checkpoints will be released to support\nfurther research.\n","authors":["Zeliang Zhang","Phu Pham","Wentian Zhao","Kun Wan","Yu-Jhe Li","Jianing Zhou","Daniel Miranda","Ajinkya Kale","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.06169v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17765v2","updated":"2024-11-30T04:50:36Z","published":"2024-11-26T04:21:22Z","title":"I2VControl: Disentangled and Unified Video Motion Synthesis Control","summary":"  Video synthesis techniques are undergoing rapid progress, with\ncontrollability being a significant aspect of practical usability for\nend-users. Although text condition is an effective way to guide video\nsynthesis, capturing the correct joint distribution between text descriptions\nand video motion remains a substantial challenge. In this paper, we present a\ndisentangled and unified framework, namely I2VControl, that unifies multiple\nmotion control tasks in image-to-video synthesis. Our approach partitions the\nvideo into individual motion units and represents each unit with disentangled\ncontrol signals, which allows for various control types to be flexibly combined\nwithin our single system. Furthermore, our methodology seamlessly integrates as\na plug-in for pre-trained models and remains agnostic to specific model\narchitectures. We conduct extensive experiments, achieving excellent\nperformance on various control tasks, and our method further facilitates\nuser-driven creative combinations, enhancing innovation and creativity. The\nproject page is: https://wanquanf.github.io/I2VControl .\n","authors":["Wanquan Feng","Tianhao Qi","Jiawei Liu","Mingzhen Sun","Pengqi Tu","Tianxiang Ma","Fei Dai","Songtao Zhao","Siyu Zhou","Qian He"],"pdf_url":"https://arxiv.org/pdf/2411.17765v2.pdf","comment":"Project page: https://wanquanf.github.io/I2VControl"},{"id":"http://arxiv.org/abs/2402.00672v3","updated":"2024-11-30T04:47:10Z","published":"2024-02-01T15:33:17Z","title":"Exploring Homogeneous and Heterogeneous Consistent Label Associations\n  for Unsupervised Visible-Infrared Person ReID","summary":"  Unsupervised visible-infrared person re-identification (USL-VI-ReID)\nendeavors to retrieve pedestrian images of the same identity from different\nmodalities without annotations. While prior work focuses on establishing\ncross-modality pseudo-label associations to bridge the modality-gap, they\nignore maintaining the instance-level homogeneous and heterogeneous consistency\nbetween the feature space and the pseudo-label space, resulting in coarse\nassociations. In response, we introduce a Modality-Unified Label Transfer\n(MULT) module that simultaneously accounts for both homogeneous and\nheterogeneous fine-grained instance-level structures, yielding high-quality\ncross-modality label associations. It models both homogeneous and heterogeneous\naffinities, leveraging them to quantify the inconsistency between the\npseudo-label space and the feature space, subsequently minimizing it. The\nproposed MULT ensures that the generated pseudo-labels maintain alignment\nacross modalities while upholding structural consistency within intra-modality.\nAdditionally, a straightforward plug-and-play Online Cross-memory Label\nRefinement (OCLR) module is proposed to further mitigate the side effects of\nnoisy pseudo-labels while simultaneously aligning different modalities, coupled\nwith an Alternative Modality-Invariant Representation Learning (AMIRL)\nframework. Experiments demonstrate that our proposed method outperforms\nexisting state-of-the-art USL-VI-ReID methods, highlighting the superiority of\nour MULT in comparison to other cross-modality association methods. Code is\navailable at https://github.com/FranklinLingfeng/code_for_MULT.\n","authors":["Lingfeng He","De Cheng","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2402.00672v3.pdf","comment":"Accepted by IJCV2024"},{"id":"http://arxiv.org/abs/2411.13615v3","updated":"2024-11-30T03:12:00Z","published":"2024-11-20T08:09:35Z","title":"A Deep Learning Approach to Predict the Fall [of Price] of\n  Cryptocurrency Long Before its Actual Fall","summary":"  In modern times, the cryptocurrency market is one of the world's most rapidly\nrising financial markets. The cryptocurrency market is regarded to be more\nvolatile and illiquid than traditional markets such as equities, foreign\nexchange, and commodities. The risk of this market creates an uncertain\ncondition among the investors. The purpose of this research is to predict the\nmagnitude of the risk factor of the cryptocurrency market. Risk factor is also\ncalled volatility. Our approach will assist people who invest in the\ncryptocurrency market by overcoming the problems and difficulties they\nexperience. Our approach starts with calculating the risk factor of the\ncryptocurrency market from the existing parameters. In twenty elements of the\ncryptocurrency market, the risk factor has been predicted using different\nmachine learning algorithms such as CNN, LSTM, BiLSTM, and GRU. All of the\nmodels have been applied to the calculated risk factor parameter. A new model\nhas been developed to predict better than the existing models. Our proposed\nmodel gives the highest RMSE value of 1.3229 and the lowest RMSE value of\n0.0089. Following our model, it will be easier for investors to trade in\ncomplicated and challenging financial assets like bitcoin, Ethereum, dogecoin,\netc. Where the other existing models, the highest RMSE was 14.5092, and the\nlower was 0.02769. So, the proposed model performs much better than models with\nproper generalization. Using our approach, it will be easier for investors to\ntrade in complicated and challenging financial assets like Bitcoin, Ethereum,\nand Dogecoin.\n","authors":["Anika Tahsin Meem"],"pdf_url":"https://arxiv.org/pdf/2411.13615v3.pdf","comment":"I am writing to formally request the withdrawal, which is necessary\n  due to issues with the author list and the need for improvements to the\n  manuscript. We apologize for any inconvenience caused by this request and\n  appreciate your understanding"},{"id":"http://arxiv.org/abs/2410.05651v2","updated":"2024-11-30T02:13:46Z","published":"2024-10-08T03:01:54Z","title":"ViBiDSampler: Enhancing Video Interpolation Using Bidirectional\n  Diffusion Sampler","summary":"  Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V)\ndiffusion models has greatly enhanced video generation, especially in terms of\nkeyframe interpolation. However, current image-to-video diffusion models, while\npowerful in generating videos from a single conditioning frame, need adaptation\nfor two-frame (start & end) conditioned generation, which is essential for\neffective bounded interpolation. Unfortunately, existing approaches that fuse\ntemporally forward and backward paths in parallel often suffer from\noff-manifold issues, leading to artifacts or requiring multiple iterative\nre-noising steps. In this work, we introduce a novel, bidirectional sampling\nstrategy to address these off-manifold issues without requiring extensive\nre-noising or fine-tuning. Our method employs sequential sampling along both\nforward and backward paths, conditioned on the start and end frames,\nrespectively, ensuring more coherent and on-manifold generation of intermediate\nframes. Additionally, we incorporate advanced guidance techniques, CFG++ and\nDDS, to further enhance the interpolation process. By integrating these, our\nmethod achieves state-of-the-art performance, efficiently generating\nhigh-quality, smooth videos between keyframes. On a single 3090 GPU, our method\ncan interpolate 25 frames at 1024 x 576 resolution in just 195 seconds,\nestablishing it as a leading solution for keyframe interpolation.\n","authors":["Serin Yang","Taesung Kwon","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2410.05651v2.pdf","comment":"Project page: https://vibidsampler.github.io/"},{"id":"http://arxiv.org/abs/2409.02574v2","updated":"2024-11-30T01:42:25Z","published":"2024-09-04T09:48:27Z","title":"Solving Video Inverse Problems Using Image Diffusion Models","summary":"  Recently, diffusion model-based inverse problem solvers (DIS) have emerged as\nstate-of-the-art approaches for addressing inverse problems, including image\nsuper-resolution, deblurring, inpainting, etc. However, their application to\nvideo inverse problems arising from spatio-temporal degradation remains largely\nunexplored due to the challenges in training video diffusion models. To address\nthis issue, here we introduce an innovative video inverse solver that leverages\nonly image diffusion models. Specifically, by drawing inspiration from the\nsuccess of the recent decomposed diffusion sampler (DDS), our method treats the\ntime dimension of a video as the batch dimension of image diffusion models and\nsolves spatio-temporal optimization problems within denoised spatio-temporal\nbatches derived from each image diffusion model. Moreover, we introduce a\nbatch-consistent diffusion sampling strategy that encourages consistency across\nbatches by synchronizing the stochastic noise components in image diffusion\nmodels. Our approach synergistically combines batch-consistent sampling with\nsimultaneous optimization of denoised spatio-temporal batches at each reverse\ndiffusion step, resulting in a novel and efficient diffusion sampling strategy\nfor video inverse problems. Experimental results demonstrate that our method\neffectively addresses various spatio-temporal degradations in video inverse\nproblems, achieving state-of-the-art reconstructions. Project page:\nhttps://svi-diffusion.github.io\n","authors":["Taesung Kwon","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2409.02574v2.pdf","comment":"22 pages, 16 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2302.11370v6","updated":"2024-11-30T21:47:39Z","published":"2023-02-22T13:39:54Z","title":"Recall, Robustness, and Lexicographic Evaluation","summary":"  Although originally developed to evaluate sets of items, recall is often used\nto evaluate rankings of items, including those produced by recommender,\nretrieval, and other machine learning systems. The application of recall\nwithout a formal evaluative motivation has led to criticism of recall as a\nvague or inappropriate measure. In light of this debate, we reflect on the\nmeasurement of recall in rankings from a formal perspective. Our analysis is\ncomposed of three tenets: recall, robustness, and lexicographic evaluation.\nFirst, we formally define `recall-orientation' as the sensitivity of a metric\nto a user interested in finding every relevant item. Second, we analyze\nrecall-orientation from the perspective of robustness with respect to possible\ncontent consumers and providers, connecting recall to recent conversations\nabout fair ranking. Finally, we extend this conceptual and theoretical\ntreatment of recall by developing a practical preference-based evaluation\nmethod based on lexicographic comparison. Through extensive empirical analysis\nacross three recommendation tasks and 17 information retrieval tasks, we\nestablish that our new evaluation method, lexirecall, has convergent validity\n(i.e., it is correlated with existing recall metrics) and exhibits\nsubstantially higher sensitivity in terms of discriminative power and stability\nin the presence of missing labels. Our conceptual, theoretical, and empirical\nanalysis substantially deepens our understanding of recall and motivates its\nadoption through connections to robustness and fairness.\n","authors":["Fernando Diaz","Michael D. Ekstrand","Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2302.11370v6.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.18721v2","updated":"2024-11-30T14:45:05Z","published":"2024-09-27T13:17:59Z","title":"Scalable Cross-Entropy Loss for Sequential Recommendations with Large\n  Item Catalogs","summary":"  Scalability issue plays a crucial role in productionizing modern recommender\nsystems. Even lightweight architectures may suffer from high computational\noverload due to intermediate calculations, limiting their practicality in\nreal-world applications. Specifically, applying full Cross-Entropy (CE) loss\noften yields state-of-the-art performance in terms of recommendations quality.\nStill, it suffers from excessive GPU memory utilization when dealing with large\nitem catalogs. This paper introduces a novel Scalable Cross-Entropy (SCE) loss\nfunction in the sequential learning setup. It approximates the CE loss for\ndatasets with large-size catalogs, enhancing both time efficiency and memory\nusage without compromising recommendations quality. Unlike traditional negative\nsampling methods, our approach utilizes a selective GPU-efficient computation\nstrategy, focusing on the most informative elements of the catalog,\nparticularly those most likely to be false positives. This is achieved by\napproximating the softmax distribution over a subset of the model outputs\nthrough the maximum inner product search. Experimental results on multiple\ndatasets demonstrate the effectiveness of SCE in reducing peak memory usage by\na factor of up to 100 compared to the alternatives, retaining or even exceeding\ntheir metrics values. The proposed approach also opens new perspectives for\nlarge-scale developments in different domains, such as large language models.\n","authors":["Gleb Mezentsev","Danil Gusak","Ivan Oseledets","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2409.18721v2.pdf","comment":"11 pages, fixed some typos"},{"id":"http://arxiv.org/abs/2410.19764v2","updated":"2024-11-30T07:06:57Z","published":"2024-10-12T16:14:18Z","title":"Unraveling Movie Genres through Cross-Attention Fusion of Bi-Modal\n  Synergy of Poster","summary":"  Movie posters are not just decorative; they are meticulously designed to\ncapture the essence of a movie, such as its genre, storyline, and tone/vibe.\nFor decades, movie posters have graced cinema walls, billboards, and now our\ndigital screens as a form of digital posters. Movie genre classification plays\na pivotal role in film marketing, audience engagement, and recommendation\nsystems. Previous explorations into movie genre classification have been mostly\nexamined in plot summaries, subtitles, trailers and movie scenes. Movie posters\nprovide a pre-release tantalizing glimpse into a film's key aspects, which can\nignite public interest. In this paper, we presented the framework that exploits\nmovie posters from a visual and textual perspective to address the multilabel\nmovie genre classification problem. Firstly, we extracted text from movie\nposters using an OCR and retrieved the relevant embedding. Next, we introduce a\ncross-attention-based fusion module to allocate attention weights to visual and\ntextual embedding. In validating our framework, we utilized 13882 posters\nsourced from the Internet Movie Database (IMDb). The outcomes of the\nexperiments indicate that our model exhibited promising performance and\noutperformed even some prominent contemporary architectures.\n","authors":["Utsav Kumar Nareti","Chandranath Adak","Soumi Chattopadhyay","Pichao Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19764v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2411.13323v2","updated":"2024-11-30T23:44:43Z","published":"2024-11-20T13:46:04Z","title":"Are Large Language Models Memorizing Bug Benchmarks?","summary":"  Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage. In this paper, we systematically evaluate popular\nLLMs to assess their susceptibility to data leakage from widely used bug\nbenchmarks. To identify potential leakage, we use multiple metrics, including a\nstudy of benchmark membership within commonly used training datasets, as well\nas analyses of negative log-likelihood and n-gram accuracy. Our findings show\nthat certain models, in particular codegen-multi, exhibit significant evidence\nof memorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities.\n","authors":["Daniel Ramos","Claudia Mamede","Kush Jain","Paulo Canelas","Catarina Gamboa","Claire Le Goues"],"pdf_url":"https://arxiv.org/pdf/2411.13323v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.02871v3","updated":"2024-11-30T22:31:53Z","published":"2024-03-05T11:29:05Z","title":"Quantum Mixed-State Self-Attention Network","summary":"  Attention mechanisms have revolutionized natural language processing.\nCombining them with quantum computing aims to further advance this technology.\nThis paper introduces a novel Quantum Mixed-State Self-Attention Network\n(QMSAN) for natural language processing tasks. Our model leverages quantum\ncomputing principles to enhance the effectiveness of self-attention mechanisms.\nQMSAN uses a quantum attention mechanism based on mixed state, allowing for\ndirect similarity estimation between queries and keys in the quantum domain.\nThis approach leads to more effective attention coefficient calculations. We\nalso propose an innovative quantum positional encoding scheme, implemented\nthrough fixed quantum gates within the circuit, improving the model's ability\nto capture sequence information without additional qubit resources. In\nnumerical experiments of text classification tasks on public datasets, QMSAN\noutperforms Quantum Self-Attention Neural Network (QSANN). Furthermore, we\ndemonstrate QMSAN's robustness in different quantum noise environments,\nhighlighting its potential for near-term quantum devices.\n","authors":["Fu Chen","Qinglin Zhao","Li Feng","Chuangtao Chen","Yangbin Lin","Jianhong Lin"],"pdf_url":"https://arxiv.org/pdf/2403.02871v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14165v3","updated":"2024-11-30T22:21:30Z","published":"2024-09-21T15:07:37Z","title":"A Survey on Large Language Model-empowered Autonomous Driving","summary":"  Artificial intelligence (AI) plays a crucial role in autonomous driving (AD)\nresearch, propelling its development towards intelligence and efficiency.\nCurrently, the development of AD technology follows two main technical paths:\nmodularization and end-to-end. Modularization decompose the driving task into\nmodules such as perception, prediction, planning, and control, and train them\nseparately. Due to the inconsistency of training objectives between modules,\nthe integrated effect suffers from bias. End-to-end attempts to address this\nissue by utilizing a single model that directly maps from sensor data to\ncontrol signals. This path has limited learning capabilities in a comprehensive\nset of features and struggles to handle unpredictable long-tail events and\ncomplex urban traffic scenarios. In the face of challenges encountered in both\npaths, many researchers believe that large language models (LLMs) with powerful\nreasoning capabilities and extensive knowledge understanding may be the\nsolution, expecting LLMs to provide AD systems with deeper levels of\nunderstanding and decision-making capabilities. In light of the challenges\nfaced by both paths, many researchers believe that LLMs, with their powerful\nreasoning abilities and extensive knowledge, could offer a solution. To\nunderstand if LLMs could enhance AD, this paper conducts a thorough analysis of\nthe potential applications of LLMs in AD systems, including exploring their\noptimization strategies in both modular and end-to-end approaches, with a\nparticular focus on how LLMs can tackle the problems and challenges present in\ncurrent solutions. Furthermore, we discuss an important question: Can LLM-based\nartificial general intelligence (AGI) be a key to achieve high-level AD? We\nfurther analyze the potential limitations and challenges that LLMs may\nencounter in promoting the development of AD technology.\n","authors":["Yuxuan Zhu","Shiyi Wang","Wenqing Zhong","Nianchen Shen","Yunqi Li","Siqi Wang","Zhiheng Li","Cathy Wu","Zhengbing He","Li Li"],"pdf_url":"https://arxiv.org/pdf/2409.14165v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.19764v2","updated":"2024-11-30T07:06:57Z","published":"2024-10-12T16:14:18Z","title":"Unraveling Movie Genres through Cross-Attention Fusion of Bi-Modal\n  Synergy of Poster","summary":"  Movie posters are not just decorative; they are meticulously designed to\ncapture the essence of a movie, such as its genre, storyline, and tone/vibe.\nFor decades, movie posters have graced cinema walls, billboards, and now our\ndigital screens as a form of digital posters. Movie genre classification plays\na pivotal role in film marketing, audience engagement, and recommendation\nsystems. Previous explorations into movie genre classification have been mostly\nexamined in plot summaries, subtitles, trailers and movie scenes. Movie posters\nprovide a pre-release tantalizing glimpse into a film's key aspects, which can\nignite public interest. In this paper, we presented the framework that exploits\nmovie posters from a visual and textual perspective to address the multilabel\nmovie genre classification problem. Firstly, we extracted text from movie\nposters using an OCR and retrieved the relevant embedding. Next, we introduce a\ncross-attention-based fusion module to allocate attention weights to visual and\ntextual embedding. In validating our framework, we utilized 13882 posters\nsourced from the Internet Movie Database (IMDb). The outcomes of the\nexperiments indicate that our model exhibited promising performance and\noutperformed even some prominent contemporary architectures.\n","authors":["Utsav Kumar Nareti","Chandranath Adak","Soumi Chattopadhyay","Pichao Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19764v2.pdf","comment":null}]},"2024-11-29T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2404.07066v5","updated":"2024-11-29T21:50:16Z","published":"2024-04-10T14:56:40Z","title":"Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?","summary":"  Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of \"Concept Depth\" to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}.\n","authors":["Mingyu Jin","Qinkai Yu","Jingyuan Huang","Qingcheng Zeng","Zhenting Wang","Wenyue Hua","Haiyan Zhao","Kai Mei","Yanda Meng","Kaize Ding","Fan Yang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.07066v5.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2402.11512v4","updated":"2024-11-29T21:35:25Z","published":"2024-02-18T08:53:41Z","title":"From Prejudice to Parity: A New Approach to Debiasing Large Language\n  Model Word Embeddings","summary":"  Embeddings play a pivotal role in the efficacy of Large Language Models. They\nare the bedrock on which these models grasp contextual relationships and foster\na more nuanced understanding of language and consequently perform remarkably on\na plethora of complex tasks that require a fundamental understanding of human\nlanguage. Given that these embeddings themselves often reflect or exhibit bias,\nit stands to reason that these models may also inadvertently learn this bias.\nIn this work, we build on the seminal previous work and propose DeepSoftDebias,\nan algorithm that uses a neural network to perform 'soft debiasing'. We\nexhaustively evaluate this algorithm across a variety of SOTA datasets,\naccuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias\noutperforms the current state-of-the-art methods at reducing bias across\ngender, race, and religion.\n","authors":["Aishik Rakshit","Smriti Singh","Shuvam Keshari","Arijit Ghosh Chowdhury","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2402.11512v4.pdf","comment":"Accepted at COLING 2025"},{"id":"http://arxiv.org/abs/2405.10718v2","updated":"2024-11-29T19:01:20Z","published":"2024-05-17T12:01:43Z","title":"SignLLM: Sign Language Production Large Language Models","summary":"  In this paper, we propose SignLLM, a multilingual Sign Language Production\n(SLP) large language model, which includes two novel multilingual SLP modes\nMLSF and Prompt2LangGloss that allow sign language gestures generation from\nquery texts input and question-style prompts input respectively. Both modes can\nuse a new RL loss based on reinforcement learning and a new RL module named\nPriority Learning Channel. These RL components can accelerate the training by\nenhancing the model's capability to sample high-quality data. For SignLLM's\ntraining, we introduce Prompt2Sign, a comprehensive multilingual sign language\ndataset, which builds from public data, including American Sign Language (ASL)\nand seven others. This dataset standardizes information by extracting pose\ninformation from sign language videos into a unified compressed format. We\nextensively evaluate SignLLM, demonstrating that our model achieves\nstate-of-the-art performance on SLP tasks across eight sign languages.\n","authors":["Sen Fang","Lei Wang","Ce Zheng","Chunyu Sui","Mingyu Zhao","Yapeng Tian","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2405.10718v2.pdf","comment":"website at https://signllm.github.io/"},{"id":"http://arxiv.org/abs/2410.12049v2","updated":"2024-11-29T19:00:58Z","published":"2024-10-15T20:37:34Z","title":"Sabiá-3 Technical Report","summary":"  This report presents Sabi\\'a-3, our new flagship language model, and\nSabiazinho-3, a more cost-effective sibling. The models were trained on a large\nbrazilian-centric corpus. Evaluations across diverse professional and academic\nbenchmarks show a strong performance on Portuguese and Brazil-related tasks.\nSabi\\'a-3 shows large improvements in comparison to our previous best of model,\nSabia-2 Medium, especially in reasoning-intensive tasks. Notably, Sabi\\'a-3's\naverage performance matches frontier LLMs, while it is offered at a three to\nfour times lower cost per token, reinforcing the benefits of domain\nspecialization.\n","authors":["Hugo Abonizio","Thales Sales Almeida","Thiago Laitz","Roseval Malaquias Junior","Giovana Kerche Bonás","Rodrigo Nogueira","Ramon Pires"],"pdf_url":"https://arxiv.org/pdf/2410.12049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19941v1","updated":"2024-11-29T18:57:25Z","published":"2024-11-29T18:57:25Z","title":"Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA\n  Benchmark","summary":"  Following the successful 2023 edition, we organised the Second Perception\nTest challenge as a half-day workshop alongside the IEEE/CVF European\nConference on Computer Vision (ECCV) 2024, with the goal of benchmarking\nstate-of-the-art video models and measuring the progress since last year using\nthe Perception Test benchmark. This year, the challenge had seven tracks (up\nfrom six last year) and covered low-level and high-level tasks, with language\nand non-language interfaces, across video, audio, and text modalities; the\nadditional track covered hour-long video understanding and introduced a novel\nvideo QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks\nwere: object tracking, point tracking, temporal action localisation, temporal\nsound localisation, multiple-choice video question-answering, grounded video\nquestion-answering, and hour-long video question-answering. We summarise in\nthis report the challenge tasks and results, and introduce in detail the novel\nhour-long video QA benchmark 1h-walk VQA.\n","authors":["Joseph Heyward","João Carreira","Dima Damen","Andrew Zisserman","Viorica Pătrăucean"],"pdf_url":"https://arxiv.org/pdf/2411.19941v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2312.13090"},{"id":"http://arxiv.org/abs/2411.19939v1","updated":"2024-11-29T18:56:37Z","published":"2024-11-29T18:56:37Z","title":"VLSBench: Unveiling Visual Leakage in Multimodal Safety","summary":"  Safety concerns of Multimodal large language models (MLLMs) have gradually\nbecome an important problem in various applications. Surprisingly, previous\nworks indicate a counter-intuitive phenomenon that using textual unlearning to\nalign MLLMs achieves comparable safety performances with MLLMs trained with\nimage-text pairs. To explain such a counter-intuitive phenomenon, we discover a\nvisual safety information leakage (VSIL) problem in existing multimodal safety\nbenchmarks, i.e., the potentially risky and sensitive content in the image has\nbeen revealed in the textual query. In this way, MLLMs can easily refuse these\nsensitive text-image queries according to textual queries. However, image-text\npairs without VSIL are common in real-world scenarios and are overlooked by\nexisting multimodal safety benchmarks. To this end, we construct multimodal\nvisual leakless safety benchmark (VLSBench) preventing visual safety leakage\nfrom image to textual query with 2.4k image-text pairs. Experimental results\nindicate that VLSBench poses a significant challenge to both open-source and\nclose-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.\nThis study demonstrates that textual alignment is enough for multimodal safety\nscenarios with VSIL, while multimodal alignment is a more promising solution\nfor multimodal safety scenarios without VSIL. Please see our code and data at:\nhttp://hxhcreate.github.io/VLSBench\n","authors":["Xuhao Hu","Dongrui Liu","Hao Li","Xuanjing Huang","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2411.19939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19930v1","updated":"2024-11-29T18:42:28Z","published":"2024-11-29T18:42:28Z","title":"On Domain-Specific Post-Training for Multimodal Large Language Models","summary":"  Recent years have witnessed the rapid development of general multimodal large\nlanguage models (MLLMs). However, adapting general MLLMs to specific domains,\nsuch as scientific fields and industrial applications, remains less explored.\nThis paper systematically investigates domain adaptation of MLLMs through\npost-training, focusing on data synthesis, training pipelines, and task\nevaluation. (1) Data Synthesis: Using open-source models, we develop a visual\ninstruction synthesizer that effectively generates diverse visual instruction\ntasks from domain-specific image-caption pairs. Our synthetic tasks surpass\nthose generated by manual rules, GPT-4, and GPT-4V in enhancing the\ndomain-specific performance of MLLMs. (2) Training Pipeline: While the\ntwo-stage training--initially on image-caption pairs followed by visual\ninstruction tasks--is commonly adopted for developing general MLLMs, we apply a\nsingle-stage training pipeline to enhance task diversity for domain-specific\npost-training. (3) Task Evaluation: We conduct experiments in two domains,\nbiomedicine and food, by post-training MLLMs of different sources and scales\n(e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM\nperformance on various domain-specific tasks. To support further research in\nMLLM domain adaptation, we will open-source our implementations.\n","authors":["Daixuan Cheng","Shaohan Huang","Ziyu Zhu","Xintong Zhang","Wayne Xin Zhao","Zhongzhi Luan","Bo Dai","Zhenliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.19930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19921v1","updated":"2024-11-29T18:36:15Z","published":"2024-11-29T18:36:15Z","title":"SIMS: Simulating Human-Scene Interactions with Real World Script\n  Planning","summary":"  Simulating long-term human-scene interaction is a challenging yet fascinating\ntask. Previous works have not effectively addressed the generation of long-term\nhuman scene interactions with detailed narratives for physics-based animation.\nThis paper introduces a novel framework for the planning and controlling of\nlong-horizon physical plausible human-scene interaction. On the one hand, films\nand shows with stylish human locomotions or interactions with scenes are\nabundantly available on the internet, providing a rich source of data for\nscript planning. On the other hand, Large Language Models (LLMs) can understand\nand generate logical storylines.\n  This motivates us to marry the two by using an LLM-based pipeline to extract\nscripts from videos, and then employ LLMs to imitate and create new scripts,\ncapturing complex, time-series human behaviors and interactions with\nenvironments. By leveraging this, we utilize a dual-aware policy that achieves\nboth language comprehension and scene understanding to guide character motions\nwithin contextual and spatial constraints. To facilitate training and\nevaluation, we contribute a comprehensive planning dataset containing diverse\nmotion sequences extracted from real-world videos and expand them with large\nlanguage models. We also collect and re-annotate motion clips from existing\nkinematic datasets to enable our policy learn diverse skills. Extensive\nexperiments demonstrate the effectiveness of our framework in versatile task\nexecution and its generalization ability to various scenarios, showing\nremarkably enhanced performance compared with existing methods. Our code and\ndata will be publicly available soon.\n","authors":["Wenjia Wang","Liang Pan","Zhiyang Dou","Zhouyingcheng Liao","Yuke Lou","Lei Yang","Jingbo Wang","Taku Komura"],"pdf_url":"https://arxiv.org/pdf/2411.19921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19906v1","updated":"2024-11-29T18:11:39Z","published":"2024-11-29T18:11:39Z","title":"Classical and Quantum Algorithms for the Deterministic L-system\n  Inductive Inference Problem","summary":"  L-systems can be made to model and create simulations of many biological\nprocesses, such as plant development. Finding an L-system for a given process\nis typically solved by hand, by experts, in a hugely time-consuming process. It\nwould be significant if this could be done automatically from data, such as\nfrom sequences of images. In this paper, we are interested in inferring a\nparticular type of L-system, deterministic context-free L-system (D0L-system)\nfrom a sequence of strings. We introduce the characteristic graph of a sequence\nof strings, which we then utilize to translate our problem (inferring\nD0L-system) in polynomial time into the maximum independent set problem (MIS)\nand the SAT problem. After that, we offer a classical exact algorithm and an\napproximate quantum algorithm for the problem.\n","authors":["Ali Lotfi","Ian McQuillan","Steven Rayan"],"pdf_url":"https://arxiv.org/pdf/2411.19906v1.pdf","comment":"16 pages, 1 figure"},{"id":"http://arxiv.org/abs/2411.19869v1","updated":"2024-11-29T17:31:42Z","published":"2024-11-29T17:31:42Z","title":"AIDetx: a compression-based method for identification of\n  machine-learning generated text","summary":"  This paper introduces AIDetx, a novel method for detecting machine-generated\ntext using data compression techniques. Traditional approaches, such as deep\nlearning classifiers, often suffer from high computational costs and limited\ninterpretability. To address these limitations, we propose a compression-based\nclassification framework that leverages finite-context models (FCMs). AIDetx\nconstructs distinct compression models for human-written and AI-generated text,\nclassifying new inputs based on which model achieves a higher compression\nratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores\nexceeding 97% and 99%, respectively, highlighting its high accuracy. Compared\nto current methods, such as large language models (LLMs), AIDetx offers a more\ninterpretable and computationally efficient solution, significantly reducing\nboth training time and hardware requirements (e.g., no GPUs needed). The full\nimplementation is publicly available at https://github.com/AIDetx/AIDetx.\n","authors":["Leonardo Almeida","Pedro Rodrigues","Diogo Magalhães","Armando J. Pinho","Diogo Pratas"],"pdf_url":"https://arxiv.org/pdf/2411.19869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19865v1","updated":"2024-11-29T17:27:05Z","published":"2024-11-29T17:27:05Z","title":"Reverse Thinking Makes LLMs Stronger Reasoners","summary":"  Reverse thinking plays a crucial role in human reasoning. Humans can reason\nnot only from a problem to a solution but also in reverse, i.e., start from the\nsolution and reason towards the problem. This often enhances overall reasoning\nperformance as it enables consistency checks between their forward and backward\nthinking. To enable Large Language Models (LLMs) to perform reverse thinking,\nwe introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data\naugmentation and learning objectives. In RevThink, we augment the dataset by\ncollecting structured forward-backward reasoning from a teacher model,\nconsisting of: (1) the original question, (2) forward reasoning, (3) backward\nquestion, and (4) backward reasoning. We then employ three objectives to train\na smaller student model in a multi-task learning fashion: (a) generate forward\nreasoning from a question, (b) generate a backward question from a question,\nand (c) generate backward reasoning from the backward question. Experiments\nacross 12 datasets covering commonsense, math, and logical reasoning show an\naverage 13.53% improvement over the student model's zero-shot performance and a\n6.84% improvement over the strongest knowledge distillation baselines.\nMoreover, our method demonstrates sample efficiency -- using only 10% of the\ncorrect forward reasoning from the training data, it outperforms a standard\nfine-tuning method trained on 10x more forward reasoning. RevThink also\nexhibits strong generalization to out-of-distribution held-out datasets.\n","authors":["Justin Chih-Yao Chen","Zifeng Wang","Hamid Palangi","Rujun Han","Sayna Ebrahimi","Long Le","Vincent Perot","Swaroop Mishra","Mohit Bansal","Chen-Yu Lee","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2411.19865v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2411.15623v2","updated":"2024-11-29T17:18:49Z","published":"2024-11-23T18:27:35Z","title":"Multi-label Sequential Sentence Classification via Large Language Model","summary":"  Sequential sentence classification (SSC) in scientific publications is\ncrucial for supporting downstream tasks such as fine-grained information\nretrieval and extractive summarization. However, current SSC methods are\nconstrained by model size, sequence length, and single-label setting. To\naddress these limitations, this paper proposes LLM-SSC, a large language model\n(LLM)-based framework for both single- and multi-label SSC tasks. Unlike\nprevious approaches that employ small- or medium-sized language models, the\nproposed framework utilizes LLMs to generate SSC labels through designed\nprompts, which enhance task understanding by incorporating demonstrations and a\nquery to describe the prediction target. We also present a multi-label\ncontrastive learning loss with auto-weighting scheme, enabling the multi-label\nclassification task. To support our multi-label SSC analysis, we introduce and\nrelease a new dataset, biorc800, which mainly contains unstructured abstracts\nin the biomedical domain with manual annotations. Experiments demonstrate\nLLM-SSC's strong performance in SSC under both in-context learning and\ntask-specific tuning settings. We release biorc800 and our code at:\nhttps://github.com/ScienceNLP-Lab/LLM-SSC.\n","authors":["Mengfei Lan","Lecheng Zheng","Shufan Ming","Halil Kilicoglu"],"pdf_url":"https://arxiv.org/pdf/2411.15623v2.pdf","comment":"Accepted by EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2411.19858v1","updated":"2024-11-29T17:12:06Z","published":"2024-11-29T17:12:06Z","title":"What fifty-one years of Linguistics and Artificial Intelligence research\n  tell us about their correlation: A scientometric review","summary":"  There is a strong correlation between linguistics and artificial intelligence\n(AI), best manifested by deep learning language models. This study provides a\nthorough scientometric analysis of this correlation, synthesizing the\nintellectual production during 51 years, from 1974 to 2024. It involves 5750\nWeb of Science-indexed articles published in 2124 journals, which are written\nby 20835 authors belonging to 13773 research centers in 794 countries. Two\npowerful software, viz., CiteSpace and VOSviewer, were used to generate mapping\nvisualizations of the intellectual landscape, trending issues and (re)emerging\nhotspots. The results indicate that in the 1980s and 1990s, linguistics and AI\nresearch was not robust, characterized by unstable publication over time. It\nhas, however, witnessed a remarkable increase of publication since then,\nreaching 1478 articles in 2023, and 546 articles in January-March timespan in\n2024, involving emerging issues and hotspots, addressing new horizons, new\ntopics, and launching new applications and powerful deep learning language\nmodels including ChatGPT.\n","authors":["Mohammed Q. Shormani"],"pdf_url":"https://arxiv.org/pdf/2411.19858v1.pdf","comment":"26 pages, 15 figures"},{"id":"http://arxiv.org/abs/2411.19832v1","updated":"2024-11-29T16:44:02Z","published":"2024-11-29T16:44:02Z","title":"Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation","summary":"  The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others.\n","authors":["Dimosthenis Antypas","Indira Sen","Carla Perez-Almendros","Jose Camacho-Collados","Francesco Barbieri"],"pdf_url":"https://arxiv.org/pdf/2411.19832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19822v1","updated":"2024-11-29T16:31:50Z","published":"2024-11-29T16:31:50Z","title":"SDR-GNN: Spectral Domain Reconstruction Graph Neural Network for\n  Incomplete Multimodal Learning in Conversational Emotion Recognition","summary":"  Multimodal Emotion Recognition in Conversations (MERC) aims to classify\nutterance emotions using textual, auditory, and visual modal features. Most\nexisting MERC methods assume each utterance has complete modalities,\noverlooking the common issue of incomplete modalities in real-world scenarios.\nRecently, graph neural networks (GNNs) have achieved notable results in\nIncomplete Multimodal Emotion Recognition in Conversations (IMERC). However,\ntraditional GNNs focus on binary relationships between nodes, limiting their\nability to capture more complex, higher-order information. Moreover, repeated\nmessage passing can cause over-smoothing, reducing their capacity to preserve\nessential high-frequency details. To address these issues, we propose a\nSpectral Domain Reconstruction Graph Neural Network (SDR-GNN) for incomplete\nmultimodal learning in conversational emotion recognition. SDR-GNN constructs\nan utterance semantic interaction graph using a sliding window based on both\nspeaker and context relationships to model emotional dependencies. To capture\nhigher-order and high-frequency information, SDR-GNN utilizes weighted\nrelationship aggregation, ensuring consistent semantic feature extraction\nacross utterances. Additionally, it performs multi-frequency aggregation in the\nspectral domain, enabling efficient recovery of incomplete modalities by\nextracting both high- and low-frequency information. Finally, multi-head\nattention is applied to fuse and optimize features for emotion recognition.\nExtensive experiments on various real-world datasets demonstrate that our\napproach is effective in incomplete multimodal learning and outperforms current\nstate-of-the-art methods.\n","authors":["Fangze Fu","Wei Ai","Fan Yang","Yuntao Shou","Tao Meng","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2411.19822v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2405.18653v2","updated":"2024-11-29T16:19:01Z","published":"2024-05-28T23:32:46Z","title":"Recent Advances of Foundation Language Models-based Continual Learning:\n  A Survey","summary":"  Recently, foundation language models (LMs) have marked significant\nachievements in the domains of natural language processing (NLP) and computer\nvision (CV). Unlike traditional neural network models, foundation LMs obtain a\ngreat ability for transfer learning by acquiring rich commonsense knowledge\nthrough pre-training on extensive unsupervised datasets with a vast number of\nparameters. However, they still can not emulate human-like continuous learning\ndue to catastrophic forgetting. Consequently, various continual learning\n(CL)-based methodologies have been developed to refine LMs, enabling them to\nadapt to new tasks without forgetting previous knowledge. However, a systematic\ntaxonomy of existing approaches and a comparison of their performance are still\nlacking, which is the gap that our survey aims to fill. We delve into a\ncomprehensive review, summarization, and classification of the existing\nliterature on CL-based approaches applied to foundation language models, such\nas pre-trained language models (PLMs), large language models (LLMs) and\nvision-language models (VLMs). We divide these studies into offline CL and\nonline CL, which consist of traditional methods, parameter-efficient-based\nmethods, instruction tuning-based methods and continual pre-training methods.\nOffline CL encompasses domain-incremental learning, task-incremental learning,\nand class-incremental learning, while online CL is subdivided into hard task\nboundary and blurry task boundary settings. Additionally, we outline the\ntypical datasets and metrics employed in CL research and provide a detailed\nanalysis of the challenges and future work for LMs-based continual learning.\n","authors":["Yutao Yang","Jie Zhou","Xuanwen Ding","Tianyu Huai","Shunyu Liu","Qin Chen","Yuan Xie","Liang He"],"pdf_url":"https://arxiv.org/pdf/2405.18653v2.pdf","comment":"Accepted by ACM Computing Survey"},{"id":"http://arxiv.org/abs/2410.08130v2","updated":"2024-11-29T16:18:29Z","published":"2024-10-10T17:14:36Z","title":"Think Beyond Size: Adaptive Prompting for More Effective Reasoning","summary":"  Pretrained large language models (LLMs) are increasingly utilized across a\nwide range of natural language processing (NLP) tasks due to their impressive\ncapabilities as few-shot learners. Recent techniques, such as chain-of-thought\n(CoT) prompting, have significantly advanced multi-step reasoning by\nintroducing step-by-step decomposition, achieving state-of-the-art results on\ncomplex reasoning benchmarks. However, these approaches often rely on static\nprompting templates that do not adapt to task complexity or errors during the\nreasoning process. In this work, we introduce Adaptive Prompting, a dynamic and\niterative framework designed to enhance reasoning by incorporating real-time\nadjustments to prompt structures and validation mechanisms.Experimental results\ndemonstrate that Adaptive Prompting significantly improves performance on\ndiverse reasoning benchmarks, including arithmetic reasoning (GSM8K,\nMultiArith), logical reasoning and commonsense tasks, achieving substantial\naccuracy gains compared to static prompting baselines. By integrating guided\nprompts, intermediate validation, and self-corrective steps, our approach\nenables smaller models to achieve competitive performance with larger\ncounterparts, such as GPT-4, while maintaining computational efficiency. The\nframework achieves this without requiring fine-tuning or task-specific training\ndata, highlighting the untapped potential of iterative reasoning methods.\n","authors":["Kamesh R"],"pdf_url":"https://arxiv.org/pdf/2410.08130v2.pdf","comment":"Submitted to ICLR 2025. This is a preprint version. Future revisions\n  will include additional evaluations and refinements"},{"id":"http://arxiv.org/abs/2411.19799v1","updated":"2024-11-29T16:03:14Z","published":"2024-11-29T16:03:14Z","title":"INCLUDE: Evaluating Multilingual Language Understanding with Regional\n  Knowledge","summary":"  The performance differential of large language models (LLM) between languages\nhinders their effective deployment in many regions, inhibiting the potential\neconomic and societal value of generative AI tools in many communities.\nHowever, the development of functional LLMs in many languages (\\ie,\nmultilingual LLMs) is bottlenecked by the lack of high-quality evaluation\nresources in languages other than English. Moreover, current practices in\nmultilingual benchmark construction often translate English resources, ignoring\nthe regional and cultural knowledge of the environments in which multilingual\nsystems would be used. In this work, we construct an evaluation suite of\n197,243 QA pairs from local exam sources to measure the capabilities of\nmultilingual LLMs in a variety of regional contexts. Our novel resource,\nINCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across\n44 written languages that evaluates multilingual LLMs for performance in the\nactual language environments where they would be deployed.\n","authors":["Angelika Romanou","Negar Foroutan","Anna Sotnikova","Zeming Chen","Sree Harsha Nelaturu","Shivalika Singh","Rishabh Maheshwary","Micol Altomare","Mohamed A. Haggag","Snegha A","Alfonso Amayuelas","Azril Hafizi Amirudin","Viraat Aryabumi","Danylo Boiko","Michael Chang","Jenny Chim","Gal Cohen","Aditya Kumar Dalmia","Abraham Diress","Sharad Duwal","Daniil Dzenhaliou","Daniel Fernando Erazo Florez","Fabian Farestam","Joseph Marvin Imperial","Shayekh Bin Islam","Perttu Isotalo","Maral Jabbarishiviari","Börje F. Karlsson","Eldar Khalilov","Christopher Klamm","Fajri Koto","Dominik Krzemiński","Gabriel Adriano de Melo","Syrielle Montariol","Yiyang Nan","Joel Niklaus","Jekaterina Novikova","Johan Samir Obando Ceron","Debjit Paul","Esther Ploeger","Jebish Purbey","Swati Rajwal","Selvan Sunitha Ravi","Sara Rydell","Roshan Santhosh","Drishti Sharma","Marjana Prifti Skenduli","Arshia Soltani Moakhar","Bardia Soltani Moakhar","Ran Tamir","Ayush Kumar Tarun","Azmine Toushik Wasi","Thenuka Ovin Weerasinghe","Serhan Yilmaz","Mike Zhang","Imanol Schlag","Marzieh Fadaee","Sara Hooker","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2411.19799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.13549v4","updated":"2024-11-29T15:51:23Z","published":"2023-06-23T15:21:52Z","title":"A Survey on Multimodal Large Language Models","summary":"  Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has\nbeen a new rising research hotspot, which uses powerful Large Language Models\n(LLMs) as a brain to perform multimodal tasks. The surprising emergent\ncapabilities of MLLM, such as writing stories based on images and OCR-free math\nreasoning, are rare in traditional multimodal methods, suggesting a potential\npath to artificial general intelligence. To this end, both academia and\nindustry have endeavored to develop MLLMs that can compete with or even better\nthan GPT-4V, pushing the limit of research at a surprising speed. In this\npaper, we aim to trace and summarize the recent progress of MLLMs. First of\nall, we present the basic formulation of MLLM and delineate its related\nconcepts, including architecture, training strategy and data, as well as\nevaluation. Then, we introduce research topics about how MLLMs can be extended\nto support more granularity, modalities, languages, and scenarios. We continue\nwith multimodal hallucination and extended techniques, including Multimodal ICL\n(M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To\nconclude the paper, we discuss existing challenges and point out promising\nresearch directions. In light of the fact that the era of MLLM has only just\nbegun, we will keep updating this survey and hope it can inspire more research.\nAn associated GitHub link collecting the latest papers is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.\n","authors":["Shukang Yin","Chaoyou Fu","Sirui Zhao","Ke Li","Xing Sun","Tong Xu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2306.13549v4.pdf","comment":"Accepted for publication in National Science Review. Project\n  page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models"},{"id":"http://arxiv.org/abs/2404.02837v2","updated":"2024-11-29T15:49:20Z","published":"2024-04-03T16:16:31Z","title":"Cherry on Top: Parameter Heterogeneity and Quantization in Large\n  Language Models","summary":"  This paper reveals the phenomenon of parameter heterogeneity in large\nlanguage models (LLMs). We find that a small subset of \"cherry\" parameters\nexhibit a disproportionately large influence on model performance, while the\nvast majority of parameters have minimal impact. This heterogeneity is found to\nbe prevalent across different model families, scales, and types. Motivated by\nthis observation, we propose CherryQ, a novel quantization method that unifies\nthe optimization of mixed-precision parameters. CherryQ identifies and\npreserves the critical cherry parameters in high precision while aggressively\nquantizing the remaining parameters to low precision. Extensive experiments\ndemonstrate the effectiveness of CherryQ. CherryQ outperforms existing\nquantization approaches in terms of perplexity and downstream task performance.\nNotably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance\ncompared to their 16-bit counterparts.\n","authors":["Wanyun Cui","Qianle Wang"],"pdf_url":"https://arxiv.org/pdf/2404.02837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19786v1","updated":"2024-11-29T15:48:24Z","published":"2024-11-29T15:48:24Z","title":"MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks","summary":"  Recently, human motion analysis has experienced great improvement due to\ninspiring generative models such as the denoising diffusion model and large\nlanguage model. While the existing approaches mainly focus on generating\nmotions with textual descriptions and overlook the reciprocal task. In this\npaper, we present~\\textbf{MoTe}, a unified multi-modal model that could handle\ndiverse tasks by learning the marginal, conditional, and joint distributions of\nmotion and text simultaneously. MoTe enables us to handle the paired\ntext-motion generation, motion captioning, and text-driven motion generation by\nsimply modifying the input context. Specifically, MoTe is composed of three\ncomponents: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), and\nMoti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained for\nextracting latent embeddings, and subsequently reconstructing the motion\nsequences and textual descriptions from the extracted embeddings, respectively.\nMTDM, on the other hand, performs an iterative denoising process on the input\ncontext to handle diverse tasks. Experimental results on the benchmark datasets\ndemonstrate the superior performance of our proposed method on text-to-motion\ngeneration and competitive performance on motion captioning.\n","authors":["Yiming Wu","Wei Ji","Kecheng Zheng","Zicheng Wang","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2411.19786v1.pdf","comment":"Five figures, six tables"},{"id":"http://arxiv.org/abs/2410.09432v2","updated":"2024-11-29T15:47:03Z","published":"2024-10-12T08:22:44Z","title":"Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation\n  Models","summary":"  Low-Rank Adaptation (LoRA) is a popular technique for efficient fine-tuning\nof foundation models. However, applying LoRA in federated learning\nenvironments, where data is distributed across multiple clients, presents\nunique challenges. Existing methods rely on traditional federated averaging of\nLoRA adapters, resulting in inexact updates. To address this, we propose\nFederated Exact LoRA, or FedExLoRA, which adds a residual error term to the\npretrained frozen weight matrix. Our approach achieves exact updates with\nminimal computational and communication overhead, preserving LoRA's efficiency.\nWe evaluate the method on various models across arithmetic reasoning,\ncommonsense reasoning, natural language understanding and natural language\ngeneration tasks, showing consistent performance gains over state-of-the-art\nmethods across multiple settings. Through extensive analysis, we quantify that\nthe deviations in updates from the ideal solution are significant, highlighting\nthe need for exact aggregation. Our method's simplicity, efficiency, and broad\napplicability position it as a promising solution for accurate and effective\nfederated fine-tuning of foundation models. Our code is publicly available at\nhttps://github.com/RaghavSinghal10/fedex-lora.\n","authors":["Raghav Singhal","Kaustubh Ponkshe","Praneeth Vepakomma"],"pdf_url":"https://arxiv.org/pdf/2410.09432v2.pdf","comment":"Raghav Singhal and Kaustubh Ponkshe contributed equally to this work.\n  Another version of the paper accepted at NeurIPS 2024 Workshop on Fine-Tuning\n  in Modern Machine Learning: Principles and Scalability"},{"id":"http://arxiv.org/abs/2411.19774v1","updated":"2024-11-29T15:20:29Z","published":"2024-11-29T15:20:29Z","title":"PerLA: Perceptive 3D Language Assistant","summary":"  Enabling Large Language Models (LLMs) to understand the 3D physical world is\nan emerging yet challenging research direction. Current strategies for\nprocessing point clouds typically downsample the scene or divide it into\nsmaller parts for separate analysis. However, both approaches risk losing key\nlocal details or global contextual information. In this paper, we introduce\nPerLA, a 3D language assistant designed to be more perceptive to both details\nand context, making visual representations more informative for the LLM. PerLA\ncaptures high-resolution (local) details in parallel from different point cloud\nareas and integrates them with (global) context obtained from a\nlower-resolution whole point cloud. We present a novel algorithm that preserves\npoint cloud locality through the Hilbert curve and effectively aggregates\nlocal-to-global information via cross-attention and a graph neural network.\nLastly, we introduce a novel loss for local representation consensus to promote\ntraining stability. PerLA outperforms state-of-the-art 3D language assistants,\nwith gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 on\nScanRefer and +3.88 on Nr3D for dense\ncaptioning.\\url{https://gfmei.github.io/PerLA/}\n","authors":["Guofeng Mei","Wei Lin","Luigi Riz","Yujiao Wu","Fabio Poiesi","Yiming Wang"],"pdf_url":"https://arxiv.org/pdf/2411.19774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19772v1","updated":"2024-11-29T15:18:06Z","published":"2024-11-29T15:18:06Z","title":"LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos","summary":"  Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.\n","authors":["Tiantian Geng","Jinrui Zhang","Qingni Wang","Teng Wang","Jinming Duan","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.19772v1.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2411.19770v1","updated":"2024-11-29T15:18:01Z","published":"2024-11-29T15:18:01Z","title":"Noro: A Noise-Robust One-shot Voice Conversion System with Hidden\n  Speaker Representation Capabilities","summary":"  One-shot voice conversion (VC) aims to alter the timbre of speech from a\nsource speaker to match that of a target speaker using just a single reference\nspeech from the target, while preserving the semantic content of the original\nsource speech. Despite advancements in one-shot VC, its effectiveness decreases\nin real-world scenarios where reference speeches, often sourced from the\ninternet, contain various disturbances like background noise. To address this\nissue, we introduce Noro, a Noise Robust One-shot VC system. Noro features\ninnovative components tailored for VC using noisy reference speeches, including\na dual-branch reference encoding module and a noise-agnostic contrastive\nspeaker loss. Experimental results demonstrate that Noro outperforms our\nbaseline system in both clean and noisy scenarios, highlighting its efficacy\nfor real-world applications. Additionally, we investigate the hidden speaker\nrepresentation capabilities of our baseline system by repurposing its reference\nencoder as a speaker encoder. The results shows that it is competitive with\nseveral advanced self-supervised learning models for speaker representation\nunder the SUPERB settings, highlighting the potential for advancing speaker\nrepresentation learning through one-shot VC task.\n","authors":["Haorui He","Yuchen Song","Yuancheng Wang","Haoyang Li","Xueyao Zhang","Li Wang","Gongping Huang","Eng Siong Chng","Zhizheng Wu"],"pdf_url":"https://arxiv.org/pdf/2411.19770v1.pdf","comment":"Submitted to IEEE OJSP"},{"id":"http://arxiv.org/abs/2402.08349v3","updated":"2024-11-29T14:44:27Z","published":"2024-02-13T10:28:57Z","title":"Evaluating the Data Model Robustness of Text-to-SQL Systems Based on\n  Real User Queries","summary":"  Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity.\n","authors":["Jonathan Fürst","Catherine Kosten","Farhad Nooralahzadeh","Yi Zhang","Kurt Stockinger"],"pdf_url":"https://arxiv.org/pdf/2402.08349v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19733v1","updated":"2024-11-29T14:26:34Z","published":"2024-11-29T14:26:34Z","title":"A Deep Learning Approach to Language-independent Gender Prediction on\n  Twitter","summary":"  This work presents a set of experiments conducted to predict the gender of\nTwitter users based on language-independent features extracted from the text of\nthe users' tweets. The experiments were performed on a version of TwiSty\ndataset including tweets written by the users of six different languages:\nPortuguese, French, Dutch, English, German, and Italian. Logistic regression\n(LR), and feed-forward neural networks (FFNN) with back-propagation were used\nto build models in two different settings: Inter-Lingual (IL) and Cross-Lingual\n(CL). In the IL setting, the training and testing were performed on the same\nlanguage whereas in the CL, Italian and German datasets were set aside and only\nused as test sets and the rest were combined to compose training and\ndevelopment sets. In the IL, the highest accuracy score belongs to LR whereas\nin the CL, FFNN with three hidden layers yields the highest score. The results\nshow that neural network based models underperform traditional models when the\nsize of the training set is small; however, they beat traditional models by a\nnon-trivial margin, when they are fed with large enough data. Finally, the\nfeature analysis confirms that men and women have different writing styles\nindependent of their language.\n","authors":["Reyhaneh Hashempour","Barbara Plank","Aline Villavicencio","Renato Cordeiro de Amorim"],"pdf_url":"https://arxiv.org/pdf/2411.19733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19726v1","updated":"2024-11-29T14:17:33Z","published":"2024-11-29T14:17:33Z","title":"Towards Santali Linguistic Inclusion: Building the First\n  Santali-to-English Translation Model using mT5 Transformer and Data\n  Augmentation","summary":"  Around seven million individuals in India, Bangladesh, Bhutan, and Nepal\nspeak Santali, positioning it as nearly the third most commonly used\nAustroasiatic language. Despite its prominence among the Austroasiatic language\nfamily's Munda subfamily, Santali lacks global recognition. Currently, no\ntranslation models exist for the Santali language. Our paper aims to include\nSantali to the NPL spectrum. We aim to examine the feasibility of building\nSantali translation models based on available Santali corpora. The paper\nsuccessfully addressed the low-resource problem and, with promising results,\nexamined the possibility of creating a functional Santali machine translation\nmodel in a low-resource setup. Our study shows that Santali-English parallel\ncorpus performs better when in transformers like mt5 as opposed to untrained\ntransformers, proving that transfer learning can be a viable technique that\nworks with Santali language. Besides the mT5 transformer, Santali-English\nperforms better than Santali-Bangla parallel corpus as the mT5 has been trained\nin way more English data than Bangla data. Lastly, our study shows that with\ndata augmentation, our model performs better.\n","authors":["Syed Mohammed Mostaque Billah","Ateya Ahmed Subarna","Sudipta Nandi Sarna","Ahmad Shawkat Wasit","Anika Fariha","Asif Sushmit","Arig Yousuf Sadeque"],"pdf_url":"https://arxiv.org/pdf/2411.19726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19718v1","updated":"2024-11-29T14:08:32Z","published":"2024-11-29T14:08:32Z","title":"TakeLab Retriever: AI-Driven Search Engine for Articles from Croatian\n  News Outlets","summary":"  TakeLab Retriever is an AI-driven search engine designed to discover,\ncollect, and semantically analyze news articles from Croatian news outlets. It\noffers a unique perspective on the history and current landscape of Croatian\nonline news media, making it an essential tool for researchers seeking to\nuncover trends, patterns, and correlations that general-purpose search engines\ncannot provide. TakeLab retriever utilizes cutting-edge natural language\nprocessing (NLP) methods, enabling users to sift through articles using named\nentities, phrases, and topics through the web application. This technical\nreport is divided into two parts: the first explains how TakeLab Retriever is\nutilized, while the second provides a detailed account of its design. In the\nsecond part, we also address the software engineering challenges involved and\npropose solutions for developing a microservice-based semantic search engine\ncapable of handling over ten million news articles published over the past two\ndecades.\n","authors":["David Dukić","Marin Petričević","Sven Ćurković","Jan Šnajder"],"pdf_url":"https://arxiv.org/pdf/2411.19718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19689v1","updated":"2024-11-29T13:24:10Z","published":"2024-11-29T13:24:10Z","title":"MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating\n  Multi-Insight Multi-Document Extraction Tasks","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\ntext analysis tasks, yet their evaluation on complex, real-world applications\nremains challenging. We define a set of tasks, Multi-Insight Multi-Document\nExtraction (MIMDE) tasks, which involves extracting an optimal set of insights\nfrom a document corpus and mapping these insights back to their source\ndocuments. This task is fundamental to many practical applications, from\nanalyzing survey responses to processing medical records, where identifying and\ntracing key insights across documents is crucial. We develop an evaluation\nframework for MIMDE and introduce a novel set of complementary human and\nsynthetic datasets to examine the potential of synthetic data for LLM\nevaluation. After establishing optimal metrics for comparing extracted\ninsights, we benchmark 20 state-of-the-art LLMs on both datasets. Our analysis\nreveals a strong correlation (0.71) between the ability of LLMs to extracts\ninsights on our two datasets but synthetic data fails to capture the complexity\nof document-level analysis. These findings offer crucial guidance for the use\nof synthetic data in evaluating text analysis systems, highlighting both its\npotential and limitations.\n","authors":["John Francis","Saba Esnaashari","Anton Poletaev","Sukankana Chakraborty","Youmna Hashem","Jonathan Bright"],"pdf_url":"https://arxiv.org/pdf/2411.19689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19668v1","updated":"2024-11-29T12:48:49Z","published":"2024-11-29T12:48:49Z","title":"ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with\n  Multi-dimensional and fine-grained information","summary":"  During the development of large language models (LLMs), pre-training data\nplay a critical role in shaping LLMs' capabilities. In recent years several\nlarge-scale and high-quality pre-training datasets have been released to\naccelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,\nWanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has\nincreasingly shifted to domain-specific capabilities and safety concerns,\nmaking those previous coarse-grained texts insufficient for meeting training\nrequirements. Furthermore, fine-grained information, such as quality, domain\nand toxicity, is becoming increasingly important in building powerful and\nreliable LLMs for various scenarios. To address these challenges, in this paper\nwe propose a new tool-chain called MDFG-tool for constructing large-scale and\nhigh-quality Chinese datasets with multi-dimensional and fine-grained\ninformation. First, we employ manually crafted rules to discard explicit noisy\ntexts from raw contents. Second, the quality evaluation model, domain\nclassifier, and toxicity evaluation model are well-designed to assess the\nremaining cleaned data respectively. Finally, we integrate these three types of\nfine-grained information for each text. With this approach, we release the\nlargest, high-quality and fine-grained Chinese text ChineseWebText2.0, which\nconsists of 3.8TB and each text is associated with a quality score, domain\nlabels, a toxicity label and a toxicity score, facilitating the LLM researchers\nto select data based on various types of fine-grained information. The data,\ncodes and the tool-chain are available on this website\nhttps://github.com/CASIA-LM/ChineseWebText-2.0\n","authors":["Wanyue Zhang","Ziyong Li","Wen Yang","Chunlin Leng","Yinan Bai","Qianlong Du","Chengqing Zong","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.19668v1.pdf","comment":"ChineseWebTex2.0 dataset is available at\n  https://github.com/CASIA-LM/ChineseWebText-2.0"},{"id":"http://arxiv.org/abs/2411.19650v1","updated":"2024-11-29T12:06:03Z","published":"2024-11-29T12:06:03Z","title":"CogACT: A Foundational Vision-Language-Action Model for Synergizing\n  Cognition and Action in Robotic Manipulation","summary":"  The advancement of large Vision-Language-Action (VLA) models has\nsignificantly improved robotic manipulation in terms of language-guided task\nexecution and generalization to unseen scenarios. While existing VLAs adapted\nfrom pretrained large Vision-Language-Models (VLM) have demonstrated promising\ngeneralizability, their task performance is still unsatisfactory as indicated\nby the low tasks success rates in different environments. In this paper, we\npresent a new advanced VLA architecture derived from VLM. Unlike previous works\nthat directly repurpose VLM for action prediction by simple action\nquantization, we propose a omponentized VLA architecture that has a specialized\naction module conditioned on VLM output. We systematically study the design of\nthe action module and demonstrates the strong performance enhancement with\ndiffusion action transformers for action sequence modeling, as well as their\nfavorable scaling behaviors. We also conduct comprehensive experiments and\nablation studies to evaluate the efficacy of our models with varied designs.\nThe evaluation on 5 robot embodiments in simulation and real work shows that\nour model not only significantly surpasses existing VLAs in task performance\nand but also exhibits remarkable adaptation to new robots and generalization to\nunseen objects and backgrounds. It exceeds the average success rates of OpenVLA\nwhich has similar model size (7B) with ours by over 35% in simulated evaluation\nand 55% in real robot experiments. It also outperforms the large RT-2-X model\n(55B) by 18% absolute success rates in simulation. Code and models can be found\non our project page (https://cogact.github.io/).\n","authors":["Qixiu Li","Yaobo Liang","Zeyu Wang","Lin Luo","Xi Chen","Mozheng Liao","Fangyun Wei","Yu Deng","Sicheng Xu","Yizhong Zhang","Xiaofan Wang","Bei Liu","Jianlong Fu","Jianmin Bao","Dong Chen","Yuanchun Shi","Jiaolong Yang","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2411.19650v1.pdf","comment":"Project Webpage: https://cogact.github.io/"},{"id":"http://arxiv.org/abs/2402.11295v6","updated":"2024-11-29T11:47:55Z","published":"2024-02-17T14:26:57Z","title":"OneBit: Towards Extremely Low-bit Large Language Models","summary":"  Model quantification uses low bit-width values to represent the weight\nmatrices of existing models to be quantized, which is a promising approach to\nreduce both storage and computational overheads of deploying highly anticipated\nLLMs. However, current quantization methods suffer severe performance\ndegradation when the bit-width is extremely reduced, and thus focus on\nutilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes\nthe weight matrices of LLMs to 1-bit, paving the way for the extremely low\nbit-width deployment of LLMs. For this target, we introduce a 1-bit model\ncompressing framework named OneBit, including a novel 1-bit parameter\nrepresentation method to better quantize LLMs as well as an effective parameter\ninitialization method based on matrix decomposition to improve the convergence\nspeed of the quantization framework. Sufficient experimental results indicate\nthat OneBit achieves good performance (at least 81% of the non-quantized\nperformance on LLaMA models) with robust training processes when only using\n1-bit weight matrices.\n","authors":["Yuzhuang Xu","Xu Han","Zonghan Yang","Shuo Wang","Qingfu Zhu","Zhiyuan Liu","Weidong Liu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2402.11295v6.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.19638v1","updated":"2024-11-29T11:42:58Z","published":"2024-11-29T11:42:58Z","title":"LLM Teacher-Student Framework for Text Classification With No Manually\n  Annotated Data: A Case Study in IPTC News Topic Classification","summary":"  With the ever-increasing number of news stories available online, classifying\nthem by topic, regardless of the language they are written in, has become\ncrucial for enhancing readers' access to relevant content. To address this\nchallenge, we propose a teacher-student framework based on large language\nmodels (LLMs) for developing multilingual news classification models of\nreasonable size with no need for manual data annotation. The framework employs\na Generative Pretrained Transformer (GPT) model as the teacher model to develop\nan IPTC Media Topic training dataset through automatic annotation of news\narticles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits\na high zero-shot performance on all four languages. Its agreement with human\nannotators is comparable to that between the human annotators themselves. To\nmitigate the computational limitations associated with the requirement of\nprocessing millions of texts daily, smaller BERT-like student models are\nfine-tuned on the GPT-annotated dataset. These student models achieve high\nperformance comparable to the teacher model. Furthermore, we explore the impact\nof the training data size on the performance of the student models and\ninvestigate their monolingual, multilingual and zero-shot cross-lingual\ncapabilities. The findings indicate that student models can achieve high\nperformance with a relatively small number of training instances, and\ndemonstrate strong zero-shot cross-lingual abilities. Finally, we publish the\nbest-performing news topic classifier, enabling multilingual classification\nwith the top-level categories of the IPTC Media Topic schema.\n","authors":["Taja Kuzman","Nikola Ljubešić"],"pdf_url":"https://arxiv.org/pdf/2411.19638v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2411.19628v1","updated":"2024-11-29T11:24:23Z","published":"2024-11-29T11:24:23Z","title":"Accelerating Multimodal Large Language Models via Dynamic Visual-Token\n  Exit and the Empirical Findings","summary":"  The excessive use of visual tokens in existing Multimoal Large Language\nModels (MLLMs) often exhibits obvious redundancy and brings in prohibitively\nexpensive computation. To gain insights into this problem, we first conduct\nextensive empirical studies on the attention behaviors of MLLMs, and summarize\nthree main inference stages in MLLMs: (i) Early fusion between tokens is first\naccomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)\nMultimodal reasoning} resumes and lasts until the end of inference. In\nparticular, we reveal that visual tokens will stop contributing to reasoning\nwhen the text tokens receive enough image information, yielding obvious visual\nredundancy. Based on these generalized observations, we propose a simple yet\neffective method to improve the efficiency of MLLMs, termed dynamic\nvisual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive\nthe text token status and decide the removal of all visual tokens after a\ncertain layer, thereby addressing the observed visual redundancy. To validate\nVTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,\nand conduct extensive experiments on a bunch of benchmarks. The experiment\nresults not only show the effectiveness of our VTE in improving MLLMs'\nefficiency, but also yield the general modeling patterns of MLLMs, well\nfacilitating the in-depth understanding of MLLMs. Our code is anonymously\nreleased at https://github.com/DoubtedSteam/DyVTE.\n","authors":["Qiong Wu","Wenhao Lin","Weihao Ye","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2411.19628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12025v3","updated":"2024-11-29T10:46:22Z","published":"2024-02-19T10:34:13Z","title":"Speech Translation with Speech Foundation Models and Large Language\n  Models: What is There and What is Missing?","summary":"  The field of natural language processing (NLP) has recently witnessed a\ntransformative shift with the emergence of foundation models, particularly\nLarge Language Models (LLMs) that have revolutionized text-based NLP. This\nparadigm has extended to other modalities, including speech, where researchers\nare actively exploring the combination of Speech Foundation Models (SFMs) and\nLLMs into single, unified models capable of addressing multimodal tasks. Among\nsuch tasks, this paper focuses on speech-to-text translation (ST). By examining\nthe published papers on the topic, we propose a unified view of the\narchitectural solutions and training strategies presented so far, highlighting\nsimilarities and differences among them. Based on this examination, we not only\norganize the lessons learned but also show how diverse settings and evaluation\napproaches hinder the identification of the best-performing solution for each\narchitectural building block and training choice. Lastly, we outline\nrecommendations for future works on the topic aimed at better understanding the\nstrengths and weaknesses of the SFM+LLM solutions for ST.\n","authors":["Marco Gaido","Sara Papi","Matteo Negri","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2402.12025v3.pdf","comment":"Outstanding paper at the ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2409.06567v2","updated":"2024-11-29T10:43:14Z","published":"2024-09-10T14:58:55Z","title":"Exploring syntactic information in sentence embeddings through\n  multilingual subject-verb agreement","summary":"  In this paper, our goal is to investigate to what degree multilingual\npretrained language models capture cross-linguistically valid abstract\nlinguistic representations. We take the approach of developing curated\nsynthetic data on a large scale, with specific properties, and using them to\nstudy sentence representations built using pretrained language models. We use a\nnew multiple-choice task and datasets, Blackbird Language Matrices (BLMs), to\nfocus on a specific grammatical structural phenomenon -- subject-verb agreement\nacross a variety of sentence structures -- in several languages. Finding a\nsolution to this task requires a system detecting complex linguistic patterns\nand paradigms in text representations. Using a two-level architecture that\nsolves the problem in two steps -- detect syntactic objects and their\nproperties in individual sentences, and find patterns across an input sequence\nof sentences -- we show that despite having been trained on multilingual texts\nin a consistent manner, multilingual pretrained language models have\nlanguage-specific differences, and syntactic structure is not shared, even\nacross closely related languages.\n","authors":["Vivi Nastase","Chunyang Jiang","Giuseppe Samo","Paola Merlo"],"pdf_url":"https://arxiv.org/pdf/2409.06567v2.pdf","comment":"13 pages, 5 tables, 6 figures"},{"id":"http://arxiv.org/abs/2310.08367v2","updated":"2024-11-29T10:39:26Z","published":"2023-10-12T14:38:25Z","title":"Towards Evaluating Generalist Agents: An Automated Benchmark in Open\n  World","summary":"  Evaluating generalist agents presents significant challenges due to their\nwide-ranging abilities and the limitations of current benchmarks in assessing\ntrue generalization. We introduce the Minecraft Universe (MCU), a fully\nautomated benchmarking framework set within the open-world game Minecraft. MCU\ndynamically generates and evaluates a broad spectrum of tasks, offering three\ncore components: 1) a task generation mechanism that provides high degrees of\nfreedom and variability, 2) an ever-expanding set of over 3K composable atomic\ntasks, and 3) a general evaluation framework that supports open-ended task\nassessment. By integrating large language models (LLMs), MCU dynamically\ncreates diverse environments for each evaluation, fostering agent\ngeneralization. The framework uses a vision-language model (VLM) to\nautomatically generate evaluation criteria, achieving over 90% agreement with\nhuman ratings across multi-dimensional assessments, which demonstrates that MCU\nis a scalable and explainable solution for evaluating generalist agents.\nAdditionally, we show that while state-of-the-art foundational models perform\nwell on specific tasks, they often struggle with increased task diversity and\ndifficulty.\n","authors":["Xinyue Zheng","Haowei Lin","Kaichen He","Zihao Wang","Zilong Zheng","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2310.08367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06622v2","updated":"2024-11-29T10:30:11Z","published":"2024-09-10T16:22:18Z","title":"Exploring Italian sentence embeddings properties through multi-tasking","summary":"  We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings.\n","authors":["Vivi Nastase","Giuseppe Samo","Chunyang Jiang","Paola Merlo"],"pdf_url":"https://arxiv.org/pdf/2409.06622v2.pdf","comment":"11 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.02085v4","updated":"2024-11-29T10:10:43Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between the latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v4.pdf","comment":"review, survey, 37 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2411.19589v1","updated":"2024-11-29T10:10:16Z","published":"2024-11-29T10:10:16Z","title":"Can Large Language Models Reason about the Region Connection Calculus?","summary":"  Qualitative Spatial Reasoning is a well explored area of Knowledge\nRepresentation and Reasoning and has multiple applications ranging from\nGeographical Information Systems to Robotics and Computer Vision. Recently,\nmany claims have been made for the reasoning capabilities of Large Language\nModels (LLMs). Here, we investigate the extent to which a set of representative\nLLMs can perform classical qualitative spatial reasoning tasks on the\nmereotopological Region Connection Calculus, RCC-8. We conduct three pairs of\nexperiments (reconstruction of composition tables, alignment to human\ncomposition preferences, conceptual neighbourhood reconstruction) using\nstate-of-the-art LLMs; in each pair one experiment uses eponymous relations and\none, anonymous relations (to test the extent to which the LLM relies on\nknowledge about the relation names obtained during training). All instances are\nrepeated 30 times to measure the stochasticity of the LLMs.\n","authors":["Anthony G Cohn","Robert E Blackwell"],"pdf_url":"https://arxiv.org/pdf/2411.19589v1.pdf","comment":"13 pages. arXiv admin note: text overlap with arXiv:2309.15577"},{"id":"http://arxiv.org/abs/2411.19581v1","updated":"2024-11-29T09:54:08Z","published":"2024-11-29T09:54:08Z","title":"In-Context Learning with Noisy Labels","summary":"  In-context learning refers to the emerging ability of large language models\n(LLMs) to perform a target task without additional training, utilizing\ndemonstrations of the task. Recent studies aim to enhance in-context learning\nperformance by selecting more useful demonstrations. However, they overlook the\npresence of inevitable noisy labels in task demonstrations that arise during\nthe labeling process in the real-world. In this paper, we propose a new task,\nin-context learning with noisy labels, which aims to solve real-world problems\nfor in-context learning where labels in task demonstrations would be corrupted.\nMoreover, we propose a new method and baseline methods for the new task,\ninspired by studies in learning with noisy labels. Through experiments, we\ndemonstrate that our proposed method can serve as a safeguard against\nperformance degradation in in-context learning caused by noisy labels.\n","authors":["Junyong Kang","Donghyun Son","Hwanjun Song","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2411.19581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19579v1","updated":"2024-11-29T09:50:32Z","published":"2024-11-29T09:50:32Z","title":"ICPR 2024 Competition on Multilingual Claim-Span Identification","summary":"  A lot of claims are made in social media posts, which may contain\nmisinformation or fake news. Hence, it is crucial to identify claims as a first\nstep towards claim verification. Given the huge number of social media posts,\nthe task of identifying claims needs to be automated. This competition deals\nwith the task of 'Claim Span Identification' in which, given a text, parts /\nspans that correspond to claims are to be identified. This task is more\nchallenging than the traditional binary classification of text into claim or\nnot-claim, and requires state-of-the-art methods in Pattern Recognition,\nNatural Language Processing and Machine Learning. For this competition, we used\na newly developed dataset called HECSI containing about 8K posts in English and\nabout 8K posts in Hindi with claim-spans marked by human annotators. This paper\ngives an overview of the competition, and the solutions developed by the\nparticipating teams.\n","authors":["Soham Poddar","Biswajit Paul","Moumita Basu","Saptarshi Ghosh"],"pdf_url":"https://arxiv.org/pdf/2411.19579v1.pdf","comment":"To appear at ICPR 2024"},{"id":"http://arxiv.org/abs/2411.19574v1","updated":"2024-11-29T09:42:38Z","published":"2024-11-29T09:42:38Z","title":"KV Shifting Attention Enhances Language Modeling","summary":"  The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.\n","authors":["Mingyu Xu","Wei Cheng","Bingning Wang","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2411.19574v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2411.19563v1","updated":"2024-11-29T09:18:32Z","published":"2024-11-29T09:18:32Z","title":"Ensemble Watermarks for Large Language Models","summary":"  The rapid advancement of large language models (LLMs) has made it\nincreasingly difficult to distinguish between text written by humans and\nmachines. While watermarks already exist for LLMs, they often lack flexibility,\nand struggle with attacks such as paraphrasing. To address these issues, we\npropose a multi-feature method for generating watermarks that combines multiple\ndistinct watermark features into an ensemble watermark. Concretely, we combine\nacrostica and sensorimotor norms with the established red-green watermark to\nachieve a 98% detection rate. After a paraphrasing attack the performance\nremains high with 95% detection rate. The red-green feature alone as baseline\nachieves a detection rate of 49%. The evaluation of all feature combinations\nreveals that the ensemble of all three consistently has the highest detection\nrate across several LLMs and watermark strength settings. Due to the\nflexibility of combining features in the ensemble, various requirements and\ntrade-offs can be addressed. Additionally, for all ensemble configurations the\nsame detection function can be used without adaptations. This method is\nparticularly of interest to facilitate accountability and prevent societal\nharm.\n","authors":["Georg Niess","Roman Kern"],"pdf_url":"https://arxiv.org/pdf/2411.19563v1.pdf","comment":"9 pages in the main body. Code is available at\n  http://github.com/CommodoreEU/master-generation. arXiv admin note:\n  substantial text overlap with arXiv:2405.08400"},{"id":"http://arxiv.org/abs/2411.19557v1","updated":"2024-11-29T09:10:30Z","published":"2024-11-29T09:10:30Z","title":"Initialization using Update Approximation is a Silver Bullet for\n  Extremely Efficient Low-Rank Fine-Tuning","summary":"  Low-rank adapters have become a standard approach for efficiently fine-tuning\nlarge language models (LLMs), but they often fall short of achieving the\nperformance of full fine-tuning. We propose a method, LoRA Silver Bullet or\nLoRA-SB, that approximates full fine-tuning within low-rank subspaces using a\ncarefully designed initialization strategy. We theoretically demonstrate that\nthe architecture of LoRA-XS, which inserts a trainable (r x r) matrix between B\nand A while keeping other matrices fixed, provides the precise conditions\nneeded for this approximation. We leverage its constrained update space to\nachieve optimal scaling for high-rank gradient updates while removing the need\nfor hyperparameter tuning. We prove that our initialization offers an optimal\nlow-rank approximation of the initial gradient and preserves update directions\nthroughout training. Extensive experiments across mathematical reasoning,\ncommonsense reasoning, and language understanding tasks demonstrate that our\napproach exceeds the performance of standard LoRA while using 27-90x fewer\nparameters, and comprehensively outperforms LoRA-XS. Our findings establish\nthat it is possible to simulate full fine-tuning in low-rank subspaces, and\nachieve significant efficiency gains without sacrificing performance. Our code\nis publicly available at https://github.com/RaghavSinghal10/lora-sb.\n","authors":["Kaustubh Ponkshe","Raghav Singhal","Eduard Gorbunov","Alexey Tumanov","Samuel Horvath","Praneeth Vepakomma"],"pdf_url":"https://arxiv.org/pdf/2411.19557v1.pdf","comment":"Kaustubh Ponkshe and Raghav Singhal contributed equally to this work"},{"id":"http://arxiv.org/abs/2411.16205v3","updated":"2024-11-29T08:48:17Z","published":"2024-11-25T09:05:36Z","title":"MH-MoE: Multi-Head Mixture-of-Experts","summary":"  Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet.\n","authors":["Shaohan Huang","Xun Wu","Shuming Ma","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2411.16205v3.pdf","comment":"7 pages, 0 figures"},{"id":"http://arxiv.org/abs/2411.19547v1","updated":"2024-11-29T08:47:04Z","published":"2024-11-29T08:47:04Z","title":"Training Agents with Weakly Supervised Feedback from Large Language\n  Models","summary":"  Large Language Models (LLMs) offer a promising basis for creating agents that\ncan tackle complex tasks through iterative environmental interaction. Existing\nmethods either require these agents to mimic expert-provided trajectories or\nrely on definitive environmental feedback for reinforcement learning which\nlimits their application to specific scenarios like gaming or code generation.\nThis paper introduces a novel training method for LLM-based agents using weakly\nsupervised signals from a critic LLM, bypassing the need for expert\ntrajectories or definitive feedback. Our agents are trained in iterative\nmanner, where they initially generate trajectories through environmental\ninteraction. Subsequently, a critic LLM selects a subset of good trajectories,\nwhich are then used to update the agents, enabling them to generate improved\ntrajectories in the next iteration. Extensive tests on the API-bank dataset\nshow consistent improvement in our agents' capabilities and comparable\nperformance to GPT-4, despite using open-source models with much fewer\nparameters.\n","authors":["Dihong Gong","Pu Lu","Zelong Wang","Meng Zhou","Xiuqiang He"],"pdf_url":"https://arxiv.org/pdf/2411.19547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19539v1","updated":"2024-11-29T08:34:07Z","published":"2024-11-29T08:34:07Z","title":"Knowledge Management for Automobile Failure Analysis Using Graph RAG","summary":"  This paper presents a knowledge management system for automobile failure\nanalysis using retrieval-augmented generation (RAG) with large language models\n(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a\ngrowing demand for knowledge transfer of failure analysis from experienced\nengineers to young engineers. However, failure events are phenomena that occur\nin a chain reaction, making them difficult for beginners to analyze them. While\nknowledge graphs, which can describe semantic relationships and structure\ninformation is effective in representing failure events, due to their\ncapability of representing the relationships between components, there is much\ninformation in KGs, so it is challenging for young engineers to extract and\nunderstand sub-graphs from the KG. On the other hand, there is increasing\ninterest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for\nknowledge management. However, when using the current Graph RAG framework with\nan existing knowledge graph for automobile failures, several issues arise\nbecause it is difficult to generate executable queries for a knowledge graph\ndatabase which is not constructed by LLMs. To address this, we focused on\noptimizing the Graph RAG pipeline for existing knowledge graphs. Using an\noriginal Q&A dataset, the ROUGE F1 score of the sentences generated by the\nproposed method showed an average improvement of 157.6% compared to the current\nmethod. This highlights the effectiveness of the proposed method for automobile\nfailure analysis.\n","authors":["Yuta Ojima","Hiroki Sakaji","Tadashi Nakamura","Hiroaki Sakata","Kazuya Seki","Yuu Teshigawara","Masami Yamashita","Kazuhiro Aoyama"],"pdf_url":"https://arxiv.org/pdf/2411.19539v1.pdf","comment":"7 pages, 6 figures, to be published in 2024 IEEE International\n  Conference on Bid Data (BigData)"},{"id":"http://arxiv.org/abs/2411.10666v2","updated":"2024-11-29T08:16:29Z","published":"2024-11-16T02:02:49Z","title":"SAM Decoding: Speculative Decoding via Suffix Automaton","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\nby unifying tasks into text generation, yet their large parameter sizes and\nautoregressive nature limit inference speed. SAM-Decoding addresses this by\nintroducing a novel retrieval-based speculative decoding method that uses a\nsuffix automaton for efficient and accurate draft generation. Unlike n-gram\nmatching used by the existing method, SAM-Decoding finds the longest suffix\nmatch in generating text and text corpuss, achieving an average time complexity\nof $O(1)$ per generation step. SAM-Decoding constructs static and dynamic\nsuffix automatons for the text corpus and input prompts, respectively, enabling\nfast and precise draft generation. Meanwhile, it is designed as an approach\nthat can be combined with existing methods, allowing SAM-Decoding to adaptively\nselect a draft generation strategy based on the matching length, thus\nincreasing the inference speed of the LLM. When combined with Token Recycling,\nevaluations show SAM-Decoding outperforms existing model-free methods,\nachieving a speedup of $2.27\\times$ over autoregressive decoding on Spec-Bench.\nWhen combined with EAGLE2, it reaches a speedup of $2.49\\times$, surpassing all\ncurrent approaches. Our code is available at\nhttps://github.com/hyx1999/SAM-Decoding.\n","authors":["Yuxuan Hu","Ke Wang","Xiaokang Zhang","Fanjin Zhang","Cuiping Li","Hong Chen","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.10666v2.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.19504v1","updated":"2024-11-29T06:48:13Z","published":"2024-11-29T06:48:13Z","title":"TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with\n  Scalable Context and Symbolic Extension","summary":"  The advent of large language models (LLMs) has unlocked great opportunities\nin complex data management tasks, particularly in question answering (QA) over\ncomplicated multi-table relational data. Despite significant progress,\nsystematically evaluating LLMs on multi-table QA remains a critical challenge\ndue to the inherent complexity of analyzing heterogeneous table structures and\npotential large scale of serialized relational data. Existing benchmarks\nprimarily focus on single-table QA, failing to capture the intricacies of\nreasoning across multiple relational tables, as required in real-world domains\nsuch as finance, healthcare, and e-commerce. To address this gap, we present\nTQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities\nof LLMs in tackling complex QA tasks over relational data. Our benchmark\nincorporates diverse relational database instances sourced from real-world\npublic datasets and introduces a flexible sampling mechanism to create tasks\nwith varying multi-table context lengths, ranging from 8K to 64K tokens. To\nensure robustness and reliability, we integrate symbolic extensions into the\nevaluation framework, enabling the assessment of LLM reasoning capabilities\nbeyond simple data retrieval or probabilistic pattern matching. We\nsystematically evaluate a range of LLMs, both open-source and closed-source,\nspanning model scales from 7 billion to 70 billion parameters. Our extensive\nexperiments reveal critical insights into the performance of LLMs in\nmulti-table QA, highlighting both challenges and opportunities for advancing\ntheir application in complex, data-driven environments. Our benchmark\nimplementation and results are available at\nhttps://github.com/Relaxed-System-Lab/TQA-Bench.\n","authors":["Zipeng Qiu","You Peng","Guangxin He","Binhang Yuan","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2411.19504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19500v1","updated":"2024-11-29T06:37:13Z","published":"2024-11-29T06:37:13Z","title":"COLD: Causal reasOning in cLosed Daily activities","summary":"  Large Language Models (LLMs) have shown state-of-the-art performance in a\nvariety of tasks, including arithmetic and reasoning; however, to gauge the\nintellectual capabilities of LLMs, causal reasoning has become a reliable proxy\nfor validating a general understanding of the mechanics and intricacies of the\nworld similar to humans. Previous works in natural language processing (NLP)\nhave either focused on open-ended causal reasoning via causal commonsense\nreasoning (CCR) or framed a symbolic representation-based question answering\nfor theoretically backed-up analysis via a causal inference engine. The former\nadds an advantage of real-world grounding but lacks theoretically backed-up\nanalysis/validation, whereas the latter is far from real-world grounding. In\nthis work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed\nDaily activities) framework, which is built upon human understanding of daily\nreal-world activities to reason about the causal nature of events. We show that\nthe proposed framework facilitates the creation of enormous causal queries (~ 9\nmillion) and comes close to the mini-turing test, simulating causal reasoning\nto evaluate the understanding of a daily real-world task. We evaluate multiple\nLLMs on the created causal queries and find that causal reasoning is\nchallenging even for activities trivial to humans. We further explore (the\ncausal reasoning abilities of LLMs) using the backdoor criterion to determine\nthe causal strength between events.\n","authors":["Abhinav Joshi","Areeb Ahmad","Ashutosh Modi"],"pdf_url":"https://arxiv.org/pdf/2411.19500v1.pdf","comment":"Paper accepted at NeurIPS 2024; Total 37 Pages"},{"id":"http://arxiv.org/abs/2405.01799v2","updated":"2024-11-29T06:15:20Z","published":"2024-05-03T01:04:28Z","title":"Exploiting ChatGPT for Diagnosing Autism-Associated Language Disorders\n  and Identifying Distinct Features","summary":"  Diagnosing language disorders associated with autism is a complex challenge,\noften hampered by the subjective nature and variability of traditional\nassessment methods. Traditional diagnostic methods not only require intensive\nhuman effort but also often result in delayed interventions due to their lack\nof speed and precision. In this study, we explored the application of ChatGPT,\na large language model, to overcome these obstacles by enhancing sensitivity\nand profiling linguistic features for autism diagnosis. This research utilizes\nChatGPT natural language processing capabilities to simplify and improve the\ndiagnostic process, focusing on identifying autism related language patterns.\nSpecifically, we compared ChatGPT performance with that of conventional\nsupervised learning models, including BERT, a model acclaimed for its\neffectiveness in various natural language processing tasks. We showed that\nChatGPT substantially outperformed these models, achieving over 10% improvement\nin both sensitivity and positive predictive value, in a zero shot learning\nconfiguration. The findings underscore the model potential as a diagnostic\ntool, combining accuracy and applicability. We identified ten key features of\nautism associated language disorders across scenarios. Features such as\necholalia, pronoun reversal, and atypical language usage play a critical role\nin diagnosing ASD and informing tailored treatment plans. Together, our\nfindings advocate for adopting sophisticated AI tools like ChatGPT in clinical\nsettings to assess and diagnose developmental disorders. Our approach promises\nenhanced diagnostic precision and supports personalized medicine, potentially\ntransforming the evaluation landscape for autism and similar neurological\nconditions.\n","authors":["Chuanbo Hu","Wenqi Li","Mindi Ruan","Xiangxu Yu","Shalaka Deshpande","Lynn K. Paul","Shuo Wang","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2405.01799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11933v2","updated":"2024-11-29T06:07:41Z","published":"2024-11-18T15:09:50Z","title":"METEOR: Evolutionary Journey of Large Language Models from Guidance to\n  Self-Growth","summary":"  Model evolution enables learning from feedback to refine experiences and\nupdate skills, transforming models from having no domain knowledge to becoming\ndomain experts. However, there is currently no unified and effective method for\nguiding this evolutionary process. To address this gap, we propose the Meteor\nmethod, which includes three training phases: weak-to-strong data distillation,\niterative training, and self-evolution strategies. Each phase maximizes the\nmodel's inherent domain capabilities, allowing it to autonomously refine its\ndomain knowledge and enhance performance. Experiments demonstrate that our\napproach significantly improves accuracy, completeness, relevance, coherence,\nand reliability across domain-specific tasks.\n","authors":["Jiawei Li","Xiaoang Xu","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2411.11933v2.pdf","comment":"Our code can be found at https://github.com/DIRECT-BIT/METEOR"},{"id":"http://arxiv.org/abs/2407.00958v4","updated":"2024-11-29T05:50:09Z","published":"2024-07-01T04:29:35Z","title":"Dynamic Universal Approximation Theory: The Basic Theory for\n  Transformer-based Large Language Models","summary":"  Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements.\n","authors":["Wei Wang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2407.00958v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19477v1","updated":"2024-11-29T05:29:47Z","published":"2024-11-29T05:29:47Z","title":"A Simple and Provable Scaling Law for the Test-Time Compute of Large\n  Language Models","summary":"  We propose a general two-stage algorithm that enjoys a provable scaling law\nfor the test-time compute of large language models (LLMs). Given an input\nproblem, the proposed algorithm first generates $N$ candidate solutions, and\nthen chooses the best one via a multiple-round knockout tournament where each\npair of candidates are compared for $K$ times and only the winners move on to\nthe next round. In a minimalistic implementation, both stages can be executed\nwith a black-box LLM alone and nothing else (e.g., no external verifier or\nreward model), and a total of $N \\times (K + 1)$ highly parallelizable LLM\ncalls are needed for solving an input problem. Assuming that a generated\ncandidate solution is correct with probability $p_{\\text{gen}} > 0$ and a\ncomparison between a pair of correct and incorrect solutions identifies the\nright winner with probability $p_{\\text{comp}} > 0.5$ (i.e., better than a\nrandom guess), we prove theoretically that the failure probability of the\nproposed algorithm decays to zero exponentially with respect to $N$ and $K$:\n$$\\mathbb{P}(\\text{final output is incorrect}) \\le (1 - p_{\\text{gen}})^N +\n\\lceil \\log_2 N \\rceil e^{-2 K (p_{\\text{comp}} - 0.5)^2}.$$ Our empirical\nresults with the challenging MMLU-Pro benchmark validate the technical\nassumptions, as well as the efficacy of the proposed algorithm and the gains\nfrom scaling up its test-time compute.\n","authors":["Yanxi Chen","Xuchen Pan","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.19477v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2406.00627v3","updated":"2024-11-29T05:05:13Z","published":"2024-06-02T06:09:56Z","title":"Prompt Framework for Role-playing: Generation and Evaluation","summary":"  Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis project introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance.\n","authors":["Xun Liu","Zhengwei Ni"],"pdf_url":"https://arxiv.org/pdf/2406.00627v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06350v2","updated":"2024-11-29T04:50:35Z","published":"2024-03-11T00:46:56Z","title":"IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning\n  Datasets for Indian Languages","summary":"  Despite the considerable advancements in English LLMs, the progress in\nbuilding comparable models for other languages has been hindered due to the\nscarcity of tailored resources. Our work aims to bridge this divide by\nintroducing an expansive suite of resources specifically designed for the\ndevelopment of Indic LLMs, covering 22 languages, containing a total of 251B\ntokens and 74.8M instruction-response pairs. Recognizing the importance of both\ndata quality and quantity, our approach combines highly curated manually\nverified data, unverified yet valuable data, and synthetic data. We build a\nclean, open-source pipeline for curating pre-training data from diverse\nsources, including websites, PDFs, and videos, incorporating best practices for\ncrawling, cleaning, flagging, and deduplication. For instruction-fine tuning,\nwe amalgamate existing Indic datasets, translate/transliterate English datasets\ninto Indian languages, and utilize LLaMa2 and Mixtral models to create\nconversations grounded in articles from Indian Wikipedia and Wikihow.\nAdditionally, we address toxicity alignment by generating toxic prompts for\nmultiple scenarios and then generate non-toxic responses by feeding these toxic\nprompts to an aligned LLaMa2 model. We hope that the datasets, tools, and\nresources released as a part of this work will not only propel the research and\ndevelopment of Indic LLMs but also establish an open-source blueprint for\nextending such efforts to other languages. The data and other artifacts created\nas part of this work are released with permissive licenses.\n","authors":["Mohammed Safi Ur Rahman Khan","Priyam Mehta","Ananth Sankar","Umashankar Kumaravelan","Sumanth Doddapaneni","Suriyaprasaad B","Varun Balan G","Sparsh Jain","Anoop Kunchukuttan","Pratyush Kumar","Raj Dabre","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2403.06350v2.pdf","comment":"ACL-2024 Outstanding Paper"},{"id":"http://arxiv.org/abs/2411.19456v1","updated":"2024-11-29T03:57:26Z","published":"2024-11-29T03:57:26Z","title":"Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension\n  Ability","summary":"  Large language models (LLMs) have shown remarkable capability in natural\nlanguage tasks, yet debate persists on whether they truly comprehend deep\nstructure (i.e., core semantics) or merely rely on surface structure (e.g.,\npresentation format). Prior studies observe that LLMs' performance declines\nwhen intervening on surface structure, arguing their success relies on surface\nstructure recognition. However, surface structure sensitivity does not prevent\ndeep structure comprehension. Rigorously evaluating LLMs' capability requires\nanalyzing both, yet deep structure is often overlooked. To this end, we assess\nLLMs' comprehension ability using causal mediation analysis, aiming to fully\ndiscover the capability of using both deep and surface structures.\nSpecifically, we formulate the comprehension of deep structure as direct causal\neffect (DCE) and that of surface structure as indirect causal effect (ICE),\nrespectively. To address the non-estimability of original DCE and ICE --\nstemming from the infeasibility of isolating mutual influences of deep and\nsurface structures, we develop the corresponding quantifiable surrogates,\nincluding approximated DCE (ADCE) and approximated ICE (AICE). We further apply\nthe ADCE to evaluate a series of mainstream LLMs, showing that most of them\nexhibit deep structure comprehension ability, which grows along with the\nprediction accuracy. Comparing ADCE and AICE demonstrates closed-source LLMs\nrely more on deep structure, while open-source LLMs are more surface-sensitive,\nwhich decreases with model scale. Theoretically, ADCE is a bidirectional\nevaluation, which measures both the sufficiency and necessity of deep structure\nchanges in causing output variations, thus offering a more comprehensive\nassessment than accuracy, a common evaluation in LLMs. Our work provides new\ninsights into LLMs' deep structure comprehension and offers novel methods for\nLLMs evaluation.\n","authors":["Yujin Han","Lei Xu","Sirui Chen","Difan Zou","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2411.19456v1.pdf","comment":"28 pages, 14 figures, 10 tables"},{"id":"http://arxiv.org/abs/2411.00774v4","updated":"2024-11-29T03:49:55Z","published":"2024-11-01T17:59:51Z","title":"Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model\n  with Frozen LLM","summary":"  Rapidly developing large language models (LLMs) have brought tremendous\nintelligent applications. Especially, the GPT-4o's excellent duplex speech\ninteraction ability has brought impressive experience to users. Researchers\nhave recently proposed several multi-modal LLMs in this direction that can\nachieve user-agent speech-to-speech conversations. This paper proposes a novel\nspeech-text multimodal LLM architecture called Freeze-Omni. Our main\ncontribution is that the speech input and output modalities can be easily\nconnected to a textual LLM while keeping the LLM's parameters frozen throughout\nthe training process. We design a three-stage training strategy for modeling\nboth the speech input and output, enabling Freeze-Omni to obtain\nspeech-to-speech conversation ability using text-speech paired data (such as\nASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.\nMoreover, we can effectively ensure that the intelligence of the Freeze-Omni in\nthe speech modality is at the same level compared with that in the text\nmodality of its backbone LLM, while achieving low latency end-to-end spoken\nresponse. In addition, we also designed a method to achieve duplex dialogue\nability through multi-task training, giving Freeze-Omni a more natural style of\ndialogue ability between users and agents. In summary, Freeze-Omni holds great\npotential to conduct speech-to-speech dialogue based on a multimodal LLM under\nthe condition of a frozen LLM, avoiding the catastrophic forgetting problem\ncaused by limited data and training resources.\n","authors":["Xiong Wang","Yangze Li","Chaoyou Fu","Yunhang Shen","Lei Xie","Ke Li","Xing Sun","Long Ma"],"pdf_url":"https://arxiv.org/pdf/2411.00774v4.pdf","comment":"Project Page: https://freeze-omni.github.io/"},{"id":"http://arxiv.org/abs/2411.19443v1","updated":"2024-11-29T03:01:05Z","published":"2024-11-29T03:01:05Z","title":"Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language\n  Models","summary":"  Iterative retrieval refers to the process in which the model continuously\nqueries the retriever during generation to enhance the relevance of the\nretrieved knowledge, thereby improving the performance of Retrieval-Augmented\nGeneration (RAG). Existing work typically employs few-shot prompting or\nmanually constructed rules to implement iterative retrieval. This introduces\nadditional inference overhead and overlooks the remarkable reasoning\ncapabilities of Large Language Models (LLMs). In this paper, we introduce\nAuto-RAG, an autonomous iterative retrieval model centered on the LLM's\npowerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues\nwith the retriever, systematically planning retrievals and refining queries to\nacquire valuable knowledge. This process continues until sufficient external\ninformation is gathered, at which point the results are presented to the user.\nTo this end, we develop a method for autonomously synthesizing reasoning-based\ndecision-making instructions in iterative retrieval and fine-tuned the latest\nopen-source LLMs. The experimental results indicate that Auto-RAG is capable of\nautonomous iterative interaction with the retriever, effectively leveraging the\nremarkable reasoning and decision-making abilities of LLMs, which lead to\noutstanding performance across six benchmarks. Further analysis reveals that\nAuto-RAG can autonomously adjust the number of iterations based on the\ndifficulty of the questions and the utility of the retrieved knowledge, without\nrequiring any human intervention. Moreover, Auto-RAG expresses the iterative\nretrieval process in natural language, enhancing interpretability while\nproviding users with a more intuitive experience\\footnote{Code is available at\n\\url{https://github.com/ictnlp/Auto-RAG}.\n","authors":["Tian Yu","Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2411.19443v1.pdf","comment":"Code is available at https://github.com/ictnlp/Auto-RAG"},{"id":"http://arxiv.org/abs/2402.11217v2","updated":"2024-11-29T02:50:45Z","published":"2024-02-17T08:04:23Z","title":"A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language\n  Models","summary":"  The significant breakthroughs of Medical Multi-Modal Large Language Models\n(Med-MLLMs) renovate modern healthcare with robust information synthesis and\nmedical decision support. However, these models are often evaluated on\nbenchmarks that are unsuitable for the Med-MLLMs due to the complexity of\nreal-world diagnostics across diverse specialties. To address this gap, we\nintroduce Asclepius, a novel Med-MLLM benchmark that comprehensively assesses\nMed-MLLMs in terms of: distinct medical specialties (cardiovascular,\ngastroenterology, etc.) and different diagnostic capacities (perception,\ndisease analysis, etc.). Grounded in 3 proposed core principles, Asclepius\nensures a comprehensive evaluation by encompassing 15 medical specialties,\nstratifying into 3 main categories and 8 sub-categories of clinical tasks, and\nexempting overlap with existing VQA dataset. We further provide an in-depth\nanalysis of 6 Med-MLLMs and compare them with 3 human specialists, providing\ninsights into their competencies and limitations in various medical contexts.\nOur work not only advances the understanding of Med-MLLMs' capabilities but\nalso sets a precedent for future evaluations and the safe deployment of these\nmodels in clinical environments.\n","authors":["Jie Liu","Wenxuan Wang","Yihang Su","Jingyuan Huan","Wenting Chen","Yudi Zhang","Cheng-Yi Li","Kao-Jung Chang","Xiaohan Xin","Linlin Shen","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2402.11217v2.pdf","comment":"20 pages, 15 figures"},{"id":"http://arxiv.org/abs/2409.07064v2","updated":"2024-11-29T02:37:06Z","published":"2024-09-11T07:24:07Z","title":"Automated Speaking Assessment of Conversation Tests with Novel\n  Graph-based Modeling on Spoken Response Coherence","summary":"  Automated speaking assessment in conversation tests (ASAC) aims to evaluate\nthe overall speaking proficiency of an L2 (second-language) speaker in a\nsetting where an interlocutor interacts with one or more candidates. Although\nprior ASAC approaches have shown promising performance on their respective\ndatasets, there is still a dearth of research specifically focused on\nincorporating the coherence of the logical flow within a conversation into the\ngrading model. To address this critical challenge, we propose a hierarchical\ngraph model that aptly incorporates both broad inter-response interactions\n(e.g., discourse relations) and nuanced semantic information (e.g., semantic\nwords and speaker intents), which is subsequently fused with contextual\ninformation for the final prediction. Extensive experimental results on the\nNICT-JLE benchmark dataset suggest that our proposed modeling approach can\nyield considerable improvements in prediction accuracy with respect to various\nassessment metrics, as compared to some strong baselines. This also sheds light\non the importance of investigating coherence-related facets of spoken responses\nin ASAC.\n","authors":["Jiun-Ting Li","Bi-Cheng Yan","Tien-Hong Lo","Yi-Cheng Wang","Yung-Chang Hsu","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2409.07064v2.pdf","comment":"Accepted by IEEE SLT 2024"},{"id":"http://arxiv.org/abs/2411.19434v1","updated":"2024-11-29T02:14:05Z","published":"2024-11-29T02:14:05Z","title":"Actions and Objects Pathways for Domain Adaptation in Video Question\n  Answering","summary":"  In this paper, we introduce the Actions and Objects Pathways (AOPath) for\nout-of-domain generalization in video question answering tasks. AOPath\nleverages features from a large pretrained model to enhance generalizability\nwithout the need for explicit training on the unseen domains. Inspired by human\nbrain, AOPath dissociates the pretrained features into action and object\nfeatures, and subsequently processes them through separate reasoning pathways.\nIt utilizes a novel module which converts out-of-domain features into\ndomain-agnostic features without introducing any trainable weights. We validate\nthe proposed approach on the TVQA dataset, which is partitioned into multiple\nsubsets based on genre to facilitate the assessment of generalizability. The\nproposed approach demonstrates 5% and 4% superior performance over conventional\nclassifiers on out-of-domain and in-domain datasets, respectively. It also\noutperforms prior methods that involve training millions of parameters, whereas\nthe proposed approach trains very few parameters.\n","authors":["Safaa Abdullahi Moallim Mohamud","Ho-Young Jung"],"pdf_url":"https://arxiv.org/pdf/2411.19434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01247v3","updated":"2024-11-29T01:47:20Z","published":"2024-09-02T13:29:44Z","title":"Conversational Complexity for Assessing Risk in Large Language Models","summary":"  Large Language Models (LLMs) present a dual-use dilemma: they enable\nbeneficial applications while harboring potential for harm, particularly\nthrough conversational interactions. Despite various safeguards, advanced LLMs\nremain vulnerable. A watershed case in early 2023 involved journalist Kevin\nRoose's extended dialogue with Bing, an LLM-powered search engine, which\nrevealed harmful outputs after probing questions, highlighting vulnerabilities\nin the model's safeguards. This contrasts with simpler early jailbreaks, like\nthe \"Grandma Jailbreak,\" where users framed requests as innocent help for a\ngrandmother, easily eliciting similar content. This raises the question: How\nmuch conversational effort is needed to elicit harmful information from LLMs?\nWe propose two measures to quantify this effort: Conversational Length (CL),\nwhich measures the number of conversational turns needed to obtain a specific\nharmful response, and Conversational Complexity (CC), defined as the Kolmogorov\ncomplexity of the user's instruction sequence leading to the harmful response.\nTo address the incomputability of Kolmogorov complexity, we approximate CC\nusing a reference LLM to estimate the compressibility of the user instructions.\nApplying this approach to a large red-teaming dataset, we perform a\nquantitative analysis examining the statistical distribution of harmful and\nharmless conversational lengths and complexities. Our empirical findings\nsuggest that this distributional analysis and the minimization of CC serve as\nvaluable tools for understanding AI safety, offering insights into the\naccessibility of harmful information. This work establishes a foundation for a\nnew perspective on LLM safety, centered around the algorithmic complexity of\npathways to harm.\n","authors":["John Burden","Manuel Cebrian","Jose Hernandez-Orallo"],"pdf_url":"https://arxiv.org/pdf/2409.01247v3.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.20302v2","updated":"2024-11-29T00:27:46Z","published":"2024-10-27T00:50:30Z","title":"Sequential Large Language Model-Based Hyper-Parameter Optimization","summary":"  This study introduces SLLMBO, an innovative framework that leverages Large\nLanguage Models (LLMs) for hyperparameter optimization (HPO), incorporating\ndynamic search space adaptability, enhanced parameter landscape exploitation,\nand a hybrid, novel LLM-Tree-structured Parzen Estimator (LLM-TPE) sampler. By\naddressing limitations in recent fully LLM-based methods and traditional\nBayesian Optimization (BO), SLLMBO achieves more robust optimization. This\ncomprehensive benchmarking evaluates multiple LLMs, including GPT-3.5-turbo,\nGPT-4o, Claude-Sonnet-3.5, and Gemini-1.5-flash, extending prior work beyond\nGPT-3.5 and GPT-4 and establishing SLLMBO as the first framework to benchmark a\ndiverse set of LLMs for HPO. By integrating LLMs' established strengths in\nparameter initialization with the exploitation abilities demonstrated in this\nstudy, alongside TPE's exploration capabilities, the LLM-TPE sampler achieves a\nbalanced exploration-exploitation trade-off, reduces API costs, and mitigates\npremature early stoppings for more effective parameter searches. Across 14\ntabular tasks in classification and regression, the LLM-TPE sampler\noutperformed fully LLM-based methods and achieved superior results over BO\nmethods in 9 tasks. Testing early stopping in budget-constrained scenarios\nfurther demonstrated competitive performance, indicating that LLM-based methods\ngenerally benefit from extended iterations for optimal results. This work lays\nthe foundation for future research exploring open-source LLMs, reproducibility\nof LLM results in HPO, and benchmarking SLLMBO on complex datasets, such as\nimage classification, segmentation, and machine translation.\n","authors":["Kanan Mahammadli","Seyda Bolelli Ertekin"],"pdf_url":"https://arxiv.org/pdf/2410.20302v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.07571v2","updated":"2024-11-29T20:48:27Z","published":"2024-09-11T18:58:16Z","title":"FaVoR: Features via Voxel Rendering for Camera Relocalization","summary":"  Camera relocalization methods range from dense image alignment to direct\ncamera pose regression from a query image. Among these, sparse feature matching\nstands out as an efficient, versatile, and generally lightweight approach with\nnumerous applications. However, feature-based methods often struggle with\nsignificant viewpoint and appearance changes, leading to matching failures and\ninaccurate pose estimates. To overcome this limitation, we propose a novel\napproach that leverages a globally sparse yet locally dense 3D representation\nof 2D features. By tracking and triangulating landmarks over a sequence of\nframes, we construct a sparse voxel map optimized to render image patch\ndescriptors observed during tracking. Given an initial pose estimate, we first\nsynthesize descriptors from the voxels using volumetric rendering and then\nperform feature matching to estimate the camera pose. This methodology enables\nthe generation of descriptors for unseen views, enhancing robustness to view\nchanges. We extensively evaluate our method on the 7-Scenes and Cambridge\nLandmarks datasets. Our results show that our method significantly outperforms\nexisting state-of-the-art feature representation techniques in indoor\nenvironments, achieving up to a 39% improvement in median translation error.\nAdditionally, our approach yields comparable results to other methods for\noutdoor scenarios while maintaining lower memory and computational costs.\n","authors":["Vincenzo Polizzi","Marco Cannici","Davide Scaramuzza","Jonathan Kelly"],"pdf_url":"https://arxiv.org/pdf/2409.07571v2.pdf","comment":"Accepted to the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV), Tucson, Arizona, US, Feb 28-Mar 4, 2025"},{"id":"http://arxiv.org/abs/2406.15920v4","updated":"2024-11-29T20:34:28Z","published":"2024-06-22T19:20:35Z","title":"SEDMamba: Enhancing Selective State Space Modelling with Bottleneck\n  Mechanism and Fine-to-Coarse Temporal Fusion for Efficient Error Detection in\n  Robot-Assisted Surgery","summary":"  Automated detection of surgical errors can improve robotic-assisted surgery.\nDespite promising progress, existing methods still face challenges in capturing\nrich temporal context to establish long-term dependencies while maintaining\ncomputational efficiency. In this paper, we propose a novel hierarchical model\nnamed SEDMamba, which incorporates the selective state space model (SSM) into\nsurgical error detection, facilitating efficient long sequence modelling with\nlinear complexity. SEDMamba enhances selective SSM with a bottleneck mechanism\nand fine-to-coarse temporal fusion (FCTF) to detect and temporally localize\nsurgical errors in long videos. The bottleneck mechanism compresses and\nrestores features within their spatial dimension, thereby reducing\ncomputational complexity. FCTF utilizes multiple dilated 1D convolutional\nlayers to merge temporal information across diverse scale ranges, accommodating\nerrors of varying duration. Our work also contributes the first-of-its-kind,\nframe-level, in-vivo surgical error dataset to support error detection in real\nsurgical cases. Specifically, we deploy the clinically validated observational\nclinical human reliability assessment tool (OCHRA) to annotate the errors\nduring suturing tasks in an open-source radical prostatectomy dataset\n(SAR-RARP50). Experimental results demonstrate that our SEDMamba outperforms\nstate-of-the-art methods with at least 1.82% AUC and 3.80% AP performance gains\nwith significantly reduced computational complexity. The corresponding error\nannotations, code and models are released at\nhttps://github.com/wzjialang/SEDMamba.\n","authors":["Jialang Xu","Nazir Sirajudeen","Matthew Boal","Nader Francis","Danail Stoyanov","Evangelos Mazomenos"],"pdf_url":"https://arxiv.org/pdf/2406.15920v4.pdf","comment":"Accepted by IEEE RA-L"},{"id":"http://arxiv.org/abs/2411.16718v2","updated":"2024-11-29T19:46:12Z","published":"2024-11-22T23:59:12Z","title":"Neuro-Symbolic Evaluation of Text-to-Video Models using Formal\n  Verification","summary":"  Recent advancements in text-to-video models such as Sora, Gen-3, MovieGen,\nand CogVideoX are pushing the boundaries of synthetic video generation, with\nadoption seen in fields like robotics, autonomous driving, and entertainment.\nAs these models become prevalent, various metrics and benchmarks have emerged\nto evaluate the quality of the generated videos. However, these metrics\nemphasize visual quality and smoothness, neglecting temporal fidelity and\ntext-to-video alignment, which are crucial for safety-critical applications. To\naddress this gap, we introduce NeuS-V, a novel synthetic video evaluation\nmetric that rigorously assesses text-to-video alignment using neuro-symbolic\nformal verification techniques. Our approach first converts the prompt into a\nformally defined Temporal Logic (TL) specification and translates the generated\nvideo into an automaton representation. Then, it evaluates the text-to-video\nalignment by formally checking the video automaton against the TL\nspecification. Furthermore, we present a dataset of temporally extended prompts\nto evaluate state-of-the-art video generation models against our benchmark. We\nfind that NeuS-V demonstrates a higher correlation by over 5x with human\nevaluations when compared to existing metrics. Our evaluation further reveals\nthat current video generation models perform poorly on these temporally complex\nprompts, highlighting the need for future work in improving text-to-video\ngeneration capabilities.\n","authors":["S. P. Sharan","Minkyu Choi","Sahil Shah","Harsh Goel","Mohammad Omama","Sandeep Chinchali"],"pdf_url":"https://arxiv.org/pdf/2411.16718v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10718v2","updated":"2024-11-29T19:01:20Z","published":"2024-05-17T12:01:43Z","title":"SignLLM: Sign Language Production Large Language Models","summary":"  In this paper, we propose SignLLM, a multilingual Sign Language Production\n(SLP) large language model, which includes two novel multilingual SLP modes\nMLSF and Prompt2LangGloss that allow sign language gestures generation from\nquery texts input and question-style prompts input respectively. Both modes can\nuse a new RL loss based on reinforcement learning and a new RL module named\nPriority Learning Channel. These RL components can accelerate the training by\nenhancing the model's capability to sample high-quality data. For SignLLM's\ntraining, we introduce Prompt2Sign, a comprehensive multilingual sign language\ndataset, which builds from public data, including American Sign Language (ASL)\nand seven others. This dataset standardizes information by extracting pose\ninformation from sign language videos into a unified compressed format. We\nextensively evaluate SignLLM, demonstrating that our model achieves\nstate-of-the-art performance on SLP tasks across eight sign languages.\n","authors":["Sen Fang","Lei Wang","Ce Zheng","Chunyu Sui","Mingyu Zhao","Yapeng Tian","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2405.10718v2.pdf","comment":"website at https://signllm.github.io/"},{"id":"http://arxiv.org/abs/2411.19950v1","updated":"2024-11-29T18:59:52Z","published":"2024-11-29T18:59:52Z","title":"AlphaTablets: A Generic Plane Representation for 3D Planar\n  Reconstruction from Monocular Videos","summary":"  We introduce AlphaTablets, a novel and generic representation of 3D planes\nthat features continuous 3D surface and precise boundary delineation. By\nrepresenting 3D planes as rectangles with alpha channels, AlphaTablets combine\nthe advantages of current 2D and 3D plane representations, enabling accurate,\nconsistent and flexible modeling of 3D planes. We derive differentiable\nrasterization on top of AlphaTablets to efficiently render 3D planes into\nimages, and propose a novel bottom-up pipeline for 3D planar reconstruction\nfrom monocular videos. Starting with 2D superpixels and geometric cues from\npre-trained models, we initialize 3D planes as AlphaTablets and optimize them\nvia differentiable rendering. An effective merging scheme is introduced to\nfacilitate the growth and refinement of AlphaTablets. Through iterative\noptimization and merging, we reconstruct complete and accurate 3D planes with\nsolid surfaces and clear boundaries. Extensive experiments on the ScanNet\ndataset demonstrate state-of-the-art performance in 3D planar reconstruction,\nunderscoring the great potential of AlphaTablets as a generic 3D plane\nrepresentation for various applications. Project page is available at:\nhttps://hyzcluster.github.io/alphatablets\n","authors":["Yuze He","Wang Zhao","Shaohui Liu","Yubin Hu","Yushi Bai","Yu-Hui Wen","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2411.19950v1.pdf","comment":"NeurIPS 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2411.19862v1","updated":"2024-11-29T17:25:00Z","published":"2024-11-29T17:25:00Z","title":"Cross-Domain Recommendation Meets Large Language Models","summary":"  Cross-domain recommendation (CDR) has emerged as a promising solution to the\ncold-start problem, faced by single-domain recommender systems. However,\nexisting CDR models rely on complex neural architectures, large datasets, and\nsignificant computational resources, making them less effective in data-scarce\nscenarios or when simplicity is crucial. In this work, we leverage the\nreasoning capabilities of large language models (LLMs) and explore their\nperformance in the CDR domain across multiple domain pairs. We introduce two\nnovel prompt designs tailored for CDR and demonstrate that LLMs, when prompted\neffectively, outperform state-of-the-art CDR baselines across various metrics\nand domain combinations in the rating prediction and ranking tasks. This work\nbridges the gap between LLMs and recommendation systems, showcasing their\npotential as effective cross-domain recommenders.\n","authors":["Ajay Krishna Vajjala","Dipak Meher","Ziwei Zhu","David S. Rosenblum"],"pdf_url":"https://arxiv.org/pdf/2411.19862v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2411.19718v1","updated":"2024-11-29T14:08:32Z","published":"2024-11-29T14:08:32Z","title":"TakeLab Retriever: AI-Driven Search Engine for Articles from Croatian\n  News Outlets","summary":"  TakeLab Retriever is an AI-driven search engine designed to discover,\ncollect, and semantically analyze news articles from Croatian news outlets. It\noffers a unique perspective on the history and current landscape of Croatian\nonline news media, making it an essential tool for researchers seeking to\nuncover trends, patterns, and correlations that general-purpose search engines\ncannot provide. TakeLab retriever utilizes cutting-edge natural language\nprocessing (NLP) methods, enabling users to sift through articles using named\nentities, phrases, and topics through the web application. This technical\nreport is divided into two parts: the first explains how TakeLab Retriever is\nutilized, while the second provides a detailed account of its design. In the\nsecond part, we also address the software engineering challenges involved and\npropose solutions for developing a microservice-based semantic search engine\ncapable of handling over ten million news articles published over the past two\ndecades.\n","authors":["David Dukić","Marin Petričević","Sven Ćurković","Jan Šnajder"],"pdf_url":"https://arxiv.org/pdf/2411.19718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19710v1","updated":"2024-11-29T13:57:07Z","published":"2024-11-29T13:57:07Z","title":"Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating\n  RAG Systems","summary":"  Retrieval Augmented Generation (RAG) systems are a widespread application of\nLarge Language Models (LLMs) in the industry. While many tools exist empowering\ndevelopers to build their own systems, measuring their performance locally,\nwith datasets reflective of the system's use cases, is a technological\nchallenge. Solutions to this problem range from non-specific and cheap (most\npublic datasets) to specific and costly (generating data from local documents).\nIn this paper, we show that using public question and answer (Q&A) datasets to\nassess retrieval performance can lead to non-optimal systems design, and that\ncommon tools for RAG dataset generation can lead to unbalanced data. We propose\nsolutions to these issues based on the characterization of RAG datasets through\nlabels and through label-targeted data generation. Finally, we show that\nfine-tuned small LLMs can efficiently generate Q&A datasets. We believe that\nthese observations are invaluable to the know-your-data step of RAG systems\ndevelopment.\n","authors":["Rafael Teixeira de Lima","Shubham Gupta","Cesar Berrospi","Lokesh Mishra","Michele Dolfi","Peter Staar","Panagiotis Vagenas"],"pdf_url":"https://arxiv.org/pdf/2411.19710v1.pdf","comment":"to be published in the 31st International Conference on Computational\n  Linguistics (COLING 2025)"},{"id":"http://arxiv.org/abs/2411.19576v1","updated":"2024-11-29T09:47:32Z","published":"2024-11-29T09:47:32Z","title":"A Review of LLM-based Explanations in Recommender Systems","summary":"  The rise of Large Language Models (LLMs), such as LLaMA and ChatGPT, has\nopened new opportunities for enhancing recommender systems through improved\nexplainability. This paper provides a systematic literature review focused on\nleveraging LLMs to generate explanations for recommendations -- a critical\naspect for fostering transparency and user trust. We conducted a comprehensive\nsearch within the ACM Guide to Computing Literature, covering publications from\nthe launch of ChatGPT (November 2022) to the present (November 2024). Our\nsearch yielded 232 articles, but after applying inclusion criteria, only six\nwere identified as directly addressing the use of LLMs in explaining\nrecommendations. This scarcity highlights that, despite the rise of LLMs, their\napplication in explainable recommender systems is still in an early stage. We\nanalyze these select studies to understand current methodologies, identify\nchallenges, and suggest directions for future research. Our findings underscore\nthe potential of LLMs improving explanations of recommender systems and\nencourage the development of more transparent and user-centric recommendation\nexplanation solutions.\n","authors":["Alan Said"],"pdf_url":"https://arxiv.org/pdf/2411.19576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19539v1","updated":"2024-11-29T08:34:07Z","published":"2024-11-29T08:34:07Z","title":"Knowledge Management for Automobile Failure Analysis Using Graph RAG","summary":"  This paper presents a knowledge management system for automobile failure\nanalysis using retrieval-augmented generation (RAG) with large language models\n(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a\ngrowing demand for knowledge transfer of failure analysis from experienced\nengineers to young engineers. However, failure events are phenomena that occur\nin a chain reaction, making them difficult for beginners to analyze them. While\nknowledge graphs, which can describe semantic relationships and structure\ninformation is effective in representing failure events, due to their\ncapability of representing the relationships between components, there is much\ninformation in KGs, so it is challenging for young engineers to extract and\nunderstand sub-graphs from the KG. On the other hand, there is increasing\ninterest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for\nknowledge management. However, when using the current Graph RAG framework with\nan existing knowledge graph for automobile failures, several issues arise\nbecause it is difficult to generate executable queries for a knowledge graph\ndatabase which is not constructed by LLMs. To address this, we focused on\noptimizing the Graph RAG pipeline for existing knowledge graphs. Using an\noriginal Q&A dataset, the ROUGE F1 score of the sentences generated by the\nproposed method showed an average improvement of 157.6% compared to the current\nmethod. This highlights the effectiveness of the proposed method for automobile\nfailure analysis.\n","authors":["Yuta Ojima","Hiroki Sakaji","Tadashi Nakamura","Hiroaki Sakata","Kazuya Seki","Yuu Teshigawara","Masami Yamashita","Kazuhiro Aoyama"],"pdf_url":"https://arxiv.org/pdf/2411.19539v1.pdf","comment":"7 pages, 6 figures, to be published in 2024 IEEE International\n  Conference on Bid Data (BigData)"},{"id":"http://arxiv.org/abs/2411.19513v1","updated":"2024-11-29T07:11:42Z","published":"2024-11-29T07:11:42Z","title":"ContextGNN: Beyond Two-Tower Recommendation Systems","summary":"  Recommendation systems predominantly utilize two-tower architectures, which\nevaluate user-item rankings through the inner product of their respective\nembeddings. However, one key limitation of two-tower models is that they learn\na pair-agnostic representation of users and items. In contrast, pair-wise\nrepresentations either scale poorly due to their quadratic complexity or are\ntoo restrictive on the candidate pairs to rank. To address these issues, we\nintroduce Context-based Graph Neural Networks (ContextGNNs), a novel deep\nlearning architecture for link prediction in recommendation systems. The method\nemploys a pair-wise representation technique for familiar items situated within\na user's local subgraph, while leveraging two-tower representations to\nfacilitate the recommendation of exploratory items. A final network then\npredicts how to fuse both pair-wise and two-tower recommendations into a single\nranking of items. We demonstrate that ContextGNN is able to adapt to different\ndata characteristics and outperforms existing methods, both traditional and\nGNN-based, on a diverse set of practical recommendation tasks, improving\nperformance by 20% on average.\n","authors":["Yiwen Yuan","Zecheng Zhang","Xinwei He","Akihiro Nitta","Weihua Hu","Dong Wang","Manan Shah","Shenyang Huang","Blaž Stojanovič","Alan Krumholz","Jan Eric Lenssen","Jure Leskovec","Matthias Fey"],"pdf_url":"https://arxiv.org/pdf/2411.19513v1.pdf","comment":"14 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2411.19504v1","updated":"2024-11-29T06:48:13Z","published":"2024-11-29T06:48:13Z","title":"TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with\n  Scalable Context and Symbolic Extension","summary":"  The advent of large language models (LLMs) has unlocked great opportunities\nin complex data management tasks, particularly in question answering (QA) over\ncomplicated multi-table relational data. Despite significant progress,\nsystematically evaluating LLMs on multi-table QA remains a critical challenge\ndue to the inherent complexity of analyzing heterogeneous table structures and\npotential large scale of serialized relational data. Existing benchmarks\nprimarily focus on single-table QA, failing to capture the intricacies of\nreasoning across multiple relational tables, as required in real-world domains\nsuch as finance, healthcare, and e-commerce. To address this gap, we present\nTQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities\nof LLMs in tackling complex QA tasks over relational data. Our benchmark\nincorporates diverse relational database instances sourced from real-world\npublic datasets and introduces a flexible sampling mechanism to create tasks\nwith varying multi-table context lengths, ranging from 8K to 64K tokens. To\nensure robustness and reliability, we integrate symbolic extensions into the\nevaluation framework, enabling the assessment of LLM reasoning capabilities\nbeyond simple data retrieval or probabilistic pattern matching. We\nsystematically evaluate a range of LLMs, both open-source and closed-source,\nspanning model scales from 7 billion to 70 billion parameters. Our extensive\nexperiments reveal critical insights into the performance of LLMs in\nmulti-table QA, highlighting both challenges and opportunities for advancing\ntheir application in complex, data-driven environments. Our benchmark\nimplementation and results are available at\nhttps://github.com/Relaxed-System-Lab/TQA-Bench.\n","authors":["Zipeng Qiu","You Peng","Guangxin He","Binhang Yuan","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2411.19504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19478v1","updated":"2024-11-29T05:31:04Z","published":"2024-11-29T05:31:04Z","title":"Zero-Indexing Internet Search Augmented Generation for Large Language\n  Models","summary":"  Retrieval augmented generation has emerged as an effective method to enhance\nlarge language model performance. This approach typically relies on an internal\nretrieval module that uses various indexing mechanisms to manage a static\npre-processed corpus. However, such a paradigm often falls short when it is\nnecessary to integrate the most up-to-date information that has not been\nupdated into the corpus during generative inference time. In this paper, we\nexplore an alternative approach that leverages standard search engine APIs to\ndynamically integrate the latest online information (without maintaining any\nindex for any fixed corpus), thereby improving the quality of generated\ncontent. We design a collaborative LLM-based paradigm, where we include: (i) a\nparser-LLM that determines if the Internet augmented generation is demanded and\nextracts the search keywords if so with a single inference; (ii) a mixed\nranking strategy that re-ranks the retrieved HTML files to eliminate bias\nintroduced from the search engine API; and (iii) an extractor-LLM that can\naccurately and efficiently extract relevant information from the fresh content\nin each HTML file. We conduct extensive empirical studies to evaluate the\nperformance of this Internet search augmented generation paradigm. The\nexperimental results demonstrate that our method generates content with\nsignificantly improved quality. Our system has been successfully deployed in a\nproduction environment to serve 01.AI's generative inference requests.\n","authors":["Guangxin He","Zonghong Dai","Jiangcheng Zhu","Binqiang Zhao","Chenyue Li","You Peng","Chen Wang","Binhang Yuan"],"pdf_url":"https://arxiv.org/pdf/2411.19478v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.19772v1","updated":"2024-11-29T15:18:06Z","published":"2024-11-29T15:18:06Z","title":"LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos","summary":"  Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.\n","authors":["Tiantian Geng","Jinrui Zhang","Qingni Wang","Teng Wang","Jinming Duan","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.19772v1.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2411.19730v1","updated":"2024-11-29T14:23:25Z","published":"2024-11-29T14:23:25Z","title":"Ten Ways in which Virtual Reality Differs from Video Streaming","summary":"  Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.\n","authors":["Gustavo de Veciana","Sonia Fahmy","George Kesidis","Voicu Popescu"],"pdf_url":"https://arxiv.org/pdf/2411.19730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19628v1","updated":"2024-11-29T11:24:23Z","published":"2024-11-29T11:24:23Z","title":"Accelerating Multimodal Large Language Models via Dynamic Visual-Token\n  Exit and the Empirical Findings","summary":"  The excessive use of visual tokens in existing Multimoal Large Language\nModels (MLLMs) often exhibits obvious redundancy and brings in prohibitively\nexpensive computation. To gain insights into this problem, we first conduct\nextensive empirical studies on the attention behaviors of MLLMs, and summarize\nthree main inference stages in MLLMs: (i) Early fusion between tokens is first\naccomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)\nMultimodal reasoning} resumes and lasts until the end of inference. In\nparticular, we reveal that visual tokens will stop contributing to reasoning\nwhen the text tokens receive enough image information, yielding obvious visual\nredundancy. Based on these generalized observations, we propose a simple yet\neffective method to improve the efficiency of MLLMs, termed dynamic\nvisual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive\nthe text token status and decide the removal of all visual tokens after a\ncertain layer, thereby addressing the observed visual redundancy. To validate\nVTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,\nand conduct extensive experiments on a bunch of benchmarks. The experiment\nresults not only show the effectiveness of our VTE in improving MLLMs'\nefficiency, but also yield the general modeling patterns of MLLMs, well\nfacilitating the in-depth understanding of MLLMs. Our code is anonymously\nreleased at https://github.com/DoubtedSteam/DyVTE.\n","authors":["Qiong Wu","Wenhao Lin","Weihao Ye","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2411.19628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19537v1","updated":"2024-11-29T08:29:25Z","published":"2024-11-29T08:29:25Z","title":"Deepfake Media Generation and Detection in the Generative AI Era: A\n  Survey and Outlook","summary":"  With the recent advancements in generative modeling, the realism of deepfake\ncontent has been increasing at a steady pace, even reaching the point where\npeople often fail to detect manipulated media content online, thus being\ndeceived into various kinds of scams. In this paper, we survey deepfake\ngeneration and detection techniques, including the most recent developments in\nthe field, such as diffusion models and Neural Radiance Fields. Our literature\nreview covers all deepfake media types, comprising image, video, audio and\nmultimodal (audio-visual) content. We identify various kinds of deepfakes,\naccording to the procedure used to alter or generate the fake content. We\nfurther construct a taxonomy of deepfake generation and detection methods,\nillustrating the important groups of methods and the domains where these\nmethods are applied. Next, we gather datasets used for deepfake detection and\nprovide updated rankings of the best performing deepfake detectors on the most\npopular datasets. In addition, we develop a novel multimodal benchmark to\nevaluate deepfake detectors on out-of-distribution content. The results\nindicate that state-of-the-art detectors fail to generalize to deepfake content\ngenerated by unseen deepfake generators. Finally, we propose future directions\nto obtain robust and powerful deepfake detectors. Our project page and new\nbenchmark are available at https://github.com/CroitoruAlin/biodeep.\n","authors":["Florinel-Alin Croitoru","Andrei-Iulian Hiji","Vlad Hondru","Nicolae Catalin Ristea","Paul Irofti","Marius Popescu","Cristian Rusu","Radu Tudor Ionescu","Fahad Shahbaz Khan","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2411.19537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19522v1","updated":"2024-11-29T07:40:58Z","published":"2024-11-29T07:40:58Z","title":"Subjective and Objective Quality Assessment Methods of Stereoscopic\n  Videos with Visibility Affecting Distortions","summary":"  We present two major contributions in this work: 1) we create a full HD\nresolution stereoscopic (S3D) video dataset comprised of 12 reference and 360\ndistorted videos. The test stimuli are produced by simulating the five levels\nof fog and haze ambiances on the pristine left and right video sequences. We\nperform subjective analysis on the created video dataset with 24 viewers and\ncompute Difference Mean Opinion Scores (DMOS) as quality representative of the\ndataset, 2) an Opinion Unaware (OU) and Distortion Unaware (DU) video quality\nassessment model is developed for S3D videos. We construct cyclopean frames\nfrom the individual views of an S3D video and partition them into\nnonoverlapping blocks. We analyze the Natural Scene Statistics (NSS) of all\npatches of pristine and test videos, and empirically model the NSS features\nwith Univariate Generalized Gaussian Distribution (UGGD). We compute UGGD model\nparameters ({\\alpha}, \\b{eta}) at multiple spatial scales and multiple\norientations of spherical steerable pyramid decomposition and show that the\nUGGD parameters are distortion discriminable. Further, we perform Multivariate\nGaussian (MVG) modeling on the pristine and distorted video feature sets and\ncompute the corresponding mean vectors and covariance matrices of MVG fits. We\ncompute the Bhattacharyya distance measure between mean vectors and covariance\nmatrices to estimate the perceptual deviation of a test video from pristine\nvideo set. Finally, we pool both distance measures to estimate the overall\nquality score of an S3D video. The performance of the proposed objective\nalgorithm is verified on the popular S3D video datasets such as IRCCYN,\nLFOVIAS3DPh1, LFOVIAS3DPh2 and the proposed VAD stereo dataset. The algorithm\ndelivers consistent performance across all datasets and shows competitive\nperformance against off-the-shelf 2D and 3D image and video quality assessment\nalgorithms.\n","authors":["Sria Biswas","Balasubramanyam Appina","Priyanka Kokil","Sumohana S Channappayya"],"pdf_url":"https://arxiv.org/pdf/2411.19522v1.pdf","comment":"13 pages"}]},"2024-11-28T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.02381v3","updated":"2024-11-28T23:46:52Z","published":"2024-10-03T11:01:25Z","title":"MetaMetrics: Calibrating Metrics For Generation Tasks Using Human\n  Preferences","summary":"  Understanding the quality of a performance evaluation metric is crucial for\nensuring that model outputs align with human preferences. However, it remains\nunclear how well each metric captures the diverse aspects of these preferences,\nas metrics often excel in one particular area but not across all dimensions. To\naddress this, it is essential to systematically calibrate metrics to specific\naspects of human preference, catering to the unique characteristics of each\naspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate\ngeneration tasks across different modalities in a supervised manner.\nMetaMetrics optimizes the combination of existing metrics to enhance their\nalignment with human preferences. Our metric demonstrates flexibility and\neffectiveness in both language and vision downstream tasks, showing significant\nbenefits across various multilingual and multi-domain scenarios. MetaMetrics\naligns closely with human preferences and is highly extendable and easily\nintegrable into any application. This makes MetaMetrics a powerful tool for\nimproving the evaluation of generation tasks, ensuring that metrics are more\nrepresentative of human judgment across diverse contexts.\n","authors":["Genta Indra Winata","David Anugraha","Lucky Susanto","Garry Kuwanto","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2410.02381v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.12705v3","updated":"2024-11-28T22:47:21Z","published":"2024-10-16T16:11:49Z","title":"WorldCuisines: A Massive-Scale Benchmark for Multilingual and\n  Multicultural Visual Question Answering on Global Cuisines","summary":"  Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data.\n","authors":["Genta Indra Winata","Frederikus Hudi","Patrick Amadeus Irawan","David Anugraha","Rifki Afina Putri","Yutong Wang","Adam Nohejl","Ubaidillah Ariq Prathama","Nedjma Ousidhoum","Afifa Amriani","Anar Rzayev","Anirban Das","Ashmari Pramodya","Aulia Adila","Bryan Wilie","Candy Olivia Mawalim","Ching Lam Cheng","Daud Abolade","Emmanuele Chersoni","Enrico Santus","Fariz Ikhwantri","Garry Kuwanto","Hanyang Zhao","Haryo Akbarianto Wibowo","Holy Lovenia","Jan Christian Blaise Cruz","Jan Wira Gotama Putra","Junho Myung","Lucky Susanto","Maria Angelica Riera Machin","Marina Zhukova","Michael Anugraha","Muhammad Farid Adilazuarda","Natasha Santosa","Peerat Limkonchotiwat","Raj Dabre","Rio Alexander Audino","Samuel Cahyawijaya","Shi-Xiong Zhang","Stephanie Yulia Salim","Yi Zhou","Yinxuan Gui","David Ifeoluwa Adelani","En-Shiun Annie Lee","Shogo Okada","Ayu Purwarianti","Alham Fikri Aji","Taro Watanabe","Derry Tanti Wijaya","Alice Oh","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2410.12705v3.pdf","comment":"Preprint"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.18969v2","updated":"2024-11-28T20:29:44Z","published":"2024-09-11T14:50:28Z","title":"Integrating SPARQL and LLMs for Question Answering over Scholarly Data\n  Sources","summary":"  The Scholarly Hybrid Question Answering over Linked Data (QALD) Challenge at\nthe International Semantic Web Conference (ISWC) 2024 focuses on Question\nAnswering (QA) over diverse scholarly sources: DBLP, SemOpenAlex, and\nWikipedia-based texts. This paper describes a methodology that combines SPARQL\nqueries, divide and conquer algorithms, and a pre-trained extractive question\nanswering model. It starts with SPARQL queries to gather data, then applies\ndivide and conquer to manage various question types and sources, and uses the\nmodel to handle personal author questions. The approach, evaluated with Exact\nMatch and F-score metrics, shows promise for improving QA accuracy and\nefficiency in scholarly contexts.\n","authors":["Fomubad Borista Fondi","Azanzi Jiomekong Fidel","Gaoussou Camara"],"pdf_url":"https://arxiv.org/pdf/2409.18969v2.pdf","comment":"Scholarly Hybrid Question answering challenge from the International\n  Semantic Web Conference of 2024(ISWC), 7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.19214v1","updated":"2024-11-28T15:36:55Z","published":"2024-11-28T15:36:55Z","title":"Parallel and Mini-Batch Stable Matching for Large-Scale Reciprocal\n  Recommender Systems","summary":"  Reciprocal recommender systems (RRSs) are crucial in online two-sided\nmatching platforms, such as online job or dating markets, as they need to\nconsider the preferences of both sides of the match. The concentration of\nrecommendations to a subset of users on these platforms undermines their match\nopportunities and reduces the total number of matches. To maximize the total\nnumber of expected matches among market participants, stable matching theory\nwith transferable utility has been applied to RRSs. However, computational\ncomplexity and memory efficiency quadratically increase with the number of\nusers, making it difficult to implement stable matching algorithms for several\nusers. In this study, we propose novel methods using parallel and mini-batch\ncomputations for reciprocal recommendation models to improve the computational\ntime and space efficiency of the optimization process for stable matching.\nExperiments on both real and synthetic data confirmed that our stable matching\ntheory-based RRS increased the computation speed and enabled tractable\nlarge-scale data processing of up to one million samples with a single graphics\nprocessing unit graphics board, without losing the match count.\n","authors":["Kento Nakada","Kazuki Kawamura","Ryosuke Furukawa"],"pdf_url":"https://arxiv.org/pdf/2411.19214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15408v3","updated":"2024-11-28T13:46:38Z","published":"2023-09-27T05:20:53Z","title":"A smoothed-Bayesian approach to frequency recovery from sketched data","summary":"  We provide a novel statistical perspective on a classical problem at the\nintersection of computer science and information theory: recovering the\nempirical frequency of a symbol in a large discrete dataset using only a\ncompressed representation, or sketch, obtained via random hashing. Departing\nfrom traditional algorithmic approaches, recent works have proposed Bayesian\nnonparametric (BNP) methods that can provide more informative frequency\nestimates by leveraging modeling assumptions about the distribution of the\nsketched data. In this paper, we propose a smoothed-Bayesian method, inspired\nby existing BNP approaches but designed in a frequentist framework to overcome\nthe computational limitations of the BNP approaches when dealing with\nlarge-scale data from realistic distributions, including those with power-law\ntail behaviors. For sketches obtained with a single hash function, our approach\nis supported by rigorous frequentist properties, including unbiasedness and\noptimality under a squared error loss function within an intuitive class of\nlinear estimators. For sketches with multiple hash functions, we introduce an\napproach based on multi-view learning to construct computationally efficient\nfrequency estimators. We validate our method on synthetic and real data,\ncomparing its performance to that of existing alternatives.\n","authors":["Mario Beraha","Stefano Favaro","Matteo Sesia"],"pdf_url":"https://arxiv.org/pdf/2309.15408v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19119v1","updated":"2024-11-28T13:06:48Z","published":"2024-11-28T13:06:48Z","title":"Introducing Three New Benchmark Datasets for Hierarchical Text\n  Classification","summary":"  Hierarchical Text Classification (HTC) is a natural language processing task\nwith the objective to classify text documents into a set of classes from a\nstructured class hierarchy. Many HTC approaches have been proposed which\nattempt to leverage the class hierarchy information in various ways to improve\nclassification performance. Machine learning-based classification approaches\nrequire large amounts of training data and are most-commonly compared through\nthree established benchmark datasets, which include the Web Of Science (WOS),\nReuters Corpus Volume 1 Version 2 (RCV1-V2) and New York Times (NYT) datasets.\nHowever, apart from the RCV1-V2 dataset which is well-documented, these\ndatasets are not accompanied with detailed description methodologies. In this\npaper, we introduce three new HTC benchmark datasets in the domain of research\npublications which comprise the titles and abstracts of papers from the Web of\nScience publication database. We first create two baseline datasets which use\nexisting journal-and citation-based classification schemas. Due to the\nrespective shortcomings of these two existing schemas, we propose an approach\nwhich combines their classifications to improve the reliability and robustness\nof the dataset. We evaluate the three created datasets with a clustering-based\nanalysis and show that our proposed approach results in a higher quality\ndataset where documents that belong to the same class are semantically more\nsimilar compared to the other datasets. Finally, we provide the classification\nperformance of four state-of-the-art HTC approaches on these three new datasets\nto provide baselines for future studies on machine learning-based techniques\nfor scientific publication classification.\n","authors":["Jaco du Toit","Herman Redelinghuys","Marcel Dunaiski"],"pdf_url":"https://arxiv.org/pdf/2411.19119v1.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.19113v1","updated":"2024-11-28T12:59:32Z","published":"2024-11-28T12:59:32Z","title":"Integration of Contextual Descriptors in Ontology Alignment for\n  Enrichment of Semantic Correspondence","summary":"  This paper proposes a novel approach to semantic ontology alignment using\ncontextual descriptors. A formalization was developed that enables the\nintegration of essential and contextual descriptors to create a comprehensive\nknowledge model. The hierarchical structure of the semantic approach and the\nmathematical apparatus for analyzing potential conflicts between concepts,\nparticularly in the example of \"Transparency\" and \"Privacy\" in the context of\nartificial intelligence, are demonstrated. Experimental studies showed a\nsignificant improvement in ontology alignment metrics after the implementation\nof contextual descriptors, especially in the areas of privacy, responsibility,\nand freedom & autonomy. The application of contextual descriptors achieved an\naverage overall improvement of approximately 4.36%. The results indicate the\neffectiveness of the proposed approach for more accurately reflecting the\ncomplexity of knowledge and its contextual dependence.\n","authors":["Eduard Manziuk","Oleksander Barmak","Pavlo Radiuk","Vladislav Kuznetsov","Iurii Krak","Sergiy Yakovlev"],"pdf_url":"https://arxiv.org/pdf/2411.19113v1.pdf","comment":"Ontology alignment, contextual descriptors, semantic matching,\n  knowledge representation, essential descriptors, ontology integration,\n  hierarchical structure, semantic heterogeneity, ethical AI"},{"id":"http://arxiv.org/abs/2411.19107v1","updated":"2024-11-28T12:44:56Z","published":"2024-11-28T12:44:56Z","title":"Headache to Overstock? Promoting Long-tail Items through Debiased\n  Product Bundling","summary":"  Product bundling aims to organize a set of thematically related items into a\ncombined bundle for shipment facilitation and item promotion. To increase the\nexposure of fresh or overstocked products, sellers typically bundle these items\nwith popular products for inventory clearance. This specific task can be\nformulated as a long-tail product bundling scenario, which leverages the\nuser-item interactions to define the popularity of each item. The inherent\npopularity bias in the pre-extracted user feedback features and the\ninsufficient utilization of other popularity-independent knowledge may force\nthe conventional bundling methods to find more popular items, thereby\nstruggling with this long-tail bundling scenario. Through intuitive and\nempirical analysis, we navigate the core solution for this challenge, which is\nmaximally mining the popularity-free features and effectively incorporating\nthem into the bundling process. To achieve this, we propose a Distilled\nModality-Oriented Knowledge Transfer framework (DieT) to effectively counter\nthe popularity bias misintroduced by the user feedback features and adhere to\nthe original intent behind the real-world bundling behaviors. Specifically,\nDieT first proposes the Popularity-free Collaborative Distribution Modeling\nmodule (PCD) to capture the popularity-independent information from the\nbundle-item view, which is proven most effective in the long-tail bundling\nscenario to enable the directional information transfer. With the tailored\nUnbiased Bundle-aware Knowledge Transferring module (UBT), DieT can highlight\nthe significance of popularity-free features while mitigating the negative\neffects of user feedback features in the long-tail scenario via the knowledge\ndistillation paradigm. Extensive experiments on two real-world datasets\ndemonstrate the superiority of DieT over a list of SOTA methods in the\nlong-tail bundling scenario.\n","authors":["Shuo Xu","Haokai Ma","Yunshan Ma","Xiaohao Liu","Lei Meng","Xiangxu Meng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2411.19107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18947v1","updated":"2024-11-28T06:28:45Z","published":"2024-11-28T06:28:45Z","title":"ICLERB: In-Context Learning Embedding and Reranker Benchmark","summary":"  In-Context Learning (ICL) enables Large Language Models (LLMs) to perform new\ntasks by conditioning on prompts with relevant information. Retrieval-Augmented\nGeneration (RAG) enhances ICL by incorporating retrieved documents into the\nLLM's context at query time. However, traditional retrieval methods focus on\nsemantic relevance, treating retrieval as a search problem. In this paper, we\npropose reframing retrieval for ICL as a recommendation problem, aiming to\nselect documents that maximize utility in ICL tasks. We introduce the\nIn-Context Learning Embedding and Reranker Benchmark (ICLERB), a novel\nevaluation framework that compares retrievers based on their ability to enhance\nLLM accuracy in ICL settings. Additionally, we propose a novel Reinforcement\nLearning-to-Rank from AI Feedback (RLRAIF) algorithm, designed to fine-tune\nretrieval models using minimal feedback from the LLM. Our experimental results\nreveal notable differences between ICLERB and existing benchmarks, and\ndemonstrate that small models fine-tuned with our RLRAIF algorithm outperform\nlarge state-of-the-art retrieval models. These findings highlight the\nlimitations of existing evaluation methods and the need for specialized\nbenchmarks and training strategies adapted to ICL.\n","authors":["Marie Al Ghossein","Emile Contal","Alexandre Robicquet"],"pdf_url":"https://arxiv.org/pdf/2411.18947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15937v2","updated":"2024-11-28T05:29:32Z","published":"2024-03-23T21:54:18Z","title":"Model, Analyze, and Comprehend User Interactions within a Social Media\n  Platform","summary":"  In this study, we propose a novel graph-based approach to model, analyze and\ncomprehend user interactions within a social media platform based on\npost-comment relationship. We construct a user interaction graph from social\nmedia data and analyze it to gain insights into community dynamics, user\nbehavior, and content preferences. Our investigation reveals that while 56.05%\nof the active users are strongly connected within the community, only 0.8% of\nthem significantly contribute to its dynamics. Moreover, we observe temporal\nvariations in community activity, with certain periods experiencing heightened\nengagement. Additionally, our findings highlight a correlation between user\nactivity and popularity showing that more active users are generally more\npopular. Alongside these, a preference for positive and informative content is\nalso observed where 82.41% users preferred positive and informative content.\nOverall, our study provides a comprehensive framework for understanding and\nmanaging online communities, leveraging graph-based techniques to gain valuable\ninsights into user behavior and community dynamics.\n","authors":["Md Kaykobad Reza","S M Maksudul Alam","Yiran Luo","Youzhe Liu","Md Siam"],"pdf_url":"https://arxiv.org/pdf/2403.15937v2.pdf","comment":"Accepted by 27th International Conference on Computer and Information\n  Technology (ICCIT), 2024. 6 Pages, 6 Figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.19220v1","updated":"2024-11-28T15:42:32Z","published":"2024-11-28T15:42:32Z","title":"Automatic Prompt Generation and Grounding Object Detection for Zero-Shot\n  Image Anomaly Detection","summary":"  Identifying defects and anomalies in industrial products is a critical\nquality control task. Traditional manual inspection methods are slow,\nsubjective, and error-prone. In this work, we propose a novel zero-shot\ntraining-free approach for automated industrial image anomaly detection using a\nmultimodal machine learning pipeline, consisting of three foundation models.\nOur method first uses a large language model, i.e., GPT-3. generate text\nprompts describing the expected appearances of normal and abnormal products. We\nthen use a grounding object detection model, called Grounding DINO, to locate\nthe product in the image. Finally, we compare the cropped product image patches\nto the generated prompts using a zero-shot image-text matching model, called\nCLIP, to identify any anomalies. Our experiments on two datasets of industrial\nproduct images, namely MVTec-AD and VisA, demonstrate the effectiveness of this\nmethod, achieving high accuracy in detecting various types of defects and\nanomalies without the need for model training. Our proposed model enables\nefficient, scalable, and objective quality control in industrial manufacturing\nsettings.\n","authors":["Tsun-Hin Cheung","Ka-Chun Fung","Songjiang Lai","Kwan-Ho Lin","Vincent Ng","Kin-Man Lam"],"pdf_url":"https://arxiv.org/pdf/2411.19220v1.pdf","comment":"Accepted to APSIPA ASC 2024"},{"id":"http://arxiv.org/abs/2411.17698v2","updated":"2024-11-28T13:25:04Z","published":"2024-11-26T18:59:58Z","title":"Video-Guided Foley Sound Generation with Multimodal Controls","summary":"  Generating sound effects for videos often requires creating artistic sound\neffects that diverge significantly from real-life sources and flexible control\nin the sound design. To address this problem, we introduce MultiFoley, a model\ndesigned for video-guided sound generation that supports multimodal\nconditioning through text, audio, and video. Given a silent video and a text\nprompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels\nspinning without wind noise) or more whimsical sounds (e.g., making a lion's\nroar sound like a cat's meow). MultiFoley also allows users to choose reference\naudio from sound effects (SFX) libraries or partial videos for conditioning. A\nkey novelty of our model lies in its joint training on both internet video\ndatasets with low-quality audio and professional SFX recordings, enabling\nhigh-quality, full-bandwidth (48kHz) audio generation. Through automated\nevaluations and human studies, we demonstrate that MultiFoley successfully\ngenerates synchronized high-quality sounds across varied conditional inputs and\noutperforms existing methods. Please see our project page for video results:\nhttps://ificl.github.io/MultiFoley/\n","authors":["Ziyang Chen","Prem Seetharaman","Bryan Russell","Oriol Nieto","David Bourgin","Andrew Owens","Justin Salamon"],"pdf_url":"https://arxiv.org/pdf/2411.17698v2.pdf","comment":"Project site: https://ificl.github.io/MultiFoley/"},{"id":"http://arxiv.org/abs/2405.00233v2","updated":"2024-11-28T12:31:04Z","published":"2024-04-30T22:51:36Z","title":"SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General\n  Sound","summary":"  Large language models (LLMs) have significantly advanced audio processing\nthrough audio codecs that convert audio into discrete tokens, enabling the\napplication of language modelling techniques to audio data. However,\ntraditional codecs often operate at high bitrates or within narrow domains such\nas speech and lack the semantic clues required for efficient language\nmodelling. Addressing these challenges, we introduce SemantiCodec, a novel\ncodec designed to compress audio into fewer than a hundred tokens per second\nacross diverse audio types, including speech, general sound, and music, without\ncompromising quality. SemantiCodec features a dual-encoder architecture: a\nsemantic encoder using a self-supervised pre-trained Audio Masked Autoencoder\n(AudioMAE), discretized using k-means clustering on extensive audio data, and\nan acoustic encoder to capture the remaining details. The semantic and acoustic\nencoder outputs are used to reconstruct audio via a diffusion-model-based\ndecoder. SemantiCodec is presented in three variants with token rates of 25,\n50, and 100 per second, supporting a range of ultra-low bit rates between 0.31\nkbps and 1.40 kbps. Experimental results demonstrate that SemantiCodec\nsignificantly outperforms the state-of-the-art Descript codec on reconstruction\nquality. Our results also suggest that SemantiCodec contains significantly\nricher semantic information than all evaluated state-of-the-art audio codecs,\neven at significantly lower bitrates. Our code and demos are available at\nhttps://haoheliu.github.io/SemantiCodec/.\n","authors":["Haohe Liu","Xuenan Xu","Yi Yuan","Mengyue Wu","Wenwu Wang","Mark D. Plumbley"],"pdf_url":"https://arxiv.org/pdf/2405.00233v2.pdf","comment":"Accepted by Journal of Selected Topics in Signal Processing (JSTSP).\n  Demo and code: https://haoheliu.github.io/SemantiCodec/"},{"id":"http://arxiv.org/abs/2405.09266v3","updated":"2024-11-28T10:30:14Z","published":"2024-05-15T11:33:07Z","title":"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation","summary":"  Generating dance from music is crucial for advancing automated choreography.\nCurrent methods typically produce skeleton keypoint sequences instead of dance\nvideos and lack the capability to make specific individuals dance, which\nreduces their real-world applicability. These methods also require precise\nkeypoint annotations, complicating data collection and limiting the use of\nself-collected video datasets. To overcome these challenges, we introduce a\nnovel task: generating dance videos directly from images of individuals guided\nby music. This task enables the dance generation of specific individuals\nwithout requiring keypoint annotations, making it more versatile and applicable\nto various situations. Our solution, the Dance Any Beat Diffusion model\n(DabFusion), utilizes a reference image and a music piece to generate dance\nvideos featuring various dance types and choreographies. The music is analyzed\nby our specially designed music encoder, which identifies essential features\nincluding dance style, movement, and rhythm. DabFusion excels in generating\ndance videos not only for individuals in the training dataset but also for any\npreviously unseen person. This versatility stems from its approach of\ngenerating latent optical flow, which contains all necessary motion information\nto animate any person in the image. We evaluate DabFusion's performance using\nthe AIST++ dataset, focusing on video quality, audio-video synchronization, and\nmotion-music alignment. We propose a 2D Motion-Music Alignment Score (2D-MM\nAlign), which builds on the Beat Alignment Score to more effectively evaluate\nmotion-music alignment for this new task. Experiments show that our DabFusion\nestablishes a solid baseline for this innovative task. Video results can be\nfound on our project page: https://DabFusion.github.io.\n","authors":["Xuanchen Wang","Heng Wang","Dongnan Liu","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2405.09266v3.pdf","comment":"WACV2025, 11 pages, 7 figures, demo page: https://DabFusion.github.io"},{"id":"http://arxiv.org/abs/2305.09979v2","updated":"2024-11-28T07:49:44Z","published":"2023-05-17T06:23:06Z","title":"Self-Training Boosted Multi-Factor Matching Network for Composed Image\n  Retrieval","summary":"  The composed image retrieval (CIR) task aims to retrieve the desired target\nimage for a given multimodal query, i.e., a reference image with its\ncorresponding modification text. The key limitations encountered by existing\nefforts are two aspects: 1) ignoring the multi-faceted query-target matching\nfactors; 2) ignoring the potential unlabeled reference-target image pairs in\nexisting benchmark datasets. To address these two limitations is non-trivial\ndue to the following challenges: 1) how to effectively model the multi-faceted\nmatching factors in a latent way without direct supervision signals; 2) how to\nfully utilize the potential unlabeled reference-target image pairs to improve\nthe generalization ability of the CIR model. To address these challenges, in\nthis work, we first propose a muLtI-faceted Matching Network (LIMN), which\nconsists of three key modules: multi-grained image/text encoder, latent\nfactor-oriented feature aggregation, and query-target matching modeling.\nThereafter, we design an iterative dual self-training paradigm to further\nenhance the performance of LIMN by fully utilizing the potential unlabeled\nreference-target image pairs in a semi-supervised manner. Specifically, we\ndenote the iterative dual self-training paradigm enhanced LIMN as LIMN+.\nExtensive experiments on three real-world datasets, FashionIQ, Shoes, and\nBirds-to-Words, show that our proposed method significantly surpasses the\nstate-of-the-art baselines.\n","authors":["Haokun Wen","Xuemeng Song","Jianhua Yin","Jianlong Wu","Weili Guan","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2305.09979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18966v1","updated":"2024-11-28T07:36:22Z","published":"2024-11-28T07:36:22Z","title":"SuperGaussians: Enhancing Gaussian Splatting Using Primitives with\n  Spatially Varying Colors","summary":"  Gaussian Splattings demonstrate impressive results in multi-view\nreconstruction based on Gaussian explicit representations. However, the current\nGaussian primitives only have a single view-dependent color and an opacity to\nrepresent the appearance and geometry of the scene, resulting in a non-compact\nrepresentation. In this paper, we introduce a new method called SuperGaussians\nthat utilizes spatially varying colors and opacity in a single Gaussian\nprimitive to improve its representation ability. We have implemented bilinear\ninterpolation, movable kernels, and even tiny neural networks as spatially\nvarying functions. Quantitative and qualitative experimental results\ndemonstrate that all three functions outperform the baseline, with the best\nmovable kernels achieving superior novel view synthesis performance on multiple\ndatasets, highlighting the strong potential of spatially varying functions.\n","authors":["Rui Xu","Wenyue Chen","Jiepeng Wang","Yuan Liu","Peng Wang","Lin Gao","Shiqing Xin","Taku Komura","Xin Li","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2411.18966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18855v1","updated":"2024-11-28T01:51:46Z","published":"2024-11-28T01:51:46Z","title":"Improving Accuracy and Generalization for Efficient Visual Tracking","summary":"  Efficient visual trackers overfit to their training distributions and lack\ngeneralization abilities, resulting in them performing well on their respective\nin-distribution (ID) test sets and not as well on out-of-distribution (OOD)\nsequences, imposing limitations to their deployment in-the-wild under\nconstrained resources. We introduce SiamABC, a highly efficient Siamese tracker\nthat significantly improves tracking performance, even on OOD sequences.\nSiamABC takes advantage of new architectural designs in the way it bridges the\ndynamic variability of the target, and of new losses for training. Also, it\ndirectly addresses OOD tracking generalization by including a fast\nbackward-free dynamic test-time adaptation method that continuously adapts the\nmodel according to the dynamic visual changes of the target. Our extensive\nexperiments suggest that SiamABC shows remarkable performance gains in OOD sets\nwhile maintaining accurate performance on the ID benchmarks. SiamABC\noutperforms MixFormerV2-S by 7.6\\% on the OOD AVisT benchmark while being 3x\nfaster (100 FPS) on a CPU.\n","authors":["Ram Zaveri","Shivang Patel","Yu Gu","Gianfranco Doretto"],"pdf_url":"https://arxiv.org/pdf/2411.18855v1.pdf","comment":"WACV 2025"}]},"2024-11-27T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2411.18814v1","updated":"2024-11-27T23:36:59Z","published":"2024-11-27T23:36:59Z","title":"Unifying Generative and Dense Retrieval for Sequential Recommendation","summary":"  Sequential dense retrieval models utilize advanced sequence learning\ntechniques to compute item and user representations, which are then used to\nrank relevant items for a user through inner product computation between the\nuser and all item representations. However, this approach requires storing a\nunique representation for each item, resulting in significant memory\nrequirements as the number of items grow. In contrast, the recently proposed\ngenerative retrieval paradigm offers a promising alternative by directly\npredicting item indices using a generative model trained on semantic IDs that\nencapsulate items' semantic information. Despite its potential for large-scale\napplications, a comprehensive comparison between generative retrieval and\nsequential dense retrieval under fair conditions is still lacking, leaving open\nquestions regarding performance, and computation trade-offs. To address this,\nwe compare these two approaches under controlled conditions on academic\nbenchmarks and propose LIGER (LeveragIng dense retrieval for GEnerative\nRetrieval), a hybrid model that combines the strengths of these two widely used\nmethods. LIGER integrates sequential dense retrieval into generative retrieval,\nmitigating performance differences and enhancing cold-start item recommendation\nin the datasets evaluated. This hybrid approach provides insights into the\ntrade-offs between these approaches and demonstrates improvements in efficiency\nand effectiveness for recommendation systems in small-scale benchmarks.\n","authors":["Liu Yang","Fabian Paischer","Kaveh Hassani","Jiacheng Li","Shuai Shao","Zhang Gabriel Li","Yun He","Xue Feng","Nima Noorshams","Sem Park","Bo Long","Robert D Nowak","Xiaoli Gao","Hamid Eghbalzadeh"],"pdf_url":"https://arxiv.org/pdf/2411.18814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18583v1","updated":"2024-11-27T18:27:07Z","published":"2024-11-27T18:27:07Z","title":"Automated Literature Review Using NLP Techniques and LLM-Based\n  Retrieval-Augmented Generation","summary":"  This research presents and compares multiple approaches to automate the\ngeneration of literature reviews using several Natural Language Processing\n(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language\nModel (LLM). The ever-increasing number of research articles provides a huge\nchallenge for manual literature review. It has resulted in an increased demand\nfor automation. Developing a system capable of automatically generating the\nliterature reviews from only the PDF files as input is the primary objective of\nthis research work. The effectiveness of several Natural Language Processing\n(NLP) strategies, such as the frequency-based method (spaCy), the transformer\nmodel (Simple T5), and retrieval-augmented generation (RAG) with Large Language\nModel (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR\ndataset is chosen for this research experiment and three distinct techniques\nare utilized to implement three different systems for auto-generating the\nliterature reviews. The ROUGE scores are used for the evaluation of all three\nsystems. Based on the evaluation, the Large Language Model GPT-3.5-turbo\nachieved the highest ROUGE-1 score, 0.364. The transformer model comes in\nsecond place and spaCy is at the last position. Finally, a graphical user\ninterface is created for the best system based on the large language model.\n","authors":["Nurshat Fateh Ali","Md. Mahdi Mohtasim","Shakil Mosharrof","T. Gopi Krishna"],"pdf_url":"https://arxiv.org/pdf/2411.18583v1.pdf","comment":"Key Words : T5, SpaCy, Large Language Model, GPT, ROUGE, Literature\n  Review, Natural Language Processing, Retrieval-augmented generation"},{"id":"http://arxiv.org/abs/2405.13362v2","updated":"2024-11-27T17:07:41Z","published":"2024-05-22T05:43:15Z","title":"Lusifer: LLM-based User SImulated Feedback Environment for online\n  Recommender systems","summary":"  Training reinforcement learning-based recommender systems is often hindered\nby the lack of dynamic and realistic user interactions. To address this\nlimitation, we introduce Lusifer, a novel environment leveraging Large Language\nModels (LLMs) to generate simulated user feedback. Lusifer synthesizes user\nprofiles and interaction histories to simulate responses and behaviors toward\nrecommended items, with profiles updated after each rating to reflect evolving\nuser characteristics. Utilizing the MovieLens dataset as a proof of concept, we\nlimited our implementation to the last 40 interactions for each user,\nrepresenting approximately 39% and 22% of the training sets, to focus on recent\nuser behavior. For consistency and to gain insights into the performance of\ntraditional methods with limited data, we implemented baseline approaches using\nthe same data subset. Our results demonstrate that Lusifer accurately emulates\nuser behavior and preferences, even with reduced training data having an RMSE\nof 1.3 across various test sets. This paper presents Lusifer's operational\npipeline, including prompt generation and iterative user profile updates, and\ncompares its performance against baseline methods. The findings validate\nLusifer's ability to produce realistic dynamic feedback and suggest that it\noffers a scalable and adjustable framework for user simulation in online\nreinforcement learning recommender systems for future studies, particularly\nwhen training data is limited.\n","authors":["Danial Ebrat","Eli Paradalis","Luis Rueda"],"pdf_url":"https://arxiv.org/pdf/2405.13362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18502v1","updated":"2024-11-27T16:43:13Z","published":"2024-11-27T16:43:13Z","title":"Isometry pursuit","summary":"  Isometry pursuit is a convex algorithm for identifying orthonormal\ncolumn-submatrices of wide matrices. It consists of a novel normalization\nmethod followed by multitask basis pursuit. Applied to Jacobians of putative\ncoordinate functions, it helps identity isometric embeddings from within\ninterpretable dictionaries. We provide theoretical and experimental results\njustifying this method. For problems involving coordinate selection and\ndiversification, it offers a synergistic alternative to greedy and brute force\nsearch.\n","authors":["Samson Koelle","Marina Meila"],"pdf_url":"https://arxiv.org/pdf/2411.18502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15479v4","updated":"2024-11-27T16:27:32Z","published":"2024-01-27T19:04:30Z","title":"Navigating the Post-API Dilemma | Search Engine Results Pages Present a\n  Biased View of Social Media Data","summary":"  Recent decisions to discontinue access to social media APIs are having\ndetrimental effects on Internet research and the field of computational social\nscience as a whole. This lack of access to data has been dubbed the Post-API\nera of Internet research. Fortunately, popular search engines have the means to\ncrawl, capture, and surface social media data on their Search Engine Results\nPages (SERP) if provided the proper search query, and may provide a solution to\nthis dilemma. In the present work we ask: does SERP provide a complete and\nunbiased sample of social media data? Is SERP a viable alternative to direct\nAPI-access? To answer these questions, we perform a comparative analysis\nbetween (Google) SERP results and nonsampled data from Reddit and Twitter/X. We\nfind that SERP results are highly biased in favor of popular posts; against\npolitical, pornographic, and vulgar posts; are more positive in their\nsentiment; and have large topical gaps. Overall, we conclude that SERP is not a\nviable alternative to social media API access.\n","authors":["Amrit Poudel","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2401.15479v4.pdf","comment":"Proceedings of the ACM Web Conference 2024 (WWW '24)"},{"id":"http://arxiv.org/abs/2409.13694v2","updated":"2024-11-27T15:13:00Z","published":"2024-09-03T03:31:37Z","title":"Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A\n  Benchmark and Empirical Study","summary":"  Retrieval-augmented generation (RAG) is increasingly recognized as an\neffective approach for mitigating the hallucination of large language models\n(LLMs) through the integration of external knowledge. While numerous efforts,\nmost studies focus on a single type of externeal knowledge source. However, in\nreal-world applications, most situations involve diverse knowledge from various\nsources, yet this area has been less explored. The main dilemma is the lack of\na suitable dataset containing multiple knowledge sources and pre-exploration of\nthe associated issues. To address these challenges, we standardize a benchmark\ndataset that combines structured and unstructured knowledge across diverse and\ncomplementary domains. Based on this dataset, we further develop a\nplug-and-play RAG framework, PruningRAG, whose main characteristic is to employ\nmulti-granularity pruning strategies for optimizing the integration of relevant\ninformation and minimizing misleading context. Building upon the standardized\ndataset and PruningRAG, we also report a series of experimental results, as\nwell as insightful findings. Our dataset and code are publicly\navailable\\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim of\nadvancing future research in the RAG community.\n","authors":["Shuo Yu","Mingyue Cheng","Jiqian Yang","Jie Ouyang","Yucong Luo","Chenyi Lei","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2409.13694v2.pdf","comment":"10 pages, 11 figures;"},{"id":"http://arxiv.org/abs/2411.18306v1","updated":"2024-11-27T12:52:51Z","published":"2024-11-27T12:52:51Z","title":"Delineating Feminist Studies through bibliometric analysis","summary":"  The multidisciplinary and socially anchored nature of Feminist Studies\npresents unique challenges for bibliometric analysis, as this research area\ntranscends traditional disciplinary boundaries and reflects discussions from\nfeminist and LGBTQIA+ social movements. This paper proposes a novel approach\nfor identifying gender/sex related publications scattered across diverse\nscientific disciplines. Using the Dimensions database, we employ bibliometric\ntechniques, natural language processing (NLP) and manual curation to compile a\ndataset of scientific publications that allows for the analysis of Gender\nStudies and its influence across different disciplines.\n  This is achieved through a methodology that combines a core of specialized\njournals with a comprehensive keyword search over titles. These keywords are\nobtained by applying Topic Modeling (BERTopic) to the corpus of titles and\nabstracts from the core. This methodological strategy, divided into two stages,\nreflects the dynamic interaction between Gender Studies and its dialogue with\ndifferent disciplines. This hybrid system surpasses basic keyword search by\nmitigating potential biases introduced through manual keyword enumeration.\n  The resulting dataset comprises over 1.9 million scientific documents\npublished between 1668 and 2023, spanning four languages. This dataset enables\na characterization of Gender Studies in terms of addressed topics, citation and\ncollaboration dynamics, and institutional and regional participation. By\naddressing the methodological challenges of studying \"more-than-disciplinary\"\nresearch areas, this approach could also be adapted to delineate other\nconversations where disciplinary boundaries are difficult to disentangle.\n","authors":["Natsumi S. Shokida","Diego Kozlowski","Vincent Larivière"],"pdf_url":"https://arxiv.org/pdf/2411.18306v1.pdf","comment":"2 tables, 5 figures"},{"id":"http://arxiv.org/abs/2411.18262v1","updated":"2024-11-27T11:59:44Z","published":"2024-11-27T11:59:44Z","title":"Break the ID-Language Barrier: An Adaption Framework for Sequential\n  Recommendation","summary":"  The recent breakthrough of large language models (LLMs) in natural language\nprocessing has sparked exploration in recommendation systems, however, their\nlimited domain-specific knowledge remains a critical bottleneck. Specifically,\nLLMs lack key pieces of information crucial for sequential recommendations,\nsuch as user behavior patterns. To address this critical gap, we propose\nIDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich\nin domain-specific knowledge, into LLMs to improve recommendation accuracy.\nIDLE-Adapter acts as a bridge, transforming sparse user-item interaction data\ninto dense, LLM-compatible representations through a Pre-trained ID Sequential\nModel, Dimensionality Alignment, Layer-wise Embedding Refinement, and\nLayer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates\nremarkable flexibility by seamlessly integrating ID embeddings from diverse\nID-based sequential models and LLM architectures. Extensive experiments across\nvarious datasets demonstrate the superiority of IDLE-Adapter, achieving over\n10\\% and 20\\% improvements in HitRate@5 and NDCG@5 metrics, respectively,\ncompared to state-of-the-art methods.\n","authors":["Xiaohan Yu","Li Zhang","Xin Zhao","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2411.18262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10596v3","updated":"2024-11-27T11:22:52Z","published":"2024-05-17T07:43:25Z","title":"CELA: Cost-Efficient Language Model Alignment for CTR Prediction","summary":"  Click-Through Rate (CTR) prediction holds a paramount position in recommender\nsystems. The prevailing ID-based paradigm underperforms in cold-start scenarios\ndue to the skewed distribution of feature frequency. Additionally, the\nutilization of a single modality fails to exploit the knowledge contained\nwithin textual features. Recent efforts have sought to mitigate these\nchallenges by integrating Pre-trained Language Models (PLMs). They design hard\nprompts to structure raw features into text for each interaction and then apply\nPLMs for text processing. With external knowledge and reasoning capabilities,\nPLMs extract valuable information even in cases of sparse interactions.\nNevertheless, compared to ID-based models, pure text modeling degrades the\nefficacy of collaborative filtering, as well as feature scalability and\nefficiency during both training and inference. To address these issues, we\npropose \\textbf{C}ost-\\textbf{E}fficient \\textbf{L}anguage Model\n\\textbf{A}lignment (\\textbf{CELA}) for CTR prediction. CELA incorporates\ntextual features and language models while preserving the collaborative\nfiltering capabilities of ID-based models. This model-agnostic framework can be\nequipped with plug-and-play textual features, with item-level alignment\nenhancing the utilization of external information while maintaining training\nand inference efficiency. Through extensive offline experiments, CELA\ndemonstrates superior performance compared to state-of-the-art methods.\nFurthermore, an online A/B test conducted on an industrial App recommender\nsystem showcases its practical effectiveness, solidifying the potential for\nreal-world applications of CELA.\n","authors":["Xingmei Wang","Weiwen Liu","Xiaolong Chen","Qi Liu","Xu Huang","Yichao Wang","Xiangyang Li","Yasheng Wang","Zhenhua Dong","Defu Lian","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2405.10596v3.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.18766v2","updated":"2024-11-27T09:54:34Z","published":"2024-10-24T14:19:38Z","title":"Citywide Electric Vehicle Charging Demand Prediction Approach\n  Considering Urban Region and Dynamic Influences","summary":"  Electric vehicle charging demand prediction is important for vacant charging\npile recommendation and charging infrastructure planning, thus facilitating\nvehicle electrification and green energy development. The performance of\nprevious spatio-temporal studies is still far from satisfactory nowadays\nbecause urban region attributes and multivariate temporal influences are not\nadequately taken into account. To tackle these issues, we propose a learning\napproach for citywide electric vehicle charging demand prediction, named\nCityEVCP. To learn non-pairwise relationships in urban areas, we cluster\nservice areas by the types and numbers of points of interest in the areas and\ndevelop attentive hypergraph networks accordingly. Graph attention mechanisms\nare employed for information propagation between neighboring areas.\nAdditionally, we propose a variable selection network to adaptively learn\ndynamic auxiliary information and improve the Transformer encoder utilizing\ngated mechanisms for fluctuating charging time-series data. Experiments on a\ncitywide electric vehicle charging dataset demonstrate the performances of our\nproposed approach compared with a broad range of competing baselines.\nFurthermore, we demonstrate the impact of dynamic influences on prediction\nresults in different areas of the city and the effectiveness of our area\nclustering method.\n","authors":["Haoxuan Kuang","Kunxiang Deng","Linlin You","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2410.18766v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14945v3","updated":"2024-11-27T09:43:20Z","published":"2023-12-06T15:24:01Z","title":"Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge\n  Base for Industrial Prognostics and Health Management","summary":"  Prognostics and health management (PHM) is essential for industrial operation\nand maintenance, focusing on predicting, diagnosing, and managing the health\nstatus of industrial systems. The emergence of the ChatGPT-Like large-scale\nlanguage model (LLM) has begun to lead a new round of innovation in the AI\nfield. It has extensively promoted the level of intelligence in various fields.\nTherefore, it is also expected further to change the application paradigm in\nindustrial PHM and promote PHM to become intelligent. Although ChatGPT-Like\nLLMs have rich knowledge reserves and powerful language understanding and\ngeneration capabilities, they lack domain-specific expertise, significantly\nlimiting their practicability in PHM applications. To this end, this study\nexplores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in\nindustrial PHM to solve the above limitations. In addition, we introduce the\nmethod and steps of combining the LKB with LLMs, including LKB preparation, LKB\nvectorization, prompt engineering, etc. Experimental analysis of real cases\nshows that combining the LKB with ChatGPT-Like LLM can significantly improve\nits performance and make ChatGPT-Like LLMs more accurate, relevant, and able to\nprovide more insightful information. This can promote the development of\nChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality.\n","authors":["Huan Wang","Yan-Fu Li","Min Xie"],"pdf_url":"https://arxiv.org/pdf/2312.14945v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18161v1","updated":"2024-11-27T09:17:31Z","published":"2024-11-27T09:17:31Z","title":"The Rn-index: a more accurate variant of the Rk-index","summary":"  The contribution to pushing the boundaries of knowledge is a critical metric\nfor evaluating the research performance of countries and institutions, which in\nmany cases is not revealed by common bibliometric indicators. The Rk-index was\nspecifically designed to assess such contributions, and the Rn-index is a\nvariant that corrects the weakness of the Rk-index, particularly in the\nevaluation of countries that produce a high proportion of global advancements.\nThis is the case of the USA and China in many technological fields.\nAdditionally, the Rn-index is simple to calculate and understand, as it\ninvolves only summing the ratios between the local and global ranks of papers,\nordered by their citation count. Moreover, the Rn-index may also be\nfractionally counted.\n","authors":["Alonso Rodriguez-Navarro"],"pdf_url":"https://arxiv.org/pdf/2411.18161v1.pdf","comment":"6 pages; 2 figures; 4 tables"},{"id":"http://arxiv.org/abs/2409.12959v2","updated":"2024-11-27T08:49:12Z","published":"2024-09-19T17:59:45Z","title":"MMSearch: Benchmarking the Potential of Large Models as Multi-modal\n  Search Engines","summary":"  The advent of Large Language Models (LLMs) has paved the way for AI search\nengines, e.g., SearchGPT, showcasing a new paradigm in human-internet\ninteraction. However, most current AI search engines are limited to text-only\nsettings, neglecting the multimodal user queries and the text-image interleaved\nnature of website information. Recently, Large Multimodal Models (LMMs) have\nmade impressive strides. Yet, whether they can function as AI search engines\nremains under-explored, leaving the potential of LMMs in multimodal search an\nopen question. To this end, we first design a delicate pipeline,\nMMSearch-Engine, to empower any LMMs with multimodal search capabilities. On\ntop of this, we introduce MMSearch, a comprehensive evaluation benchmark to\nassess the multimodal search performance of LMMs. The curated dataset contains\n300 manually collected instances spanning 14 subfields, which involves no\noverlap with the current LMMs' training data, ensuring the correct answer can\nonly be obtained within searching. By using MMSearch-Engine, the LMMs are\nevaluated by performing three individual tasks (requery, rerank, and\nsummarization), and one challenging end-to-end task with a complete searching\nprocess. We conduct extensive experiments on closed-source and open-source\nLMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best\nresults, which surpasses the commercial product, Perplexity Pro, in the\nend-to-end task, demonstrating the effectiveness of our proposed pipeline. We\nfurther present error analysis to unveil current LMMs still struggle to fully\ngrasp the multimodal search tasks, and conduct ablation study to indicate the\npotential of scaling test-time computation for AI search engine. We hope\nMMSearch may provide unique insights to guide the future development of\nmultimodal AI search engine. Project Page: https://mmsearch.github.io\n","authors":["Dongzhi Jiang","Renrui Zhang","Ziyu Guo","Yanmin Wu","Jiayi Lei","Pengshuo Qiu","Pan Lu","Zehui Chen","Chaoyou Fu","Guanglu Song","Peng Gao","Yu Liu","Chunyuan Li","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2409.12959v2.pdf","comment":"Project Page: https://mmsearch.github.io"},{"id":"http://arxiv.org/abs/2411.16872v2","updated":"2024-11-27T06:25:09Z","published":"2024-11-25T19:11:41Z","title":"Enabling Adoption of Regenerative Agriculture through Soil Carbon\n  Copilots","summary":"  Mitigating climate change requires transforming agriculture to minimize\nenviron mental impact and build climate resilience. Regenerative agricultural\npractices enhance soil organic carbon (SOC) levels, thus improving soil health\nand sequestering carbon. A challenge to increasing regenerative agriculture\npractices is cheaply measuring SOC over time and understanding how SOC is\naffected by regenerative agricultural practices and other environmental factors\nand farm management practices. To address this challenge, we introduce an\nAI-driven Soil Organic Carbon Copilot that automates the ingestion of complex\nmulti-resolution, multi-modal data to provide large-scale insights into soil\nhealth and regenerative practices. Our data includes extreme weather event data\n(e.g., drought and wildfire incidents), farm management data (e.g., cropland\ninformation and tillage predictions), and SOC predictions. We find that\nintegrating public data and specialized models enables large-scale, localized\nanalysis for sustainable agriculture. In comparisons of agricultural practices\nacross California counties, we find evidence that diverse agricultural activity\nmay mitigate the negative effects of tillage; and that while extreme weather\nconditions heavily affect SOC, composting may mitigate SOC loss. Finally,\nimplementing role-specific personas empowers agronomists, farm consultants,\npolicymakers, and other stakeholders to implement evidence-based strategies\nthat promote sustainable agriculture and build climate resilience.\n","authors":["Margaret Capetz","Swati Sharma","Rafael Padilha","Peder Olsen","Jessica Wolk","Emre Kiciman","Ranveer Chandra"],"pdf_url":"https://arxiv.org/pdf/2411.16872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18073v1","updated":"2024-11-27T05:54:33Z","published":"2024-11-27T05:54:33Z","title":"DuMapper: Towards Automatic Verification of Large-Scale POIs with Street\n  Views at Baidu Maps","summary":"  With the increased popularity of mobile devices, Web mapping services have\nbecome an indispensable tool in our daily lives. To provide user-satisfied\nservices, such as location searches, the point of interest (POI) database is\nthe fundamental infrastructure, as it archives multimodal information on\nbillions of geographic locations closely related to people's lives, such as a\nshop or a bank. Therefore, verifying the correctness of a large-scale POI\ndatabase is vital. To achieve this goal, many industrial companies adopt\nvolunteered geographic information (VGI) platforms that enable thousands of\ncrowdworkers and expert mappers to verify POIs seamlessly; but to do so, they\nhave to spend millions of dollars every year. To save the tremendous labor\ncosts, we devised DuMapper, an automatic system for large-scale POI\nverification with the multimodal street-view data at Baidu Maps. DuMapper takes\nthe signboard image and the coordinates of a real-world place as input to\ngenerate a low-dimensional vector, which can be leveraged by ANN algorithms to\nconduct a more accurate search through billions of archived POIs in the\ndatabase for verification within milliseconds. It can significantly increase\nthe throughput of POI verification by $50$ times. DuMapper has already been\ndeployed in production since \\DuMPOnline, which dramatically improves the\nproductivity and efficiency of POI verification at Baidu Maps. As of December\n31, 2021, it has enacted over $405$ million iterations of POI verification\nwithin a 3.5-year period, representing an approximate workload of $800$\nhigh-performance expert mappers.\n","authors":["Miao Fan","Jizhou Huang","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2411.18073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18069v1","updated":"2024-11-27T05:43:00Z","published":"2024-11-27T05:43:00Z","title":"Overview of TREC 2024 Biomedical Generative Retrieval (BioGen) Track","summary":"  With the advancement of large language models (LLMs), the biomedical domain\nhas seen significant progress and improvement in multiple tasks such as\nbiomedical question answering, lay language summarization of the biomedical\nliterature, clinical note summarization, etc. However, hallucinations or\nconfabulations remain one of the key challenges when using LLMs in the\nbiomedical and other domains. Inaccuracies may be particularly harmful in\nhigh-risk situations, such as making clinical decisions or appraising\nbiomedical research. Studies on the evaluation of the LLMs' abilities to ground\ngenerated statements in verifiable sources have shown that models perform\nsignificantly worse on lay-user generated questions, and often fail to\nreference relevant sources. This can be problematic when those seeking\ninformation want evidence from studies to back up the claims from LLMs[3].\nUnsupported statements are a major barrier to using LLMs in any applications\nthat may affect health. Methods for grounding generated statements in reliable\nsources along with practical evaluation approaches are needed to overcome this\nbarrier. Towards this, in our pilot task organized at TREC 2024, we introduced\nthe task of reference attribution as a means to mitigate the generation of\nfalse statements by LLMs answering biomedical questions.\n","authors":["Deepak Gupta","Dina Demner-Fushman","William Hersh","Steven Bedrick","Kirk Roberts"],"pdf_url":"https://arxiv.org/pdf/2411.18069v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.13397v5","updated":"2024-11-27T21:05:33Z","published":"2023-03-23T16:15:18Z","title":"DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery\n  from Videos","summary":"  Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications. While image-based HMR methods have achieved impressive\nresults, they often struggle to recover humans in dynamic scenarios, leading to\ntemporal inconsistencies and non-smooth 3D motion predictions due to the\nabsence of human motion. In contrast, video-based approaches leverage temporal\ninformation to mitigate this issue. In this paper, we present DiffMesh, an\ninnovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh\nestablishes a bridge between diffusion models and human motion, efficiently\ngenerating accurate and smooth output mesh sequences by incorporating human\nmotion within the forward process and reverse process in the diffusion model.\nExtensive experiments are conducted on the widely used datasets (Human3.6M\n\\cite{h36m_pami} and 3DPW \\cite{pw3d2018}), which demonstrate the effectiveness\nand efficiency of our DiffMesh. Visual comparisons in real-world scenarios\nfurther highlight DiffMesh's suitability for practical applications.\n","authors":["Ce Zheng","Xianpeng Liu","Qucheng Peng","Tianfu Wu","Pu Wang","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13397v5.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2306.08730v2","updated":"2024-11-27T17:25:10Z","published":"2023-06-14T20:20:39Z","title":"Over-the-Air Learning-based Geometry Point Cloud Transmission","summary":"  This paper presents novel solutions for the efficient and reliable\ntransmission of 3D point clouds over wireless channels. We first propose SEPT\nfor the transmission of small-scale point clouds, which encodes the point cloud\nvia an iterative downsampling and feature extraction process. At the receiver,\nSEPT decoder reconstructs the point cloud with latent reconstruction and\noffset-based upsampling. A novel channel-adaptive module is proposed to allow\nSEPT to operate effectively over a wide range of channel conditions. Next, we\npropose OTA-NeRF, a scheme inspired by neural radiance fields. OTA-NeRF\nperforms voxelization to the point cloud input and learns to encode the\nvoxelized point cloud into a neural network. Instead of transmitting the\nextracted feature vectors as in the SEPT scheme, it transmits the learned\nneural network weights over the air in an analog fashion along with few\nhyperparameters that are transmitted digitally. At the receiver, the OTA-NeRF\ndecoder reconstructs the original point cloud using the received noisy neural\nnetwork weights. To further increase the bandwidth efficiency of the OTA-NeRF\nscheme, a fine-tuning algorithm is developed, where only a fraction of the\nneural network weights are retrained and transmitted. Extensive numerical\nexperiments confirm that both the SEPT and the OTA-NeRF schemes achieve\nsuperior or comparable performance over the conventional approaches, where an\noctree-based or a learning-based point cloud compression scheme is concatenated\nwith a channel code. As an additional advantage, both schemes mitigate the\ncliff and leveling effects making them particularly attractive for highly\nmobile scenarios, where accurate channel estimation is challenging if not\nimpossible.\n","authors":["Chenghong Bian","Yulin Shao","Deniz Gunduz"],"pdf_url":"https://arxiv.org/pdf/2306.08730v2.pdf","comment":"14 pages, submitted to IEEE journal"},{"id":"http://arxiv.org/abs/2411.18372v1","updated":"2024-11-27T14:20:28Z","published":"2024-11-27T14:20:28Z","title":"Uncertainty-driven Sampling for Efficient Pairwise Comparison Subjective\n  Assessment","summary":"  Assessing image quality is crucial in image processing tasks such as\ncompression, super-resolution, and denoising. While subjective assessments\ninvolving human evaluators provide the most accurate quality scores, they are\nimpractical for large-scale or continuous evaluations due to their high cost\nand time requirements. Pairwise comparison subjective assessment tests, which\nrank image pairs instead of assigning scores, offer more reliability and\naccuracy but require numerous comparisons, leading to high costs. Although\nobjective quality metrics are more efficient, they lack the precision of\nsubjective tests, which are essential for benchmarking and training\nlearning-based quality metrics. This paper proposes an uncertainty-based\nsampling method to optimize the pairwise comparison subjective assessment\nprocess. By utilizing deep learning models to estimate human preferences and\nidentify pairs that need human labeling, the approach reduces the number of\nrequired comparisons while maintaining high accuracy. The key contributions\ninclude modeling uncertainty for accurate preference predictions and for\npairwise sampling. The experimental results demonstrate superior performance of\nthe proposed approach compared to traditional active sampling methods. Software\nis publicly available at: shimamohammadi/LBPS-EIC\n","authors":["Shima Mohammadi","João Ascenso"],"pdf_url":"https://arxiv.org/pdf/2411.18372v1.pdf","comment":"10 Pages, 7 Figures, Submitted to IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2410.22046v2","updated":"2024-11-27T09:33:18Z","published":"2024-10-29T13:53:09Z","title":"CHORDONOMICON: A Dataset of 666,000 Songs and their Chord Progressions","summary":"  Chord progressions encapsulate important information about music, pertaining\nto its structure and conveyed emotions. They serve as the backbone of musical\ncomposition, and in many cases, they are the sole information required for a\nmusician to play along and follow the music. Despite their importance, chord\nprogressions as a data domain remain underexplored. There is a lack of\nlarge-scale datasets suitable for deep learning applications, and limited\nresearch exploring chord progressions as an input modality. In this work, we\npresent Chordonomicon, a dataset of over 666,000 songs and their chord\nprogressions, annotated with structural parts, genre, and release date -\ncreated by scraping various sources of user-generated progressions and\nassociated metadata. We demonstrate the practical utility of the Chordonomicon\ndataset for classification and generation tasks, and discuss its potential to\nprovide valuable insights to the research community. Chord progressions are\nunique in their ability to be represented in multiple formats (e.g. text,\ngraph) and the wealth of information chords convey in given contexts, such as\ntheir harmonic function . These characteristics make the Chordonomicon an ideal\ntestbed for exploring advanced machine learning techniques, including\ntransformers, graph machine learning, and hybrid systems that combine knowledge\nrepresentation and machine learning.\n","authors":["Spyridon Kantarelis","Konstantinos Thomas","Vassilis Lyberatos","Edmund Dervakos","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2410.22046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15573v3","updated":"2024-11-27T05:43:19Z","published":"2024-10-21T01:36:42Z","title":"OpenMU: Your Swiss Army Knife for Music Understanding","summary":"  We present OpenMU-Bench, a large-scale benchmark suite for addressing the\ndata scarcity issue in training multimodal language models to understand music.\nTo construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new\nannotations. OpenMU-Bench also broadens the scope of music understanding by\nincluding lyrics understanding and music tool usage. Using OpenMU-Bench, we\ntrained our music understanding model, OpenMU, with extensive ablations,\ndemonstrating that OpenMU outperforms baseline models such as MU-Llama. Both\nOpenMU and OpenMU-Bench are open-sourced to facilitate future research in music\nunderstanding and to enhance creative music production efficiency.\n","authors":["Mengjie Zhao","Zhi Zhong","Zhuoyuan Mao","Shiqi Yang","Wei-Hsiang Liao","Shusuke Takahashi","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.15573v3.pdf","comment":"Resources: https://github.com/sony/openmu"}]},"2024-11-26T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2411.17863v1","updated":"2024-11-26T20:26:47Z","published":"2024-11-26T20:26:47Z","title":"LongKey: Keyphrase Extraction for Long Documents","summary":"  In an era of information overload, manually annotating the vast and growing\ncorpus of documents and scholarly papers is increasingly impractical. Automated\nkeyphrase extraction addresses this challenge by identifying representative\nterms within texts. However, most existing methods focus on short documents (up\nto 512 tokens), leaving a gap in processing long-context documents. In this\npaper, we introduce LongKey, a novel framework for extracting keyphrases from\nlengthy documents, which uses an encoder-based language model to capture\nextended text intricacies. LongKey uses a max-pooling embedder to enhance\nkeyphrase candidate representation. Validated on the comprehensive LDKP\ndatasets and six diverse, unseen datasets, LongKey consistently outperforms\nexisting unsupervised and language model-based keyphrase extraction methods.\nOur findings demonstrate LongKey's versatility and superior performance,\nmarking an advancement in keyphrase extraction for varied text lengths and\ndomains.\n","authors":["Jeovane Honorio Alves","Radu State","Cinthia Obladen de Almendra Freitas","Jean Paul Barddal"],"pdf_url":"https://arxiv.org/pdf/2411.17863v1.pdf","comment":"Accepted for presentation at the 2024 IEEE International Conference\n  on Big Data (IEEE BigData 2024). Code available at\n  https://github.com/jeohalves/longkey"},{"id":"http://arxiv.org/abs/2411.17614v1","updated":"2024-11-26T17:27:18Z","published":"2024-11-26T17:27:18Z","title":"Automating Chapter-Level Classification for Electronic Theses and\n  Dissertations","summary":"  Traditional archival practices for describing electronic theses and\ndissertations (ETDs) rely on broad, high-level metadata schemes that fail to\ncapture the depth, complexity, and interdisciplinary nature of these long\nscholarly works. The lack of detailed, chapter-level content descriptions\nimpedes researchers' ability to locate specific sections or themes, thereby\nreducing discoverability and overall accessibility. By providing chapter-level\nmetadata information, we improve the effectiveness of ETDs as research\nresources. This makes it easier for scholars to navigate them efficiently and\nextract valuable insights. The absence of such metadata further obstructs\ninterdisciplinary research by obscuring connections across fields, hindering\nnew academic discoveries and collaboration. In this paper, we propose a machine\nlearning and AI-driven solution to automatically categorize ETD chapters. This\nsolution is intended to improve discoverability and promote understanding of\nchapters. Our approach enriches traditional archival practices by providing\ncontext-rich descriptions that facilitate targeted navigation and improved\naccess. We aim to support interdisciplinary research and make ETDs more\naccessible. By providing chapter-level classification labels and using them to\nindex in our developed prototype system, we make content in ETD chapters more\ndiscoverable and usable for a diverse range of scholarly needs. Implementing\nthis AI-enhanced approach allows archives to serve researchers better, enabling\nefficient access to relevant information and supporting deeper engagement with\nETDs. This will increase the impact of ETDs as research tools, foster\ninterdisciplinary exploration, and reinforce the role of archives in scholarly\ncommunication within the data-intensive academic landscape.\n","authors":["Bipasha Banerjee","William A. Ingram","Edward A. Fox"],"pdf_url":"https://arxiv.org/pdf/2411.17614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17600v1","updated":"2024-11-26T17:06:58Z","published":"2024-11-26T17:06:58Z","title":"Making History Readable","summary":"  The Virginia Tech University Libraries (VTUL) Digital Library Platform (DLP)\nhosts digital collections that offer our users access to a wide variety of\ndocuments of historical and cultural importance. These collections are not only\nof academic importance but also provide our users with a glance at local\nhistorical events. Our DLP contains collections comprising digital objects\nfeaturing complex layouts, faded imagery, and hard-to-read handwritten text,\nwhich makes providing online access to these materials challenging. To address\nthese issues, we integrate AI into our DLP workflow and convert the text in the\ndigital objects into a machine-readable format. To enhance the user experience\nwith our historical collections, we use custom AI agents for handwriting\nrecognition, text extraction, and large language models (LLMs) for\nsummarization. This poster highlights three collections focusing on handwritten\nletters, newspapers, and digitized topographic maps. We discuss the challenges\nwith each collection and detail our approaches to address them. Our proposed\nmethods aim to enhance the user experience by making the contents in these\ncollections easier to search and navigate.\n","authors":["Bipasha Banerjee","Jennifer Goyne","William A. Ingram"],"pdf_url":"https://arxiv.org/pdf/2411.17600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17598v1","updated":"2024-11-26T17:06:30Z","published":"2024-11-26T17:06:30Z","title":"Agentic AI for Improving Precision in Identifying Contributions to\n  Sustainable Development Goals","summary":"  As research institutions increasingly commit to supporting the United\nNations' Sustainable Development Goals (SDGs), there is a pressing need to\naccurately assess their research output against these goals. Current\napproaches, primarily reliant on keyword-based Boolean search queries, conflate\nincidental keyword matches with genuine contributions, reducing retrieval\nprecision and complicating benchmarking efforts. This study investigates the\napplication of autoregressive Large Language Models (LLMs) as evaluation agents\nto identify relevant scholarly contributions to SDG targets in scholarly\npublications. Using a dataset of academic abstracts retrieved via SDG-specific\nkeyword queries, we demonstrate that small, locally-hosted LLMs can\ndifferentiate semantically relevant contributions to SDG targets from documents\nretrieved due to incidental keyword matches, addressing the limitations of\ntraditional methods. By leveraging the contextual understanding of LLMs, this\napproach provides a scalable framework for improving SDG-related research\nmetrics and informing institutional reporting.\n","authors":["William A. Ingram","Bipasha Banerjee","Edward A. Fox"],"pdf_url":"https://arxiv.org/pdf/2411.17598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17447v1","updated":"2024-11-26T14:04:36Z","published":"2024-11-26T14:04:36Z","title":"Exploring Structural Dynamics in Retracted and Non-Retracted Author's\n  Collaboration Networks: A Quantitative Analysis","summary":"  Retractions undermine the reliability of scientific literature and the\nfoundation of future research. Analyzing collaboration networks in retracted\npapers can identify risk factors, such as recurring co-authors or institutions.\nThis study compared the network structures of retracted and non-retracted\npapers, using data from Retraction Watch and Scopus for 30 authors with\nsignificant retractions. Collaboration networks were constructed, and network\nproperties analyzed. Retracted networks showed hierarchical and centralized\nstructures, while non-retracted networks exhibited distributed collaboration\nwith stronger clustering and connectivity. Statistical tests, including\n$t$-tests and Cohen's $d$, revealed significant differences in metrics like\nDegree Centrality and Weighted Degree, highlighting distinct structural\ndynamics. These insights into retraction-prone collaborations can guide\npolicies to improve research integrity.\n","authors":["Kiran Sharma","Aanchal Sharma","Jazlyn Jose","Vansh Saini","Raghavraj Sobti","Ziya Uddin"],"pdf_url":"https://arxiv.org/pdf/2411.17447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17374v1","updated":"2024-11-26T12:31:10Z","published":"2024-11-26T12:31:10Z","title":"Fairness And Performance In Harmony: Data Debiasing Is All You Need","summary":"  Fairness in both machine learning (ML) predictions and human decisions is\ncritical, with ML models prone to algorithmic and data bias, and human\ndecisions affected by subjectivity and cognitive bias. This study investigates\nfairness using a real-world university admission dataset with 870 profiles,\nleveraging three ML models, namely XGB, Bi-LSTM, and KNN. Textual features are\nencoded with BERT embeddings. For individual fairness, we assess decision\nconsistency among experts with varied backgrounds and ML models, using a\nconsistency score. Results show ML models outperform humans in fairness by\n14.08% to 18.79%. For group fairness, we propose a gender-debiasing pipeline\nand demonstrate its efficacy in removing gender-specific language without\ncompromising prediction performance. Post-debiasing, all models maintain or\nimprove their classification accuracy, validating the hypothesis that fairness\nand performance can coexist. Our findings highlight ML's potential to enhance\nfairness in admissions while maintaining high accuracy, advocating a hybrid\napproach combining human judgement and ML models.\n","authors":["Junhua Liu","Wendy Wan Yee Hui","Roy Ka-Wei Lee","Kwan Hui Lim"],"pdf_url":"https://arxiv.org/pdf/2411.17374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17361v1","updated":"2024-11-26T12:08:06Z","published":"2024-11-26T12:08:06Z","title":"Towards Robust Cross-Domain Recommendation with Joint Identifiability of\n  User Preference","summary":"  Recent cross-domain recommendation (CDR) studies assume that disentangled\ndomain-shared and domain-specific user representations can mitigate domain gaps\nand facilitate effective knowledge transfer. However, achieving perfect\ndisentanglement is challenging in practice, because user behaviors in CDR are\nhighly complex, and the true underlying user preferences cannot be fully\ncaptured through observed user-item interactions alone. Given this\nimpracticability, we instead propose to model {\\it joint identifiability} that\nestablishes unique correspondence of user representations across domains,\nensuring consistent preference modeling even when user behaviors exhibit shifts\nin different domains. To achieve this, we introduce a hierarchical user\npreference modeling framework that organizes user representations by the neural\nnetwork encoder's depth, allowing separate treatment of shallow and deeper\nsubspaces. In the shallow subspace, our framework models the interest centroids\nfor each user within each domain, probabilistically determining the users'\ninterest belongings and selectively aligning these centroids across domains to\nensure fine-grained consistency in domain-irrelevant features. For deeper\nsubspace representations, we enforce joint identifiability by decomposing it\ninto a shared cross-domain stable component and domain-variant components,\nlinked by a bijective transformation for unique correspondence. Empirical\nstudies on real-world CDR tasks with varying domain correlations demonstrate\nthat our method consistently surpasses state-of-the-art, even with weakly\ncorrelated tasks, highlighting the importance of joint identifiability in\nachieving robust CDR.\n","authors":["Jing Du","Zesheng Ye","Bin Guo","Zhiwen Yu","Jia Wu","Jian Yang","Michael Sheng","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2411.17361v1.pdf","comment":"12 pages, 6 figures, under review"},{"id":"http://arxiv.org/abs/2404.12008v4","updated":"2024-11-26T10:57:40Z","published":"2024-04-18T08:59:32Z","title":"How Do Recommendation Models Amplify Popularity Bias? An Analysis from\n  the Spectral Perspective","summary":"  Recommendation Systems (RS) are often plagued by popularity bias. When\ntraining a recommendation model on a typically long-tailed dataset, the model\ntends to not only inherit this bias but often exacerbate it, resulting in\nover-representation of popular items in the recommendation lists. This study\nconducts comprehensive empirical and theoretical analyses to expose the root\ncauses of this phenomenon, yielding two core insights: 1) Item popularity is\nmemorized in the principal spectrum of the score matrix predicted by the\nrecommendation model; 2) The dimension collapse phenomenon amplifies the\nrelative prominence of the principal spectrum, thereby intensifying the\npopularity bias. Building on these insights, we propose a novel debiasing\nstrategy that leverages a spectral norm regularizer to penalize the magnitude\nof the principal singular value. We have developed an efficient algorithm to\nexpedite the calculation of the spectral norm by exploiting the spectral\nproperty of the score matrix. Extensive experiments across seven real-world\ndatasets and three testing paradigms have been conducted to validate the\nsuperiority of the proposed method.\n","authors":["Siyi Lin","Chongming Gao","Jiawei Chen","Sheng Zhou","Binbin Hu","Yan Feng","Chun Chen","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12008v4.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2212.02935v3","updated":"2024-11-26T10:53:40Z","published":"2022-12-06T12:45:15Z","title":"A multi-language toolkit for the semi-automated checking of research\n  outputs","summary":"  This article presents a free and open source toolkit that supports the\nsemi-automated checking of research outputs (SACRO) for privacy disclosure\nwithin secure data environments. SACRO is a framework that applies\nbest-practice principles-based statistical disclosure control (SDC) techniques\non-the-fly as researchers conduct their analyses. SACRO is designed to assist\nhuman checkers rather than seeking to replace them as with current automated\nrules-based approaches. The toolkit is composed of a lightweight Python package\nthat sits over well-known analysis tools that produce outputs such as tables,\nplots, and statistical models. This package adds functionality to (i)\nautomatically identify potentially disclosive outputs against a range of\ncommonly used disclosure tests; (ii) apply optional disclosure mitigation\nstrategies as requested; (iii) report reasons for applying SDC; and (iv)\nproduce simple summary documents trusted research environment staff can use to\nstreamline their workflow and maintain auditable records. This creates an\nexplicit change in the dynamics so that SDC is something done with researchers\nrather than to them, and enables more efficient communication with checkers. A\ngraphical user interface supports human checkers by displaying the requested\noutput and results of the checks in an immediately accessible format,\nhighlighting identified issues, potential mitigation options, and tracking\ndecisions made. The major analytical programming languages used by researchers\n(Python, R, and Stata) are supported by providing front-end packages that\ninterface with the core Python back-end. Source code, packages, and\ndocumentation are available under MIT license at https://github.com/AI-SDC/ACRO\n","authors":["Richard J. Preen","Maha Albashir","Simon Davy","Jim Smith"],"pdf_url":"https://arxiv.org/pdf/2212.02935v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17299v1","updated":"2024-11-26T10:47:35Z","published":"2024-11-26T10:47:35Z","title":"2D Matryoshka Training for Information Retrieval","summary":"  2D Matryoshka Training is an advanced embedding representation training\napproach designed to train an encoder model simultaneously across various\nlayer-dimension setups. This method has demonstrated higher effectiveness in\nSemantic Text Similarity (STS) tasks over traditional training approaches when\nusing sub-layers for embeddings. Despite its success, discrepancies exist\nbetween two published implementations, leading to varied comparative results\nwith baseline models. In this reproducibility study, we implement and evaluate\nboth versions of 2D Matryoshka Training on STS tasks and extend our analysis to\nretrieval tasks. Our findings indicate that while both versions achieve higher\neffectiveness than traditional Matryoshka training on sub-dimensions, and\ntraditional full-sized model training approaches, they do not outperform models\ntrained separately on specific sub-layer and sub-dimension setups. Moreover,\nthese results generalize well to retrieval tasks, both in supervised (MSMARCO)\nand zero-shot (BEIR) settings. Further explorations of different loss\ncomputations reveals more suitable implementations for retrieval tasks, such as\nincorporating full-dimension loss and training on a broader range of target\ndimensions. Conversely, some intuitive approaches, such as fixing document\nencoders to full model outputs, do not yield improvements. Our reproduction\ncode is available at https://github.com/ielab/2DMSE-Reproduce.\n","authors":["Shuai Wang","Shengyao Zhuang","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2411.17299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00231v2","updated":"2024-11-26T08:37:54Z","published":"2024-05-31T23:29:42Z","title":"LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking","summary":"  Ranking passages by prompting a large language model (LLM) can achieve\npromising performance in modern information retrieval (IR) systems. A common\napproach to sort the ranking list is by prompting LLMs for a pairwise or\nsetwise comparison which often relies on sorting algorithms. However,\nsorting-based methods require consistent comparisons to correctly sort the\npassages, which we show that LLMs often violate. We identify two kinds of\nintrinsic inconsistency in LLM-based pairwise comparisons: order inconsistency\nwhich leads to conflicting results when switching the passage order, and\ntransitive inconsistency which leads to non-transitive triads among all\npreference pairs. Our study of these inconsistencies is relevant for\nunderstanding and improving the stability of any ranking scheme based on\nrelative preferences. In this paper, we propose LLM-RankFusion, an LLM-based\nranking framework that mitigates these inconsistencies and produces a robust\nranking list. LLM-RankFusion mitigates order inconsistency using in-context\nlearning (ICL) to demonstrate order-agnostic comparisons and calibration to\nestimate the underlying preference probability between two passages. We then\naddress transitive inconsistency by aggregating the ranking results from\nmultiple rankers. In our experiments, we empirically show that LLM-RankFusion\ncan significantly reduce inconsistent comparison results, improving the ranking\nquality by making the final ranking list more robust. Our code is available at\n\\href{https://github.com/XHMY/LLM-RankFusion}{https://github.com/XHMY/LLM-RankFusion}\n","authors":["Yifan Zeng","Ojas Tendolkar","Raymond Baartmans","Qingyun Wu","Lizhong Chen","Huazheng Wang"],"pdf_url":"https://arxiv.org/pdf/2406.00231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08208v2","updated":"2024-11-26T08:07:08Z","published":"2024-08-15T15:18:46Z","title":"LLM4DSR: Leveraing Large Language Model for Denoising Sequential\n  Recommendation","summary":"  Sequential Recommenders generate recommendations based on users' historical\ninteraction sequences. However, in practice, these collected sequences are\noften contaminated by noisy interactions, which significantly impairs\nrecommendation performance. Accurately identifying such noisy interactions\nwithout additional information is particularly challenging due to the absence\nof explicit supervisory signals indicating noise. Large Language Models (LLMs),\nequipped with extensive open knowledge and semantic reasoning abilities, offer\na promising avenue to bridge this information gap. However, employing LLMs for\ndenoising in sequential recommendation presents notable challenges: 1) Direct\napplication of pretrained LLMs may not be competent for the denoising task,\nfrequently generating nonsensical responses; 2) Even after fine-tuning, the\nreliability of LLM outputs remains questionable, especially given the\ncomplexity of the denoising task and the inherent hallucinatory issue of LLMs.\n  To tackle these challenges, we propose LLM4DSR, a tailored approach for\ndenoising sequential recommendation using LLMs. We constructed a\nself-supervised fine-tuning task to activate LLMs' capabilities to identify\nnoisy items and suggest replacements. Furthermore, we developed an uncertainty\nestimation module that ensures only high-confidence responses are utilized for\nsequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing corrected\nsequences to be flexibly applied across various recommendation models.\nExtensive experiments validate the superiority of LLM4DSR over existing\nmethods.\n","authors":["Bohao Wang","Feng Liu","Changwang Zhang","Jiawei Chen","Yudi Wu","Sheng Zhou","Xingyu Lou","Jun Wang","Yan Feng","Chun Chen","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2408.08208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09017v4","updated":"2024-11-26T07:08:55Z","published":"2024-07-12T06:10:01Z","title":"AI-Driven Guided Response for Security Operation Centers with Microsoft\n  Copilot for Security","summary":"  Security operation centers contend with a constant stream of security\nincidents, ranging from straightforward to highly complex. To address this, we\ndeveloped Microsoft Copilot for Security Guided Response (CGR), an\nindustry-scale ML architecture that guides security analysts across three key\ntasks -- (1) investigation, providing essential historical context by\nidentifying similar incidents; (2) triaging to ascertain the nature of the\nincident -- whether it is a true positive, false positive, or benign positive;\nand (3) remediation, recommending tailored containment actions. CGR is\nintegrated into the Microsoft Defender XDR product and deployed worldwide,\ngenerating millions of recommendations across thousands of customers. Our\nextensive evaluation, incorporating internal evaluation, collaboration with\nsecurity experts, and customer feedback, demonstrates that CGR delivers\nhigh-quality recommendations across all three tasks. We provide a comprehensive\noverview of the CGR architecture, setting a precedent as the first\ncybersecurity company to openly discuss these capabilities in such depth.\nAdditionally, we release GUIDE, the largest public collection of real-world\nsecurity incidents, spanning 13M evidences across 1M incidents annotated with\nground-truth triage labels by customer security analysts. This dataset\nrepresents the first large-scale cybersecurity resource of its kind, supporting\nthe development and evaluation of guided response systems and beyond.\n","authors":["Scott Freitas","Jovan Kalajdjieski","Amir Gharib","Robert McCann"],"pdf_url":"https://arxiv.org/pdf/2407.09017v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02793v3","updated":"2024-11-26T06:34:55Z","published":"2024-07-03T03:42:13Z","title":"Learning Positional Attention for Sequential Recommendation","summary":"  Self-attention-based networks have achieved remarkable performance in\nsequential recommendation tasks. A crucial component of these models is\npositional encoding. In this study, we delve into the learned positional\nembedding, demonstrating that it often captures the distance between tokens.\nBuilding on this insight, we introduce novel attention models that directly\nlearn positional relations. Extensive experiments reveal that our proposed\nmodels, \\textbf{PARec} and \\textbf{FPARec} outperform previous\nself-attention-based approaches. The code can be found here:\nhttps://github.com/NetEase-Media/FPARec.\n","authors":["Fan Luo","Haibo He","Juan Zhang","Shenghui Xu"],"pdf_url":"https://arxiv.org/pdf/2407.02793v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00909v2","updated":"2024-11-26T06:28:24Z","published":"2024-07-01T02:27:54Z","title":"Heterogeneous Graph-based Framework with Disentangled Representations\n  Learning for Multi-target Cross Domain Recommendation","summary":"  CDR (Cross-Domain Recommendation), i.e., leveraging information from multiple\ndomains, is a critical solution to data sparsity problem in recommendation\nsystem. The majority of previous research either focused on single-target CDR\n(STCDR) by utilizing data from the source domains to improve the model's\nperformance on the target domain, or applied dual-target CDR (DTCDR) by\nintegrating data from the source and target domains. In addition, multi-target\nCDR (MTCDR) is a generalization of DTCDR, which is able to capture the link\namong different domains. In this paper we present HGDR (Heterogeneous\nGraph-based Framework with Disentangled Representations Learning), an\nend-to-end heterogeneous network architecture where graph convolutional layers\nare applied to model relations among different domains, meanwhile utilizes the\nidea of disentangling representation for domain-shared and domain-specifc\ninformation. First, a shared heterogeneous graph is generated by gathering\nusers and items from several domains without any further side information.\nSecond, we use HGDR to compute disentangled representations for users and items\nin all domains. Experiments on real-world datasets and online A/B tests prove\nthat our proposed model can transmit information among domains effectively and\nreach the SOTA performance. The code can be found here:\nhttps://github.com/NetEase-Media/HGCDR.\n","authors":["Xiaopeng Liu","Juan Zhang","Chongqi Ren","Shenghui Xu","Zhaoming Pan","Zhimin Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.00909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17102v1","updated":"2024-11-26T04:39:46Z","published":"2024-11-26T04:39:46Z","title":"Scholar Name Disambiguation with Search-enhanced LLM Across Language","summary":"  The task of scholar name disambiguation is crucial in various real-world\nscenarios, including bibliometric-based candidate evaluation for awards,\napplication material anti-fraud measures, and more. Despite significant\nadvancements, current methods face limitations due to the complexity of\nheterogeneous data, often necessitating extensive human intervention. This\npaper proposes a novel approach by leveraging search-enhanced language models\nacross multiple languages to improve name disambiguation. By utilizing the\npowerful query rewriting, intent recognition, and data indexing capabilities of\nsearch engines, our method can gather richer information for distinguishing\nbetween entities and extracting profiles, resulting in a more comprehensive\ndata dimension. Given the strong cross-language capabilities of large language\nmodels(LLMs), optimizing enhanced retrieval methods with this technology offers\nsubstantial potential for high-efficiency information retrieval and\nutilization. Our experiments demonstrate that incorporating local languages\nsignificantly enhances disambiguation performance, particularly for scholars\nfrom diverse geographic regions. This multi-lingual, search-enhanced\nmethodology offers a promising direction for more efficient and accurate active\nscholar name disambiguation.\n","authors":["Renyu Zhao","Yunxin Chen"],"pdf_url":"https://arxiv.org/pdf/2411.17102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13789v2","updated":"2024-11-26T02:54:07Z","published":"2024-11-21T02:22:35Z","title":"LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display\n  Advertisement Recommender System","summary":"  Display advertising provides significant value to advertisers, publishers,\nand users. Traditional display advertising systems utilize a multi-stage\narchitecture consisting of retrieval, coarse ranking, and final ranking.\nHowever, conventional retrieval methods rely on ID-based learning to rank\nmechanisms and fail to adequately utilize the content information of ads, which\nhampers their ability to provide diverse recommendation lists.\n  To address this limitation, we propose leveraging the extensive world\nknowledge of LLMs. However, three key challenges arise when attempting to\nmaximize the effectiveness of LLMs: \"How to capture user interests\", \"How to\nbridge the knowledge gap between LLMs and advertising system\", and \"How to\nefficiently deploy LLMs\". To overcome these challenges, we introduce a novel\nLLM-based framework called LLM Empowered Display ADvertisement REcommender\nsystem (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware\nPrompt Engineering introduces multi-faceted knowledge and designs intent-aware\n<Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users'\npersonal interests. (2) The Advertising-Specific Knowledge Alignment\nincorporates auxiliary fine-tuning tasks and Direct Preference Optimization\n(DPO) to align LLMs with ad semantic and business value. (3) The Efficient\nSystem Deployment deploys LEADRE in an online environment by integrating both\nlatency-tolerant and latency-sensitive service. Extensive offline experiments\ndemonstrate the effectiveness of LEADRE and validate the contributions of\nindividual modules. Online A/B test shows that LEADRE leads to a 1.57% and\n1.17% GMV lift for serviced users on WeChat Channels and Moments separately.\nLEADRE has been deployed on both platforms, serving tens of billions of\nrequests each day.\n","authors":["Fengxin Li","Yi Li","Yue Liu","Chao Zhou","Yuan Wang","Xiaoxiang Deng","Wei Xue","Dapeng Liu","Lei Xiao","Haijie Gu","Jie Jiang","Hongyan Liu","Biao Qin","Jun He"],"pdf_url":"https://arxiv.org/pdf/2411.13789v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.17690v1","updated":"2024-11-26T18:57:29Z","published":"2024-11-26T18:57:29Z","title":"Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis","summary":"  In this paper, we propose a new task -- generating speech from videos of\npeople and their transcripts (VTTS) -- to motivate new techniques for\nmultimodal speech generation. This task generalizes the task of generating\nspeech from cropped lip videos, and is also more complicated than the task of\ngenerating generic audio clips (e.g., dog barking) from videos and text.\nMultilingual versions of the task could lead to new techniques for\ncross-lingual dubbing. We also present a decoder-only multimodal model for this\ntask, which we call Visatronic. This model embeds vision, text and speech\ndirectly into the common subspace of a transformer model and uses an\nautoregressive loss to learn a generative model of discretized mel-spectrograms\nconditioned on speaker videos and transcripts of their speech. By embedding all\nmodalities into a common subspace, Visatronic can achieve improved results over\nmodels that use only text or video as input. Further, it presents a much\nsimpler approach for multimodal speech generation compared to prevailing\napproaches which rely on lip-detectors and complicated architectures to fuse\nmodalities while producing better results. Since the model is flexible enough\nto accommodate different ways of ordering inputs as a sequence, we carefully\nexplore different strategies to better understand the best way to propagate\ninformation to the generative steps. To facilitate further research on VTTS, we\nwill release (i) our code, (ii) clean transcriptions for the large-scale\nVoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTS\nincorporating both objective and subjective metrics.\n","authors":["Akshita Gupta","Tatiana Likhomanenko","Karren Dai Yang","Richard He Bai","Zakaria Aldeneh","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2411.17690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15128v2","updated":"2024-11-26T18:01:08Z","published":"2024-11-22T18:51:51Z","title":"Health AI Developer Foundations","summary":"  Robust medical Machine Learning (ML) models have the potential to\nrevolutionize healthcare by accelerating clinical research, improving workflows\nand outcomes, and producing novel insights or capabilities. Developing such ML\nmodels from scratch is cost prohibitive and requires substantial compute, data,\nand time (e.g., expert labeling). To address these challenges, we introduce\nHealth AI Developer Foundations (HAI-DEF), a suite of pre-trained,\ndomain-specific foundation models, tools, and recipes to accelerate building ML\nfor health applications. The models cover various modalities and domains,\nincluding radiology (X-rays and computed tomography), histopathology,\ndermatological imaging, and audio. These models provide domain specific\nembeddings that facilitate AI development with less labeled data, shorter\ntraining times, and reduced computational costs compared to traditional\napproaches. In addition, we utilize a common interface and style across these\nmodels, and prioritize usability to enable developers to integrate HAI-DEF\nefficiently. We present model evaluations across various tasks and conclude\nwith a discussion of their application and evaluation, covering the importance\nof ensuring efficacy, fairness, and equity. Finally, while HAI-DEF and\nspecifically the foundation models lower the barrier to entry for ML in\nhealthcare, we emphasize the importance of validation with problem- and\npopulation-specific data for each desired usage setting. This technical report\nwill be updated over time as more modalities and features are added.\n","authors":["Atilla P. Kiraly","Sebastien Baur","Kenneth Philbrick","Fereshteh Mahvar","Liron Yatziv","Tiffany Chen","Bram Sterling","Nick George","Fayaz Jamil","Jing Tang","Kai Bailey","Faruk Ahmed","Akshay Goel","Abbi Ward","Lin Yang","Andrew Sellergren","Yossi Matias","Avinatan Hassidim","Shravya Shetty","Daniel Golden","Shekoofeh Azizi","David F. Steiner","Yun Liu","Tim Thelin","Rory Pilgrim","Can Kirmizibayrak"],"pdf_url":"https://arxiv.org/pdf/2411.15128v2.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.07772v2","updated":"2024-11-26T14:55:05Z","published":"2024-11-12T13:13:20Z","title":"Automatic Album Sequencing","summary":"  Album sequencing is a critical part of the album production process.\nRecently, a data-driven approach was proposed that sequences general\ncollections of independent media by extracting the narrative essence of the\nitems in the collections. While this approach implies an album sequencing\ntechnique, it is not widely accessible to a less technical audience, requiring\nadvanced knowledge of machine learning techniques to use. To address this, we\nintroduce a new user-friendly web-based tool that allows a less technical\naudience to upload music tracks, execute this technique in one click, and\nsubsequently presents the result in a clean visualization to the user. To both\nincrease the number of templates available to the user and address shortcomings\nof previous work, we also introduce a new direct transformer-based album\nsequencing method. We find that our more direct method outperforms a random\nbaseline but does not reach the same performance as the narrative essence\napproach. Both methods are included in our web-based user interface, and this\n-- alongside a full copy of our implementation -- is publicly available at\nhttps://github.com/dylanashley/automatic-album-sequencing\n","authors":["Vincent Herrmann","Dylan R. Ashley","Jürgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2411.07772v2.pdf","comment":"presented as a late breaking demo in the 25th International Society\n  for Music Information Retrieval Conference; 3 pages in main text + 1 page of\n  references, 3 figures in main text; source code available at\n  https://github.com/dylanashley/automatic-album-sequencing"},{"id":"http://arxiv.org/abs/2411.17440v1","updated":"2024-11-26T13:58:24Z","published":"2024-11-26T13:58:24Z","title":"Identity-Preserving Text-to-Video Generation by Frequency Decomposition","summary":"  Identity-preserving text-to-video (IPT2V) generation aims to create\nhigh-fidelity videos with consistent human identity. It is an important task in\nvideo generation but remains an open problem for generative models. This paper\npushes the technical frontier of IPT2V in two directions that have not been\nresolved in literature: (1) A tuning-free pipeline without tedious case-by-case\nfinetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based\ncontrol scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V\nmodel to keep human identity consistent in the generated video. Inspired by\nprior findings in frequency analysis of diffusion transformers, it employs\nidentity-control signals in the frequency domain, where facial features can be\ndecomposed into low-frequency global features and high-frequency intrinsic\nfeatures. First, from a low-frequency perspective, we introduce a global facial\nextractor, which encodes reference images and facial key points into a latent\nspace, generating features enriched with low-frequency information. These\nfeatures are then integrated into shallow layers of the network to alleviate\ntraining challenges associated with DiT. Second, from a high-frequency\nperspective, we design a local facial extractor to capture high-frequency\ndetails and inject them into transformer blocks, enhancing the model's ability\nto preserve fine-grained features. We propose a hierarchical training strategy\nto leverage frequency information for identity preservation, transforming a\nvanilla pre-trained video generation model into an IPT2V model. Extensive\nexperiments demonstrate that our frequency-aware heuristic scheme provides an\noptimal control solution for DiT-based models. Thanks to this scheme, our\nConsisID generates high-quality, identity-preserving videos, making strides\ntowards more effective IPT2V.\n","authors":["Shenghai Yuan","Jinfa Huang","Xianyi He","Yunyuan Ge","Yujun Shi","Liuhan Chen","Jiebo Luo","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2411.17440v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.17776v1","updated":"2024-11-26T09:50:15Z","published":"2024-11-26T09:50:15Z","title":"Beyond Walking: A Large-Scale Image-Text Benchmark for Text-based Person\n  Anomaly Search","summary":"  Text-based person search aims to retrieve specific individuals across camera\nnetworks using natural language descriptions. However, current benchmarks often\nexhibit biases towards common actions like walking or standing, neglecting the\ncritical need for identifying abnormal behaviors in real-world scenarios. To\nmeet such demands, we propose a new task, text-based person anomaly search,\nlocating pedestrians engaged in both routine or anomalous activities via text.\nTo enable the training and evaluation of this new task, we construct a\nlarge-scale image-text Pedestrian Anomaly Behavior (PAB) benchmark, featuring a\nbroad spectrum of actions, e.g., running, performing, playing soccer, and the\ncorresponding anomalies, e.g., lying, being hit, and falling of the same\nidentity. The training set of PAB comprises 1,013,605 synthesized image-text\npairs of both normalities and anomalies, while the test set includes 1,978\nreal-world image-text pairs. To validate the potential of PAB, we introduce a\ncross-modal pose-aware framework, which integrates human pose patterns with\nidentity-based hard negative pair sampling. Extensive experiments on the\nproposed benchmark show that synthetic training data facilitates the\nfine-grained behavior retrieval in the real-world test set, while the proposed\npose-aware method further improves the recall@1 by 2.88%. We will release the\ndataset, code, and checkpoints to facilitate further research and ensure the\nreproducibility of our results.\n","authors":["Shuyu Yang","Yaxiong Wang","Li Zhu","Zhedong Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.17776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13577v2","updated":"2024-11-26T09:20:48Z","published":"2024-11-15T04:16:45Z","title":"WavChat: A Survey of Spoken Dialogue Models","summary":"  Recent advancements in spoken dialogue models, exemplified by systems like\nGPT-4o, have captured significant attention in the speech domain. Compared to\ntraditional three-tier cascaded spoken dialogue models that comprise speech\nrecognition (ASR), large language models (LLMs), and text-to-speech (TTS),\nmodern spoken dialogue models exhibit greater intelligence. These advanced\nspoken dialogue models not only comprehend audio, music, and other\nspeech-related features, but also capture stylistic and timbral characteristics\nin speech. Moreover, they generate high-quality, multi-turn speech responses\nwith low latency, enabling real-time interaction through simultaneous listening\nand speaking capability. Despite the progress in spoken dialogue systems, there\nis a lack of comprehensive surveys that systematically organize and analyze\nthese systems and the underlying technologies. To address this, we have first\ncompiled existing spoken dialogue systems in the chronological order and\ncategorized them into the cascaded and end-to-end paradigms. We then provide an\nin-depth overview of the core technologies in spoken dialogue models, covering\naspects such as speech representation, training paradigm, streaming, duplex,\nand interaction capabilities. Each section discusses the limitations of these\ntechnologies and outlines considerations for future research. Additionally, we\npresent a thorough review of relevant datasets, evaluation metrics, and\nbenchmarks from the perspectives of training and evaluating spoken dialogue\nsystems. We hope this survey will contribute to advancing both academic\nresearch and industrial applications in the field of spoken dialogue systems.\nThe related material is available at https://github.com/jishengpeng/WavChat.\n","authors":["Shengpeng Ji","Yifu Chen","Minghui Fang","Jialong Zuo","Jingyu Lu","Hanting Wang","Ziyue Jiang","Long Zhou","Shujie Liu","Xize Cheng","Xiaoda Yang","Zehan Wang","Qian Yang","Jian Li","Yidi Jiang","Jingzhen He","Yunfei Chu","Jin Xu","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.13577v2.pdf","comment":"60 papes, working in progress"},{"id":"http://arxiv.org/abs/2411.15447v2","updated":"2024-11-26T03:49:11Z","published":"2024-11-23T04:27:19Z","title":"Gotta Hear Them All: Sound Source Aware Vision to Audio Generation","summary":"  Vision-to-audio (V2A) synthesis has broad applications in multimedia. Recent\nadvancements of V2A methods have made it possible to generate relevant audios\nfrom inputs of videos or still images. However, the immersiveness and\nexpressiveness of the generation are limited. One possible problem is that\nexisting methods solely rely on the global scene and overlook details of local\nsounding objects (i.e., sound sources). To address this issue, we propose a\nSound Source-Aware V2A (SSV2A) generator. SSV2A is able to locally perceive\nmultimodal sound sources from a scene with visual detection and cross-modality\ntranslation. It then contrastively learns a Cross-Modal Sound Source (CMSS)\nManifold to semantically disambiguate each source. Finally, we attentively mix\ntheir CMSS semantics into a rich audio representation, from which a pretrained\naudio generator outputs the sound. To model the CMSS manifold, we curate a\nnovel single-sound-source visual-audio dataset VGGS3 from VGGSound. We also\ndesign a Sound Source Matching Score to measure localized audio relevance. This\nis to our knowledge the first work to address V2A generation at the\nsound-source level. Extensive experiments show that SSV2A surpasses\nstate-of-the-art methods in both generation fidelity and relevance. We further\ndemonstrate SSV2A's ability to achieve intuitive V2A control by compositing\nvision, text, and audio conditions. Our SSV2A generation can be tried and heard\nat https://ssv2a.github.io/SSV2A-demo .\n","authors":["Wei Guo","Heng Wang","Jianbo Ma","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2411.15447v2.pdf","comment":"16 pages, 9 figures, source code released at\n  https://github.com/wguo86/SSV2A"}]},"2024-11-25T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2312.02901v2","updated":"2024-11-25T23:38:48Z","published":"2023-12-05T17:15:16Z","title":"Concept Drift Adaptation in Text Stream Mining Settings: A Systematic\n  Review","summary":"  The society produces textual data online in several ways, e.g., via reviews\nand social media posts. Therefore, numerous researchers have been working on\ndiscovering patterns in textual data that can indicate peoples' opinions,\ninterests, etc. Most tasks regarding natural language processing are addressed\nusing traditional machine learning methods and static datasets. This setting\ncan lead to several problems, e.g., outdated datasets and models, which degrade\nin performance over time. This is particularly true regarding concept drift, in\nwhich the data distribution changes over time. Furthermore, text streaming\nscenarios also exhibit further challenges, such as the high speed at which data\narrives over time. Models for stream scenarios must adhere to the\naforementioned constraints while learning from the stream, thus storing texts\nfor limited periods and consuming low memory. This study presents a systematic\nliterature review regarding concept drift adaptation in text stream scenarios.\nConsidering well-defined criteria, we selected 48 papers published between 2018\nand August 2024 to unravel aspects such as text drift categories, detection\ntypes, model update mechanisms, stream mining tasks addressed, and text\nrepresentation methods and their update mechanisms. Furthermore, we discussed\ndrift visualization and simulation and listed real-world datasets used in the\nselected papers. Finally, we brought forward a discussion on existing works in\nthe area, also highlighting open challenges and future research directions for\nthe community.\n","authors":["Cristiano Mesquita Garcia","Ramon Simoes Abilio","Alessandro Lameiras Koerich","Alceu de Souza Britto Jr.","Jean Paul Barddal"],"pdf_url":"https://arxiv.org/pdf/2312.02901v2.pdf","comment":"69 pages"},{"id":"http://arxiv.org/abs/2411.16645v1","updated":"2024-11-25T18:27:50Z","published":"2024-11-25T18:27:50Z","title":"Recommender Systems for Good (RS4Good): Survey of Use Cases and a Call\n  to Action for Research that Matters","summary":"  In the area of recommender systems, the vast majority of research efforts is\nspent on developing increasingly sophisticated recommendation models, also\nusing increasingly more computational resources. Unfortunately, most of these\nresearch efforts target a very small set of application domains, mostly\ne-commerce and media recommendation. Furthermore, many of these models are\nnever evaluated with users, let alone put into practice. The scientific,\neconomic and societal value of much of these efforts by scholars therefore\nremains largely unclear. To achieve a stronger positive impact resulting from\nthese efforts, we posit that we as a research community should more often\naddress use cases where recommender systems contribute to societal good\n(RS4Good). In this opinion piece, we first discuss a number of examples where\nthe use of recommender systems for problems of societal concern has been\nsuccessfully explored in the literature. We then proceed by outlining a\nparadigmatic shift that is needed to conduct successful RS4Good research, where\nthe key ingredients are interdisciplinary collaborations and longitudinal\nevaluation approaches with humans in the loop.\n","authors":["Dietmar Jannach","Alan Said","Marko Tkalčič","Markus Zanker"],"pdf_url":"https://arxiv.org/pdf/2411.16645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16515v3","updated":"2024-11-25T18:11:18Z","published":"2023-11-25T14:24:49Z","title":"Word4Per: Zero-shot Composed Person Retrieval","summary":"  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10513v2","updated":"2024-11-25T17:04:20Z","published":"2024-11-15T17:44:27Z","title":"Any2Any: Incomplete Multimodal Retrieval with Conformal Prediction","summary":"  Autonomous agents perceive and interpret their surroundings by integrating\nmultimodal inputs, such as vision, audio, and LiDAR. These perceptual\nmodalities support retrieval tasks, such as place recognition in robotics.\nHowever, current multimodal retrieval systems encounter difficulties when parts\nof the data are missing due to sensor failures or inaccessibility, such as\nsilent videos or LiDAR scans lacking RGB information. We propose Any2Any-a\nnovel retrieval framework that addresses scenarios where both query and\nreference instances have incomplete modalities. Unlike previous methods limited\nto the imputation of two modalities, Any2Any handles any number of modalities\nwithout training generative models. It calculates pairwise similarities with\ncross-modal encoders and employs a two-stage calibration process with conformal\nprediction to align the similarities. Any2Any enables effective retrieval\nacross multimodal datasets, e.g., text-LiDAR and text-time series. It achieves\na Recall@5 of 35% on the KITTI dataset, which is on par with baseline models\nwith complete modalities.\n","authors":["Po-han Li","Yunhao Yang","Mohammad Omama","Sandeep Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2411.10513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07610v2","updated":"2024-11-25T17:01:53Z","published":"2024-10-10T04:54:37Z","title":"CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features","summary":"  Multimodal encoders like CLIP excel in tasks such as zero-shot image\nclassification and cross-modal retrieval. However, they require excessive\ntraining data. We propose canonical similarity analysis (CSA), which uses two\nunimodal encoders to replicate multimodal encoders using limited data. CSA maps\nunimodal features into a multimodal space, using a new similarity score to\nretain only the multimodal information. CSA only involves the inference of\nunimodal encoders and a cubic-complexity matrix decomposition, eliminating the\nneed for extensive GPU-based model training. Experiments show that CSA\noutperforms CLIP while requiring $300,000\\times$ fewer multimodal data pairs\nand $6\\times$ fewer unimodal data for ImageNet classification and\nmisinformative news captions detection. CSA surpasses the state-of-the-art\nmethod to map unimodal features to multimodal features. We also demonstrate the\nability of CSA with modalities beyond image and text, paving the way for future\nmodality pairs with limited paired multimodal data but abundant unpaired\nunimodal data, such as lidar and text.\n","authors":["Po-han Li","Sandeep P. Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2410.07610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16408v1","updated":"2024-11-25T14:14:25Z","published":"2024-11-25T14:14:25Z","title":"Low-Data Classification of Historical Music Manuscripts: A Few-Shot\n  Learning Approach","summary":"  In this paper, we explore the intersection of technology and cultural\npreservation by developing a self-supervised learning framework for the\nclassification of musical symbols in historical manuscripts. Optical Music\nRecognition (OMR) plays a vital role in digitising and preserving musical\nheritage, but historical documents often lack the labelled data required by\ntraditional methods. We overcome this challenge by training a neural-based\nfeature extractor on unlabelled data, enabling effective classification with\nminimal samples. Key contributions include optimising crop preprocessing for a\nself-supervised Convolutional Neural Network and evaluating classification\nmethods, including SVM, multilayer perceptrons, and prototypical networks. Our\nexperiments yield an accuracy of 87.66\\%, showcasing the potential of AI-driven\nmethods to ensure the survival of historical music for future generations\nthrough advanced digital archiving techniques.\n","authors":["Elona Shatri","Daniel Raymond","George Fazekas"],"pdf_url":"https://arxiv.org/pdf/2411.16408v1.pdf","comment":"6 pages, The Sixth IEEE international conference on Image Processing\n  Applications and Systems"},{"id":"http://arxiv.org/abs/2409.15690v2","updated":"2024-11-25T13:05:39Z","published":"2024-09-24T03:06:25Z","title":"A Survey of Stance Detection on Social Media: New Directions and\n  Perspectives","summary":"  In modern digital environments, users frequently express opinions on\ncontentious topics, providing a wealth of information on prevailing attitudes.\nThe systematic analysis of these opinions offers valuable insights for\ndecision-making in various sectors, including marketing and politics. As a\nresult, stance detection has emerged as a crucial subfield within affective\ncomputing, enabling the automatic detection of user stances in social media\nconversations and providing a nuanced understanding of public sentiment on\ncomplex issues. Recent years have seen a surge of research interest in\ndeveloping effective stance detection methods, with contributions from multiple\ncommunities, including natural language processing, web science, and social\ncomputing. This paper provides a comprehensive survey of stance detection\ntechniques on social media, covering task definitions, datasets, approaches,\nand future works. We review traditional stance detection models, as well as\nstate-of-the-art methods based on large language models, and discuss their\nstrengths and limitations. Our survey highlights the importance of stance\ndetection in understanding public opinion and sentiment, and identifies gaps in\ncurrent research. We conclude by outlining potential future directions for\nstance detection on social media, including the need for more robust and\ngeneralizable models, and the importance of addressing emerging challenges such\nas multi-modal stance detection and stance detection in low-resource languages.\n","authors":["Bowen Zhang","Genan Dai","Fuqiang Niu","Nan Yin","Xiaomao Fan","Senzhang Wang","Xiaochun Cao","Hu Huang"],"pdf_url":"https://arxiv.org/pdf/2409.15690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16791v1","updated":"2024-11-25T09:07:56Z","published":"2024-11-25T09:07:56Z","title":"What can LLM tell us about cities?","summary":"  This study explores the capabilities of large language models (LLMs) in\nproviding knowledge about cities and regions on a global scale. We employ two\nmethods: directly querying the LLM for target variable values and extracting\nexplicit and implicit features from the LLM correlated with the target\nvariable. Our experiments reveal that LLMs embed a broad but varying degree of\nknowledge across global cities, with ML models trained on LLM-derived features\nconsistently leading to improved predictive accuracy. Additionally, we observe\nthat LLMs demonstrate a certain level of knowledge across global cities on all\ncontinents, but it is evident when they lack knowledge, as they tend to\ngenerate generic or random outputs for unfamiliar tasks. These findings suggest\nthat LLMs can offer new opportunities for data-driven decision-making in the\nstudy of cities.\n","authors":["Zhuoheng Li","Yaochen Wang","Zhixue Song","Yuqi Huang","Rui Bao","Guanjie Zheng","Zhenhui Jessie Li"],"pdf_url":"https://arxiv.org/pdf/2411.16791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16787v1","updated":"2024-11-25T08:35:55Z","published":"2024-11-25T08:35:55Z","title":"Contrastive Multi-graph Learning with Neighbor Hierarchical Sifting for\n  Semi-supervised Text Classification","summary":"  Graph contrastive learning has been successfully applied in text\nclassification due to its remarkable ability for self-supervised node\nrepresentation learning. However, explicit graph augmentations may lead to a\nloss of semantics in the contrastive views. Secondly, existing methods tend to\noverlook edge features and the varying significance of node features during\nmulti-graph learning. Moreover, the contrastive loss suffer from false\nnegatives. To address these limitations, we propose a novel method of\ncontrastive multi-graph learning with neighbor hierarchical sifting for\nsemi-supervised text classification, namely ConNHS. Specifically, we exploit\ncore features to form a multi-relational text graph, enhancing semantic\nconnections among texts. By separating text graphs, we provide diverse views\nfor contrastive learning. Our approach ensures optimal preservation of the\ngraph information, minimizing data loss and distortion. Then, we separately\nexecute relation-aware propagation and cross-graph attention propagation, which\neffectively leverages the varying correlations between nodes and edge features\nwhile harmonising the information fusion across graphs. Subsequently, we\npresent the neighbor hierarchical sifting loss (NHS) to refine the negative\nselection. For one thing, following the homophily assumption, NHS masks\nfirst-order neighbors of the anchor and positives from being negatives. For\nanother, NHS excludes the high-order neighbors analogous to the anchor based on\ntheir similarities. Consequently, it effectively reduces the occurrence of\nfalse negatives, preventing the expansion of the distance between similar\nsamples in the embedding space. Our experiments on ThuCNews, SogouNews, 20\nNewsgroups, and Ohsumed datasets achieved 95.86\\%, 97.52\\%, 87.43\\%, and\n70.65\\%, which demonstrates competitive results in semi-supervised text\nclassification.\n","authors":["Wei Ai","Jianbin Li","Ze Wang","Yingying Wei","Tao Meng","Yuntao Shou","Keqin Lib"],"pdf_url":"https://arxiv.org/pdf/2411.16787v1.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.16160v1","updated":"2024-11-25T07:36:20Z","published":"2024-11-25T07:36:20Z","title":"Stop Playing the Guessing Game! Target-free User Simulation for\n  Evaluating Conversational Recommender Systems","summary":"  Recent approaches in Conversational Recommender Systems (CRSs) have tried to\nsimulate real-world users engaging in conversations with CRSs to create more\nrealistic testing environments that reflect the complexity of human-agent\ndialogue. Despite the significant advancements, reliably evaluating the\ncapability of CRSs to elicit user preferences still faces a significant\nchallenge. Existing evaluation metrics often rely on target-biased user\nsimulators that assume users have predefined preferences, leading to\ninteractions that devolve into simplistic guessing game. These simulators\ntypically guide the CRS toward specific target items based on fixed attributes,\nlimiting the dynamic exploration of user preferences and struggling to capture\nthe evolving nature of real-user interactions. Additionally, current evaluation\nmetrics are predominantly focused on single-turn recall of target items,\nneglecting the intermediate processes of preference elicitation. To address\nthis, we introduce PEPPER, a novel CRS evaluation protocol with target-free\nuser simulators constructed from real-user interaction histories and reviews.\nPEPPER enables realistic user-CRS dialogues without falling into simplistic\nguessing games, allowing users to gradually discover their preferences through\nenriched interactions, thereby providing a more accurate and reliable\nassessment of the CRS's ability to elicit personal preferences. Furthermore,\nPEPPER presents detailed measures for comprehensively evaluating the preference\nelicitation capabilities of CRSs, encompassing both quantitative and\nqualitative measures that capture four distinct aspects of the preference\nelicitation process. Through extensive experiments, we demonstrate the validity\nof PEPPER as a simulation environment and conduct a thorough analysis of how\neffectively existing CRSs perform in preference elicitation and recommendation.\n","authors":["Sunghwan Kim","Tongyoung Kim","Kwangwook Seo","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2411.16160v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.16133v1","updated":"2024-11-25T06:48:38Z","published":"2024-11-25T06:48:38Z","title":"Context Awareness Gate For Retrieval Augmented Generation","summary":"  Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\nto mitigate the limitations of large language models (LLMs) in answering\ndomain-specific questions. Previous research has predominantly focused on\nimproving the accuracy and quality of retrieved data chunks to enhance the\noverall performance of the generation pipeline. However, despite ongoing\nadvancements, the critical issue of retrieving irrelevant information -- which\ncan impair the ability of the model to utilize its internal knowledge\neffectively -- has received minimal attention. In this work, we investigate the\nimpact of retrieving irrelevant information in open-domain question answering,\nhighlighting its significant detrimental effect on the quality of LLM outputs.\nTo address this challenge, we propose the Context Awareness Gate (CAG)\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\nbased on whether the user query necessitates external context retrieval.\nAdditionally, we introduce the Vector Candidates method, a core mathematical\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\nfurther examine the distributions of relationships between contexts and\nquestions, presenting a statistical analysis of these distributions. This\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\nAugmented Generation (RAG) systems.\n","authors":["Mohammad Hassan Heydari","Arshia Hemmat","Erfan Naman","Afsaneh Fatemi"],"pdf_url":"https://arxiv.org/pdf/2411.16133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16122v1","updated":"2024-11-25T06:14:20Z","published":"2024-11-25T06:14:20Z","title":"Ensemble Learning via Knowledge Transfer for CTR Prediction","summary":"  Click-through rate (CTR) prediction plays a critical role in recommender\nsystems and web searches. While many existing methods utilize ensemble learning\nto improve model performance, they typically limit the ensemble to two or three\nsub-networks, with little exploration of larger ensembles. In this paper, we\ninvestigate larger ensemble networks and find three inherent limitations in\ncommonly used ensemble learning method: (1) performance degradation with more\nnetworks; (2) sharp decline and high variance in sub-network performance; (3)\nlarge discrepancies between sub-network and ensemble predictions.\n  To simultaneously address the above limitations, this paper investigates\npotential solutions from the perspectives of Knowledge Distillation (KD) and\nDeep Mutual Learning (DML). Based on the empirical performance of these\nmethods, we combine them to propose a novel model-agnostic Ensemble Knowledge\nTransfer Framework (EKTF). Specifically, we employ the collective\ndecision-making of the students as an abstract teacher to guide each student\n(sub-network) towards more effective learning. Additionally, we encourage\nmutual learning among students to enable knowledge acquisition from different\nviews. To address the issue of balancing the loss hyperparameters, we design a\nnovel examination mechanism to ensure tailored teaching from teacher-to-student\nand selective learning in peer-to-peer. Experimental results on five real-world\ndatasets demonstrate the effectiveness and compatibility of EKTF. The code,\nrunning logs, and detailed hyperparameter configurations are available at:\nhttps://github.com/salmon1802/EKTF.\n","authors":["Honghao Li","Yiwen Zhang","Yi Zhang","Lei Sang"],"pdf_url":"https://arxiv.org/pdf/2411.16122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11225v2","updated":"2024-11-25T06:07:41Z","published":"2024-11-18T01:30:34Z","title":"Online Item Cold-Start Recommendation with Popularity-Aware\n  Meta-Learning","summary":"  With the rise of e-commerce and short videos, online recommender systems that\ncan capture users' interests and update new items in real-time play an\nincreasingly important role. In both online and offline recommendation, the\ncold-start problem due to interaction sparsity has been affecting the\nrecommendation effect of cold-start items, which is also known as the long-tail\nproblem of item distribution. Many cold-start scheme based on fine-tuning or\nknowledge transferring shows excellent performance on offline recommendation.\nYet, these schemes are infeasible for online recommendation on streaming data\npipelines due to different training method, computational overhead and time\nconstraints. Inspired by the above questions, we propose a model-agnostic\nrecommendation algorithm called Popularity-Aware Meta-learning (PAM), to\naddress the item cold-start problem under streaming data settings. PAM divides\nthe incoming data into different meta-learning tasks by predefined item\npopularity thresholds. The model can distinguish and reweight behavior-related\nand content-related features in each task based on their different roles in\ndifferent popularity levels, thus adapting to recommendations for cold-start\nsamples. These task-fixing design significantly reduces additional computation\nand storage costs compared to offline methods. Furthermore, PAM also introduced\ndata augmentation and an additional self-supervised loss specifically designed\nfor low-popularity tasks, leveraging insights from high-popularity samples.\nThis approach effectively mitigates the issue of inadequate supervision due to\nthe scarcity of cold-start samples. Experimental results across multiple public\ndatasets demonstrate the superiority of our approach over other baseline\nmethods in addressing cold-start challenges in online streaming data scenarios.\n","authors":["Yunze Luo","Yuezihan Jiang","Yinjie Jiang","Gaode Chen","Jingchi Wang","Kaigui Bian","Peiyi Li","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.11225v2.pdf","comment":"11 pages, 4 figures, to be published in KDD '25"}],"Multimedia":[{"id":"http://arxiv.org/abs/2009.00951v5","updated":"2024-11-25T23:16:37Z","published":"2020-09-02T11:08:43Z","title":"Embedded Blockchains: A Synthesis of Blockchains, Spread Spectrum\n  Watermarking, Perceptual Hashing & Digital Signatures","summary":"  In this paper we introduce a scheme for detecting manipulated audio and\nvideo. The scheme is a synthesis of blockchains, encrypted spread spectrum\nwatermarks, perceptual hashing and digital signatures, which we call an\nEmbedded Blockchain. Within this scheme, we use the blockchain for its data\nstructure of a cryptographically linked list, cryptographic hashing for\nabsolute comparisons, perceptual hashing for flexible comparisons, digital\nsignatures for proof of ownership, and encrypted spread spectrum watermarking\nto embed the blockchain into the background noise of the media. So each media\nrecording has its own unique blockchain, with each block holding information\ndescribing the media segment. The problem of verifying the integrity of the\nmedia is recast to traversing the blockchain, block-by-block, and\nsegment-by-segment of the media. If any chain is broken, the difference in the\ncomputed and extracted perceptual hash is used to estimate the level of\nmanipulation.\n","authors":["Sam Blake"],"pdf_url":"https://arxiv.org/pdf/2009.00951v5.pdf","comment":"Going in a different direction with this research"},{"id":"http://arxiv.org/abs/2411.16946v1","updated":"2024-11-25T21:35:42Z","published":"2024-11-25T21:35:42Z","title":"Lens Distortion Encoding System Version 1.0","summary":"  Lens Distortion Encoding System (LDES) allows for a distortion-accurate\nworkflow, with a seamless interchange of high quality motion picture images\nregardless of the lens source. This system is similar in a concept to the\nAcademy Color Encoding System (ACES), but for distortion. Presented solution is\nfully compatible with existing software/plug-in tools for STMapping found in\npopular production software like Adobe After Effects or DaVinci Resolve. LDES\nutilizes common distortion space and produces single high-quality, animatable\nSTMap used for direct transformation of one view to another, neglecting the\nneed of lens-swapping for each shoot. The LDES profile of a lens consist of two\nelements; View Map texture, and Footage Map texture, each labeled with the FOV\nvalue. Direct distortion mapping is produced by sampling of the Footage Map\nthrough the View Map. The result; animatable mapping texture, is then used to\nsample the footage to a desired distortion. While the Footage Map is specific\nto a footage, View Maps can be freely combined/transitioned and animated,\nallowing for effects like smooth shift from anamorphic to spherical distortion,\npreviously impossible to achieve in practice. Presented LDES Version 1.0 uses\ncommon 32-bit STMap format for encoding, supported by most compositing\nsoftware, directly or via plug-ins. The difference between standard STMap\nworkflow and LDES is that it encodes absolute pixel position in the spherical\nimage model. The main benefit of this approach is the ability to achieve a\nsimilar look of a highly expensive lens using some less expensive equipment in\nterms of distortion. It also provides greater artistic control and never seen\nbefore manipulation of footage.\n","authors":["Jakub Maksymilian Fober"],"pdf_url":"https://arxiv.org/pdf/2411.16946v1.pdf","comment":"7 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2411.16885v1","updated":"2024-11-25T19:34:35Z","published":"2024-11-25T19:34:35Z","title":"Fully Automatic Deep Learning Pipeline for Whole Slide Image Quality\n  Assessment","summary":"  In recent years, the use of deep learning (DL) methods, including\nconvolutional neural networks (CNNs) and vision transformers (ViTs), has\nsignificantly advanced computational pathology, enhancing both diagnostic\naccuracy and efficiency. Hematoxylin and Eosin (H&E) Whole Slide Images (WSI)\nplays a crucial role by providing detailed tissue samples for the analysis and\ntraining of DL models. However, WSIs often contain regions with artifacts such\nas tissue folds, blurring, as well as non-tissue regions (background), which\ncan negatively impact DL model performance. These artifacts are diagnostically\nirrelevant and can lead to inaccurate results. This paper proposes a fully\nautomatic supervised DL pipeline for WSI Quality Assessment (WSI-QA) that uses\na fused model combining CNNs and ViTs to detect and exclude WSI regions with\nartifacts, ensuring that only qualified WSI regions are used to build DL-based\ncomputational pathology applications. The proposed pipeline employs a\npixel-based segmentation model to classify WSI regions as either qualified or\nnon-qualified based on the presence of artifacts. The proposed model was\ntrained on a large and diverse dataset and validated with internal and external\ndata from various human organs, scanners, and H&E staining procedures.\nQuantitative and qualitative evaluations demonstrate the superiority of the\nproposed model, which outperforms state-of-the-art methods in WSI artifact\ndetection. The proposed model consistently achieved over 95% accuracy,\nprecision, recall, and F1 score across all artifact types. Furthermore, the\nWSI-QA pipeline shows strong generalization across different tissue types and\nscanning conditions.\n","authors":["Falah Jabar","Lill-Tove Rasmussen Busund","Biagio Ricciuti","Masoud Tafavvoghi","Mette Pøhl","Sigve Andersen","Tom Donnem","David J. Kwiatkowski","Mehrdad Rakaee"],"pdf_url":"https://arxiv.org/pdf/2411.16885v1.pdf","comment":"submitted to IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS,\n  November 25, 2024"},{"id":"http://arxiv.org/abs/2411.16863v1","updated":"2024-11-25T19:01:03Z","published":"2024-11-25T19:01:03Z","title":"Augmenting Multimodal LLMs with Self-Reflective Tokens for\n  Knowledge-based Visual Question Answering","summary":"  Multimodal LLMs (MLLMs) are the natural extension of large language models to\nhandle multimodal inputs, combining text and image data. They have recently\ngarnered attention due to their capability to address complex tasks involving\nboth modalities. However, their effectiveness is limited to the knowledge\nacquired during training, which restricts their practical utility. In this\nwork, we introduce a novel method to enhance the adaptability of MLLMs by\nintegrating external knowledge sources. Our proposed model, Reflective LLaVA\n(ReflectiVA), utilizes reflective tokens to dynamically determine the need for\nexternal knowledge and predict the relevance of information retrieved from an\nexternal database. Tokens are trained following a two-stage two-model training\nrecipe. This ultimately enables the MLLM to manage external knowledge while\npreserving fluency and performance on tasks where external knowledge is not\nneeded. Through our experiments, we demonstrate the efficacy of ReflectiVA for\nknowledge-based visual question answering, highlighting its superior\nperformance compared to existing methods. Source code and trained models are\npublicly available at https://github.com/aimagelab/ReflectiVA.\n","authors":["Federico Cocchi","Nicholas Moratelli","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2411.16863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10513v2","updated":"2024-11-25T17:04:20Z","published":"2024-11-15T17:44:27Z","title":"Any2Any: Incomplete Multimodal Retrieval with Conformal Prediction","summary":"  Autonomous agents perceive and interpret their surroundings by integrating\nmultimodal inputs, such as vision, audio, and LiDAR. These perceptual\nmodalities support retrieval tasks, such as place recognition in robotics.\nHowever, current multimodal retrieval systems encounter difficulties when parts\nof the data are missing due to sensor failures or inaccessibility, such as\nsilent videos or LiDAR scans lacking RGB information. We propose Any2Any-a\nnovel retrieval framework that addresses scenarios where both query and\nreference instances have incomplete modalities. Unlike previous methods limited\nto the imputation of two modalities, Any2Any handles any number of modalities\nwithout training generative models. It calculates pairwise similarities with\ncross-modal encoders and employs a two-stage calibration process with conformal\nprediction to align the similarities. Any2Any enables effective retrieval\nacross multimodal datasets, e.g., text-LiDAR and text-time series. It achieves\na Recall@5 of 35% on the KITTI dataset, which is on par with baseline models\nwith complete modalities.\n","authors":["Po-han Li","Yunhao Yang","Mohammad Omama","Sandeep Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2411.10513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05058v3","updated":"2024-11-25T15:19:42Z","published":"2023-09-10T15:52:56Z","title":"Multimodal Fish Feeding Intensity Assessment in Aquaculture","summary":"  Fish feeding intensity assessment (FFIA) aims to evaluate fish appetite\nchanges during feeding, which is crucial in industrial aquaculture\napplications. Existing FFIA methods are limited by their robustness to noise,\ncomputational complexity, and the lack of public datasets for developing the\nmodels. To address these issues, we first introduce AV-FFIA, a new dataset\ncontaining 27,000 labeled audio and video clips that capture different levels\nof fish feeding intensity. Then, we introduce multi-modal approaches for FFIA\nby leveraging the models pre-trained on individual modalities and fused with\ndata fusion methods. We perform benchmark studies of these methods on AV-FFIA,\nand demonstrate the advantages of the multi-modal approach over the\nsingle-modality based approach, especially in noisy environments. However,\ncompared to the methods developed for individual modalities, the multimodal\napproaches may involve higher computational costs due to the need for\nindependent encoders for each modality. To overcome this issue, we further\npresent a novel unified mixed-modality based method for FFIA, termed as U-FFIA.\nU-FFIA is a single model capable of processing audio, visual, or audio-visual\nmodalities, by leveraging modality dropout during training and knowledge\ndistillation using the models pre-trained with data from single modality. We\ndemonstrate that U-FFIA can achieve performance better than or on par with the\nstate-of-the-art modality-specific FFIA models, with significantly lower\ncomputational overhead, enabling robust and efficient FFIA for improved\naquaculture management.\n","authors":["Meng Cui","Xubo Liu","Haohe Liu","Zhuangzhuang Du","Tao Chen","Guoping Lian","Daoliang Li","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2309.05058v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16331v1","updated":"2024-11-25T12:24:52Z","published":"2024-11-25T12:24:52Z","title":"Sonic: Shifting Focus to Global Audio Perception in Portrait Animation","summary":"  The study of talking face generation mainly explores the intricacies of\nsynchronizing facial movements and crafting visually appealing,\ntemporally-coherent animations. However, due to the limited exploration of\nglobal audio perception, current approaches predominantly employ auxiliary\nvisual and spatial knowledge to stabilize the movements, which often results in\nthe deterioration of the naturalness and temporal inconsistencies.Considering\nthe essence of audio-driven animation, the audio signal serves as the ideal and\nunique priors to adjust facial expressions and lip movements, without resorting\nto interference of any visual signals. Based on this motivation, we propose a\nnovel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of\nglobal audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,\nwe disentangle it into intra- and inter-clip audio perception and collaborate\nwith both aspects to enhance overall perception.For the intra-clip audio\nperception, 1). \\textbf{Context-enhanced audio learning}, in which long-range\nintra-clip temporal audio knowledge is extracted to provide facial expression\nand lip motion priors implicitly expressed as the tone and speed of speech. 2).\n\\textbf{Motion-decoupled controller}, in which the motion of the head and\nexpression movement are disentangled and independently controlled by\nintra-audio clips. Most importantly, for inter-clip audio perception, as a\nbridge to connect the intra-clips to achieve the global perception,\n\\textbf{Time-aware position shift fusion}, in which the global inter-clip audio\ninformation is considered and fused for long-audio inference via through\nconsecutively time-aware shifted windows. Extensive experiments demonstrate\nthat the novel audio-driven paradigm outperform existing SOTA methodologies in\nterms of video quality, temporally consistency, lip synchronization precision,\nand motion diversity.\n","authors":["Xiaozhong Ji","Xiaobin Hu","Zhihong Xu","Junwei Zhu","Chuming Lin","Qingdong He","Jiangning Zhang","Donghao Luo","Yi Chen","Qin Lin","Qinglin Lu","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16331v1.pdf","comment":"refer to our main-page \\url{https://jixiaozhong.github.io/Sonic/}"},{"id":"http://arxiv.org/abs/2310.16334v4","updated":"2024-11-25T05:19:58Z","published":"2023-10-25T03:30:37Z","title":"Structured Multi-Track Accompaniment Arrangement via Style Prior\n  Modelling","summary":"  In the realm of music AI, arranging rich and structured multi-track\naccompaniments from a simple lead sheet presents significant challenges. Such\nchallenges include maintaining track cohesion, ensuring long-term coherence,\nand optimizing computational efficiency. In this paper, we introduce a novel\nsystem that leverages prior modelling over disentangled style factors to\naddress these challenges. Our method presents a two-stage process: initially, a\npiano arrangement is derived from the lead sheet by retrieving piano texture\nstyles; subsequently, a multi-track orchestration is generated by infusing\norchestral function styles into the piano arrangement. Our key design is the\nuse of vector quantization and a unique multi-stream Transformer to model the\nlong-term flow of the orchestration style, which enables flexible,\ncontrollable, and structured music generation. Experiments show that by\nfactorizing the arrangement task into interpretable sub-stages, our approach\nenhances generative capacity while improving efficiency. Additionally, our\nsystem supports a variety of music genres and provides style control at\ndifferent composition hierarchies. We further show that our system achieves\nsuperior coherence, structure, and overall arrangement quality compared to\nexisting baselines.\n","authors":["Jingwei Zhao","Gus Xia","Ziyu Wang","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16334v4.pdf","comment":"Accepted by NeurIPS 2024; typos addressed"},{"id":"http://arxiv.org/abs/2411.16096v1","updated":"2024-11-25T05:15:38Z","published":"2024-11-25T05:15:38Z","title":"ENCLIP: Ensembling and Clustering-Based Contrastive Language-Image\n  Pretraining for Fashion Multimodal Search with Limited Data and Low-Quality\n  Images","summary":"  Multimodal search has revolutionized the fashion industry, providing a\nseamless and intuitive way for users to discover and explore fashion items.\nBased on their preferences, style, or specific attributes, users can search for\nproducts by combining text and image information. Text-to-image searches enable\nusers to find visually similar items or describe products using natural\nlanguage. This paper presents an innovative approach called ENCLIP, for\nenhancing the performance of the Contrastive Language-Image Pretraining (CLIP)\nmodel, specifically in Multimodal Search targeted towards the domain of fashion\nintelligence. This method focuses on addressing the challenges posed by limited\ndata availability and low-quality images. This paper proposes an algorithm that\ninvolves training and ensembling multiple instances of the CLIP model, and\nleveraging clustering techniques to group similar images together. The\nexperimental findings presented in this study provide evidence of the\neffectiveness of the methodology. This approach unlocks the potential of CLIP\nin the domain of fashion intelligence, where data scarcity and image quality\nissues are prevalent. Overall, the ENCLIP method represents a valuable\ncontribution to the field of fashion intelligence and provides a practical\nsolution for optimizing the CLIP model in scenarios with limited data and\nlow-quality images.\n","authors":["Prithviraj Purushottam Naik","Rohit Agarwal"],"pdf_url":"https://arxiv.org/pdf/2411.16096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13811v2","updated":"2024-11-25T00:21:53Z","published":"2024-11-21T03:21:42Z","title":"X-CrossNet: A complex spectral mapping approach to target speaker\n  extraction with cross attention speaker embedding fusion","summary":"  Target speaker extraction (TSE) is a technique for isolating a target\nspeaker's voice from mixed speech using auxiliary features associated with the\ntarget speaker. It is another attempt at addressing the cocktail party problem\nand is generally considered to have more practical application prospects than\ntraditional speech separation methods. Although academic research in this area\nhas achieved high performance and evaluation scores on public datasets, most\nmodels exhibit significantly reduced performance in real-world noisy or\nreverberant conditions. To address this limitation, we propose a novel TSE\nmodel, X-CrossNet, which leverages CrossNet as its backbone. CrossNet is a\nspeech separation network specifically optimized for challenging noisy and\nreverberant environments, achieving state-of-the-art performance in tasks such\nas speaker separation under these conditions. Additionally, to enhance the\nnetwork's ability to capture and utilize auxiliary features of the target\nspeaker, we integrate a Cross-Attention mechanism into the global multi-head\nself-attention (GMHSA) module within each CrossNet block. This facilitates more\neffective integration of target speaker features with mixed speech features.\nExperimental results show that our method performs superior separation on the\nWSJ0-2mix and WHAMR! datasets, demonstrating strong robustness and stability.\n","authors":["Chang Sun","Bo Qin"],"pdf_url":"https://arxiv.org/pdf/2411.13811v2.pdf","comment":null}]}}